{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pltrdy/rouge\n",
    "from rouge import Rouge\n",
    "\n",
    "# https://github.com/li-plus/rouge-metric\n",
    "from rouge_metric import PyRouge\n",
    "\n",
    "# https://huggingface.co/metrics/rouge\n",
    "from datasets import load_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Input\n",
    "preds=[\"Model output 1\",\"Model output 2\",\"Model output 3\"]\n",
    "\n",
    "# Single Reference for each sample\n",
    "references=[\"Reference 1\",\"Reference 2\",\"Reference 3\"]\n",
    "\n",
    "# Multiple reference for each sample\n",
    "multiple_references=[\n",
    "\t[\"Sample 1 Reference 1\",\"Sample 1 Reference 2\",\"Sample 1 Reference 3\"],\n",
    "\t[\"Sample 2 Reference 1\",\"Sample 2 Reference 2\",\"Sample 2 Reference 3\"],\n",
    "\t[\"Sample 3 Reference 1\",\"Sample 3 Reference 2\",\"Sample 3 Reference 3\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE Paper Examples\n",
    "s1=\"police killed the gunman\"\n",
    "s2=\"police kill the gunman\"\n",
    "s3=\"the gunman kill police\"\n",
    "s4=\"the gunman police killed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: rouge\n",
      "Version: 1.0.1\n",
      "Summary: Full Python ROUGE Score Implementation (not a wrapper)\n",
      "Home-page: http://github.com/pltrdy/rouge\n",
      "Author: pltrdy\n",
      "Author-email: pltrdy@gmail.com\n",
      "License: LICENCE.txt\n",
      "Location: /home/tslab/anaconda3/envs/comet2020/lib/python3.8/site-packages\n",
      "Requires: six\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'r': 0.5, 'p': 0.3333333333333333, 'f': 0.3999999952000001}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.5, 'p': 0.3333333333333333, 'f': 0.3999999952000001}}\n"
     ]
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "scores=rouge.get_scores(preds, references, avg=True)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: rouge-metric\n",
      "Version: 1.0.1\n",
      "Summary: A fast python implementation of full ROUGE metrics for automatic summarization.\n",
      "Home-page: https://github.com/li-plus/rouge-metric\n",
      "Author: Jiahao Li\n",
      "Author-email: liplus17@163.com\n",
      "License: MIT\n",
      "Location: /home/tslab/anaconda3/envs/comet2020/lib/python3.8/site-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show rouge_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'r': 0.10000000000000002, 'p': 0.030303030303030304, 'f': 0.04651162790697676}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-4': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.10000000000000002, 'p': 0.030303030303030304, 'f': 0.04651162790697676}, 'rouge-w-1.2': {'r': 0.14677992676220694, 'p': 0.045190953800397234, 'f': 0.06910553173165694}, 'rouge-s*': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-su*': {'r': 0.0, 'p': 0.0, 'f': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate document-wise ROUGE scores\n",
    "# skip_gap: The maximum gap between two words in skip-bigram\n",
    "# mode: 'average', 'individual'\n",
    "rouge = PyRouge(rouge_n=(1, 2, 4), rouge_l=True, rouge_w=True,\n",
    "                rouge_w_weight=1.2, rouge_s=True, rouge_su=True#, skip_gap=4\n",
    "\t\t\t\t,mode='average')\n",
    "scores=scores = rouge.evaluate(preds, references)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rougeL': AggregateScore(low=Score(precision=0.3333333333333333, recall=0.5, fmeasure=0.4000000000000001), mid=Score(precision=0.3333333333333333, recall=0.5, fmeasure=0.4000000000000001), high=Score(precision=0.3333333333333333, recall=0.5, fmeasure=0.4000000000000001))}\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"rouge\")\n",
    "scores=metric.compute(predictions=preds, references=references, rouge_types=[\"rougeL\"])\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'r': 0.25, 'p': 0.3333333333333333, 'f': 0.28571428571428575}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-4': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.25, 'p': 0.3333333333333333, 'f': 0.28571428571428575}, 'rouge-w-1.2': {'r': 0.18946457081379978, 'p': 0.3333333333333333, 'f': 0.24160332869337336}, 'rouge-s*': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-su*': {'r': 0.0, 'p': 0.0, 'f': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "rouge = PyRouge(rouge_n=(1, 2, 4), rouge_l=True, rouge_w=True,\n",
    "                rouge_w_weight=1.2, rouge_s=True, rouge_su=True#, skip_gap=4\n",
    "\t\t\t\t,mode='average')\n",
    "scores = rouge.evaluate(preds, multiple_references)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge\n",
      "Final ROUGE-L F1 0.3333333283333334\n",
      "rouge_metric - PyRouge\n",
      "{'rouge-1': {'r': 0.25, 'p': 0.3333333333333333, 'f': 0.28571428571428575}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.25, 'p': 0.3333333333333333, 'f': 0.28571428571428575}}\n",
      "Multi-refs ROUGE-1 0.28571428571428575\n",
      "Multi-refs ROUGE-L 0.28571428571428575\n"
     ]
    }
   ],
   "source": [
    "#Compare Multiple Pred-Ref Pairs Avg and Pred-Refs Pair Score\n",
    "preds=[]\n",
    "multiple_references=[\n",
    "    []\n",
    "]\n",
    "\n",
    "print(\"Rouge\")\n",
    "rouge = Rouge()\n",
    "\n",
    "#Test for ROUGE-L\n",
    "avg_scores=[]\n",
    "for pred,refs in zip(preds,multiple_references):\n",
    "    pair_scores=[]\n",
    "    for ref in refs:\n",
    "        scores=rouge.get_scores([pred], [ref], avg=True)\n",
    "        # print(scores)\n",
    "        pair_scores.append(scores[\"rouge-l\"][\"f\"])\n",
    "    avg_scores.append(max(pair_scores))\n",
    "print(\"Final ROUGE-L F1\",sum(avg_scores)/len(avg_scores))\n",
    "\n",
    "print(\"rouge_metric - PyRouge\")\n",
    "\n",
    "#mode: individual, average\n",
    "rouge = PyRouge(rouge_n=(1, 2), rouge_l=True,mode='average')\n",
    "\n",
    "scores = rouge.evaluate(preds, multiple_references)\n",
    "print(scores)\n",
    "print(\"Multi-refs ROUGE-1\",scores[\"rouge-l\"][\"f\"])\n",
    "print(\"Multi-refs ROUGE-L\",scores[\"rouge-1\"][\"f\"])\n",
    "\n",
    "# scores = rouge.get_scores(preds, multiple_references, avg=True)\n",
    "# print(\"Multi-refs\",scores)\n",
    "\n",
    "\n",
    "#Datasets metric\n",
    "# metric = load_metric(\"rouge\")\n",
    "\n",
    "# avg_scores=[]\n",
    "# for pred,refs in zip(preds,multiple_references):\n",
    "#     pair_scores=[]\n",
    "#     for ref in refs:\n",
    "#         scores=metric.compute(predictions=[pred], references=[ref], rouge_types=[\"rougeL\"])\n",
    "#         pair_scores.append(scores[\"rougeL\"][\"f\"])\n",
    "#     avg_scores.append(max(pair_scores))\n",
    "# print(\"Final ROUGE-L F1\",sum(avg_scores)/len(avg_scores))\n",
    "\n",
    "# scores=metric.compute(predictions=preds_repeated, references=multiple_references_unsqueeze, rouge_types=[\"rougeL\"])\n",
    "# print(\"Multi Pref-ref pair\",scores)\n",
    "\n",
    "# scores=metric.compute(predictions=preds, references=multiple_references, rouge_types=[\"rougeL\"])\n",
    "# print(\"Multi-refs\",scores)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "490463624bb20166a8667b7fa728dcad49f92cff15a6e94f417ad6118e23fbcd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('comet2020': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
