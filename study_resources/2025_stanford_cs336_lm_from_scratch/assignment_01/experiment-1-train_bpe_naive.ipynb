{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721706e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from typing import BinaryIO, Dict, List, Set, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "sys.path.append('assignment1-basics')\n",
    "from cs336_basics.tokenizer.train import find_chunk_boundaries, PAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "066588d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6453f2",
   "metadata": {},
   "source": [
    "# 1. Initialize Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a7e885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xc4\\xa0'\n",
      "13 b'<|endoftext|>'\n"
     ]
    }
   ],
   "source": [
    "vocab: Dict[int, bytes] = {\n",
    "    i: chr(i).encode('utf-8') for i in range(256)\n",
    "}\n",
    "vocab_size = 256\n",
    "\n",
    "# For representing whitespace\n",
    "print(\"Ġ\".encode('utf-8'))\n",
    "vocab[vocab_size]=\"Ġ\".encode('utf-8')\n",
    "vocab_size+=1\n",
    "\n",
    "# Add Special Tokens\n",
    "special_tokens = ['<|endoftext|>']\n",
    "\n",
    "encoded_special_tokens = [\n",
    "    x.encode('utf-8') for x in special_tokens\n",
    "]\n",
    "\n",
    "for tok in encoded_special_tokens:\n",
    "    vocab[vocab_size]=tok\n",
    "    vocab_size+=1\n",
    "\n",
    "split_special_token = \"<|endoftext|>\".encode('utf-8')\n",
    "print(len(split_special_token), split_special_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "490362ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inverse Vocab\n",
    "vocab_inv = {tok:i for i,tok in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab4b60be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'\\x00': 0,\n",
       " b'\\x01': 1,\n",
       " b'\\x02': 2,\n",
       " b'\\x03': 3,\n",
       " b'\\x04': 4,\n",
       " b'\\x05': 5,\n",
       " b'\\x06': 6,\n",
       " b'\\x07': 7,\n",
       " b'\\x08': 8,\n",
       " b'\\t': 9,\n",
       " b'\\n': 10,\n",
       " b'\\x0b': 11,\n",
       " b'\\x0c': 12,\n",
       " b'\\r': 13,\n",
       " b'\\x0e': 14,\n",
       " b'\\x0f': 15,\n",
       " b'\\x10': 16,\n",
       " b'\\x11': 17,\n",
       " b'\\x12': 18,\n",
       " b'\\x13': 19,\n",
       " b'\\x14': 20,\n",
       " b'\\x15': 21,\n",
       " b'\\x16': 22,\n",
       " b'\\x17': 23,\n",
       " b'\\x18': 24,\n",
       " b'\\x19': 25,\n",
       " b'\\x1a': 26,\n",
       " b'\\x1b': 27,\n",
       " b'\\x1c': 28,\n",
       " b'\\x1d': 29,\n",
       " b'\\x1e': 30,\n",
       " b'\\x1f': 31,\n",
       " b' ': 32,\n",
       " b'!': 33,\n",
       " b'\"': 34,\n",
       " b'#': 35,\n",
       " b'$': 36,\n",
       " b'%': 37,\n",
       " b'&': 38,\n",
       " b\"'\": 39,\n",
       " b'(': 40,\n",
       " b')': 41,\n",
       " b'*': 42,\n",
       " b'+': 43,\n",
       " b',': 44,\n",
       " b'-': 45,\n",
       " b'.': 46,\n",
       " b'/': 47,\n",
       " b'0': 48,\n",
       " b'1': 49,\n",
       " b'2': 50,\n",
       " b'3': 51,\n",
       " b'4': 52,\n",
       " b'5': 53,\n",
       " b'6': 54,\n",
       " b'7': 55,\n",
       " b'8': 56,\n",
       " b'9': 57,\n",
       " b':': 58,\n",
       " b';': 59,\n",
       " b'<': 60,\n",
       " b'=': 61,\n",
       " b'>': 62,\n",
       " b'?': 63,\n",
       " b'@': 64,\n",
       " b'A': 65,\n",
       " b'B': 66,\n",
       " b'C': 67,\n",
       " b'D': 68,\n",
       " b'E': 69,\n",
       " b'F': 70,\n",
       " b'G': 71,\n",
       " b'H': 72,\n",
       " b'I': 73,\n",
       " b'J': 74,\n",
       " b'K': 75,\n",
       " b'L': 76,\n",
       " b'M': 77,\n",
       " b'N': 78,\n",
       " b'O': 79,\n",
       " b'P': 80,\n",
       " b'Q': 81,\n",
       " b'R': 82,\n",
       " b'S': 83,\n",
       " b'T': 84,\n",
       " b'U': 85,\n",
       " b'V': 86,\n",
       " b'W': 87,\n",
       " b'X': 88,\n",
       " b'Y': 89,\n",
       " b'Z': 90,\n",
       " b'[': 91,\n",
       " b'\\\\': 92,\n",
       " b']': 93,\n",
       " b'^': 94,\n",
       " b'_': 95,\n",
       " b'`': 96,\n",
       " b'a': 97,\n",
       " b'b': 98,\n",
       " b'c': 99,\n",
       " b'd': 100,\n",
       " b'e': 101,\n",
       " b'f': 102,\n",
       " b'g': 103,\n",
       " b'h': 104,\n",
       " b'i': 105,\n",
       " b'j': 106,\n",
       " b'k': 107,\n",
       " b'l': 108,\n",
       " b'm': 109,\n",
       " b'n': 110,\n",
       " b'o': 111,\n",
       " b'p': 112,\n",
       " b'q': 113,\n",
       " b'r': 114,\n",
       " b's': 115,\n",
       " b't': 116,\n",
       " b'u': 117,\n",
       " b'v': 118,\n",
       " b'w': 119,\n",
       " b'x': 120,\n",
       " b'y': 121,\n",
       " b'z': 122,\n",
       " b'{': 123,\n",
       " b'|': 124,\n",
       " b'}': 125,\n",
       " b'~': 126,\n",
       " b'\\x7f': 127,\n",
       " b'\\xc2\\x80': 128,\n",
       " b'\\xc2\\x81': 129,\n",
       " b'\\xc2\\x82': 130,\n",
       " b'\\xc2\\x83': 131,\n",
       " b'\\xc2\\x84': 132,\n",
       " b'\\xc2\\x85': 133,\n",
       " b'\\xc2\\x86': 134,\n",
       " b'\\xc2\\x87': 135,\n",
       " b'\\xc2\\x88': 136,\n",
       " b'\\xc2\\x89': 137,\n",
       " b'\\xc2\\x8a': 138,\n",
       " b'\\xc2\\x8b': 139,\n",
       " b'\\xc2\\x8c': 140,\n",
       " b'\\xc2\\x8d': 141,\n",
       " b'\\xc2\\x8e': 142,\n",
       " b'\\xc2\\x8f': 143,\n",
       " b'\\xc2\\x90': 144,\n",
       " b'\\xc2\\x91': 145,\n",
       " b'\\xc2\\x92': 146,\n",
       " b'\\xc2\\x93': 147,\n",
       " b'\\xc2\\x94': 148,\n",
       " b'\\xc2\\x95': 149,\n",
       " b'\\xc2\\x96': 150,\n",
       " b'\\xc2\\x97': 151,\n",
       " b'\\xc2\\x98': 152,\n",
       " b'\\xc2\\x99': 153,\n",
       " b'\\xc2\\x9a': 154,\n",
       " b'\\xc2\\x9b': 155,\n",
       " b'\\xc2\\x9c': 156,\n",
       " b'\\xc2\\x9d': 157,\n",
       " b'\\xc2\\x9e': 158,\n",
       " b'\\xc2\\x9f': 159,\n",
       " b'\\xc2\\xa0': 160,\n",
       " b'\\xc2\\xa1': 161,\n",
       " b'\\xc2\\xa2': 162,\n",
       " b'\\xc2\\xa3': 163,\n",
       " b'\\xc2\\xa4': 164,\n",
       " b'\\xc2\\xa5': 165,\n",
       " b'\\xc2\\xa6': 166,\n",
       " b'\\xc2\\xa7': 167,\n",
       " b'\\xc2\\xa8': 168,\n",
       " b'\\xc2\\xa9': 169,\n",
       " b'\\xc2\\xaa': 170,\n",
       " b'\\xc2\\xab': 171,\n",
       " b'\\xc2\\xac': 172,\n",
       " b'\\xc2\\xad': 173,\n",
       " b'\\xc2\\xae': 174,\n",
       " b'\\xc2\\xaf': 175,\n",
       " b'\\xc2\\xb0': 176,\n",
       " b'\\xc2\\xb1': 177,\n",
       " b'\\xc2\\xb2': 178,\n",
       " b'\\xc2\\xb3': 179,\n",
       " b'\\xc2\\xb4': 180,\n",
       " b'\\xc2\\xb5': 181,\n",
       " b'\\xc2\\xb6': 182,\n",
       " b'\\xc2\\xb7': 183,\n",
       " b'\\xc2\\xb8': 184,\n",
       " b'\\xc2\\xb9': 185,\n",
       " b'\\xc2\\xba': 186,\n",
       " b'\\xc2\\xbb': 187,\n",
       " b'\\xc2\\xbc': 188,\n",
       " b'\\xc2\\xbd': 189,\n",
       " b'\\xc2\\xbe': 190,\n",
       " b'\\xc2\\xbf': 191,\n",
       " b'\\xc3\\x80': 192,\n",
       " b'\\xc3\\x81': 193,\n",
       " b'\\xc3\\x82': 194,\n",
       " b'\\xc3\\x83': 195,\n",
       " b'\\xc3\\x84': 196,\n",
       " b'\\xc3\\x85': 197,\n",
       " b'\\xc3\\x86': 198,\n",
       " b'\\xc3\\x87': 199,\n",
       " b'\\xc3\\x88': 200,\n",
       " b'\\xc3\\x89': 201,\n",
       " b'\\xc3\\x8a': 202,\n",
       " b'\\xc3\\x8b': 203,\n",
       " b'\\xc3\\x8c': 204,\n",
       " b'\\xc3\\x8d': 205,\n",
       " b'\\xc3\\x8e': 206,\n",
       " b'\\xc3\\x8f': 207,\n",
       " b'\\xc3\\x90': 208,\n",
       " b'\\xc3\\x91': 209,\n",
       " b'\\xc3\\x92': 210,\n",
       " b'\\xc3\\x93': 211,\n",
       " b'\\xc3\\x94': 212,\n",
       " b'\\xc3\\x95': 213,\n",
       " b'\\xc3\\x96': 214,\n",
       " b'\\xc3\\x97': 215,\n",
       " b'\\xc3\\x98': 216,\n",
       " b'\\xc3\\x99': 217,\n",
       " b'\\xc3\\x9a': 218,\n",
       " b'\\xc3\\x9b': 219,\n",
       " b'\\xc3\\x9c': 220,\n",
       " b'\\xc3\\x9d': 221,\n",
       " b'\\xc3\\x9e': 222,\n",
       " b'\\xc3\\x9f': 223,\n",
       " b'\\xc3\\xa0': 224,\n",
       " b'\\xc3\\xa1': 225,\n",
       " b'\\xc3\\xa2': 226,\n",
       " b'\\xc3\\xa3': 227,\n",
       " b'\\xc3\\xa4': 228,\n",
       " b'\\xc3\\xa5': 229,\n",
       " b'\\xc3\\xa6': 230,\n",
       " b'\\xc3\\xa7': 231,\n",
       " b'\\xc3\\xa8': 232,\n",
       " b'\\xc3\\xa9': 233,\n",
       " b'\\xc3\\xaa': 234,\n",
       " b'\\xc3\\xab': 235,\n",
       " b'\\xc3\\xac': 236,\n",
       " b'\\xc3\\xad': 237,\n",
       " b'\\xc3\\xae': 238,\n",
       " b'\\xc3\\xaf': 239,\n",
       " b'\\xc3\\xb0': 240,\n",
       " b'\\xc3\\xb1': 241,\n",
       " b'\\xc3\\xb2': 242,\n",
       " b'\\xc3\\xb3': 243,\n",
       " b'\\xc3\\xb4': 244,\n",
       " b'\\xc3\\xb5': 245,\n",
       " b'\\xc3\\xb6': 246,\n",
       " b'\\xc3\\xb7': 247,\n",
       " b'\\xc3\\xb8': 248,\n",
       " b'\\xc3\\xb9': 249,\n",
       " b'\\xc3\\xba': 250,\n",
       " b'\\xc3\\xbb': 251,\n",
       " b'\\xc3\\xbc': 252,\n",
       " b'\\xc3\\xbd': 253,\n",
       " b'\\xc3\\xbe': 254,\n",
       " b'\\xc3\\xbf': 255,\n",
       " b'\\xc4\\xa0': 256,\n",
       " b'<|endoftext|>': 257}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df60856a",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3044523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "input_path = 'data/owt_valid.txt'\n",
    "num_processes=8\n",
    "\n",
    "with open(input_path, 'rb') as f:\n",
    "    boundaries = find_chunk_boundaries(\n",
    "        f,\n",
    "        num_processes,\n",
    "        split_special_token\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05329088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 36335216,\n",
       " 72505172,\n",
       " 108752143,\n",
       " 145027268,\n",
       " 181256470,\n",
       " 217499287,\n",
       " 253752435,\n",
       " 289998753]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eabe8551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 36335216\n",
      "begin 'LOUISVILLE'\n",
      "end 'evel above 4th.'\n",
      "2 36335229 72505172\n",
      "begin 'Story high'\n",
      "end 'tion,” he said.'\n",
      "3 72505185 108752143\n",
      "begin '1705-hill-'\n",
      "end 'Newswire posts:'\n",
      "4 108752156 145027268\n",
      "begin 'Get the bi'\n",
      "end 'graphy [ edit ]'\n",
      "5 145027281 181256470\n",
      "begin 'Soos Goes '\n",
      "end 'k my swan song.'\n",
      "6 181256483 217499287\n",
      "begin 'Monday, Au'\n",
      "end 'elsh and Irish.'\n",
      "7 217499300 253752435\n",
      "begin 'There’s an'\n",
      "end '9s&w=600&h=315]'\n",
      "8 253752448 289998740\n",
      "begin 'Address 68'\n",
      "end 'ce on March 1.\"'\n"
     ]
    }
   ],
   "source": [
    "with open(input_path, 'rb') as f:\n",
    "    for b_i in range(1, len(boundaries)):\n",
    "        start = boundaries[b_i-1]\n",
    "        # every chunk except first contains split_special_token at start\n",
    "        if b_i!=1:\n",
    "            start+=len(split_special_token)\n",
    "            \n",
    "        end = boundaries[b_i]\n",
    "        # Last Chunk contains split_special_token at the end\n",
    "        if b_i==len(boundaries)-1:\n",
    "            end-=len(split_special_token)\n",
    "        \n",
    "        f.seek(start)\n",
    "        chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "        print(b_i, start, end)\n",
    "        print('begin',repr(chunk[:10]))\n",
    "        print('end', repr(chunk[-15:]))\n",
    "        \n",
    "    # for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "    #     f.seek(start)\n",
    "    #     chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "    #     print('begin',chunk[:10])\n",
    "    #     print('end', chunk[-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6287f9ef",
   "metadata": {},
   "source": [
    "# 3. Handle Pretokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f049ca4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "2\n",
      "space\n",
      "32\n",
      "tab\n",
      "9\n",
      "newline\n",
      "10\n",
      "' '\n",
      "'\\t'\n",
      "'\\n'\n",
      "{'input_ids': [[39]], 'attention_mask': [[1]]}\n",
      "{'input_ids': [[367]], 'attention_mask': [[1]]}\n",
      "{'input_ids': [[198, 39]], 'attention_mask': [[1, 1]]}\n",
      "{'input_ids': [[128, 232, 39]], 'attention_mask': [[1, 1, 1]]}\n",
      "{'input_ids': [[197, 39]], 'attention_mask': [[1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# Test whitespace\n",
    "print(\" \".isspace())\n",
    "print(\"\\n\".isspace())\n",
    "print(\"\\t\".isspace())\n",
    "\n",
    "print(len(\" h\".encode('utf-8')))\n",
    "print(\"space\")\n",
    "print(\" h\".encode('utf-8')[0])\n",
    "print(\"tab\")\n",
    "print(\"\\th\".encode('utf-8')[0])\n",
    "print(\"newline\")\n",
    "print(\"\\nh\".encode('utf-8')[0])\n",
    "\n",
    "print(repr(bytes([32]).decode('utf-8')))    # space\n",
    "print(repr(bytes([9]).decode('utf-8'))) # tab\n",
    "print(repr(bytes([10]).decode('utf-8'))) # newline\n",
    "\n",
    "print(gpt2_tokenizer(['H'])) # merged into one\n",
    "print(gpt2_tokenizer([' H'])) # merged into one\n",
    "print(gpt2_tokenizer(['\\nH']))\n",
    "print(gpt2_tokenizer(['ĊH']))\n",
    "print(gpt2_tokenizer(['\\tH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb5d1c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xc4\\xa0'\n",
      "b'\\xc4\\x8a'\n"
     ]
    }
   ],
   "source": [
    "whitespace_token = \"Ġ\"\n",
    "whitespace_token_bytes = whitespace_token.encode('utf-8')\n",
    "print(whitespace_token_bytes)\n",
    "\n",
    "newline_token = \"Ċ\"\n",
    "newline_token_bytes = newline_token.encode('utf-8')\n",
    "print(newline_token_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1678d1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "x = 'hi'.encode('utf-8')[0]\n",
    "print(type(newline_token_bytes))\n",
    "print(type(x))\n",
    "# b''.join([newline_token_bytes, bytes([x])])\n",
    "# bytes([newline_token_bytes, x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b389a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk Pretokenization\n",
    "import regex as re\n",
    "\n",
    "class TokenNode:\n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "        self.prev = None\n",
    "        self.next = None\n",
    "        # For determining pre-tokenization boundary\n",
    "        self.is_next_connected = True\n",
    "\n",
    "def add_node(byte_val, prev):\n",
    "    \"\"\"Helper to create and link a new TokenNode.\"\"\"\n",
    "    node = TokenNode(byte_val)\n",
    "    if prev:\n",
    "        prev.next = node\n",
    "        node.prev = prev\n",
    "    return node\n",
    "\n",
    "i=0\n",
    "\n",
    "head=None\n",
    "prev=None\n",
    "\n",
    "# Outer: Pre-tokenized Tokens\n",
    "for pre_tok in re.finditer(PAT, chunk):\n",
    "    text = pre_tok.group()\n",
    "    bytes_to_process = []\n",
    "\n",
    "    if text[0] in (' ', '\\n'):\n",
    "        # Determine the prefix byte token (space or newline)\n",
    "        prefix_bytes = whitespace_token_bytes if text[0] == ' ' else newline_token_bytes\n",
    "        rest = text[1:].encode('utf-8') if len(text) > 1 else b\"\"\n",
    "\n",
    "        # Merge prefix with first byte of rest, or use prefix alone\n",
    "        if rest:\n",
    "            first = bytes([rest[0]])\n",
    "            node = add_node(prefix_bytes + first, prev)\n",
    "            prev = node\n",
    "            bytes_to_process = rest[1:]\n",
    "        else:\n",
    "            node = add_node(prefix_bytes, prev)\n",
    "            prev = node\n",
    "            bytes_to_process = b\"\"\n",
    "    else:\n",
    "        bytes_to_process = text.encode('utf-8')\n",
    "\n",
    "    # Add remaining bytes as separate nodes\n",
    "    for byte in bytes_to_process:\n",
    "        prev = add_node(bytes([byte]), prev)\n",
    "        if head is None:\n",
    "            head = prev\n",
    "\n",
    "    if prev:\n",
    "        prev.is_next_connected = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9efbfbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "pair_positions = defaultdict(set)\n",
    "node = head\n",
    "while node and node.next:\n",
    "    # print(node.val, node.is_next_connected)\n",
    "    if not node.is_next_connected:\n",
    "        node=node.next\n",
    "        continue\n",
    "    \n",
    "    pair_positions[\n",
    "        (node.val, node.next.val)\n",
    "    ].add(node)\n",
    "    node = node.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3cf4904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'h', b'e') 516999 <class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "pair_counts = {pair: len(nodes) for pair, nodes in pair_positions.items()}\n",
    "# print(pair_counts)\n",
    "\n",
    "max_count_pair = max(pair_counts, key=pair_counts.get)\n",
    "print(max_count_pair, pair_counts[max_count_pair], type(max_count_pair[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f954b8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'he'\n"
     ]
    }
   ],
   "source": [
    "print(repr(b''.join(max_count_pair).decode('utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6520c024",
   "metadata": {},
   "source": [
    "# 4. Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f2c5c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERGE 0 b'he'\n",
      "MERGE 1 b'er'\n",
      "MERGE 2 b'\\xc4\\xa0the'\n",
      "MERGE 3 b'in'\n",
      "MERGE 4 b'on'\n"
     ]
    }
   ],
   "source": [
    "merges: List[Tuple[bytes, bytes]] = []\n",
    "\n",
    "num_merges = 5\n",
    "\n",
    "for merge_i in range(num_merges):\n",
    "    max_count_pair = max(pair_counts, key=pair_counts.get)\n",
    "    # Add to merges\n",
    "    merges.append(max_count_pair)\n",
    "    \n",
    "    # Add new vocab\n",
    "    merged_val = b''.join(max_count_pair)\n",
    "    vocab[vocab_size]=merged_val\n",
    "    vocab_size+=1\n",
    "    \n",
    "    print(\"MERGE {} {}\".format(merge_i, merged_val))\n",
    "    \n",
    "    for node_a in list(pair_positions[max_count_pair]):\n",
    "        node_b = node_a.next\n",
    "        \n",
    "        # 1. Merge Node\n",
    "        new_node = TokenNode(merged_val)\n",
    "        new_node.prev=node_a.prev\n",
    "        new_node.next=node_b.next\n",
    "        new_node.is_next_connected=node_b.is_next_connected\n",
    "        \n",
    "        # 2. Update Left\n",
    "        if node_a.prev:\n",
    "            if node_a.prev.is_next_connected:\n",
    "                # Remove previous\n",
    "                prev_pair = (node_a.prev.val, node_a.val)\n",
    "                pair_counts[prev_pair]-=1\n",
    "                pair_positions[prev_pair].discard(node_a.prev)\n",
    "                \n",
    "                # Add new merged version\n",
    "                new_pair = (node_a.prev.val, merged_val)\n",
    "                pair_counts[new_pair] = pair_counts.get(new_pair, 0) + 1\n",
    "                pair_positions[new_pair].add(node_a.prev)\n",
    "            node_a.prev.next=new_node\n",
    "        \n",
    "        # 3. Update Right\n",
    "        if node_b.next and node_b.is_next_connected:\n",
    "            if node_b.is_next_connected:\n",
    "                # Remove previous\n",
    "                prev_pair = (node_b.val, node_b.next.val)\n",
    "                pair_counts[prev_pair]-=1\n",
    "                pair_positions[prev_pair].discard(node_b)\n",
    "                \n",
    "                # Add new merged version\n",
    "                new_pair = (merged_val, node_b.next.val)\n",
    "                pair_counts[new_pair] = pair_counts.get(new_pair, 0) + 1\n",
    "                pair_positions[new_pair].add(new_node)\n",
    "            node_b.next.prev=new_node\n",
    "        \n",
    "        del node_a\n",
    "        del node_b\n",
    "    \n",
    "    # Delete pair count, positions\n",
    "    del pair_counts[max_count_pair]\n",
    "    del pair_positions[max_count_pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1d7af7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġ'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b'\\xc4\\xa0'.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b7e69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
