{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb6b549d",
   "metadata": {},
   "source": [
    "# data-1 analysis\n",
    "* testing pretokenization_example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4402bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import BinaryIO\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce0419a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<|endoftext|>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_token = \"<|endoftext|>\".encode(\"utf-8\")\n",
    "split_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "804f7bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289998753\n",
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "mini_chunk_size = 4096\n",
    "\n",
    "with open('data/owt_valid.txt', 'rb') as file:\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    print(file_size)\n",
    "    file.seek(0)\n",
    "    \n",
    "    mini_chunk = file.read(mini_chunk_size) \n",
    "    print(type(mini_chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87160dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO, \n",
    "    desired_num_chunks: int, \n",
    "    split_special_token: bytes\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), (\n",
    "        \"Must represent special token as a bytestring\"\n",
    "    )\n",
    "\n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
    "    # Chunks start on previous index, don't include last index\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        file.seek(initial_position)  # Start at boundary guess\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
    "\n",
    "            # If EOF, this boundary should be at the end of the file\n",
    "            if mini_chunk == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # Find the special token in the mini chunk\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            if found_at != -1:\n",
    "                chunk_boundaries[bi] = initial_position + found_at\n",
    "                break\n",
    "            initial_position += mini_chunk_size\n",
    "\n",
    "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
    "    return sorted(set(chunk_boundaries))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f955a30",
   "metadata": {},
   "source": [
    "# OWT Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bb4c938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK 0 Size: 36009830\n",
      "CHUNK 36335216 Size: 35852690\n",
      "CHUNK 72505172 Size: 35941460\n",
      "CHUNK 108752143 Size: 35900753\n",
      "CHUNK 145027268 Size: 35914750\n",
      "CHUNK 181256470 Size: 35917461\n",
      "CHUNK 217499287 Size: 35927603\n",
      "CHUNK 253752435 Size: 35889585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 36335216,\n",
       " 72505172,\n",
       " 108752143,\n",
       " 145027268,\n",
       " 181256470,\n",
       " 217499287,\n",
       " 253752435,\n",
       " 289998753]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_chunk_size = 4096\n",
    "num_processes = 8\n",
    "\n",
    "with open('data/owt_valid.txt', 'rb') as file:\n",
    "        boundaries = find_chunk_boundaries(\n",
    "                file,\n",
    "                num_processes,\n",
    "                \"<|endoftext|>\".encode(\"utf-8\")\n",
    "        )\n",
    "\n",
    "        for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "                file.seek(start)\n",
    "                chunk = file.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "                print(f\"CHUNK {start} Size:\", len(chunk))\n",
    "boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24e13054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000 '\\x00'\n"
     ]
    }
   ],
   "source": [
    "print(chr(0), repr(chr(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb224552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi I am\\x00hello'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Hi I am\" + chr(0) + \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "117e1491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi I am\u0000hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hi I am\" + chr(0) + \"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94936425",
   "metadata": {},
   "source": [
    "# corpus.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cee6742",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXTURES_PATH=\"assignment1-basics/tests/fixtures\"\n",
    "input_path =  os.path.join(FIXTURES_PATH, \"corpus.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b8adef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK 0 Size: 132878\n",
      "iron cement is a ready for use paste which is laid as a fillet by putty knife or finger in the mould\n",
      "chäftsordnung .\n",
      "Frau Präsidentin , zur Geschäftsordnung .\n",
      "Frau Präsidentin , zur Geschäftsordnung .\n",
      "\n",
      "iron cement is a ready for use paste which is laid as a fillet by putty knife or finger in the mould\n",
      "tin , zur Geschäftsordnung .\n",
      "Frau Präsidentin , zur Geschäftsordnung .\n",
      "Frau Präsidentin , zur Geschä\n"
     ]
    }
   ],
   "source": [
    "num_processes=8\n",
    "split_special_token = \"<|endoftext|>\".encode('utf-8')\n",
    "\n",
    "with open(input_path, 'rb') as file:\n",
    "        boundaries = find_chunk_boundaries(\n",
    "                file,\n",
    "                num_processes,\n",
    "                \"<|endoftext|>\".encode(\"utf-8\")\n",
    "        )\n",
    "\n",
    "        for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "                file.seek(start)\n",
    "                chunk = file.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "                print(f\"CHUNK {start} Size:\", len(chunk))\n",
    "                print(chunk[:100])\n",
    "                print(chunk[-100:])\n",
    "                \n",
    "        for b_i in range(1, len(boundaries)):\n",
    "                start = boundaries[b_i-1]\n",
    "                # every chunk except first contains split_special_token at start\n",
    "                if b_i!=1:\n",
    "                        start+=len(split_special_token)\n",
    "                \n",
    "                end = boundaries[b_i]\n",
    "                # Last Chunk contains split_special_token at the end\n",
    "                if b_i==len(boundaries)-1:\n",
    "                        end-=len(split_special_token)\n",
    "                \n",
    "                file.seek(start)\n",
    "                chunk = file.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "                print(chunk[:100])\n",
    "                print(chunk[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7a6500",
   "metadata": {},
   "source": [
    "# tinystories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4585485",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXTURES_PATH=\"assignment1-basics/tests/fixtures\"\n",
    "input_path =  os.path.join(FIXTURES_PATH, \"tinystories_sample_5M.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c40b6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHUNK 0 Size: 656379\n",
      "u don't have to be scared of the loud dog, I'll protect you\". The mole felt so safe with the little \n",
      " kiss. They played with their ball in the backyard. They were not worried anymore. They were happy.\n",
      "\n",
      "CHUNK 656657 Size: 654068\n",
      "<|endoftext|>\n",
      "One day, a sad cat named Tom was walking in the rain. The rain made a big flood near h\n",
      "lived happily in the forest, always remembering to follow the signs and protect the important root.\n",
      "\n",
      "CHUNK 1310951 Size: 655158\n",
      "<|endoftext|>\n",
      "Once upon a time, there was a pink cat named Kitty. Kitty loved to nap in the warm sun\n",
      "nough spraying, let’s go home now.”\n",
      "So, they both went back home with a happy smile on their faces.\n",
      "\n",
      "CHUNK 1966391 Size: 655280\n",
      "<|endoftext|>\n",
      "Once upon a time, in a small town, there was a store. The store was incredible. It had\n",
      "e big bird helped Tim get down safely. Tim learned to listen to his friend Sam and be more careful.\n",
      "\n",
      "CHUNK 2621933 Size: 654814\n",
      "<|endoftext|>\n",
      "Once upon a time, there was a little boy named Tim. Tim loved to paint. He would paint\n",
      "were at his house, and they all played together inside. The rain did not stop them from having fun.\n",
      "\n",
      "CHUNK 3277029 Size: 655289\n",
      "<|endoftext|>\n",
      "Once upon a time, there was a weak little jellyfish. The jellyfish lived in the big bl\n",
      " sat on the attractive couch. They learned that it is always better to stay calm and work together.\n",
      "\n",
      "CHUNK 3932548 Size: 654816\n",
      "<|endoftext|>\n",
      "\n",
      "\n",
      "Lila and Tom are friends. They like to play in the hut in the woods. The hut is made\n",
      ", even her toys. Mia and her toys played happily together, and they all became the best of friends.\n",
      "\n",
      "CHUNK 4587660 Size: 654918\n",
      "<|endoftext|>\n",
      "\n",
      "There was once a small ant who was unique in a very special way. She had a passion fo\n",
      "d thanked Sue. They cleaned the rocks together and made them shiny.\n",
      "After cleaning the rocks, Bob an\n",
      "u don't have to be scared of the loud dog, I'll protect you\". The mole felt so safe with the little \n",
      " kiss. They played with their ball in the backyard. They were not worried anymore. They were happy.\n",
      "\n",
      "\n",
      "One day, a sad cat named Tom was walking in the rain. The rain made a big flood near his home. Tom \n",
      "lived happily in the forest, always remembering to follow the signs and protect the important root.\n",
      "\n",
      "\n",
      "Once upon a time, there was a pink cat named Kitty. Kitty loved to nap in the warm sun. One day, wh\n",
      "nough spraying, let’s go home now.”\n",
      "So, they both went back home with a happy smile on their faces.\n",
      "\n",
      "\n",
      "Once upon a time, in a small town, there was a store. The store was incredible. It had many toys, b\n",
      "e big bird helped Tim get down safely. Tim learned to listen to his friend Sam and be more careful.\n",
      "\n",
      "\n",
      "Once upon a time, there was a little boy named Tim. Tim loved to paint. He would paint all day long\n",
      "were at his house, and they all played together inside. The rain did not stop them from having fun.\n",
      "\n",
      "\n",
      "Once upon a time, there was a weak little jellyfish. The jellyfish lived in the big blue sea. The j\n",
      " sat on the attractive couch. They learned that it is always better to stay calm and work together.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lila and Tom are friends. They like to play in the hut in the woods. The hut is made of sticks an\n",
      ", even her toys. Mia and her toys played happily together, and they all became the best of friends.\n",
      "\n",
      "\n",
      "\n",
      "There was once a small ant who was unique in a very special way. She had a passion for rotation, w\n",
      " was happy and thanked Sue. They cleaned the rocks together and made them shiny.\n",
      "After cleaning the \n"
     ]
    }
   ],
   "source": [
    "num_processes=8\n",
    "split_special_token = \"<|endoftext|>\".encode('utf-8')\n",
    "\n",
    "with open(input_path, 'rb') as file:\n",
    "        boundaries = find_chunk_boundaries(\n",
    "                file,\n",
    "                num_processes,\n",
    "                \"<|endoftext|>\".encode(\"utf-8\")\n",
    "        )\n",
    "\n",
    "        for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "                file.seek(start)\n",
    "                chunk = file.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "                print(f\"CHUNK {start} Size:\", len(chunk))\n",
    "                print(chunk[:100])\n",
    "                print(chunk[-100:])\n",
    "                \n",
    "        for b_i in range(1, len(boundaries)):\n",
    "                start = boundaries[b_i-1]\n",
    "                # every chunk except first contains split_special_token at start\n",
    "                if b_i!=1:\n",
    "                        start+=len(split_special_token)\n",
    "                \n",
    "                end = boundaries[b_i]\n",
    "                # Last Chunk contains split_special_token at the end\n",
    "                if b_i==len(boundaries)-1:\n",
    "                        end-=len(split_special_token)\n",
    "                \n",
    "                file.seek(start)\n",
    "                chunk = file.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "                print(chunk[:100])\n",
    "                print(chunk[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848f9882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
