{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8e553cc",
   "metadata": {},
   "source": [
    "# 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4337d33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n",
      "<class 'bytes'>\n",
      "[104, 101, 108, 108, 111, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 33]\n",
      "13\n",
      "23\n",
      "hello! „Åì„Çì„Å´„Å°„ÅØ!\n"
     ]
    }
   ],
   "source": [
    "## 2.2\n",
    "test_string = \"hello! „Åì„Çì„Å´„Å°„ÅØ!\"\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(utf8_encoded)\n",
    "# b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n",
    "\n",
    "print(type(utf8_encoded))\n",
    "# <class 'bytes'>\n",
    "\n",
    "# Get the byte values for the encoded string (integers from 0 to 255).\n",
    "print(list(utf8_encoded))\n",
    "# [104, 101, 108, 108, 111, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129,\n",
    "# 161, 227, 129, 175, 33]\n",
    "\n",
    "# One byte does not necessarily correspond to one Unicode character!\n",
    "print(len(test_string))\n",
    "# 13\n",
    "\n",
    "print(len(utf8_encoded))\n",
    "# 23\n",
    "\n",
    "print(utf8_encoded.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14dab049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n",
      "28 b'\\xff\\xfeh\\x00e\\x00l\\x00l\\x00o\\x00!\\x00 \\x00S0\\x930k0a0o0!\\x00'\n",
      "56 b'\\xff\\xfe\\x00\\x00h\\x00\\x00\\x00e\\x00\\x00\\x00l\\x00\\x00\\x00l\\x00\\x00\\x00o\\x00\\x00\\x00!\\x00\\x00\\x00 \\x00\\x00\\x00S0\\x00\\x00\\x930\\x00\\x00k0\\x00\\x00a0\\x00\\x00o0\\x00\\x00!\\x00\\x00\\x00'\n"
     ]
    }
   ],
   "source": [
    "# Problem a\n",
    "# testing utf-16, utf-32\n",
    "# same string gets longer -> sequeunce inefficiency\n",
    "\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(len(utf8_encoded), utf8_encoded)\n",
    "\n",
    "utf16_encoded = test_string.encode(\"utf-16\")\n",
    "print(len(utf16_encoded), utf16_encoded)\n",
    "\n",
    "utf32_encoded = test_string.encode(\"utf-32\")\n",
    "print(len(utf32_encoded), utf32_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf735818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'utf-8' codec can't decode byte 0xf0 in position 0: unexpected end of data\n"
     ]
    }
   ],
   "source": [
    "# Problem b\n",
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "\n",
    "# wrong case\n",
    "try:\n",
    "    decode_utf8_bytes_to_str_wrong(\"hello üëã\".encode(\"utf-8\"))\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69c017d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0x4'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex(4)\n",
    "# b'\\xc2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34ec5fb",
   "metadata": {},
   "source": [
    "### Problem C\n",
    "Due to utf-8 encoding rules:\n",
    "```\n",
    "UTF-8 uses 1 to 4 bytes to encode Unicode characters. Each byte in a valid sequence must follow specific bit patterns:\n",
    "\t‚Ä¢\t1-byte (ASCII): 0xxxxxxx\n",
    "\t‚Ä¢\t2-byte: 110xxxxx 10xxxxxx\n",
    "\t‚Ä¢\t3-byte: 1110xxxx 10xxxxxx 10xxxxxx\n",
    "\t‚Ä¢\t4-byte: 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx\n",
    "\n",
    "In multi-byte sequences, the first byte determines how many bytes follow, and continuation bytes must begin with 10.\n",
    "```\n",
    "\n",
    "`xff` (11111111) is invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2003c974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'UnicodeDecodeError'> 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "# Problem c - 2 byte sequence that doesn't decode to any Unicode\n",
    "\n",
    "encoded = b'\\xff\\xff'\n",
    "try:\n",
    "    print(encoded.decode('utf-8'))\n",
    "except Exception as e:\n",
    "    print(type(e), str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138300f0",
   "metadata": {},
   "source": [
    "# 2.5 training bpe\n",
    "\n",
    "## optimizing the merging step\n",
    "```\n",
    "only pair counts that change after each merge are those that overlap with the merged pair\n",
    "index the counts of all pars -> incrementally update the counts\n",
    "\n",
    "```\n",
    "\n",
    "Reference: Linked-list based method\n",
    "* https://github.com/huggingface/tokenizers/issues/1400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8f3fce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ab'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(('a', 'b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4dba0b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 'a')\n",
      "0\n",
      "5\n",
      "['aa', 'a', 'b', 'd', 'aa', 'a', 'b', 'a', 'c']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({('a', 'b'): 2,\n",
       "         ('aa', 'a'): 2,\n",
       "         ('b', 'd'): 1,\n",
       "         ('b', 'a'): 1,\n",
       "         ('a', 'c'): 1,\n",
       "         ('d', 'aa'): 1,\n",
       "         ('d', 'a'): 0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test small scale\n",
    "from collections import Counter\n",
    "\n",
    "tokens = ['a', 'a', 'a', 'b', 'd', 'a', 'a', 'a', 'b', 'a', 'c']\n",
    "\n",
    "# try merging 2 times\n",
    "pair_counts = Counter()\n",
    "pair_positions = {} # start positions\n",
    "merges = {}\n",
    "\n",
    "for i in range(1, len(tokens)):\n",
    "    a = tokens[i-1]\n",
    "    b = tokens[i]\n",
    "    \n",
    "    if (a,b) not in pair_positions:\n",
    "        pair_positions[(a,b)]=[i-1]\n",
    "    else:\n",
    "        pair_positions[(a,b)].append(i-1)\n",
    "        \n",
    "    pair_counts[(a,b)]+=1\n",
    "    \n",
    "# pair_counts.most_common()\n",
    "target_pair = max(pair_counts, key=pair_counts.get)\n",
    "print(target_pair)\n",
    "new_vocab_token = ''.join(target_pair)\n",
    "\n",
    "del pair_counts[target_pair]\n",
    "\n",
    "## Merge\n",
    "new_tokens = []\n",
    "prev_idx = 0\n",
    "for pos in pair_positions[target_pair]:\n",
    "    print(pos)\n",
    "    if pos>0:\n",
    "        # Decrement\n",
    "        x = (tokens[pos-1], target_pair[0])\n",
    "        pair_positions[x].remove(pos-1)\n",
    "        if x!=target_pair:\n",
    "            pair_counts[x]-=1\n",
    "        \n",
    "        # Increment\n",
    "        x = (tokens[pos-1], new_vocab_token)\n",
    "        pair_counts[x]+=1\n",
    "        \n",
    "    if pos+2<len(tokens):\n",
    "        # Decrement\n",
    "        x = (target_pair[1], tokens[pos+2])\n",
    "        pair_positions[x].remove(pos+1)\n",
    "        if x!=target_pair:\n",
    "            pair_counts[x]-=1\n",
    "        \n",
    "        # Increment \n",
    "        x = (new_vocab_token, tokens[pos+2])\n",
    "        pair_counts[x]+=1\n",
    "        \n",
    "    new_tokens.extend(tokens[prev_idx:pos])\n",
    "    new_tokens.append(new_vocab_token)\n",
    "    prev_idx = pos+2\n",
    "    \n",
    "new_tokens.extend(tokens[prev_idx:])\n",
    "print(new_tokens)\n",
    "pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592abf3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
