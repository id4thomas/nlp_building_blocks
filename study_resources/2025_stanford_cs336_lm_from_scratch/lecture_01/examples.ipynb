{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cc349f",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "```\n",
    "intro_to_tokenization()\n",
    "tokenization_examples()\n",
    "character_tokenizer()\n",
    "byte_tokenizer()\n",
    "word_tokenizer()\n",
    "bpe_tokenizer()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad780a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "import tiktoken\n",
    "import regex\n",
    "\n",
    "class Tokenizer(ABC):\n",
    "    \"\"\"Abstract interface for a tokenizer.\"\"\"\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        raise NotImplementedError\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebec6893",
   "metadata": {},
   "source": [
    "## tokenization_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23de566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt2_tokenizer():\n",
    "    # Code: https://github.com/openai/tiktoken\n",
    "    # You can use cl100k_base for the gpt3.5-turbo or gpt4 tokenizer\n",
    "    return tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "tokenizer = get_gpt2_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'byte' size per token - larger means longer text can get more compressed\n",
    "\n",
    "def get_compression_ratio(string: str, indices: list[int]) -> float:\n",
    "    \"\"\"Given `string` that has been tokenized into `indices`, .\"\"\"\n",
    "    num_bytes = len(bytes(string, encoding=\"utf-8\"))  # @inspect num_bytes\n",
    "    \n",
    "    num_tokens = len(indices)                       # @inspect num_tokens\n",
    "    return num_bytes / num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947a6247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, \\xf0\\x9f\\x8c\\x8d! \\xe4\\xbd\\xa0\\xe5\\xa5\\xbd!'\n",
      "b'Hello'\n",
      "20\n",
      "[15496, 11, 12520, 234, 235, 0, 220, 19526, 254, 25001, 121, 0]\n",
      "Hello, ðŸŒ! ä½ å¥½!\n",
      "1.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "string = \"Hello, ðŸŒ! ä½ å¥½!\"\n",
    "print(bytes(string, encoding=\"utf-8\"))\n",
    "print(bytes(\"Hello\", encoding=\"utf-8\"))\n",
    "print(len(bytes(string, encoding=\"utf-8\")))\n",
    "\n",
    "\n",
    "indices = tokenizer.encode(string)\n",
    "print(indices)\n",
    "\n",
    "reconstructed_string = tokenizer.decode(indices)\n",
    "print(reconstructed_string)\n",
    "\n",
    "# utf-8 maps unicodes into 2~4 bytes, ascii into 1 byte\n",
    "compression_ratio = get_compression_ratio(string, indices)\n",
    "print(compression_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66448eb",
   "metadata": {},
   "source": [
    "## character_tokenizer()\n",
    "* each character can be converted into code point (int)\n",
    "    * code point: unique int value that represents unicode text\n",
    "    * `ord` str->int, `chr` int-> str\n",
    "\n",
    "Issues with character_tokenizer\n",
    "* has very large vocabulary (approx 150K unicode characters)\n",
    "    * allocate one slot for evey character -> inefficient\n",
    "* many characters are rare -> inefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc067192",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTokenizer(Tokenizer):\n",
    "    \"\"\"Represent a string as a sequence of Unicode code points.\"\"\"\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        return list(map(ord, string))\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        return \"\".join(map(chr, indices))\n",
    "    \n",
    "tokenizer = CharacterTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c82d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5384615384615385\n"
     ]
    }
   ],
   "source": [
    "string = \"Hello, ðŸŒ! ä½ å¥½!\"\n",
    "\n",
    "indices = tokenizer.encode(string)\n",
    "reconstructed_string = tokenizer.decode(indices)\n",
    "\n",
    "vocabulary_size = max(indices) + 1\n",
    "compression_ratio = get_compression_ratio(string, indices)\n",
    "print(compression_ratio) # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b11fa83",
   "metadata": {},
   "source": [
    "## byte_tokenizer()\n",
    "* unicode (utf-8) -> **sequence of bytes** -> each byte is integer between 0~255 (8 bits)\n",
    "    * utf-8: single unicode code point can be represented by 1~4 bytes (32bits)\n",
    "* vocab size is small (256), but compression ratio is terrible (1)\n",
    "    * -> token sequence will be too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "534cdcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteTokenizer(Tokenizer):\n",
    "    \"\"\"Represent a string as a sequence of bytes.\"\"\"\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        string_bytes = string.encode(\"utf-8\")  # @inspect string_bytes\n",
    "        indices = list(map(int, string_bytes))  # @inspect indices\n",
    "        return indices\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        string_bytes = bytes(indices)  # @inspect string_bytes\n",
    "        string = string_bytes.decode(\"utf-8\")  # @inspect string\n",
    "        return string\n",
    "tokenizer = ByteTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03dc0116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 [72, 101, 108, 108, 111, 44, 32, 240, 159, 140, 141, 33, 32, 228, 189, 160, 229, 165, 189, 33]\n"
     ]
    }
   ],
   "source": [
    "string = \"Hello, ðŸŒ! ä½ å¥½!\"\n",
    "indices = tokenizer.encode(string)\n",
    "print(len(indices), indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc33ab43",
   "metadata": {},
   "source": [
    "## word_tokenizer()\n",
    "* split strings into words (`r\"\\w+|.\"`)\n",
    "    * `\\w+` (ë‹¨ì–´ ë¬¸ìž) or `.` any\n",
    "    * includes whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eec87009",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_TOKENIZER_REGEX=r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baf3a592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'\", 'll', ' ', 'say', ' ', 'supercalifragilisticexpialidocious', '!']\n"
     ]
    }
   ],
   "source": [
    "string = \"I'll say supercalifragilisticexpialidocious!\"\n",
    "segments = regex.findall(r\"\\w+|.\", string)\n",
    "print(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781fb7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'ll\", ' say', ' supercalifragilisticexpialidocious', '!']\n"
     ]
    }
   ],
   "source": [
    "# fancier (gpt-2)\n",
    "\n",
    "segments = regex.findall(GPT2_TOKENIZER_REGEX, string)\n",
    "print(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9c6919",
   "metadata": {},
   "source": [
    "## bpe_tokenizer()\n",
    "* byte-pair encoding: train the tokenizer on raw text\n",
    "    * common sequences will merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75d40115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:  # @inspect indices, @inspect pair, @inspect new_index\n",
    "    \"\"\"Return `indices`, but with all instances of `pair` replaced with `new_index`.\"\"\"\n",
    "    new_indices = []  # @inspect new_indices\n",
    "    i = 0  # @inspect i\n",
    "    while i < len(indices):\n",
    "        if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:\n",
    "            new_indices.append(new_index)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_indices.append(indices[i])\n",
    "            i += 1\n",
    "    return new_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "406f6ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BPETokenizerParams:\n",
    "    \"\"\"All you need to specify a BPETokenizer.\"\"\"\n",
    "    vocab: dict[int, bytes]     # index -> bytes\n",
    "    merges: dict[tuple[int, int], int]  # index1,index2 -> new_index\n",
    "    \n",
    "class BPETokenizer(Tokenizer):\n",
    "    \"\"\"BPE tokenizer given a set of merges and a vocabulary.\"\"\"\n",
    "    def __init__(self, params: BPETokenizerParams):\n",
    "        self.params = params\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        indices = list(map(int, string.encode(\"utf-8\")))  # @inspect indices\n",
    "        # Note: this is a very slow implementation\n",
    "        for pair, new_index in self.params.merges.items():  # @inspect pair, @inspect new_index\n",
    "            indices = merge(indices, pair, new_index)\n",
    "        return indices\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        bytes_list = list(map(self.params.vocab.get, indices))  # @inspect bytes_list\n",
    "        string = b\"\".join(bytes_list).decode(\"utf-8\")  # @inspect string\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8286c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def train_bpe(string: str, num_merges: int) -> BPETokenizerParams:  # @inspect string, @inspect num_merges\n",
    "    # Start with the list of bytes of string.\n",
    "    indices = list(map(int, string.encode(\"utf-8\")))  # @inspect indices\n",
    "    merges: dict[tuple[int, int], int] = {}  # index1, index2 => merged index\n",
    "    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  # index -> bytes\n",
    "    for i in range(num_merges):\n",
    "        # Count the number of occurrences of each pair of tokens\n",
    "        counts = defaultdict(int)\n",
    "        for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair\n",
    "            counts[(index1, index2)] += 1  # @inspect counts\n",
    "        # Find the most common pair.\n",
    "        pair = max(counts, key=counts.get)  # @inspect pair\n",
    "        index1, index2 = pair\n",
    "\n",
    "        # Merge that pair.\n",
    "        new_index = 256 + i  # @inspect new_index\n",
    "        merges[pair] = new_index  # @inspect merges\n",
    "        vocab[new_index] = vocab[index1] + vocab[index2]  # @inspect vocab\n",
    "        indices = merge(indices, pair, new_index)  # @inspect indices\n",
    "    return BPETokenizerParams(vocab=vocab, merges=merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff09c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"the cat in the hat\"  # @inspect string\n",
    "params = train_bpe(string, num_merges=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57382bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(116, 104): 256, (256, 101): 257, (257, 32): 258}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(params.vocab)) # 256 + 3 merges\n",
    "params.merges # 3 merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aafcd5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[258, 113, 117, 105, 99, 107, 32, 98, 114, 111, 119, 110, 32, 102, 111, 120]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BPETokenizer(params)\n",
    "\n",
    "string = \"the quick brown fox\"\n",
    "indices = tokenizer.encode(string)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa2c91a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
