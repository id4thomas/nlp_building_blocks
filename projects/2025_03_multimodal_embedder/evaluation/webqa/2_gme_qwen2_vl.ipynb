{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alibaba-NLP/gme-Qwen2-VL\n",
    "* general-multimodal-embedding from alibaba [[2B]](https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-2B-Instruct), [[7B]](https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-7B-Instruct)\n",
    "    * Qwen2-VL base model\n",
    "    * uses separate `GmeQwen2VL` model class [[code]](https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-2B-Instruct/blob/main/gme_inference.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/llm/lib/python3.10/site-packages/pydantic/_internal/_fields.py:152: UserWarning: Field \"model_dir\" in Settings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n",
      "/Users/id4thomas/github/nlp_building_blocks/projects/2025_03_multimodal_embedder/evaluation/webqa/src/gme_inference.py:12: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from src.config import settings\n",
    "from src.gme_inference import GmeQwen2VL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model\n",
    "Set to `attn_implementation=\"eager\"` to prevent following error\n",
    "```\n",
    "attn_output = F.scaled_dot_product_attention(q, k, v, attention_mask, dropout_p=0.0)\n",
    "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "IndexError: Dimension out of range (expected to be in range of [-3, 2], but got 3)\n",
    "```\n",
    "* https://github.com/hiyouga/LLaMA-Factory/issues/6838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f911e0059a4848ba94b3a1d2a661ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GmeQwen2VL(\n",
    "    model_path=os.path.join(settings.model_dir, \"embedding/gme-Qwen2-VL-2B-Instruct\"),\n",
    "    device=\"mps\",\n",
    "    max_length=8192,\n",
    "    attn_implementation=\"eager\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9717b73116e144ec83747c9f889d7a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "encode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6baa60df6c445db66a610c950af550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "encode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable To disable this warning, you can either:\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable \t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable To disable this warning, you can either:\n",
      "TOKENIZERS_PARALLELISM\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable =(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISMTOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3860, 0.5542], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "## Inference Example\n",
    "texts = [\n",
    "    \"What kind of car is this?\",\n",
    "    \"The Tesla Cybertruck is a battery electric pickup truck built by Tesla, Inc. since 2023.\"\n",
    "]\n",
    "images = [\n",
    "    'file://./resources/Tesla_Cybertruck_damaged_window.jpg',\n",
    "    'file://./resources/2024_Tesla_Cybertruck_Foundation_Series,_front_left_(Greenwich).jpg',\n",
    "    # 'https://en.wikipedia.org/wiki/File:Tesla_Cybertruck_damaged_window.jpg',\n",
    "    # 'https://en.wikipedia.org/wiki/File:2024_Tesla_Cybertruck_Foundation_Series,_front_left_(Greenwich).jpg',\n",
    "]\n",
    "\n",
    "e_text = model.get_text_embeddings(texts=texts)\n",
    "e_image = model.get_image_embeddings(images=images)\n",
    "print((e_text * e_image).sum(-1))\n",
    "## expected value (from hf page): tensor([0.2281, 0.6001], dtype=torch.float16)\n",
    "## macos inference value: tensor([0.3860, 0.5542], dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from io import BytesIO\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.dataset import (\n",
    "    load_webqa_data,\n",
    "    WebQAQueryDataset,\n",
    "    WebQATCandidateDataset,\n",
    "    WebQATICandidateDataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries, candidates = load_webqa_data(\n",
    "    os.path.join(settings.webqa_data_dir, \"WebQA_test.json\"),\n",
    "    task=\"t2ti\",\n",
    "    text_template = \"{title} {fact}\",\n",
    "    image_text_template = \"{title} {caption}\"\n",
    ")\n",
    "\n",
    "query_ds = WebQAQueryDataset(data=queries)\n",
    "candidates_ds = WebQATICandidateDataset(\n",
    "    data=candidates,\n",
    "    lineidx_fpath=os.path.join(settings.webqa_data_dir, \"imgs.lineidx\"),\n",
    "    images_fpath=os.path.join(settings.webqa_data_dir, \"images/imgs.tsv\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['\"Are both the Original Playboy Mansion and Gage Park High School made of brick?\"', '\"Are there bears in the background of the painting \"Greek Landscape\"?\"', '\"Are there flowering trees in front of both the Georgia Tech Library and the Newman Library at Virginia Tech?\"', '\"Is the surface of the egg next to the handrail at the Big Egg Hunt  in Covent Garden London shiny or dull?\"']}\n"
     ]
    }
   ],
   "source": [
    "query_dl = DataLoader(query_ds, batch_size=4)\n",
    "for x in query_dl:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
