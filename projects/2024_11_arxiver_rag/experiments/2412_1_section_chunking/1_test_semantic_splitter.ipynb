{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from semantic_chunkers import StatisticalChunker\n",
    "# from semantic_router.encoders import OpenAIEncoder\n",
    "from src.encoder import OpenAIEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    model_config = SettingsConfigDict(\n",
    "        env_file=\"../.env\", env_file_encoding=\"utf-8\", extra=\"ignore\"\n",
    "    )\n",
    "    embedding_base_url: str\n",
    "    embedding_api_key: str\n",
    "    embedding_model: str\n",
    "    \n",
    "    sample_data_dir: str\n",
    "    pipeline_src_dir: str\n",
    "settings = Settings()\n",
    "\n",
    "import sys\n",
    "sys.path.append(settings.pipeline_src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.data.paper import ArxivPaperSection, ArxivPaperMetadata, ArxivPaper\n",
    "from core.parser.md2py import TreeOfContents\n",
    "\n",
    "from modules.extractor.section_splitter import MarkdownArxivPaperSectionSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a custom OpenAIEncoder implementation to use local models\n",
    "encoder = OpenAIEncoder(\n",
    "    name=settings.embedding_model,\n",
    "    base_url=settings.embedding_base_url,\n",
    "    api_key=settings.embedding_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StatisticalChunker options\n",
    "# https://github.com/aurelio-labs/semantic-chunkers/blob/43ee0ac6ecdf76790de7d4ac4029f59438f34769/semantic_chunkers/chunkers/statistical.py#L49\n",
    "chunker = StatisticalChunker(\n",
    "    encoder=encoder,\n",
    "    dynamic_threshold = True,\n",
    "    window_size = 5,\n",
    "    min_split_tokens = 256,\n",
    "    max_split_tokens = 2048,\n",
    "    plot_chunks = True,\n",
    "    enable_statistics = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 7) Index(['id', 'title', 'abstract', 'authors', 'published_date', 'link',\n",
      "       'markdown'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## Load Sample\n",
    "df = pd.read_parquet(settings.sample_data_dir)\n",
    "df = df.sample(100)\n",
    "print(df.shape, df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                       2302.05334\n",
       "title             The Role of Codeword-to-Class Assignments in E...\n",
       "abstract          Error-correcting codes (ECC) are used to reduc...\n",
       "authors           Itay Evron, Ophir Onn, Tamar Weiss Orzech, Hai...\n",
       "published_date                                 2023-02-10T15:48:51Z\n",
       "link                              http://arxiv.org/abs/2302.05334v1\n",
       "markdown          # The Role of Codeword-to-Class Assignments in...\n",
       "Name: 61715, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sections(idx, text):\n",
    "    # row = df.iloc[idx]\n",
    "    # text = row['markdown']\n",
    "    # found_filter = False\n",
    "    \n",
    "    sections = None\n",
    "    for filter_cls in MarkdownArxivPaperSectionSplitter.__subclasses__():\n",
    "        try:\n",
    "            if filter_cls.is_type(text):\n",
    "                # print(\"FOUND\",filter_cls)\n",
    "                found_filter = True\n",
    "                sections = filter_cls().split(text)\n",
    "                break\n",
    "        except RecursionError as e:\n",
    "            print(\"{} RECURSION ERROR {}\".format(idx, str(e)))\n",
    "            return idx, None\n",
    "        except Exception as e:\n",
    "            print(\"{} ERROR {}\".format(idx, str(e)))\n",
    "            # print(traceback.format_exc())\n",
    "            raise e\n",
    "    return idx, sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 100 failed 0\n"
     ]
    }
   ],
   "source": [
    "texts = df.markdown.values.tolist()\n",
    "\n",
    "results = [get_sections(idx, text) for idx, text in enumerate(texts)]\n",
    "# Failed Count\n",
    "failed_count = sum(1 for _, sections in results if sections is None)\n",
    "print(\"Total {} failed {}\".format(len(results), failed_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " [ArxivPaperSection(header='h2', title='1 Introduction', text='Error\\\\-correcting codes (ECC) have been long used in machine learning as a reduction from multiclass classification tasks to binary classification tasks (Dietterich and Bakiri, 1994\\\\). This scheme encodes classes using rows of a binary matrix called a codebook. The codebook columns induce binary partitions of classes, or subproblems, to be learned using any binary classification algorithm.\\n\\nRecently, error\\\\-correcting codes have been used as output embeddings of deep networks (Yang et al., 2015; Rodriguez et al., 2018; Kusupati et al., 2021\\\\), on top of features extracted by deep CNNs (Dori et al., 2018\\\\), and as a means to combine ensembles of several networks (Zheng et al., 2018\\\\). Moreover, they were recently used for their robustness in adversarial learning (Verma and Swami, 2019; Gupta and Amin, 2021; Song et al., 2021\\\\) and for their redundancy in regression tasks (Shah et al., 2022\\\\) and heterogeneous domain adaptation (Zhou et al., 2019\\\\).\\n\\nIn extreme multiclass classification, where the number of classes is extremely large, ECC can be particularly beneficial. Several works (Jasinska and Karampatziakis, 2016; Evron et al., 2018\\\\) employed ECC to shrink the output space, decreasing the number of learned predictors, as well as the prediction time, to *logarithmic* in the number of classes. In comparison, both one\\\\-hot encoding and hierarchical models train a linear number of predictors (even though the latter enjoy a logarithmic prediction time).\\n\\n*logarithmic*The first step in employing ECC consists of selecting a *good* codebook. Some codebook properties are universally important for error correction, e.g., the minimum hamming distance between rows. Other properties are only important in some regimes, e.g., the decoding complexity which is essential mainly in extreme classification.\\n\\n*good*Roughly, codebooks can be divided into two categories: *predefined codebooks* and *problem\\\\-dependent codebooks*. Predefined codebooks are independent of the problem at hand, but offer simplicity (e.g., random codebooks), favorable error\\\\-correction properties (e.g., Hadamard codebooks in Zhang et al., 2003 or optimized codebooks in Gupta and Amin, 2022\\\\), or regime\\\\-specific advantages like fast decoding algorithms (Evron et al., 2018\\\\). On the other hand, problem\\\\-dependent approaches attempt to induce binary subproblems that are tailored for a given dataset, often by balancing against other codebook properties.\\n\\n*predefined codebooks**problem\\\\-dependent codebooks*Problem\\\\-dependent codebooks are commonly designed by optimizing over codebooks while taking *class\\\\-similarity* into account. However, there are two *opposite* intuitions in the literature as to *how* to incorporate class\\\\-similarity in the\\ndesign process. Some works follow an intuition that to induce easy subproblems, similar classes should be encoded by *similar* codewords (Zhang et al., 2009; Cisse et al., 2012; Zhao and Xing, 2013; Zhou et al., 2016; Rodriguez et al., 2018\\\\). In contrast, other works encode similar classes by *distant* codewords to improve the error correction between hardly\\\\-separable classes (Pujol et al., 2008; Martin et al., 2017; Youn et al., 2021; Gupta and Amin, 2021\\\\). We examine this *controversy* in depth and provide evidence from multiple regimes that generalization is superior when encoding similar classes by *similar* codewords.\\n\\n*class\\\\-similarity**opposite**how**similar**distant**controversy**similar*In *predefined* codebooks, the mapping between codewords and classes, i.e., the *codeword\\\\-to\\\\-class assignment*, is usually set arbitrarily (e.g., using a random assignment). Dietterich and Bakiri (1994\\\\) showed that randomly\\\\-sampled assignments perform similarly, and since, these assignments have been commonly overlooked.\\n\\n*predefined**codeword\\\\-to\\\\-class assignment*Our paper shows that *codeword\\\\-to\\\\-class assignments do matter* and cause a large variation in the performance of many predefined codebooks (Section 4\\\\.1\\\\.1\\\\). We explain this by showing that, given a codebook, some assignments induce substantially easier binary subproblems than other assignments do (Section 4\\\\.1\\\\.2\\\\). Moreover, we show that the easiest subproblems are induced by assigning similar codewords to similar classes (Section 4\\\\.1\\\\.3\\\\).\\n\\n*codeword\\\\-to\\\\-class assignments do matter*Finally, we employ our observations on extreme multi\\\\-class classification datasets (having 1K to 104K classes). By assigning similar codewords to similar classes, we significantly improve predefined extreme classification codebooks that enjoy fast decoding algorithms (Section 4\\\\.2\\\\).\\n\\nTo the best of our knowledge, this is the first work to point out the large performance variation explained solely by codeword\\\\-to\\\\-class assignments, and to explicitly examine these assignments as a means to control the difficulty of the induced learning\\\\-subproblems in problem\\\\-*independent* predefined codebooks. We conclude that choosing an informed assignment improves predefined codebooks by turning them problem\\\\-*dependent* and better suited for the solved task. Importantly, other useful properties of these codebooks are *not* harmed in this process.\\n\\n*independent**dependent**not*', children=[]),\n",
       "  ArxivPaperSection(header='h2', title='2 Error-Correcting Codes (ECC)', text=\"Error\\\\-correcting codes are widely used for transmitting messages over noisy channels in communication systems, storage systems, and more. By adding redundant bits to transmitted messages, the receiver can recover messages despite errors caused by a disruptive channel (Roth, 2006\\\\).\\n\\nTraining.The seminal work of Dietterich and Bakiri (1994\\\\) employed error\\\\-correcting codes to encode the (K) classes of a classification dataset. They set a binary codebook (\\\\\\\\mathbf{M}\\\\\\\\in\\\\\\\\left{\\\\-1,\\\\+1\\\\\\\\right}^{K\\\\\\\\times\\\\\\\\ell}) with (K\\\\\\\\in\\\\\\\\mathbb{N}) codewords (each belonging to one class) and (\\\\\\\\ell) columns (where (\\\\\\\\ell\\\\\\\\geq\\\\\\\\log\\\\_{2}K)). Each column induces a *binary subproblem*, i.e., a binary partition of classes. Each such subproblem is learned using a base learner (\\\\\\\\mathcal{A}) (i.e., a binary classification learning algorithm), yielding (\\\\\\\\ell) predictors (f\\\\_{1},...,f\\\\_{\\\\\\\\ell}:\\\\\\\\mathcal{X}\\\\\\\\rightarrow\\\\\\\\mathbb{R}). More formally, given a training set (\\\\\\\\left{\\\\\\\\left(\\\\\\\\mathbf{x}*{i},y*{i}\\\\\\\\right)\\\\\\\\right}*{i\\\\=1}^{m}), where (x*{i}\\\\\\\\in\\\\\\\\mathcal{X}) and (y\\\\_{i}\\\\\\\\in\\\\\\\\left\\\\[K\\\\\\\\right]\\\\\\\\triangleq\\\\\\\\left{1,...,K\\\\\\\\right}), the (j)th predictor is the output of (\\\\\\\\mathcal{A}) when trained using the induced binary labels (M\\\\_{y\\\\_{i},j}):\\n\\n*binary subproblem**{i},y**{i\\\\=1}^{m}), where (x*\\\\[f\\\\_{j}\\\\=\\\\\\\\mathcal{A}\\\\\\\\left(\\\\\\\\left{\\\\\\\\left(\\\\\\\\mathbf{x}*{i},M*{y\\\\_{i},j}\\\\\\\\right)\\\\\\\\right}\\\\_ {i\\\\=1}^{m}\\\\\\\\right)\\\\\\\\;. \\\\\\\\tag{1}]\\n\\n*{i},M*Prediction.At prediction time, an example (\\\\\\\\mathbf{x}\\\\\\\\in\\\\\\\\mathcal{X}) is treated as a transmitted message encoding the unknown class (y\\\\\\\\in\\\\\\\\left\\\\[K\\\\\\\\right]). The (\\\\\\\\ell) predictors' scores for (\\\\\\\\mathbf{x}) constitute the vector (\\\\\\\\mathbf{f}(\\\\\\\\mathbf{x})\\\\\\\\triangleq\\\\\\\\left\\\\[f\\\\_{1}(\\\\\\\\mathbf{x}),...,f\\\\_{\\\\\\\\ell}(\\\\\\\\mathbf{ x})\\\\\\\\right]^{\\\\\\\\top}). These scores can be prediction margins from a linear model, confidences from a probabilistic model, outputs of a neural network, etc.\\n\\nFinally, the prediction vector (\\\\\\\\mathbf{f}(\\\\\\\\mathbf{x})) is *decoded* into a codeword belonging to a class. The simplest approach is *hard decoding* that consists of finding the nearest neighbor, that is, the codeword closest (in Hamming distance) to the thresholded prediction vector, (\\\\\\\\operatorname{sign}(\\\\\\\\mathbf{f}(\\\\\\\\mathbf{x}))\\\\\\\\in\\\\\\\\left{\\\\-1,\\\\+1\\\\\\\\right}^{\\\\\\\\ell}).\\n\\n*decoded**hard decoding*Hard decoding ignores the score magnitudes which entail valuable information for prediction. As a remedy, *soft decoding*, or loss\\\\-based decoding (Allwein et al., 2000\\\\), minimizes a decoding loss (\\\\\\\\mathcal{L}:\\\\\\\\mathbb{R}\\\\\\\\rightarrow\\\\\\\\mathbb{R}\\\\_{\\\\\\\\geq 0}):\\n\\n*soft decoding*\\\\[\\\\\\\\hat{y}\\\\\\\\left(\\\\\\\\mathbf{x}\\\\\\\\right)\\\\=\\\\\\\\operatorname\\\\*{arg\\\\\\\\,min}*{y\\\\\\\\in\\\\\\\\left\\\\[K\\\\\\\\right]} \\\\\\\\sum\\\\\\\\nolimits*{j\\\\=1}^{\\\\\\\\ell}\\\\\\\\mathcal{L}\\\\\\\\left(M\\\\_{y,j}f\\\\_{j}\\\\\\\\left(\\\\\\\\mathbf{x} \\\\\\\\right)\\\\\\\\right)\\\\\\\\;. \\\\\\\\tag{2}]\\n\\n*{y\\\\\\\\in\\\\\\\\left\\\\[K\\\\\\\\right]} \\\\\\\\sum\\\\\\\\nolimits*Two popular decoding losses are the hinge loss (\\\\\\\\mathcal{L}\\\\\\\\left(z\\\\\\\\right)\\\\=\\\\\\\\max\\\\\\\\left{0,1\\\\-z\\\\\\\\right}) and the exponential loss (\\\\\\\\mathcal{L}\\\\\\\\left(z\\\\\\\\right)\\\\=e^{\\\\-z}). Notice that soft decoding generalizes hard decoding with (\\\\\\\\mathcal{L}\\\\\\\\left(z\\\\\\\\right)\\\\=\\\\\\\\frac{1\\\\-\\\\\\\\operatorname{sign}(z)}{2}).\\n\\nWe illustrate the entire ECC scheme in App. A.\\n\\nMulticlass error upper bound.Allwein et al. (2000\\\\) proved an insightful upper bound1 that will facilitate our discussion throughout this paper. Let\\n\\nFootnote 1: Zhou et al. (2019\\\\) derived a bound for more general N\\\\-ary codes (where subproblems are also multiclass instead of binary), but this remains out of our scope in this work.\\n\\n\\\\[\\\\\\\\varepsilon\\\\\\\\triangleq\\\\\\\\varepsilon(\\\\\\\\mathbf{M},\\\\\\\\mathcal{L})\\\\=\\\\\\\\frac{1}{m\\\\\\\\ell}{\\\\\\\\sum \\\\\\\\nolimits\\\\_{i\\\\=1}^{m}}{\\\\\\\\sum\\\\\\\\nolimits\\\\_{j\\\\=1}^{\\\\\\\\ell}}\\\\\\\\mathcal{L}\\\\\\\\left(M\\\\_{y\\\\_{i},j}f\\\\_ {j}\\\\\\\\left(\\\\\\\\mathbf{x}\\\\_{i}\\\\\\\\right)\\\\\\\\right) \\\\\\\\tag{3}]\\n\\nbe the *average binary loss* of the binary predictors on a given training set (\\\\\\\\left{\\\\\\\\left(\\\\\\\\mathbf{x}*{i},y*{i}\\\\\\\\right)\\\\\\\\right}*{i\\\\=1}^{m}) with respect to a codebook (\\\\\\\\mathbf{M}) and a decoding loss (\\\\\\\\mathcal{L}). Assume (\\\\\\\\mathcal{L}) satisfies mild conditions (e.g., convexity is sufficient). Then, the \\\\_multiclass training error* when decoding with (\\\\\\\\mathcal{L}) is upper bounded as:\\n\\n*average binary loss**{i},y**{i\\\\=1}^{m}) with respect to a codebook (\\\\\\\\mathbf{M}) and a decoding loss (\\\\\\\\mathcal{L}). Assume (\\\\\\\\mathcal{L}) satisfies mild conditions (e.g., convexity is sufficient). Then, the \\\\_multiclass training error*\\\\[\\\\\\\\frac{1}{m}{\\\\\\\\sum\\\\\\\\nolimits\\\\_{i\\\\=1}^{m}}\\\\\\\\mathbb{I}\\\\[y\\\\_{i}\\\\\\\\neq\\\\\\\\hat{y}(\\\\\\\\mathbf{x}\\\\_{i} )]\\\\\\\\leq\\\\\\\\frac{\\\\\\\\ell\\\\\\\\varepsilon}{\\\\\\\\rho\\\\\\\\mathcal{L}\\\\\\\\left(0\\\\\\\\right)}\\\\\\\\;, \\\\\\\\tag{4}]\\n\\nwhere (\\\\\\\\rho\\\\\\\\triangleq\\\\\\\\rho(\\\\\\\\mathbf{M})\\\\=\\\\\\\\min\\\\_{a\\\\\\\\neq b}\\\\\\\\frac{1}{2}\\\\\\\\left\\\\|\\\\\\\\mathbf{M}*{a, :}\\\\-\\\\\\\\mathbf{M}*{b,:}\\\\\\\\right\\\\|*{1}) is the codebook's \\\\_minimum inter\\\\-row Hamming distance* ((\\\\\\\\mathbf{M}\\\\_{a,:}) being the (a)th row of (\\\\\\\\mathbf{M})) and (\\\\\\\\mathcal{L}\\\\\\\\left(0\\\\\\\\right)) is a scaling factor of (\\\\\\\\mathcal{L}).\\n\\n*{a, :}\\\\-\\\\\\\\mathbf{M}**{1}) is the codebook's \\\\_minimum inter\\\\-row Hamming distance*### Properties of a Good Codebook\\n\\nWe now review favorable properties of error\\\\-correcting codebooks. The first two properties are discussed more often in the literature (e.g., Dietterich and Bakiri, 1994; Zhang et al., 2003\\\\), while the latter two are seldom addressed despite their importance. In many cases improving one property comes at the expense of another.\\n\\n1. **High minimum row distance (\\\\\\\\rho) (between codewords).** With hard decoding (i.e., nearest neighbor), the maximal number of prediction errors the scheme can recover from (\\\\\\\\left\\\\[\\\\\\\\left(\\\\\\\\rho\\\\-1\\\\\\\\right)/2\\\\\\\\right]). Using soft decoding, a high minimum distance is still vital for error correction, as seen from the error bound (4\\\\).\\n2. **Low column correlation (between subproblems).** Intuitively, if two binary predictors often make errors on the same inputs, their mistakes become twice as hard to correct. Thus, uncorrelated columns (that yield uncorrelated binary subproblems) are generally considered advantageous.\\n3. **Efficient decoding algorithm.** Traditionally ignored in many ECC works, the complexity of decoding prediction scores into codewords becomes essential in extreme classification tasks with thousands of codewords or more. Recently, Jasinska and Karampatziakis (2016\\\\) and Evron et al. (2018\\\\) utilized codebooks with a special structure to allow soft decoding using any decoding loss in a time complexity that depends only on the codebook width (\\\\\\\\ell) (which can be logarithmic in the number of codewords (K)). In contrast, exact soft decoding of arbitrary codebooks (e.g., random or optimized ones) requires a time complexity at least linear in (K).\\n4. **Easy binary subproblems (low average loss (\\\\\\\\varepsilon)).** The binary subproblems yield binary predictors with an average binary loss (\\\\\\\\varepsilon). The lower this loss is, the better the multiclass accuracy of the scheme becomes (see (4\\\\)). One way to lower (\\\\\\\\varepsilon) is to use high\\\\-capacity base learners (e.g., kernel SVMs), but such rich models are often prone to overfitting or require more computation. A proper codebook design can lower (\\\\\\\\varepsilon), by making the subproblems *easier*, even for low\\\\-capacity learners. Following are design choices that can achieve this. 1\\\\. **Sparse or imbalanced codebooks.** Allwein et al. (2000\\\\) extended the ECC scheme to ternary codes where (\\\\\\\\mathbf{M}\\\\\\\\in\\\\\\\\left{\\\\-1,0,\\\\+1\\\\\\\\right}^{K\\\\\\\\times\\\\\\\\ell}). They showed that sparse columns generalize the one\\\\-vs\\\\-one scheme and that imbalanced columns generalize the one\\\\-vs\\\\-all scheme. Both options can be seen as ways to create easier subproblems at the expense of the row distance or column correlation. See Zhou et al. (2016\\\\) and Section 6 in Allwein et al. (2000\\\\) for further discussion. 2\\\\. **Problem\\\\-dependent aspects.** Many papers design codebooks that are specifically suitable for the problem at hand while implicitly tuning the difficulty of the binary subproblems. Most of these works are guided by notions of class similarity. Some try (implicitly or explicitly) to create codebooks where similar classes have similar codewords (e.g., Cisse et al., 2012\\\\) in order to create easier subproblems. Others try the opposite (e.g., Martin et al., 2017\\\\) in order to enhance error correction between classes that are hard to separate, at the expense of harder subproblems. Notably, most methods balance preserving the similarity against other codebook properties (e.g., the codeword distance between two very similar classes is encouraged to be 1, whereas (\\\\\\\\rho) is encouraged to be maximal). They create codebooks from scratch or alter existing ones. On the other hand, our observations next allow making *pre\\\\-defined* codebooks more problem\\\\-dependent, by simply assigning codewords to classes in an informed manner, and without harming other codebook properties which may be important.\\n\\n- **High minimum row distance (\\\\\\\\rho) (between codewords).** With hard decoding (i.e., nearest neighbor), the maximal number of prediction errors the scheme can recover from (\\\\\\\\left\\\\[\\\\\\\\left(\\\\\\\\rho\\\\-1\\\\\\\\right)/2\\\\\\\\right]). Using soft decoding, a high minimum distance is still vital for error correction, as seen from the error bound (4\\\\).\\n**High minimum row distance (\\\\\\\\rho) (between codewords).**- **Low column correlation (between subproblems).** Intuitively, if two binary predictors often make errors on the same inputs, their mistakes become twice as hard to correct. Thus, uncorrelated columns (that yield uncorrelated binary subproblems) are generally considered advantageous.\\n**Low column correlation (between subproblems).**- **Efficient decoding algorithm.** Traditionally ignored in many ECC works, the complexity of decoding prediction scores into codewords becomes essential in extreme classification tasks with thousands of codewords or more. Recently, Jasinska and Karampatziakis (2016\\\\) and Evron et al. (2018\\\\) utilized codebooks with a special structure to allow soft decoding using any decoding loss in a time complexity that depends only on the codebook width (\\\\\\\\ell) (which can be logarithmic in the number of codewords (K)). In contrast, exact soft decoding of arbitrary codebooks (e.g., random or optimized ones) requires a time complexity at least linear in (K).\\n**Efficient decoding algorithm.**- **Easy binary subproblems (low average loss (\\\\\\\\varepsilon)).** The binary subproblems yield binary predictors with an average binary loss (\\\\\\\\varepsilon). The lower this loss is, the better the multiclass accuracy of the scheme becomes (see (4\\\\)). One way to lower (\\\\\\\\varepsilon) is to use high\\\\-capacity base learners (e.g., kernel SVMs), but such rich models are often prone to overfitting or require more computation. A proper codebook design can lower (\\\\\\\\varepsilon), by making the subproblems *easier*, even for low\\\\-capacity learners. Following are design choices that can achieve this. 1\\\\. **Sparse or imbalanced codebooks.** Allwein et al. (2000\\\\) extended the ECC scheme to ternary codes where (\\\\\\\\mathbf{M}\\\\\\\\in\\\\\\\\left{\\\\-1,0,\\\\+1\\\\\\\\right}^{K\\\\\\\\times\\\\\\\\ell}). They showed that sparse columns generalize the one\\\\-vs\\\\-one scheme and that imbalanced columns generalize the one\\\\-vs\\\\-all scheme. Both options can be seen as ways to create easier subproblems at the expense of the row distance or column correlation. See Zhou et al. (2016\\\\) and Section 6 in Allwein et al. (2000\\\\) for further discussion. 2\\\\. **Problem\\\\-dependent aspects.** Many papers design codebooks that are specifically suitable for the problem at hand while implicitly tuning the difficulty of the binary subproblems. Most of these works are guided by notions of class similarity. Some try (implicitly or explicitly) to create codebooks where similar classes have similar codewords (e.g., Cisse et al., 2012\\\\) in order to create easier subproblems. Others try the opposite (e.g., Martin et al., 2017\\\\) in order to enhance error correction between classes that are hard to separate, at the expense of harder subproblems. Notably, most methods balance preserving the similarity against other codebook properties (e.g., the codeword distance between two very similar classes is encouraged to be 1, whereas (\\\\\\\\rho) is encouraged to be maximal). They create codebooks from scratch or alter existing ones. On the other hand, our observations next allow making *pre\\\\-defined* codebooks more problem\\\\-dependent, by simply assigning codewords to classes in an informed manner, and without harming other codebook properties which may be important.\\n**Easy binary subproblems (low average loss (\\\\\\\\varepsilon)).***easier***Sparse or imbalanced codebooks.****Problem\\\\-dependent aspects.***pre\\\\-defined*\", children=[]),\n",
       "  ArxivPaperSection(header='h2', title='3 Codeword-to-class assignments', text='The error\\\\-correcting scheme implicitly assigns codewords to classes. Both during training and during decoding, we arbitrarily assumed that the (k)th row in the codebook belongs to the (k)th class (see (1\\\\) and (2\\\\)). In an attempt to show robustness to codeword\\\\-to\\\\-class assignments, Dietterich and Bakiri (1994\\\\) (Section 3\\\\.3\\\\.2 therein) experimented on several random assignments and reported no significant accuracy variation. However, they did not rule out the possibility that some assignments *are* better than others.\\n\\n*are*We hypothesize that some assignments are *significantly* better than others. We first notice that given a codebook, different assignments induce different binary subproblems, potentially changing their difficulty and consequently the average binary loss (\\\\\\\\varepsilon). Next, we define a scoring function that measures the extent to which close codewords are assigned to close classes. This score later helps us conclude that *similarity\\\\-preserving* assignments (i.e., similar codewords to similar classes) are preferable.\\n\\n*significantly**similarity\\\\-preserving*Class\\\\-codeword score.Consider a class metric in the form of a distance matrix (\\\\\\\\mathbf{D}*{\\\\\\\\text{cls}}\\\\\\\\in\\\\\\\\mathbb{R}*{\\\\\\\\geq 0}^{K\\\\\\\\times K}). For instance, (\\\\\\\\mathbf{D}*{\\\\\\\\text{cls}}) can be (inversely proportional to) a symmetrized confusion matrix, a matrix of distances between class embeddings, or a matrix of distances between classes on a hierarchy tree. Define the codeword distance matrix (\\\\\\\\mathbf{D}*{\\\\\\\\mathbf{M}}\\\\\\\\in\\\\\\\\mathbb{R}*{\\\\\\\\geq 0}^{K\\\\\\\\times K}) where (\\\\\\\\left(\\\\\\\\mathbf{D}*{\\\\\\\\mathbf{M}}\\\\\\\\right)*{a,b}\\\\\\\\triangleq\\\\\\\\frac{1}{2}\\\\\\\\left\\\\| \\\\\\\\mathbf{M}*{a,:}\\\\-\\\\\\\\mathbf{M}*{b,:}\\\\\\\\right\\\\|*{1}). To account for the different scales of these matrices, we normalize them such that (\\\\\\\\left\\\\|\\\\\\\\mathbf{D}*{\\\\\\\\text{cls}}\\\\\\\\right\\\\|*{\\\\\\\\text{F}}\\\\=\\\\\\\\left\\\\|\\\\\\\\mathbf{D}*{\\\\\\\\mathbf{ M}}\\\\\\\\right\\\\|*{\\\\\\\\text{F}}\\\\=1\\\\).\\nNotice that an assignment corresponds to reordering, or permuting, the rows of the codebook (\\\\\\\\mathbf{M}) using a (K\\\\\\\\times K) permutation matrix (\\\\\\\\mathbf{P}). Consequently, such an assignment corresponds to permuting the rows *and* columns of the distance matrix (\\\\\\\\mathbf{D}\\\\_{\\\\\\\\mathbf{M}}).\\n\\n*{\\\\\\\\text{cls}}\\\\\\\\in\\\\\\\\mathbb{R}**{\\\\\\\\text{cls}}) can be (inversely proportional to) a symmetrized confusion matrix, a matrix of distances between class embeddings, or a matrix of distances between classes on a hierarchy tree. Define the codeword distance matrix (\\\\\\\\mathbf{D}**{\\\\\\\\geq 0}^{K\\\\\\\\times K}) where (\\\\\\\\left(\\\\\\\\mathbf{D}**{a,b}\\\\\\\\triangleq\\\\\\\\frac{1}{2}\\\\\\\\left\\\\| \\\\\\\\mathbf{M}**{b,:}\\\\\\\\right\\\\|**{\\\\\\\\text{cls}}\\\\\\\\right\\\\|**{\\\\\\\\mathbf{ M}}\\\\\\\\right\\\\|**and*Given a codebook (\\\\\\\\mathbf{M}) and a class metric (\\\\\\\\mathbf{D}*{\\\\\\\\text{cls}}). We assess an assignment, or a permutation (\\\\\\\\mathbf{P}) of the rows in (\\\\\\\\mathbf{M}), by defining the \\\\_class\\\\-codeword score* as the Frobenius distance between (\\\\\\\\mathbf{D}*{\\\\\\\\text{cls}}) and the permuted (\\\\\\\\mathbf{D}*{\\\\\\\\mathbf{M}}):\\n\\n*{\\\\\\\\text{cls}}). We assess an assignment, or a permutation (\\\\\\\\mathbf{P}) of the rows in (\\\\\\\\mathbf{M}), by defining the \\\\_class\\\\-codeword score**{\\\\\\\\text{cls}}) and the permuted (\\\\\\\\mathbf{D}*\\\\[s\\\\_{\\\\\\\\text{cc}}\\\\\\\\left(\\\\\\\\mathbf{P}\\\\\\\\right)\\\\\\\\triangleq\\\\\\\\left\\\\|\\\\\\\\mathbf{D}*{\\\\\\\\text{cls}}\\\\- \\\\\\\\mathbf{D}*{\\\\\\\\mathbf{PM}}\\\\\\\\right\\\\|*{\\\\\\\\text{F}}\\\\=\\\\\\\\left\\\\|\\\\\\\\mathbf{D}*{\\\\\\\\text{cls}}\\\\- \\\\\\\\mathbf{P}\\\\\\\\mathbf{D}*{\\\\\\\\mathbf{M}}\\\\\\\\mathbf{P}^{\\\\\\\\top}\\\\\\\\right\\\\|*{\\\\\\\\text{F}}. \\\\\\\\tag{5}]\\n\\n*{\\\\\\\\text{cls}}\\\\- \\\\\\\\mathbf{D}**{\\\\\\\\text{F}}\\\\=\\\\\\\\left\\\\|\\\\\\\\mathbf{D}**{\\\\\\\\mathbf{M}}\\\\\\\\mathbf{P}^{\\\\\\\\top}\\\\\\\\right\\\\|*Intuitively, an extreme case where (s\\\\_{\\\\\\\\text{cc}}\\\\\\\\left(\\\\\\\\mathbf{P}\\\\\\\\right)\\\\=0\\\\) means that (\\\\\\\\mathbf{D}*{\\\\\\\\text{cls}}) and the permuted (\\\\\\\\mathbf{D}*{\\\\\\\\mathbf{M}}) completely \"agree\", i.e., similar codewords are assigned to similar classes, and dissimilar codewords are assigned to dissimilar classes (realistically, given (\\\\\\\\mathbf{D}*{\\\\\\\\text{cls}}) and (\\\\\\\\mathbf{D}*{\\\\\\\\mathbf{M}}), the minimum is often larger than zero).\\n\\n*{\\\\\\\\text{cls}}) and the permuted (\\\\\\\\mathbf{D}**{\\\\\\\\text{cls}}) and (\\\\\\\\mathbf{D}*Synthetic dataset.App. B illustrates some of the above ideas using a synthetic dataset. For a specific codebook, we show that only *one* assignment can perfectly fit the data, while *all* other ((K!\\\\-1\\\\)) assignments fail. Moreover, the only successful assignment assigns similar codewords to similar classes.\\n\\n*one**all*', children=[]),\n",
       "  ArxivPaperSection(header='h2', title='4 Experiments', text=\"We test our hypothesis and demonstrate the validity of our claims in two regimes. First, in Section 4\\\\.1 we run extensive experiments on small datasets and illustrate how codeword\\\\-to\\\\-class assignments vary greatly in their accuracy. We show that this variation is mostly explained by the average binary loss (\\\\\\\\varepsilon) from (3\\\\) and the class\\\\-codeword score from (5\\\\). We conclude that similarity\\\\-preserving assignments are vital for inducing easy binary subproblems. Then, in Section 4\\\\.2 we employ similarity\\\\-preserving assignments on codebooks for extreme classification. We show how the structure of specific predefined codebooks facilitates finding good assignments and improve performance on datasets with up to 104K classes.\\n\\n### Exhaustive Experiments\\n\\nDatasets.We start by testing our hypothesis on (3\\\\) small datasets with (K\\\\=10\\\\) classes: MNIST(LeCun et al., 1998\\\\), CIFAR\\\\-10(Krizhevsky et al., 2009\\\\), and yeast(Dua and Graff, 2017\\\\).\\n\\nCodebooks.We experiment on 3 predefined codebooks: Two random dense codebooks (generated like in Allwein et al., 2000\\\\) of widths (\\\\\\\\ell\\\\=8,15\\\\) having row distances of (\\\\\\\\rho\\\\=3,5\\\\)(respectively) and a truncated Hadamard matrix (see Hoffer et al., 2018\\\\) with (\\\\\\\\ell\\\\=15\\\\) and (\\\\\\\\rho\\\\=8\\\\).\\n\\nExperimental setup.Working with only (K\\\\=10\\\\) classes allows us to extensively validate our claims on **all** possible (K!\\\\\\\\approx 3\\\\.6M) assignments of each combination of a dataset and a predefined codebook. Notice that given such a combination, we need not *train*(K!) assignments from scratch. Instead, we train only (2^{K\\\\-1}\\\\=512\\\\) binary predictors and construct every possible assignment from them. This technique saves time and decreases the variance of the evaluated test accuracy (details in App. C.1\\\\).\\n\\n**all***train*To demonstrate the flexibility of our observations, we use two different base learners. For MNIST and CIFAR\\\\-10, we train (\\\\\\\\ell) linear predictors using the (soft\\\\-margin) SVM algorithm. For yeast, each binary predictor is a decision tree (built by the Gini splitting criterion and a minimum of 3 samples to split a node). Hyperparameters were tuned using cross\\\\-validation (details in App. C.2\\\\).\\n\\nIn the decoding step (2\\\\), we use the hinge loss, corresponding also to the loss minimized by the SVM used for training the linear base learners.\\n\\n#### 4\\\\.1\\\\.1 Variation in performance of assignments\\n\\nFigure 1 illustrates the large variation in performance for different assignments of given codebooks. For instance, in MNIST we observe that using the random dense codebook of width (\\\\\\\\ell\\\\=8\\\\), the worst assignment achieves (\\\\\\\\approx 77\\\\\\\\%) test accuracy, while the best assignment achieves (\\\\\\\\approx 88\\\\.5\\\\\\\\%).\\n\\nIn all 3 datasets, the narrow ((\\\\\\\\ell\\\\<K)) codebook exhibits higher variation in performance. This can be explained by the low minimum distance ((\\\\\\\\rho\\\\=3\\\\)) which does not allow for meaningful error correction, making the average binary loss (\\\\\\\\varepsilon) a more dominant factor in performance.\\n\\n\\\\\\\\begin{table}\\n\\\\\\\\begin{tabular}{l\\\\|l\\\\|l l l\\\\|l} \\\\\\\\hline \\\\\\\\hline Dataset \\\\& Area \\\\& Feat. \\\\& Train \\\\& Test \\\\& Model \\\\\\\\ \\\\\\\\hline MNIST \\\\& Vision \\\\& 784 \\\\& 60K \\\\& 10K \\\\& Linear \\\\\\\\ CIFAR\\\\-10 \\\\& Vision \\\\& 3,072 \\\\& 50K \\\\& 10K \\\\& Linear \\\\\\\\ yeast \\\\& Life \\\\& 8 \\\\& 1,284 \\\\& 200 \\\\& DT \\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\n\\\\\\\\end{table}\\nTable 1: Exhaustive Experiments’ Datasets\\n\\nFigure 1: Variation of test accuracy across all (10!\\\\\\\\approx 3\\\\.6M) assignments of 3 codebooks on 3 datasets. Dashed lines indicate quartiles (except where the plot is too narrow). There is a large variation in performance across different assignments of the same codebooks.\\nEquidistant codebooks.The low variation in the Hadamard codebook (especially in MNIST) probably stems from it being an *equidistant codebook* (every two codewords are in the same distance from each other). In such codebooks, the class\\\\-codeword score (5\\\\) remains constant across all assignments (since (\\\\\\\\forall\\\\\\\\mathbf{P}\\\\\\\\colon\\\\\\\\mathbf{D}*{\\\\\\\\mathbf{M}}\\\\=\\\\\\\\mathbf{PD}*{\\\\\\\\mathbf{M}}\\\\\\\\mathbf{P} ^{\\\\\\\\top})). This also supports the following findings (Section 4\\\\.1\\\\.3\\\\) that the class\\\\-codeword score is a lead factor in the observed performance variation.\\n\\n*equidistant codebook**{\\\\\\\\mathbf{M}}\\\\=\\\\\\\\mathbf{PD}*#### 4\\\\.1\\\\.2 Some assignments induce easier subproblems\\n\\nFigure 1(a) shows the correlation between the average binary train loss (\\\\\\\\varepsilon) and the test accuracy. We plot the empirical distribution (using kernel density estimation) of all 3\\\\.6M assignments ran on the 3 datasets using the (10\\\\\\\\times 8\\\\) random dense codebook.\\n\\nFor MNIST (top left), the correlation between the test accuracy and (\\\\\\\\varepsilon) is the highest ((r^{2}\\\\=0\\\\.78\\\\)). The other two datasets exhibit lower correlations, but large performance gaps are still explained by (\\\\\\\\varepsilon) which roughly quantifies the difficulty of subproblems induced by each assignment.\\n\\nWe observe a similar behavior in another (10\\\\\\\\times 8\\\\) codebook and a wider (10\\\\\\\\times 15\\\\) codebook as well (App. D).\\n\\nThe observed correlation between performance and the average binary loss (\\\\\\\\varepsilon) is itself not surprising and can be expected from the error bound in (4\\\\). However, our results stress that different *assignments* of the *same* codebook induce binary subproblems of different difficulty.\\n\\n*assignments**same*#### 4\\\\.1\\\\.3 Similarity\\\\-preserving assignments are better\\n\\nWe now test the effect of class similarity on an assignment's performance. We use the class\\\\-codeword score (5\\\\) to assess how close are codewords of similar classes.\\n\\nSources of class similarity.Our class\\\\-codeword score requires a matrix (\\\\\\\\mathbf{D}*{\\\\\\\\text{cls}}) corresponding to a class metric. Here, we use two \\\\_different* class metrics to strengthen our findings. First, we use the (training) confusion matrices of one\\\\-vs\\\\-all predictors, assuming that confusable classes are semantically similar (a common assumption; see Zhou et al., 2016\\\\). Then, in App. D, we use Euclidean distances between the means of raw features of each class. App. C.3 explains how we turn a confusion matrix (a similarity matrix) into a distance matrix.\\n\\n*{\\\\\\\\text{cls}}) corresponding to a class metric. Here, we use two \\\\_different*Results.Figure 1(b) shows the correlation between our class\\\\-codeword score and test accuracy. We use the same random dense (10\\\\\\\\times 8\\\\) codebook as before, and compute the class\\\\-codeword score from confusion matrices (see above).\\n\\nFor example, the plot on the bottom\\\\-middle shows the distribution of all 3\\\\.6M assignments ran on CIFAR\\\\-10\\\\. On average, assigning similar codewords to similar classes (thus minimizing the class\\\\-codeword score) improves the test accuracy from (\\\\\\\\approx!29\\\\\\\\%) to (\\\\\\\\approx!32\\\\.5\\\\\\\\%). Moreover, assigning similar codewords to *dissimilar* classes evidently worsens the performance significantly (to (\\\\\\\\approx!25\\\\.5\\\\\\\\%))\\n\\n*dissimilar*We observe a similar behavior in another (10\\\\\\\\times 8\\\\) codebook and a wider (10\\\\\\\\times 15\\\\) codebook as well (App. D).\\n\\nFigure 2: The empirical distributions of *all* the 3\\\\.6M assignments of the random (10\\\\\\\\times 8\\\\) codebook on the 3 datasets. **Top:** Test accuracy vs. average binary (train) loss from (3\\\\). **Bottom:** Test accuracy vs. the class\\\\-codeword score from (5\\\\). Each level set contains (\\\\\\\\approx 10\\\\\\\\%) of the assignments. The (10^{\\\\-3}) least probable assignments are scattered as individual points. Regressors computed on all assignments are plotted in orange. Also written are the coefficients of determination ((r^{2})).\\n\\n*all***Top:****Bottom:**#### 4\\\\.1\\\\.4 Summary\\n\\nSome assignments of *the same* codebook induce much easier binary subproblems than others do. Our class\\\\-codeword score largely explains the performance of an assignment.\\n\\n*the same*Computing the class\\\\-codeword score of one assignment is cheap and mainly requires calculating the distance between two (K\\\\\\\\times K) matrices. Thus, when (K\\\\=10\\\\), exhaustively iterating *all* 3\\\\.6M assignments to find the one minimizing that score, takes only a few minutes on a single CPU. Overall, a similarity\\\\-preserving assignment found exhaustively *before* training should yield a much better test accuracy than a random assignment.\\n\\n*all**before*In App. E we show that the class\\\\-codeword score also controls performance in a larger dataset (CIFAR\\\\-100\\\\), where any exhaustive experiment becomes intractable. We demonstrate that similarity\\\\-preserving assignments, originating from the distances between fastText embeddings of *class names*, significantly improve performance.\\n\\n*class names*### Extreme Multiclass Classification (XMC)\\n\\nWe now utilize our understanding that similar codewords should be assigned to similar classes on four XMC benchmarks trained using XMC\\\\-dedicated codebooks. We show that in the extreme regime as well \\\\-\\\\- similarity\\\\-preserving assignments are significantly better than random ones.\\n\\nDatasets.We experiment on four XMC preprocessed benchmarks \\\\- LSHTC1, LSHTC2 (Partalas et al., 2015\\\\), aloi.bin (Rocha and Goldenstein, 2013; Yen et al., 2016\\\\), and ODP (Bennett and Nguyen, 2009\\\\). The datasets are described briefly below and in detail in App. F.\\n\\nSources of class similarity.For all datasets, our algorithm below uses class taxonomies given in a form of a tree. These taxonomies are either known in advance (in LSHTC1 and LSHTC2\\\\) *or computed* by a simple hierarchical clustering algorithm on class means (in aloi.bin and ODP). Again, using multiple sources of class similarities corroborates the soundness of our findings below.\\n\\n*or computed*Experimental setup.We use the code from the publicly available repository of Evron et al. (2018\\\\) to learn using their WLTLS codebooks. To use our similarity\\\\-preserving codeword\\\\-to\\\\-class assignments, we edit their scripts to allow for fixed assignments (rather than random ones).2 We also use the same learning setup \\\\-\\\\- as a base learner, we use AROW (Crammer et al., 2009\\\\), which is an online algorithm for learning linear classifiers, and we also use the exponential loss for the soft decoding step in (2\\\\). We run all experiments sequentially on a single i7 CPU. In practice, each binary predictor can be trained on a separate CPU.\\n\\nFootnote 2: The updated GitHub repository is available on <https://github.com/ievron/wltls>\\n\\n<https://github.com/ievron/wltls>For each dataset, we train several WLTLS codebooks of various widths (\\\\\\\\ell). Each codebook is learned 5 times using random assignments and 5 times using similarity\\\\-preserving assignments, found as described below (here, randomness stems from shuffling the training set).\\n\\nFor comparison, we also train one\\\\-vs\\\\-all (OVA) models using the same base learner \\\\- AROW. Our OVA results are better than the ones reported in Evron et al. (2018\\\\), since we apply oversampling (Ling and Li, 1998\\\\) to overcome the high imbalance in each OVA subproblem.\\n\\nFinding similarity\\\\-preserving assignments.We exploit the graph structure of WLTLS codebooks which embed (K) codewords on source\\\\-to\\\\-target paths of a directed acyclic graph (DAG) with exactly (K) such paths. Since the class taxonomies are also DAGs, a quick\\\\-and\\\\-simple algorithm arises for assigning similar codewords to similar classes.\\n\\nInput:\\n1\\\\. The dataset's class taxonomy (given or learned)\\n2\\\\. A WLTLS coding DAG suitable for (K) classes\\n\\nAlgorithm:\\n1\\\\. Traverse the class tree with DFS to obtain an ordering ((a\\\\_{1},...,a\\\\_{K})) of leaves (i.e., classes);\\n2\\\\. Recursively iterate *all*(K) paths in the coding DAG, to obtain an ordering ((b\\\\_{1},...,b\\\\_{K})) of paths (i.e., codewords);\\n3\\\\. Assign class (a\\\\_{i}) to codeword (b\\\\_{i}). \\\\`\\\\`\\\\`\\n\\n*all***Algorithm Sketch:** Naive assignment for WLTLS\\n\\n**Algorithm Sketch:**The proposed algorithm preserves similarities by assigning similar classes to similar codewords. Intuitively, in most cases classes (a\\\\_{i}) and (a\\\\_{i\\\\+1}) are close on the taxonomy and paths (b\\\\_{i}) and (b\\\\_{i\\\\+1}) are similar on the codebook's DAG. We illustrate this algorithm in App. F.2\\\\.\\n\\nDespite its simplicity, the algorithm finds assignments with exceptionally low class\\\\-codeword scores (5\\\\) compared to the scores of random assignments. For example, for the smallest codebook of LSHTC1 ((\\\\\\\\ell\\\\=56\\\\)), random assignments exhibit an average score of (\\\\\\\\widehat{s\\\\_{\\\\\\\\text{cc}}}\\\\\\\\approx 0\\\\.061\\\\) with an empirical standard deviation of (1\\\\.33\\\\\\\\cdot 10^{\\\\-5}); while the assignment our algorithm finds has a score of (s\\\\_{\\\\\\\\text{cc}}\\\\\\\\approx 0\\\\.049\\\\). That is, compared to random assignments, our algorithm decreases the score by more than (900\\\\) standard deviations (!).\\n\\n\\\\\\\\begin{table}\\n\\\\\\\\begin{tabular}{l\\\\|c\\\\|c c\\\\|l} \\\\\\\\hline \\\\\\\\hline Dataset \\\\& Area \\\\& Classes \\\\& Features \\\\& Similarity \\\\\\\\ \\\\\\\\hline aloi \\\\& Vision \\\\& 1K \\\\& 637K \\\\& Clustering \\\\\\\\ LSHTC1 \\\\& Text \\\\& 12K \\\\& 1\\\\.2M \\\\& Given \\\\\\\\ LSHTC2 \\\\& Text \\\\& 27K \\\\& 575K \\\\& Given \\\\\\\\ ODP \\\\& Text \\\\& 104K \\\\& 423K \\\\& Clustering \\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\n\\\\\\\\end{table}\\nTable 2: Extreme Benchmarks. Further details in App. F.\\nResults.Figure 3 demonstrates the advantage of similarity\\\\-preserving codeword\\\\-to\\\\-class assignments. For each dataset, we compare the test accuracy of random assignments to that of similarity\\\\-preserving assignments across various codebook widths (\\\\\\\\ell).\\n\\nWe plot the test accuracy averages of the 5 runs of each combination of a codebook width and an assignment method, accompanied by 2 empirical standard deviations (full result tables are given in App. F.3\\\\). In almost all cases, similarity\\\\-preserving assignments lead to a statistically\\\\-significant improvement over random assignments. Moreover, in 16 out of 18 cases, similarity\\\\-preserving assignments exhibit a lower variance. In LSHTC1 and LSHTC2, similarity\\\\-preserving assignments make the codebooks competitive with OVA while training up to 32 times fewer predictors.\\n\\nIn the two larger codebooks of aloi.bin, our assignments do not improve much over random ones. This probably happens because when (\\\\\\\\ell) approaches (K), the underlying WLTLS codebooks become almost equidistant.\\n\\nSummary.Similarity\\\\-preserving assignments significantly improve codebooks dedicated to extreme classification. By exploiting class semantics, such assignments turn *predefined* codebooks with regime\\\\-specific advantages (e.g., fast decoding algorithms) into problem\\\\-dependent codebooks, without losing those advantages.\\n\\n*predefined*\", children=[]),\n",
       "  ArxivPaperSection(header='h2', title='5 Related Work', text=\"Our work is of a retrospective nature and calls for an elaborate discussion of its connections with decades of existing research on error\\\\-correcting codes.\\n\\nCodebooks with easy subproblems are obviously preferable. Bai et al. (2016\\\\) design a codebook by selecting a subset of the easiest columns out of all possible columns. They exhaustively train on *all* these columns and select a column subset based on the trained predictors' accuracy. This works well but does not scale gracefully (e.g., for merely (K\\\\=10\\\\) classes, it requires *training*(2^{K\\\\-1}\\\\=512\\\\) predictors). Instead, many works (including ours) exploit extra knowledge on classes to create easy subproblems.\\n\\n*all**training*Codebook design methods.While we point out that similarity\\\\-preserving assignments improve a *predefined* codebook by making it *problem dependent*, most works try to design the *entire* codebook. Given a dataset, designing optimal codebooks is a hard problem due to their discrete nature (Crammer and Singer, 2002\\\\). As a remedy, some papers take greedy approaches, e.g., sequentially adding optimized columns (Pujol et al., 2008\\\\) or solving integer programming formulations (Gupta and Amin, 2021, 2022\\\\); while others take approximate approaches, like solving relaxed continuous optimization problems (e.g., Zhang et al., 2009; Rodriguez et al., 2018\\\\).\\n\\n*predefined**problem dependent**entire*The class\\\\-similarity controversy.Many papers incorporate different notions of class similarity into their design process. Interestingly, some encode similar classes with *similar* codewords (Zhang et al., 2009; Cisse et al., 2012; Zhao and Xing, 2013; Zhou et al., 2016; Rodriguez et al., 2018; McVay, 2020\\\\), whereas others encode similar classes with *dissimilar* codewords (Pujol et al., 2008; Martin et al., 2017; Jaiswal et al., 2020; Gupta and Amin, 2021; Wan et al., 2022\\\\). For instance, Martin et al. (2017\\\\) look for a codebook (\\\\\\\\mathbf{M}!\\\\\\\\in!{\\\\-1,\\\\+1}^{K\\\\\\\\times\\\\\\\\ell}) that *minimizes*(\\\\\\\\big{\\\\|}\\\\\\\\mathbf{D}*{\\\\\\\\text{cls}}\\\\-\\\\\\\\mathbf{M}\\\\\\\\mathbf{M}^{\\\\\\\\top}\\\\\\\\big{\\\\|}*{\\\\\\\\text{F}}^ {2}), while balancing against other codebook properties. In fact, they *maximize* our score (5\\\\) instead of minimizing it, since (\\\\\\\\mathbf{M}\\\\\\\\mathbf{M}^{\\\\\\\\top}\\\\=\\\\\\\\ell\\\\\\\\mathbf{1}*{K\\\\\\\\times K}\\\\-\\\\\\\\mathbf{D}*{\\\\\\\\mathbf{M}}).\\n\\n*similar**dissimilar**minimizes**{\\\\\\\\text{cls}}\\\\-\\\\\\\\mathbf{M}\\\\\\\\mathbf{M}^{\\\\\\\\top}\\\\\\\\big{\\\\|}**maximize**{K\\\\\\\\times K}\\\\-\\\\\\\\mathbf{D}*Existing literature on adversarial robustness has thus far considered assigning *dissimilar* codewords to similar classes (e.g., Gupta and Amin (2021\\\\); Wan et al. (2022\\\\)). in order to improve the error\\\\-correcting capabilities between easily\\\\-confusable classes, especially in the presence of an adversary. On the other hand, our study shows that similarity\\\\-preserving assignments improve the separability and classification performance in traditional settings. An interesting future direction should be to perform adequate ablation studies in the adversarial learning regime and examine the tradeoff between separability (maximized by similarity\\\\-preserving assignments) and robustness (maximized by similarity\\\\-breaking ones).\\n\\n*dissimilar*Class similarity in extreme classification (XMC).In Section 4\\\\.2 we use a class taxonomy to improve a codebook that requires training very few predictors compared\\n\\nFigure 3: Results on extreme datasets. We run WLTLS codebooks using different predictor numbers (\\\\\\\\ell). Errorbars indicate (\\\\\\\\pm 2\\\\) empirical standard deviations of (5\\\\) runs). Results are available in a tabular form in App. F.3\\\\. Due to computational infeasibility, we do not report the performance of one\\\\-vs\\\\-all (OVA) on the largest dataset (LSHTC2\\\\), but just mark its number of binary predictors instead (where (\\\\\\\\ell!\\\\=!K)). In all datasets, assigning similar codewords to similar classes significantly improves performance.\\nto one\\\\-vs\\\\-all or hierarchical models. A closely related work (Cisse et al., 2012\\\\) designs XMC\\\\-codebooks using a learned class\\\\-similarity. However, their codebooks do not allow fast decoding like the ones we use. Other related approaches learn hierarchical models using a given (or learned) class taxonomy, to either benefit from a (\\\\\\\\mathcal{O}\\\\\\\\left(\\\\\\\\log K\\\\\\\\right)) prediction time (Bengio et al., 2010\\\\), or to alleviate the computation of the softmax while training a deep network (Morin and Bengio, 2005\\\\). Another approach directly builds a codebook from a class taxonomy (Pujol et al., 2006\\\\). However, these approaches train (\\\\\\\\mathcal{O}\\\\\\\\left(K\\\\\\\\right)) predictors, implying longer training and *linear* space requirements. Recently, Mittal et al. (2021\\\\) incorporated label metadata in the training of *deep* extreme classification models (much larger than the linear WLTLS models we use). Finally, Rahman et al. (2018\\\\) use class semantics to improve zero\\\\-shot performance, which may be relevant to XMC tasks which often suffer from a long tail of classes (Babbar et al., 2014\\\\), some having few to no training examples.\\n\\n*linear**deep*Ordinal classification and regression taskscan also be tackled with ECC. Interestingly, successful assignments used implicitly in these areas often follow a similar rule\\\\-of\\\\-thumb like we do \\\\- they encode target labels that are similar (i.e., close on the real line) using similar codewords. For instance, see the Unary and HEXJ codebooks in Shah et al. (2022\\\\) (the first codebook is equivalent to the underlying codebook in Li and Lin, 2006\\\\) or the random ordered\\\\-splits in Huhn and Hullermeier (2008\\\\). However, similarities in these areas (i.e., distances on the real line) are much simpler than the inter\\\\-class relations examined in our paper.\\n\\nNested dichotomies (ND)offer another reduction from multiclass tasks to binary ones. Basically, ND models split classes recursively in a binary hierarchical structure, where each tree node corresponds to a binary classification subproblem. One could either use a single tree (Fox, 1997\\\\) or an ensemble of trees (Frank and Kramer, 2004\\\\). The resulting models can be seen as a special case of ECC.\\n\\nMelnikov and Hullermeier (2018\\\\) conduct an experiment that is closely related to our variation demonstration in Section 4\\\\.1\\\\.1\\\\. They show that the assignment of classes to leaves of a *single* ND tree greatly affects the model's performance, and report a high variation in the performance of *randomly\\\\-sampled* NDs (the tree structure was also shown to be important in Mnih and Hinton, 2008\\\\). However, their tree corresponds to a codebook with a minimum Hamming distance of (\\\\\\\\rho\\\\=1\\\\) (i.e., a prediction mistake in *one* inner node necessarily results in a multiclass error). Thus, it is not immediate that their findings generalize to codebooks with higher error\\\\-correcting capabilities (like the ones we use). Importantly, we do not only point out the performance variation of codeword\\\\-to\\\\-class assignments, but also clearly show it is explained by class\\\\-similarity (Section 4\\\\.1\\\\.3\\\\).\\n\\n*single**randomly\\\\-sampled**one*Model capacity.Codeword\\\\-to\\\\-class assignments control the difficulty of the binary subproblems, which is naturally more crucial when the base learners are weaker (see the (\\\\\\\\varepsilon/\\\\\\\\rho) factor in (4\\\\)). Related phenomena have been exhibited in ordinal classification (Huhn and Hullermeier, 2008\\\\) and nested dichotomies (Melnikov and Hullermeier, 2018\\\\) as well. In this paper, we demonstrated our findings using relatively weak linear models and decision trees over raw features (Section 4\\\\.1\\\\) and preprocessed ones (Section 4\\\\.2; App. E).\\n\\nEven high\\\\-capacity models like neural networks are likely to favor similarity\\\\-preserving assignments. Zhai and Wu (2019\\\\) show that a deep classification network implicitly performs *metric learning* \\\\-\\\\- training embeds the classes' weight vectors in the last linear layer (preceding the softmax) in a way that reflects underlying class semantics (see also Kusupati et al. (2021\\\\)). Similarity\\\\-preserving codebooks can be seen as fixing the last layer using a matrix that already reflects such semantics at initialization (see also Sec. 3\\\\.3 in Hoffer et al., 2018\\\\).\\n\\n*metric learning*Notably, complex models can attain a very low average training binary loss (\\\\\\\\varepsilon) such that the training error bound (4\\\\) becomes (\\\\<1/m), implying no *training* mistakes. However, this does not make assignments unimportant. If, for example, we train and decode using an exponential loss, then complex learners can obtain an extremely low loss (\\\\\\\\varepsilon), but never (0\\\\). In such cases, similarity\\\\-preserving assignments should *still* yield a lower (\\\\\\\\varepsilon). In turn, a lower training loss, even when the error is already (0\\\\), is linked to better generalization, both theoretically and practically (e.g., Soudry et al. (2018\\\\)).\\n\\n*training**still*Limitations of design methods.Similarity\\\\-preserving assignments can enhance almost *any* predefined codebook, while design methods are often restricted to codebooks with certain properties. For instance, the spectral method (Zhang et al., 2009\\\\) creates only narrow codebooks (where (\\\\\\\\ell\\\\\\\\leq K)) and does not explicitly take the minimum row distance (\\\\\\\\rho) into account, which may not be best suited for small datasets (e.g., on CIFAR\\\\-10 with (\\\\\\\\ell\\\\=8\\\\), their method yielded two identical rows). Other methods scale poorly with the number of classes (Bai et al., 2016; Escalera et al., 2008\\\\). Some are more suitable for creating balanced dense columns (Zhang et al., 2009; Rodriguez et al., 2018\\\\) while others focus on sparse columns (Pujol et al., 2006\\\\).\\n\\n*any*Limitations of finding informed assignments.Designing problem\\\\-dependent codebooks from scratch is naturally more flexible than only assigning classes to predefined ones. Objective scores can be optimized more freely when the codebook itself is not fixed like in predefined codebooks. However, predefined codebooks can have favorable properties like fast decoding algorithms, hence it is important to be able to find informed assignments for them.\\nWe use our class\\\\-codeword score (\\\\\\\\left\\\\\\\\lVert\\\\\\\\mathbf{D}*{\\\\\\\\mathrm{cls}}\\\\-\\\\\\\\mathbf{PD}*{\\\\\\\\mathbf{M}}\\\\\\\\mathbf{P}^{\\\\\\\\top} \\\\\\\\right\\\\\\\\rVert\\\\_{\\\\\\\\mathrm{F}}) mainly to demonstrate the superiority of similarity\\\\-preserving assignments (Section 4\\\\.1\\\\.3\\\\). One could also employ this score as a surrogate to control the difficulty of subproblems, and directly minimize it on a given codebook to find an optimal similarity\\\\-preserving assignment. However, finding this optimum corresponds to solving a weighted graph\\\\-matching problem, which does not have a known efficient algorithm (Umeyama, 1988\\\\). Instead, one could settle for assignments with a low (but possibly suboptimal) score. We exemplify this using a local search on a CIFAR\\\\-100 codebook (App. E). An exception where our score is constant and assignments are less impactful, is in equidistant codebooks (e.g., Hadamard, OVA, OVO; see Section 4\\\\.1\\\\.1\\\\). This suggests that equidistant codebooks are perhaps more suitable when no class semantics are available. They can also be expected to yield smaller variation (see Figure 1\\\\). See also James and Hastie (1998\\\\) who linked such codebooks to Bayes optimality. As a downside, these codebooks must be wide ((\\\\\\\\ell\\\\\\\\geq K)), which is unacceptable in many cases such as extreme classification.\\n\\n*{\\\\\\\\mathrm{cls}}\\\\-\\\\\\\\mathbf{PD}*Greedy assignment policies.After submitting our paper, we became aware of two recent works closely related to ours that also improve the performance of a given codebook using codeword\\\\-to\\\\-class assignments. McVay (2020\\\\) exploits a sparse class\\\\-similarity matrix to greedily assign similar codewords to similar classes. Wan et al. (2022\\\\) employ ECC for adversarial learning, by altering a Hadamrd codebook (to break its equidistance property) and using a confusion matrix to greedily assign *dissimilar* codewords to similar classes (in contrast to our policy; see the discussion on this controversy above).\\n\\n*dissimilar*Both these works focus on specific greedy assignment policies for specific codebooks. We on the other hand extensively test our hypotheses on many codebooks and demonstrate the superiority of similarity\\\\-preserving assignments over similarity\\\\-breaking ones in traditional classification settings. We exhaustively evaluate *all* possible assignments in several codebooks on three small datasets (see Figure 2 and App. D); and also evaluate different greedy assignment policies on larger datasets (see 4\\\\.2 and App. E).\\n\\n*all*\", children=[]),\n",
       "  ArxivPaperSection(header='h2', title='6 Conclusion', text='Codeword\\\\-to\\\\-class assignments matter because they vary greatly in the difficulty of subproblems they induce, even for a *predefined* codebook. In classification tasks (of both small and large scales), similarity\\\\-preserving assignments lead to easier subproblems and better generalization performance. Predefined codebooks can be advantageous when certain properties are crucial, e.g., specific minimum distance (\\\\\\\\rho) and number of predictors (\\\\\\\\ell), a given sparsity level, or an efficient decoding algorithm. Choosing an informed assignment according to class semantics, allows for improving predefined codebooks by making them more problem\\\\-dependent.\\n\\n*predefined*Further research might discover that different usages require different assignment policies. For instance, perhaps similarity\\\\-preserving assignments benefit generalization, while similarity\\\\-breaking assignments benefit robustness (see the discussion in Section 5\\\\).\\n\\n', children=[]),\n",
       "  ArxivPaperSection(header='h2', title='Acknowledgements', text='We thank Koby Crammer and Thomas G. Dietterich for the fruitful discussions. The research of DS was Funded by the European Union (ERC, A\\\\-B\\\\-C\\\\-Deep, 101039436\\\\). Views and opinions expressed are however those of the author only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency (ERCEA). Neither the European Union nor the granting authority can be held responsible for them. DS also acknowledges the support of Schmidt Career Advancement Chair in AI. Finally, we thank the Control Robotics \\\\& Machine Learning (CRML) Lab at the Technion for their support.\\n\\n', children=[])])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test Chunking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
