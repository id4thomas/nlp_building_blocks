{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import List, Literal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    model_config = SettingsConfigDict(\n",
    "        env_file=\"../.env\", env_file_encoding=\"utf-8\", extra=\"ignore\"\n",
    "    )\n",
    "    pipeline_src_dir: str\n",
    "settings = Settings()\n",
    "\n",
    "import sys\n",
    "sys.path.append(settings.pipeline_src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.data.paper import ArxivPaperSection, ArxivPaperMetadata, ArxivPaper\n",
    "from core.parser.md2py import TreeOfContents\n",
    "\n",
    "from modules.extractor.section_splitter import MarkdownArxivPaperSectionSplitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivPaperSection(BaseModel):\n",
    "    header: Literal[\"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]\n",
    "    title:str = Field(..., description=\"Section title\")\n",
    "    text: str = Field(\"\", description = \"Section contents\")\n",
    "    children: List[\"ArxivPaperSection\"] = Field(list(), description=\"child sections\")\n",
    "\n",
    "class ArxivPaperMetadata(BaseModel):\n",
    "    authors: str\n",
    "    published_date: datetime\n",
    "    link: str\n",
    "\n",
    "class ArxivPaper(BaseModel):\n",
    "    id: str\n",
    "    title: str\n",
    "    abstract: str\n",
    "    sections: List[ArxivPaperSection]\n",
    "    metadata: ArxivPaperMetadata = Field(None, description=\"paper metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Test with Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 7) Index(['id', 'title', 'abstract', 'authors', 'published_date', 'link',\n",
      "       'markdown'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## Load Sample\n",
    "df = pd.read_parquet(\"../sample.parquet\")\n",
    "df = df.sample(100)\n",
    "print(df.shape, df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                       2307.16062\n",
       "title             Using Implicit Behavior Cloning and Dynamic Mo...\n",
       "abstract          Reinforcement learning (RL) for motion plannin...\n",
       "authors           Zengjie Zhang, Jayden Hong, Amir Soufi Enayati...\n",
       "published_date                                 2023-07-29T19:46:09Z\n",
       "link                              http://arxiv.org/abs/2307.16062v2\n",
       "markdown          Using Implicit Behavior Cloning and Dynamic Mo...\n",
       "Name: 24219, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sections(idx, text):\n",
    "    # row = df.iloc[idx]\n",
    "    # text = row['markdown']\n",
    "    # found_filter = False\n",
    "    \n",
    "    sections = None\n",
    "    for filter_cls in MarkdownArxivPaperSectionSplitter.__subclasses__():\n",
    "        try:\n",
    "            if filter_cls.is_type(text):\n",
    "                # print(\"FOUND\",filter_cls)\n",
    "                found_filter = True\n",
    "                sections = filter_cls().split(text)\n",
    "                break\n",
    "        except RecursionError as e:\n",
    "            print(\"{} RECURSION ERROR {}\".format(idx, str(e)))\n",
    "            return idx, None\n",
    "        except Exception as e:\n",
    "            print(\"{} ERROR {}\".format(idx, str(e)))\n",
    "            # print(traceback.format_exc())\n",
    "            raise e\n",
    "    return idx, sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df.markdown.values.tolist()\n",
    "\n",
    "results = [get_sections(idx, text) for idx, text in enumerate(texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 100 failed 0\n"
     ]
    }
   ],
   "source": [
    "# Failed Count\n",
    "failed_count = sum(1 for _, sections in results if sections is None)\n",
    "print(\"Total {} failed {}\".format(len(results), failed_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " [ArxivPaperSection(header='h2', title='I Introduction', text='The next\\\\-generation manufacturing is expected to have a higher level of automation and involve less human power. Intelligent robots are needed to actively learn skills instead of being programmed by experts explicitly \\\\[1, 2]. Reinforcement learning (RL) is a powerful approach that enables robots to automatically learn an ideal manipulation policy via trial and error. A typical application of RL is robot motion planning which requires the robot to move from an initial position to a goal position without colliding with the obstacles in the environment \\\\[3]. As illustrated in Fig. 1, motion planning is an essential problem for more complicated tasks such as grasping \\\\[4], assembly \\\\[5], and manipulation \\\\[6]. The conventional approaches used for robot motion planning include optimization\\\\-based methods, such as trajectory optimization \\\\[7] and sequential convex optimization \\\\[8], and the sampling\\\\-based methods, including exploration trees \\\\[9], probabilistic roadmaps \\\\[10], and model predictive control \\\\[11] which highly depend on the precise model of the environment. More recent methods, such as stochastic optimization\\\\-based planning (STOMP) \\\\[12], attempt to combine the advantages of both types of approaches. A review of the conventional robot motion planning methods can be referred to in \\\\[13, 14]. Compared to them, RL does not directly solve an optimization problem based on the precise model of the environment. Instead, an optimal planner, or an RL agent, can be trained automatically during interactions with the environment.\\n\\nA typical case of RL is deep reinforcement learning (DRL) which adopts an end\\\\-to\\\\-end learning scheme using deep neural networks \\\\[15]. It aims at constructing an all\\\\-in\\\\-one planner for highly\\\\-coupled and complicated robot tasks, especially the ones that depend on computer vision, such as grasping \\\\[16] and autonomous navigation \\\\[17]. Nevertheless, end\\\\-to\\\\-end learning suffers from the slow convergence rate of the training process and the sensitivity to environmental changes \\\\[2]. This is due to the large scale of the deep neural network and the high likelihood of overfitting. Efforts to resolve these issues include developing high\\\\-fidelity simulation platforms \\\\[18] or designing sim\\\\-to\\\\-real schemes to improve its adaptability and robustness \\\\[19]. Compared to end\\\\-to\\\\-end learning, exploiting heuristics can effectively speed up the training of RL agents and avoid overfitting by splitting a big RL problem into several\\n\\nFigure 1: Motion planning is the essential problem of more complicated robotic tasks, e.g. grasping and manipulation.\\nsmaller learning problems. Effective heuristic methods for RL include feature extraction layers \\\\[20], modularized neural networks \\\\[21], and hierarchical structures \\\\[22]. In \\\\[23, 24], experimental studies are used to address that hierarchical\\\\-RL ensures faster convergence and better robustness than end\\\\-to\\\\-end learning. Besides, transfer learning facilitated by heuristic mappings is used to transfer a simple\\\\-task policy to a complicated task without additional training \\\\[25]. Heuristic models, such as motion primitives \\\\[26, 27] are also widely used to simplify an RL problem by transforming a complex decision\\\\-making problem into a simpler domain.\\n\\nBehavior cloning (BC) from demonstration is another technology to promote the training performance of RL agents. It is used by imitation learning (IL) and programming by demonstration (PbD) to learn human\\\\-like policies for robot manipulation tasks \\\\[28, 29]. The main idea of BC is to duplicate the human policy encoded in the demonstration data to a robot motion planner by supervised learning methods \\\\[30]. For RL, a common approach is to use BC to obtain a decent human\\\\-like policy which then serves as the initial policy for the agent training \\\\[31, 32, 33]. The human\\\\-like policy is not necessarily to be the optimal one but is at least a decent policy for robot motion generation. This method, however, renders complete separation between BC and agent training, such that BC is not helpful in improving the performance of RL. A recent study proposed a novel method to integrate BC into the training process of an RL agent, which greatly improves the convergence speed \\\\[34]. However, the demonstration used for BC is generated by a PID controller in a simulation environment, instead of real human data. Also, BC is still performed in a *explicit* manner which directly penalizes the deviation between the cloned and the demonstration actions. This may lead to the overfitting of the demonstration policy. Recent work tries to solve this problem by proposing an *implicit* BC (IBC) method that performs BC by penalizing a certain energy function of the cloned policy, such that the cloned policy is less sensitive to the action deviations \\\\[35].\\n\\n*explicit**implicit*We believe that proper usage of both heuristics and human demonstration can leverage the training speed and the generalizability of RL agents. Out of this motivation, we propose a novel RL method for robot motion planning facilitated by DMP and IBC, which has not been investigated by existing work, to our best knowledge. The efficacy of the proposed method and its advantages over conventional RL agents are validated using simulation studies. We also use an experimental study to demonstrate its applicability to practical robotic tasks. Our detailed contributions are summarized as follows.\\n\\n* Firstly, we created a dataset of human demonstrations in a point\\\\-to\\\\-point reaching task. The dataset is published online and can be openly used for studies on demonstration\\\\-facilitated methods for robot motion planning.\\n* Secondly, we propose the framework for developing an IBC\\\\-DMP RL agent for the motion planning of a robot manipulator based on Multi\\\\-degree\\\\-of\\\\-freedom (DoF) DMP and IBC. The details of the framework are given. This framework provides a novel perspective for improving the learning performance and generalizability of RL agents using demonstrations and heuristic models.\\n* Thirdly, we present a series of technical points that are important to ensuring decent training performance of an IBC\\\\-DMP RL agent, including DMP dimension extension, human action generation, IBC loss computation, and critical loss refinement. They can also inspire other RL\\\\-based methods with heuristic models and demonstrations.\\n* Besides the simulation studies to validate the efficacy of the IBC\\\\-DMP RL method, we also conduct an experimental study to demonstrate how this method can be used to solve practical robotic problems.\\n\\n- Firstly, we created a dataset of human demonstrations in a point\\\\-to\\\\-point reaching task. The dataset is published online and can be openly used for studies on demonstration\\\\-facilitated methods for robot motion planning.\\n- Secondly, we propose the framework for developing an IBC\\\\-DMP RL agent for the motion planning of a robot manipulator based on Multi\\\\-degree\\\\-of\\\\-freedom (DoF) DMP and IBC. The details of the framework are given. This framework provides a novel perspective for improving the learning performance and generalizability of RL agents using demonstrations and heuristic models.\\n- Thirdly, we present a series of technical points that are important to ensuring decent training performance of an IBC\\\\-DMP RL agent, including DMP dimension extension, human action generation, IBC loss computation, and critical loss refinement. They can also inspire other RL\\\\-based methods with heuristic models and demonstrations.\\n- Besides the simulation studies to validate the efficacy of the IBC\\\\-DMP RL method, we also conduct an experimental study to demonstrate how this method can be used to solve practical robotic problems.\\nThe rest part of the paper is organized as follows. Sec. II introduces the related work and the preliminary knowledge of the main technologies used in this paper. Sec. III interprets the framework of the proposed method and formulates the problem. Sec. IV introduces how we generate human demonstrations from a hand trajectory recording experiment. Sec. V presents the important technical details of the IBC\\\\-DMP agent. Sec. VI and Sec. VII present our simulation and experiment studies, respectively. Finally, Sec. IX concludes the paper.\\n\\n*Notation*: In this paper, we use (\\\\\\\\mathbb{R}), (\\\\\\\\mathbb{R}*{\\\\\\\\geq 0}), and (\\\\\\\\mathbb{R}^{\\\\+}) to represent the sets of real numbers, non\\\\-negative numbers, and positive numbers, respectively. Also, we use (\\\\\\\\mathbb{N}), (\\\\\\\\mathbb{N}*{\\\\\\\\geq 0}), and (\\\\\\\\mathbb{N}^{\\\\+}) to represent the sets of real integers, non\\\\-negative integers, and positive integers, respectively.\\n\\n*Notation**{\\\\\\\\geq 0}), and (\\\\\\\\mathbb{R}^{\\\\+}) to represent the sets of real numbers, non\\\\-negative numbers, and positive numbers, respectively. Also, we use (\\\\\\\\mathbb{N}), (\\\\\\\\mathbb{N}*', children=[ArxivPaperSection(header='p', title='', text='The next\\\\-generation manufacturing is expected to have a higher level of automation and involve less human power. Intelligent robots are needed to actively learn skills instead of being programmed by experts explicitly \\\\[1, 2]. Reinforcement learning (RL) is a powerful approach that enables robots to automatically learn an ideal manipulation policy via trial and error. A typical application of RL is robot motion planning which requires the robot to move from an initial position to a goal position without colliding with the obstacles in the environment \\\\[3]. As illustrated in Fig. 1, motion planning is an essential problem for more complicated tasks such as grasping \\\\[4], assembly \\\\[5], and manipulation \\\\[6]. The conventional approaches used for robot motion planning include optimization\\\\-based methods, such as trajectory optimization \\\\[7] and sequential convex optimization \\\\[8], and the sampling\\\\-based methods, including exploration trees \\\\[9], probabilistic roadmaps \\\\[10], and model predictive control \\\\[11] which highly depend on the precise model of the environment. More recent methods, such as stochastic optimization\\\\-based planning (STOMP) \\\\[12], attempt to combine the advantages of both types of approaches. A review of the conventional robot motion planning methods can be referred to in \\\\[13, 14]. Compared to them, RL does not directly solve an optimization problem based on the precise model of the environment. Instead, an optimal planner, or an RL agent, can be trained automatically during interactions with the environment.  \\nA typical case of RL is deep reinforcement learning (DRL) which adopts an end\\\\-to\\\\-end learning scheme using deep neural networks \\\\[15]. It aims at constructing an all\\\\-in\\\\-one planner for highly\\\\-coupled and complicated robot tasks, especially the ones that depend on computer vision, such as grasping \\\\[16] and autonomous navigation \\\\[17]. Nevertheless, end\\\\-to\\\\-end learning suffers from the slow convergence rate of the training process and the sensitivity to environmental changes \\\\[2]. This is due to the large scale of the deep neural network and the high likelihood of overfitting. Efforts to resolve these issues include developing high\\\\-fidelity simulation platforms \\\\[18] or designing sim\\\\-to\\\\-real schemes to improve its adaptability and robustness \\\\[19]. Compared to end\\\\-to\\\\-end learning, exploiting heuristics can effectively speed up the training of RL agents and avoid overfitting by splitting a big RL problem into several  \\nFigure 1: Motion planning is the essential problem of more complicated robotic tasks, e.g. grasping and manipulation.\\nsmaller learning problems. Effective heuristic methods for RL include feature extraction layers \\\\[20], modularized neural networks \\\\[21], and hierarchical structures \\\\[22]. In \\\\[23, 24], experimental studies are used to address that hierarchical\\\\-RL ensures faster convergence and better robustness than end\\\\-to\\\\-end learning. Besides, transfer learning facilitated by heuristic mappings is used to transfer a simple\\\\-task policy to a complicated task without additional training \\\\[25]. Heuristic models, such as motion primitives \\\\[26, 27] are also widely used to simplify an RL problem by transforming a complex decision\\\\-making problem into a simpler domain.  \\nBehavior cloning (BC) from demonstration is another technology to promote the training performance of RL agents. It is used by imitation learning (IL) and programming by demonstration (PbD) to learn human\\\\-like policies for robot manipulation tasks \\\\[28, 29]. The main idea of BC is to duplicate the human policy encoded in the demonstration data to a robot motion planner by supervised learning methods \\\\[30]. For RL, a common approach is to use BC to obtain a decent human\\\\-like policy which then serves as the initial policy for the agent training \\\\[31, 32, 33]. The human\\\\-like policy is not necessarily to be the optimal one but is at least a decent policy for robot motion generation. This method, however, renders complete separation between BC and agent training, such that BC is not helpful in improving the performance of RL. A recent study proposed a novel method to integrate BC into the training process of an RL agent, which greatly improves the convergence speed \\\\[34]. However, the demonstration used for BC is generated by a PID controller in a simulation environment, instead of real human data. Also, BC is still performed in a *explicit* manner which directly penalizes the deviation between the cloned and the demonstration actions. This may lead to the overfitting of the demonstration policy. Recent work tries to solve this problem by proposing an *implicit* BC (IBC) method that performs BC by penalizing a certain energy function of the cloned policy, such that the cloned policy is less sensitive to the action deviations \\\\[35].  \\n*explicit**implicit*We believe that proper usage of both heuristics and human demonstration can leverage the training speed and the generalizability of RL agents. Out of this motivation, we propose a novel RL method for robot motion planning facilitated by DMP and IBC, which has not been investigated by existing work, to our best knowledge. The efficacy of the proposed method and its advantages over conventional RL agents are validated using simulation studies. We also use an experimental study to demonstrate its applicability to practical robotic tasks. Our detailed contributions are summarized as follows.  \\n* Firstly, we created a dataset of human demonstrations in a point\\\\-to\\\\-point reaching task. The dataset is published online and can be openly used for studies on demonstration\\\\-facilitated methods for robot motion planning.\\n* Secondly, we propose the framework for developing an IBC\\\\-DMP RL agent for the motion planning of a robot manipulator based on Multi\\\\-degree\\\\-of\\\\-freedom (DoF) DMP and IBC. The details of the framework are given. This framework provides a novel perspective for improving the learning performance and generalizability of RL agents using demonstrations and heuristic models.\\n* Thirdly, we present a series of technical points that are important to ensuring decent training performance of an IBC\\\\-DMP RL agent, including DMP dimension extension, human action generation, IBC loss computation, and critical loss refinement. They can also inspire other RL\\\\-based methods with heuristic models and demonstrations.\\n* Besides the simulation studies to validate the efficacy of the IBC\\\\-DMP RL method, we also conduct an experimental study to demonstrate how this method can be used to solve practical robotic problems.  \\n- Firstly, we created a dataset of human demonstrations in a point\\\\-to\\\\-point reaching task. The dataset is published online and can be openly used for studies on demonstration\\\\-facilitated methods for robot motion planning.\\n- Secondly, we propose the framework for developing an IBC\\\\-DMP RL agent for the motion planning of a robot manipulator based on Multi\\\\-degree\\\\-of\\\\-freedom (DoF) DMP and IBC. The details of the framework are given. This framework provides a novel perspective for improving the learning performance and generalizability of RL agents using demonstrations and heuristic models.\\n- Thirdly, we present a series of technical points that are important to ensuring decent training performance of an IBC\\\\-DMP RL agent, including DMP dimension extension, human action generation, IBC loss computation, and critical loss refinement. They can also inspire other RL\\\\-based methods with heuristic models and demonstrations.\\n- Besides the simulation studies to validate the efficacy of the IBC\\\\-DMP RL method, we also conduct an experimental study to demonstrate how this method can be used to solve practical robotic problems.\\nThe rest part of the paper is organized as follows. Sec. II introduces the related work and the preliminary knowledge of the main technologies used in this paper. Sec. III interprets the framework of the proposed method and formulates the problem. Sec. IV introduces how we generate human demonstrations from a hand trajectory recording experiment. Sec. V presents the important technical details of the IBC\\\\-DMP agent. Sec. VI and Sec. VII present our simulation and experiment studies, respectively. Finally, Sec. IX concludes the paper.  \\n*Notation*: In this paper, we use (\\\\\\\\mathbb{R}), (\\\\\\\\mathbb{R}*{\\\\\\\\geq 0}), and (\\\\\\\\mathbb{R}^{\\\\+}) to represent the sets of real numbers, non\\\\-negative numbers, and positive numbers, respectively. Also, we use (\\\\\\\\mathbb{N}), (\\\\\\\\mathbb{N}*{\\\\\\\\geq 0}), and (\\\\\\\\mathbb{N}^{\\\\+}) to represent the sets of real integers, non\\\\-negative integers, and positive integers, respectively.  \\n*Notation**{\\\\\\\\geq 0}), and (\\\\\\\\mathbb{R}^{\\\\+}) to represent the sets of real numbers, non\\\\-negative numbers, and positive numbers, respectively. Also, we use (\\\\\\\\mathbb{N}), (\\\\\\\\mathbb{N}*', children=[])]),\n",
       "  ArxivPaperSection(header='h2', title='II Related Work and Preliminaries', text='In this section, we introduce the related work and the preliminary knowledge of the three important technologies used in this paper, namely DMP, off\\\\-policy RL, and BC.\\n\\n### *Dynamic Movement Primitive (DMP)*\\n\\n*Dynamic Movement Primitive (DMP)*A motion primitive is a heuristic model commonly used for robot motion generation. A special motion primitive is the dynamic movement primitive (DMP) designed for robot manipulators. The mathematical formulation of DMP is a virtual second\\\\-order linear dynamic model. The state of the DMP model is the position and velocity of the robot trajectory to be generated. Its input is an actuation function to be learned. The trajectories generated by DMP are ensured with inherent smoothness and stability due to the continuity of second\\\\-order dynamic models. DMP was originally proposed as an abstract model to describe human motions \\\\[36]. Then, it is applied to IL for the learning of human movement patterns \\\\[37] since DMP provides an elegant and simple model to depict human motions. When human demonstrations are not available, DMP can be solved using RL with a predefined cost function \\\\[38]. It is very effective to simplify a complicated robot motion planning problem via hierarchical RL \\\\[39]. DMP\\\\-facilitated RL has become a popular method for robot motion planning in recent work \\\\[40, 41, 42]. A survey on the application of DMP to robot manipulation problems is presented in \\\\[43]. Nevertheless, DMP has been mainly solved using on\\\\-policy RL methods, which makes it difficult to promote the current policy using demonstration data. Using off\\\\-policy methods to train a DMP model is still an unsolved problem.\\nDMP is a virtual dynamic model used to generate desired trajectories for a moving point \\\\[39, 42], defined as follows,\\n\\n\\\\[\\\\\\\\tau\\\\\\\\ddot{x}*{t}\\\\= \\\\\\\\alpha(\\\\\\\\beta(x*{\\\\\\\\text{g}}\\\\-x\\\\_{t})\\\\-\\\\\\\\dot{x}*{t})\\\\+\\\\\\\\zeta*{t}(x\\\\_{\\\\\\\\text{g }}\\\\-x\\\\_{0})f(\\\\\\\\zeta\\\\_{t}), \\\\\\\\tag{1a}] \\\\[\\\\\\\\tau\\\\\\\\dot{\\\\\\\\zeta}*{t}^{\\\\\\\\prime}\\\\= \\\\-\\\\\\\\omega\\\\\\\\zeta*{t}, \\\\\\\\tag{1b}]\\n\\n*{t}\\\\= \\\\\\\\alpha(\\\\\\\\beta(x**{t})\\\\+\\\\\\\\zeta**{t}^{\\\\\\\\prime}\\\\= \\\\-\\\\\\\\omega\\\\\\\\zeta*where (x\\\\_{t},\\\\\\\\dot{x}*{t},\\\\\\\\ddot{x}*{t}\\\\\\\\in\\\\\\\\mathbb{R}) are, respectively, the time\\\\-dependent position, velocity, and acceleration of the desired trajectory to be generated, (x\\\\_{0},x\\\\_{\\\\\\\\text{g}}\\\\\\\\in\\\\\\\\mathbb{R}) are the initial and the goal positions of the desired trajectory, (\\\\\\\\zeta\\\\_{t}\\\\\\\\in\\\\\\\\mathbb{R}) is a canonical dynamic variable with a non\\\\-zero initial value (\\\\\\\\zeta\\\\_{0}\\\\\\\\in\\\\\\\\mathbb{R}), (\\\\\\\\tau\\\\\\\\in\\\\\\\\mathbb{R}^{\\\\+}) is a constant temporal scalar that depicts the inertia of DMP, (\\\\\\\\alpha,\\\\\\\\beta\\\\\\\\in\\\\\\\\mathbb{R}^{\\\\+}) are constant parameters that determine the damping and stiffness of the DMP, (\\\\\\\\omega\\\\\\\\in\\\\\\\\mathbb{R}) is a constant parameter that controls the duration of the DMP, and (f:\\\\\\\\mathbb{R}\\\\\\\\rightarrow\\\\\\\\mathbb{R}) is the actuation function that determines the shape of the generated trajectory. With a proper actuation function (f), the DMP model (1\\\\) generates a trajectory (x\\\\_{t}) which has the continuous position, velocity, and acceleration with respect to time. Therefore, it guarantees smoothness when applied to robot motion planning. The actuation function (f) is solved by minimizing a certain cost. Conventionally, DMP approximates the optimal actuation function using radius\\\\-basis\\\\-function (RBF) neural networks which can be solved using on\\\\-policy RL methods \\\\[39]. DMP is usually used to directly generate the desired trajectory for a robot end\\\\-effector in the Cartesian space, where an inverse\\\\-kinematic (IK) algorithm is needed to map the trajectory to the joint space \\\\[27].\\n\\n*{t},\\\\\\\\ddot{x}*### *Off\\\\-policy Reinforcement Learning*\\n\\n*Off\\\\-policy Reinforcement Learning*RL methods can be categorized into on\\\\-policy and off\\\\-policy approaches depending on whether the policy to be promoted is the one that is interacting with the environment. Both types of approaches are widely used for robot motion planning \\\\[44, 3, 45]. A typical on\\\\-policy method is proximal policy optimization (PPO) which updates the policy using the policy gradient calculated from the perturbed system trajectories \\\\[46]. On\\\\-policy methods typically promote policies only after complete episodes. Also, historical policies are no longer used for policy promotion. On the contrary, the policy update of off\\\\-policy RL methods, such as deep deterministic policy gradient (DDPG), can be performed at any time. Off\\\\-policy methods can also utilize the historical data stored in the experience replay buffer to promote the current policy \\\\[47]. This means that off\\\\-policy methods can learn from multiple policies encoded in the historical data, which is its main advantage over on\\\\-policy methods. Nevertheless, the main shortcomings of off\\\\-policy methods are long training times and unstable training behaviors due to the bootstrapping effect in the initial stage of the training process. In fact, the training performance of an off\\\\-policy agent is highly dependent on the quality of the experience data. In the application of robot motion planning, it is commonly witnessed that on\\\\-policy methods outperform off\\\\-policy methods when the quality of the experience data is not sufficiently good \\\\[48, 49]. Therefore, many efforts are devoted to improving the performance of off\\\\-policy agents by refining the experience data \\\\[50, 51].\\n\\nMarkov Decision Process (MDP) is the essential mathematic model of RL. An MDP is defined as a tuple (\\\\\\\\mathcal{M}\\\\=(\\\\\\\\mathcal{S},\\\\\\\\mathcal{A},\\\\\\\\mathscr{F},\\\\\\\\mathscr{R})), where (\\\\\\\\mathcal{S}) and (\\\\\\\\mathcal{A}) are the state space and the action space of the agent, respectively, and (\\\\\\\\mathscr{F}:\\\\\\\\mathcal{S}\\\\\\\\times\\\\\\\\mathcal{A}\\\\\\\\rightarrow\\\\\\\\mathcal{S}) and (\\\\\\\\mathscr{R}:\\\\\\\\mathcal{S}\\\\\\\\times\\\\\\\\mathcal{A}\\\\\\\\rightarrow\\\\\\\\mathbb{R}) are the state transition and the reward function of the agent. RL is dedicated to solving the optimal policy (\\\\\\\\pi:\\\\\\\\mathcal{S}\\\\\\\\rightarrow\\\\\\\\mathcal{A}) subject to the following optimization problem,\\n\\n\\\\[\\\\\\\\pi^{\\\\*}\\\\=\\\\\\\\arg\\\\\\\\max\\\\_{\\\\\\\\pi}\\\\\\\\sum\\\\_{t\\\\=0}^{T}\\\\\\\\gamma^{t}\\\\\\\\mathscr{R}(s\\\\_{t},\\\\\\\\pi(s\\\\_{t}))\\\\\\\\,, \\\\\\\\tag{2}]\\n\\nwhere (T\\\\\\\\in\\\\\\\\mathbb{N}^{\\\\+}) is the length of a trajectory (s\\\\_{0}s\\\\_{1}\\\\\\\\cdots s\\\\_{T}), (0\\\\<\\\\\\\\gamma\\\\<1\\\\) is a discount factor, and (s\\\\_{t}\\\\\\\\in\\\\\\\\mathcal{S}) is the state of the MDP at time (t\\\\=0,1,\\\\\\\\cdots,T). The solution (\\\\\\\\pi^{\\\\*}) to (2\\\\) is referred to as the optimal policy.\\n\\nThe optimal policy (\\\\\\\\pi^{*}) can be solved either with an on\\\\-policy method, such as proximal policy optimization (PPO), or an off\\\\-policy approach, like deep deterministic policy gradient (DDPG). In this paper, our main focus is on off\\\\-policy RL and we use DDPG as the baseline model of our approach since it allows for improving the current policy using the experience data. A basic DDPG agent typically consists of two neural networks, namely an actor (\\\\\\\\pi\\\\_{\\\\\\\\theta}) and a critic (Q\\\\_{w}) which are respectively used to approximate the optimal policy (\\\\\\\\pi^{*}) as shown in (2\\\\) and the value function (Q:\\\\\\\\mathcal{S}\\\\\\\\times\\\\\\\\mathcal{A}\\\\\\\\rightarrow\\\\\\\\mathbb{R}), where (\\\\\\\\theta) and (w) denote the parameters of the neural networks. Details about the definition of the value function can be found in \\\\[50]. The main objective of the training process for a DDPG agent is to constantly update the values of the parameters (\\\\\\\\theta) and (w), such that the approximations (\\\\\\\\pi\\\\_{\\\\\\\\theta}\\\\\\\\rightarrow\\\\\\\\pi^{\\\\*}) and (Q\\\\_{w}\\\\\\\\to Q) are as close as possible. Another two neural networks (\\\\\\\\pi\\\\_{\\\\\\\\theta^{\\\\\\\\prime}}:\\\\\\\\mathcal{S}\\\\\\\\rightarrow\\\\\\\\mathcal{A}) and (Q\\\\_{w^{\\\\\\\\prime}}:\\\\\\\\mathcal{S}\\\\\\\\times\\\\\\\\mathcal{A}\\\\\\\\rightarrow\\\\\\\\mathbb{R}) with parameters (\\\\\\\\theta^{\\\\\\\\prime}) and (w^{\\\\\\\\prime}), referred to as *target neural networks* are typically used to smooth out the approximation process. The parameters (\\\\\\\\theta^{\\\\\\\\prime}) and (w^{\\\\\\\\prime}) are iteratively updated following the updates of (\\\\\\\\theta) and (w),\\n\\n*}) can be solved either with an on\\\\-policy method, such as proximal policy optimization (PPO), or an off\\\\-policy approach, like deep deterministic policy gradient (DDPG). In this paper, our main focus is on off\\\\-policy RL and we use DDPG as the baseline model of our approach since it allows for improving the current policy using the experience data. A basic DDPG agent typically consists of two neural networks, namely an actor (\\\\\\\\pi\\\\_{\\\\\\\\theta}) and a critic (Q\\\\_{w}) which are respectively used to approximate the optimal policy (\\\\\\\\pi^{**target neural networks*\\\\[\\\\\\\\theta^{\\\\\\\\prime}\\\\\\\\leftarrow\\\\\\\\lambda\\\\\\\\theta^{\\\\\\\\prime}\\\\+(1\\\\-\\\\\\\\lambda)\\\\\\\\theta,\\\\\\\\,\\\\\\\\,w^{ \\\\\\\\prime}\\\\\\\\leftarrow\\\\\\\\lambda w^{\\\\\\\\prime}\\\\+(1\\\\-\\\\\\\\lambda)w, \\\\\\\\tag{3}]\\n\\nwhere (0\\\\<\\\\\\\\lambda\\\\<1\\\\) is an interpolation factor used to average the updates of the target networks and stabilize the approximation.\\n\\nThe training of the critic network (Q\\\\_{w}) is a supervised learning process. The samples used to train the network are from an *experience replay buffer*(\\\\\\\\mathcal{B}) sized (n\\\\\\\\in\\\\\\\\mathbb{N}^{\\\\+}), which is randomly sampled from an experience replay buffer (\\\\\\\\mathcal{B}). Each sample in the buffer (\\\\\\\\mathcal{B}) is organized as the format ({\\\\\\\\,s\\\\_{j},a\\\\_{j},s^{\\\\\\\\prime}*{j},r*{j},d\\\\_{j}}), where (s\\\\_{j}\\\\\\\\in\\\\\\\\mathcal{S}) and (a\\\\_{j}\\\\\\\\in\\\\\\\\mathcal{A}) are the state and the action of the agent at a certain history instant (j\\\\=1,2\\\\), (\\\\\\\\cdots), (n), (s^{\\\\\\\\prime}*{j}!\\\\=!\\\\\\\\mathscr{F}(s*{j},a\\\\_{j})) is the *successive state*, (r\\\\_{j}!\\\\=!\\\\\\\\mathscr{R}(s\\\\_{j},a\\\\_{j})) is the corresponding instant reward, and (d\\\\_{j}) is a boolean termination flag that determines whether an episode terminates or not. The critic loss function of the buffer (\\\\\\\\mathcal{B}) is computed as\\n\\n*experience replay buffer**{j},r**{j}!\\\\=!\\\\\\\\mathscr{F}(s**successive state*\\\\[\\\\\\\\mathcal{L}*{C}(\\\\\\\\mathcal{B})\\\\=\\\\\\\\tfrac{1}{n}\\\\\\\\sum*{j\\\\=1}^{n}\\\\\\\\left(l\\\\_{j}\\\\-Q\\\\_{w}(s\\\\_{j}, a\\\\_{j})\\\\\\\\right)^{2} \\\\\\\\tag{4}]\\n\\n*{C}(\\\\\\\\mathcal{B})\\\\=\\\\\\\\tfrac{1}{n}\\\\\\\\sum*where (l\\\\_{j}\\\\=r\\\\_{j}\\\\+\\\\\\\\gamma(1\\\\-d\\\\_{j})Q\\\\_{w^{\\\\\\\\prime}}(s^{\\\\\\\\prime}*{j},\\\\\\\\pi*{\\\\\\\\theta^{\\\\\\\\prime}}(s^{ \\\\\\\\prime}*{j}))) is the label of sample ((s*{j},a\\\\_{j})). The actor network (\\\\\\\\pi\\\\_{\\\\\\\\theta}) is also trained using the buffer (\\\\\\\\mathcal{B}) with the following loss function,\\n\\n*{j},\\\\\\\\pi**{j}))) is the label of sample ((s*\\\\[\\\\\\\\mathcal{L}*{A}(\\\\\\\\mathcal{B})\\\\=\\\\-\\\\\\\\tfrac{1}{n}\\\\\\\\sum*{j\\\\=1}^{n}Q\\\\_{w}(s\\\\_{j},\\\\\\\\pi\\\\_{ \\\\\\\\theta}(s\\\\_{j})). \\\\\\\\tag{5}]\\nGiven the computed losses (\\\\\\\\mathcal{L}*{A}) and (\\\\\\\\mathcal{L}*{C}), the parameters of the actor and the critic networks are updated with the following gradient\\\\-based law, in an iterative and alternative manner,\\n\\n*{A}(\\\\\\\\mathcal{B})\\\\=\\\\-\\\\\\\\tfrac{1}{n}\\\\\\\\sum**{A}) and (\\\\\\\\mathcal{L}*\\\\[\\\\\\\\Delta\\\\\\\\theta\\\\=\\\\-\\\\\\\\alpha\\\\_{\\\\\\\\theta}\\\\\\\\nabla\\\\_{\\\\\\\\theta}\\\\\\\\mathcal{L}*{A},\\\\\\\\ \\\\\\\\Delta w\\\\=\\\\-\\\\\\\\alpha*{w} \\\\\\\\nabla\\\\_{w}\\\\\\\\mathcal{L}\\\\_{C}, \\\\\\\\tag{6}]\\n\\n*{A},\\\\\\\\ \\\\\\\\Delta w\\\\=\\\\-\\\\\\\\alpha*where (\\\\\\\\Delta\\\\\\\\theta) and (\\\\\\\\Delta w) are the parameter increments at each iteration, (\\\\\\\\alpha\\\\_{\\\\\\\\theta},\\\\\\\\alpha\\\\_{w}\\\\\\\\in\\\\\\\\mathbb{R}^{\\\\+}) are the learning rates of the actor and the critic networks, and (\\\\\\\\nabla\\\\_{\\\\\\\\theta}\\\\\\\\mathcal{L}*{A}) and (\\\\\\\\nabla*{w}\\\\\\\\mathcal{L}\\\\_{C}) are the gradients of the loss functions to the network parameters.\\n\\n*{A}) and (\\\\\\\\nabla*### *Behavior Cloning (BC)*\\n\\n*Behavior Cloning (BC)*BC is an important technology of IL to duplicate human policy from demonstrations. It has been widely applied to robot motion planning and autonomous driving \\\\[30, 52]. Here, behavior refers to what *actions* humans tend to take under certain *states*. Cloning means learning a new policy to fit human behaviors. BC formulates a supervised learning problem where human actions serve as the ground truth labels of the states. Then, the policy to be cloned can be trained using deep neural networks (DNN) \\\\[53] or Gaussian mixture models (GMM) \\\\[54]. When human demonstrations are recorded as movement trajectories, states refer to the position and velocity of human trajectories at a certain time and action is the corresponding acceleration \\\\[28]. DMP has also been used to simplify the BC process \\\\[55]. Nevertheless, calculating acceleration from human demonstrations requires the second\\\\-order derivative operation which brings up differential noise to the human demonstration samples. Additional procedures such as locally weighted regression (LWR) are often used to mitigate the effects of the noise \\\\[56].\\n\\n*actions**states*Another issue of BC is that the pre\\\\-trained policy might overfit human behaviors. Note that human likeness does not necessarily indicate the best planning policy. Thus, overfitting human behaviors may degrade the performance of robot motion planning with respect to the predefined reward. It is addressed that BC performs worse than nominal RL agents in many cases due to overfitting \\\\[57]. This motivates us to combine BC and nominal RL to develop a better robot motion planner, instead of directly using BC to generate human\\\\-like trajectories. The conventional BC\\\\-facilitated RL methods usually perform a fully separate scheme, i.e., to learn an initial policy with BC before training the agent using nominal RL methods \\\\[31, 32, 33]. Nevertheless, in such a separate scheme, BC is not fully exploited to promote the training of the RL agent. It also lacks the flexibility of achieving a balance between human likeness and the predefined reward. Recent studies made some attempts to integrate BC into the off\\\\-policy RL training using a dual\\\\-buffer structure \\\\[34, 58], which inspires us to use BC from human demonstration to leverage the training performance of RL.\\n\\nBC refers to the process of training a target policy from a source policy encoded in a demonstration. Given a set of demonstrations that are stored in a buffer (\\\\\\\\mathcal{B}) as defined in Sec. II\\\\-B, the source pattern refers to the policy (\\\\\\\\tilde{\\\\\\\\pi}) that fits the data ((s\\\\_{j},a\\\\_{j})) for all (j\\\\=1,2,\\\\\\\\cdots,n). The objective of BC is to train a parameterized policy (\\\\\\\\pi\\\\_{\\\\\\\\theta}) to approximate (\\\\\\\\tilde{\\\\\\\\pi}). Conventionally, BC renders a supervised learning problem, where the parameter (\\\\\\\\theta) is updated via a gradient\\\\-based law (\\\\\\\\Delta\\\\\\\\theta\\\\=\\\\-\\\\\\\\alpha\\\\_{\\\\\\\\theta}\\\\\\\\nabla\\\\_{\\\\\\\\theta}\\\\\\\\mathcal{L}*{I}), where (\\\\\\\\Delta\\\\\\\\theta) is the parameter increment, (\\\\\\\\alpha*{\\\\\\\\theta}\\\\\\\\in\\\\\\\\mathbb{R}^{\\\\+}) is the learning rate, and (\\\\\\\\nabla\\\\_{\\\\\\\\theta}\\\\\\\\mathcal{L}*{I}) is the gradient of a loss function (\\\\\\\\mathcal{L}*{I}) defined as\\n\\n*{I}), where (\\\\\\\\Delta\\\\\\\\theta) is the parameter increment, (\\\\\\\\alpha**{I}) is the gradient of a loss function (\\\\\\\\mathcal{L}*\\\\[\\\\\\\\mathcal{L}*{I}(\\\\\\\\mathcal{B})\\\\=\\\\\\\\sum*{j\\\\=1}^{n}\\\\\\\\mathcal{M}^{2}(a\\\\_{j}\\\\-\\\\\\\\pi\\\\_{\\\\\\\\theta} (s\\\\_{j})) \\\\\\\\tag{7}]\\n\\n*{I}(\\\\\\\\mathcal{B})\\\\=\\\\\\\\sum*where (\\\\\\\\mathcal{M}:\\\\\\\\mathcal{A}!\\\\\\\\rightarrow!\\\\\\\\mathbb{R}*{\\\\\\\\geq 0}) is a non\\\\-negative function that evaluates the deviation between the two policies, which is commonly the 2\\\\-norm of vectors. Such an approach is also referred to as \\\\_explicit BC (EBC)* since the loss function (7\\\\) explicitly penalizes the deviation between actions of the source and the target policies. EBC is very likely to cause overfitting since it is obsessed with the fitting of two policies. It may not be successful when the demonstration data is noisy and subject to large variance. Recent study proposes an *implicit BC (IBC)* method which suggests the following loss function \\\\[35],\\n\\n*{\\\\\\\\geq 0}) is a non\\\\-negative function that evaluates the deviation between the two policies, which is commonly the 2\\\\-norm of vectors. Such an approach is also referred to as \\\\_explicit BC (EBC)**implicit BC (IBC)*\\\\[\\\\\\\\mathcal{L}*{I}(\\\\\\\\mathcal{B})\\\\=\\\\\\\\sum*{j\\\\=1}^{n}E\\\\_{\\\\\\\\theta}(s\\\\_{j},a\\\\_{j}), \\\\\\\\tag{8}]\\n\\n*{I}(\\\\\\\\mathcal{B})\\\\=\\\\\\\\sum*where (E\\\\_{\\\\\\\\theta}:\\\\\\\\mathcal{S}!\\\\\\\\times!\\\\\\\\mathcal{A}!\\\\\\\\rightarrow!\\\\\\\\mathbb{R}*{\\\\\\\\geq 0}) is a non\\\\-negative energy function of demo data ((s*{j},a\\\\_{j})) parameterized by (\\\\\\\\theta). Thus, IBC generates the target policy through an implicit energy function, which can effectively avoid the overfitting problem. It also provides a flexible interface to combine BC with an off\\\\-policy RL agent. A technical question to be answered in this paper is how to design a proper energy function to facilitate RL with IBC.\\n\\n*{\\\\\\\\geq 0}) is a non\\\\-negative energy function of demo data ((s*', children=[ArxivPaperSection(header='p', title='', text='In this section, we introduce the related work and the preliminary knowledge of the three important technologies used in this paper, namely DMP, off\\\\-policy RL, and BC.', children=[]), ArxivPaperSection(header='h3', title='*Dynamic Movement Primitive (DMP)*', text='*Dynamic Movement Primitive (DMP)*A motion primitive is a heuristic model commonly used for robot motion generation. A special motion primitive is the dynamic movement primitive (DMP) designed for robot manipulators. The mathematical formulation of DMP is a virtual second\\\\-order linear dynamic model. The state of the DMP model is the position and velocity of the robot trajectory to be generated. Its input is an actuation function to be learned. The trajectories generated by DMP are ensured with inherent smoothness and stability due to the continuity of second\\\\-order dynamic models. DMP was originally proposed as an abstract model to describe human motions \\\\[36]. Then, it is applied to IL for the learning of human movement patterns \\\\[37] since DMP provides an elegant and simple model to depict human motions. When human demonstrations are not available, DMP can be solved using RL with a predefined cost function \\\\[38]. It is very effective to simplify a complicated robot motion planning problem via hierarchical RL \\\\[39]. DMP\\\\-facilitated RL has become a popular method for robot motion planning in recent work \\\\[40, 41, 42]. A survey on the application of DMP to robot manipulation problems is presented in \\\\[43]. Nevertheless, DMP has been mainly solved using on\\\\-policy RL methods, which makes it difficult to promote the current policy using demonstration data. Using off\\\\-policy methods to train a DMP model is still an unsolved problem.\\nDMP is a virtual dynamic model used to generate desired trajectories for a moving point \\\\[39, 42], defined as follows,  \\n\\\\[\\\\\\\\tau\\\\\\\\ddot{x}*{t}\\\\= \\\\\\\\alpha(\\\\\\\\beta(x*{\\\\\\\\text{g}}\\\\-x\\\\_{t})\\\\-\\\\\\\\dot{x}*{t})\\\\+\\\\\\\\zeta*{t}(x\\\\_{\\\\\\\\text{g }}\\\\-x\\\\_{0})f(\\\\\\\\zeta\\\\_{t}), \\\\\\\\tag{1a}] \\\\[\\\\\\\\tau\\\\\\\\dot{\\\\\\\\zeta}*{t}^{\\\\\\\\prime}\\\\= \\\\-\\\\\\\\omega\\\\\\\\zeta*{t}, \\\\\\\\tag{1b}]  \\n*{t}\\\\= \\\\\\\\alpha(\\\\\\\\beta(x**{t})\\\\+\\\\\\\\zeta**{t}^{\\\\\\\\prime}\\\\= \\\\-\\\\\\\\omega\\\\\\\\zeta*where (x\\\\_{t},\\\\\\\\dot{x}*{t},\\\\\\\\ddot{x}*{t}\\\\\\\\in\\\\\\\\mathbb{R}) are, respectively, the time\\\\-dependent position, velocity, and acceleration of the desired trajectory to be generated, (x\\\\_{0},x\\\\_{\\\\\\\\text{g}}\\\\\\\\in\\\\\\\\mathbb{R}) are the initial and the goal positions of the desired trajectory, (\\\\\\\\zeta\\\\_{t}\\\\\\\\in\\\\\\\\mathbb{R}) is a canonical dynamic variable with a non\\\\-zero initial value (\\\\\\\\zeta\\\\_{0}\\\\\\\\in\\\\\\\\mathbb{R}), (\\\\\\\\tau\\\\\\\\in\\\\\\\\mathbb{R}^{\\\\+}) is a constant temporal scalar that depicts the inertia of DMP, (\\\\\\\\alpha,\\\\\\\\beta\\\\\\\\in\\\\\\\\mathbb{R}^{\\\\+}) are constant parameters that determine the damping and stiffness of the DMP, (\\\\\\\\omega\\\\\\\\in\\\\\\\\mathbb{R}) is a constant parameter that controls the duration of the DMP, and (f:\\\\\\\\mathbb{R}\\\\\\\\rightarrow\\\\\\\\mathbb{R}) is the actuation function that determines the shape of the generated trajectory. With a proper actuation function (f), the DMP model (1\\\\) generates a trajectory (x\\\\_{t}) which has the continuous position, velocity, and acceleration with respect to time. Therefore, it guarantees smoothness when applied to robot motion planning. The actuation function (f) is solved by minimizing a certain cost. Conventionally, DMP approximates the optimal actuation function using radius\\\\-basis\\\\-function (RBF) neural networks which can be solved using on\\\\-policy RL methods \\\\[39]. DMP is usually used to directly generate the desired trajectory for a robot end\\\\-effector in the Cartesian space, where an inverse\\\\-kinematic (IK) algorithm is needed to map the trajectory to the joint space \\\\[27].  \\n*{t},\\\\\\\\ddot{x}*### *Off\\\\-policy Reinforcement Learning*  \\n*Off\\\\-policy Reinforcement Learning*RL methods can be categorized into on\\\\-policy and off\\\\-policy approaches depending on whether the policy to be promoted is the one that is interacting with the environment. Both types of approaches are widely used for robot motion planning \\\\[44, 3, 45]. A typical on\\\\-policy method is proximal policy optimization (PPO) which updates the policy using the policy gradient calculated from the perturbed system trajectories \\\\[46]. On\\\\-policy methods typically promote policies only after complete episodes. Also, historical policies are no longer used for policy promotion. On the contrary, the policy update of off\\\\-policy RL methods, such as deep deterministic policy gradient (DDPG), can be performed at any time. Off\\\\-policy methods can also utilize the historical data stored in the experience replay buffer to promote the current policy \\\\[47]. This means that off\\\\-policy methods can learn from multiple policies encoded in the historical data, which is its main advantage over on\\\\-policy methods. Nevertheless, the main shortcomings of off\\\\-policy methods are long training times and unstable training behaviors due to the bootstrapping effect in the initial stage of the training process. In fact, the training performance of an off\\\\-policy agent is highly dependent on the quality of the experience data. In the application of robot motion planning, it is commonly witnessed that on\\\\-policy methods outperform off\\\\-policy methods when the quality of the experience data is not sufficiently good \\\\[48, 49]. Therefore, many efforts are devoted to improving the performance of off\\\\-policy agents by refining the experience data \\\\[50, 51].  \\nMarkov Decision Process (MDP) is the essential mathematic model of RL. An MDP is defined as a tuple (\\\\\\\\mathcal{M}\\\\=(\\\\\\\\mathcal{S},\\\\\\\\mathcal{A},\\\\\\\\mathscr{F},\\\\\\\\mathscr{R})), where (\\\\\\\\mathcal{S}) and (\\\\\\\\mathcal{A}) are the state space and the action space of the agent, respectively, and (\\\\\\\\mathscr{F}:\\\\\\\\mathcal{S}\\\\\\\\times\\\\\\\\mathcal{A}\\\\\\\\rightarrow\\\\\\\\mathcal{S}) and (\\\\\\\\mathscr{R}:\\\\\\\\mathcal{S}\\\\\\\\times\\\\\\\\mathcal{A}\\\\\\\\rightarrow\\\\\\\\mathbb{R}) are the state transition and the reward function of the agent. RL is dedicated to solving the optimal policy (\\\\\\\\pi:\\\\\\\\mathcal{S}\\\\\\\\rightarrow\\\\\\\\mathcal{A}) subject to the following optimization problem,  \\n\\\\[\\\\\\\\pi^{\\\\*}\\\\=\\\\\\\\arg\\\\\\\\max\\\\_{\\\\\\\\pi}\\\\\\\\sum\\\\_{t\\\\=0}^{T}\\\\\\\\gamma^{t}\\\\\\\\mathscr{R}(s\\\\_{t},\\\\\\\\pi(s\\\\_{t}))\\\\\\\\,, \\\\\\\\tag{2}]  \\nwhere (T\\\\\\\\in\\\\\\\\mathbb{N}^{\\\\+}) is the length of a trajectory (s\\\\_{0}s\\\\_{1}\\\\\\\\cdots s\\\\_{T}), (0\\\\<\\\\\\\\gamma\\\\<1\\\\) is a discount factor, and (s\\\\_{t}\\\\\\\\in\\\\\\\\mathcal{S}) is the state of the MDP at time (t\\\\=0,1,\\\\\\\\cdots,T). The solution (\\\\\\\\pi^{\\\\*}) to (2\\\\) is referred to as the optimal policy.  \\nThe optimal policy (\\\\\\\\pi^{*}) can be solved either with an on\\\\-policy method, such as proximal policy optimization (PPO), or an off\\\\-policy approach, like deep deterministic policy gradient (DDPG). In this paper, our main focus is on off\\\\-policy RL and we use DDPG as the baseline model of our approach since it allows for improving the current policy using the experience data. A basic DDPG agent typically consists of two neural networks, namely an actor (\\\\\\\\pi\\\\_{\\\\\\\\theta}) and a critic (Q\\\\_{w}) which are respectively used to approximate the optimal policy (\\\\\\\\pi^{*}) as shown in (2\\\\) and the value function (Q:\\\\\\\\mathcal{S}\\\\\\\\times\\\\\\\\mathcal{A}\\\\\\\\rightarrow\\\\\\\\mathbb{R}), where (\\\\\\\\theta) and (w) denote the parameters of the neural networks. Details about the definition of the value function can be found in \\\\[50]. The main objective of the training process for a DDPG agent is to constantly update the values of the parameters (\\\\\\\\theta) and (w), such that the approximations (\\\\\\\\pi\\\\_{\\\\\\\\theta}\\\\\\\\rightarrow\\\\\\\\pi^{\\\\*}) and (Q\\\\_{w}\\\\\\\\to Q) are as close as possible. Another two neural networks (\\\\\\\\pi\\\\_{\\\\\\\\theta^{\\\\\\\\prime}}:\\\\\\\\mathcal{S}\\\\\\\\rightarrow\\\\\\\\mathcal{A}) and (Q\\\\_{w^{\\\\\\\\prime}}:\\\\\\\\mathcal{S}\\\\\\\\times\\\\\\\\mathcal{A}\\\\\\\\rightarrow\\\\\\\\mathbb{R}) with parameters (\\\\\\\\theta^{\\\\\\\\prime}) and (w^{\\\\\\\\prime}), referred to as *target neural networks* are typically used to smooth out the approximation process. The parameters (\\\\\\\\theta^{\\\\\\\\prime}) and (w^{\\\\\\\\prime}) are iteratively updated following the updates of (\\\\\\\\theta) and (w),  \\n*}) can be solved either with an on\\\\-policy method, such as proximal policy optimization (PPO), or an off\\\\-policy approach, like deep deterministic policy gradient (DDPG). In this paper, our main focus is on off\\\\-policy RL and we use DDPG as the baseline model of our approach since it allows for improving the current policy using the experience data. A basic DDPG agent typically consists of two neural networks, namely an actor (\\\\\\\\pi\\\\_{\\\\\\\\theta}) and a critic (Q\\\\_{w}) which are respectively used to approximate the optimal policy (\\\\\\\\pi^{**target neural networks*\\\\[\\\\\\\\theta^{\\\\\\\\prime}\\\\\\\\leftarrow\\\\\\\\lambda\\\\\\\\theta^{\\\\\\\\prime}\\\\+(1\\\\-\\\\\\\\lambda)\\\\\\\\theta,\\\\\\\\,\\\\\\\\,w^{ \\\\\\\\prime}\\\\\\\\leftarrow\\\\\\\\lambda w^{\\\\\\\\prime}\\\\+(1\\\\-\\\\\\\\lambda)w, \\\\\\\\tag{3}]  \\nwhere (0\\\\<\\\\\\\\lambda\\\\<1\\\\) is an interpolation factor used to average the updates of the target networks and stabilize the approximation.  \\nThe training of the critic network (Q\\\\_{w}) is a supervised learning process. The samples used to train the network are from an *experience replay buffer*(\\\\\\\\mathcal{B}) sized (n\\\\\\\\in\\\\\\\\mathbb{N}^{\\\\+}), which is randomly sampled from an experience replay buffer (\\\\\\\\mathcal{B}). Each sample in the buffer (\\\\\\\\mathcal{B}) is organized as the format ({\\\\\\\\,s\\\\_{j},a\\\\_{j},s^{\\\\\\\\prime}*{j},r*{j},d\\\\_{j}}), where (s\\\\_{j}\\\\\\\\in\\\\\\\\mathcal{S}) and (a\\\\_{j}\\\\\\\\in\\\\\\\\mathcal{A}) are the state and the action of the agent at a certain history instant (j\\\\=1,2\\\\), (\\\\\\\\cdots), (n), (s^{\\\\\\\\prime}*{j}!\\\\=!\\\\\\\\mathscr{F}(s*{j},a\\\\_{j})) is the *successive state*, (r\\\\_{j}!\\\\=!\\\\\\\\mathscr{R}(s\\\\_{j},a\\\\_{j})) is the corresponding instant reward, and (d\\\\_{j}) is a boolean termination flag that determines whether an episode terminates or not. The critic loss function of the buffer (\\\\\\\\mathcal{B}) is computed as  \\n*experience replay buffer**{j},r**{j}!\\\\=!\\\\\\\\mathscr{F}(s**successive state*\\\\[\\\\\\\\mathcal{L}*{C}(\\\\\\\\mathcal{B})\\\\=\\\\\\\\tfrac{1}{n}\\\\\\\\sum*{j\\\\=1}^{n}\\\\\\\\left(l\\\\_{j}\\\\-Q\\\\_{w}(s\\\\_{j}, a\\\\_{j})\\\\\\\\right)^{2} \\\\\\\\tag{4}]  \\n*{C}(\\\\\\\\mathcal{B})\\\\=\\\\\\\\tfrac{1}{n}\\\\\\\\sum*where (l\\\\_{j}\\\\=r\\\\_{j}\\\\+\\\\\\\\gamma(1\\\\-d\\\\_{j})Q\\\\_{w^{\\\\\\\\prime}}(s^{\\\\\\\\prime}*{j},\\\\\\\\pi*{\\\\\\\\theta^{\\\\\\\\prime}}(s^{ \\\\\\\\prime}*{j}))) is the label of sample ((s*{j},a\\\\_{j})). The actor network (\\\\\\\\pi\\\\_{\\\\\\\\theta}) is also trained using the buffer (\\\\\\\\mathcal{B}) with the following loss function,  \\n*{j},\\\\\\\\pi**{j}))) is the label of sample ((s*\\\\[\\\\\\\\mathcal{L}*{A}(\\\\\\\\mathcal{B})\\\\=\\\\-\\\\\\\\tfrac{1}{n}\\\\\\\\sum*{j\\\\=1}^{n}Q\\\\_{w}(s\\\\_{j},\\\\\\\\pi\\\\_{ \\\\\\\\theta}(s\\\\_{j})). \\\\\\\\tag{5}]\\nGiven the computed losses (\\\\\\\\mathcal{L}*{A}) and (\\\\\\\\mathcal{L}*{C}), the parameters of the actor and the critic networks are updated with the following gradient\\\\-based law, in an iterative and alternative manner,  \\n*{A}(\\\\\\\\mathcal{B})\\\\=\\\\-\\\\\\\\tfrac{1}{n}\\\\\\\\sum**{A}) and (\\\\\\\\mathcal{L}*\\\\[\\\\\\\\Delta\\\\\\\\theta\\\\=\\\\-\\\\\\\\alpha\\\\_{\\\\\\\\theta}\\\\\\\\nabla\\\\_{\\\\\\\\theta}\\\\\\\\mathcal{L}*{A},\\\\\\\\ \\\\\\\\Delta w\\\\=\\\\-\\\\\\\\alpha*{w} \\\\\\\\nabla\\\\_{w}\\\\\\\\mathcal{L}\\\\_{C}, \\\\\\\\tag{6}]  \\n*{A},\\\\\\\\ \\\\\\\\Delta w\\\\=\\\\-\\\\\\\\alpha*where (\\\\\\\\Delta\\\\\\\\theta) and (\\\\\\\\Delta w) are the parameter increments at each iteration, (\\\\\\\\alpha\\\\_{\\\\\\\\theta},\\\\\\\\alpha\\\\_{w}\\\\\\\\in\\\\\\\\mathbb{R}^{\\\\+}) are the learning rates of the actor and the critic networks, and (\\\\\\\\nabla\\\\_{\\\\\\\\theta}\\\\\\\\mathcal{L}*{A}) and (\\\\\\\\nabla*{w}\\\\\\\\mathcal{L}\\\\_{C}) are the gradients of the loss functions to the network parameters.  \\n*{A}) and (\\\\\\\\nabla*### *Behavior Cloning (BC)*  \\n*Behavior Cloning (BC)*BC is an important technology of IL to duplicate human policy from demonstrations. It has been widely applied to robot motion planning and autonomous driving \\\\[30, 52]. Here, behavior refers to what *actions* humans tend to take under certain *states*. Cloning means learning a new policy to fit human behaviors. BC formulates a supervised learning problem where human actions serve as the ground truth labels of the states. Then, the policy to be cloned can be trained using deep neural networks (DNN) \\\\[53] or Gaussian mixture models (GMM) \\\\[54]. When human demonstrations are recorded as movement trajectories, states refer to the position and velocity of human trajectories at a certain time and action is the corresponding acceleration \\\\[28]. DMP has also been used to simplify the BC process \\\\[55]. Nevertheless, calculating acceleration from human demonstrations requires the second\\\\-order derivative operation which brings up differential noise to the human demonstration samples. Additional procedures such as locally weighted regression (LWR) are often used to mitigate the effects of the noise \\\\[56].  \\n*actions**states*Another issue of BC is that the pre\\\\-trained policy might overfit human behaviors. Note that human likeness does not necessarily indicate the best planning policy. Thus, overfitting human behaviors may degrade the performance of robot motion planning with respect to the predefined reward. It is addressed that BC performs worse than nominal RL agents in many cases due to overfitting \\\\[57]. This motivates us to combine BC and nominal RL to develop a better robot motion planner, instead of directly using BC to generate human\\\\-like trajectories. The conventional BC\\\\-facilitated RL methods usually perform a fully separate scheme, i.e., to learn an initial policy with BC before training the agent using nominal RL methods \\\\[31, 32, 33]. Nevertheless, in such a separate scheme, BC is not fully exploited to promote the training of the RL agent. It also lacks the flexibility of achieving a balance between human likeness and the predefined reward. Recent studies made some attempts to integrate BC into the off\\\\-policy RL training using a dual\\\\-buffer structure \\\\[34, 58], which inspires us to use BC from human demonstration to leverage the training performance of RL.  \\nBC refers to the process of training a target policy from a source policy encoded in a demonstration. Given a set of demonstrations that are stored in a buffer (\\\\\\\\mathcal{B}) as defined in Sec. II\\\\-B, the source pattern refers to the policy (\\\\\\\\tilde{\\\\\\\\pi}) that fits the data ((s\\\\_{j},a\\\\_{j})) for all (j\\\\=1,2,\\\\\\\\cdots,n). The objective of BC is to train a parameterized policy (\\\\\\\\pi\\\\_{\\\\\\\\theta}) to approximate (\\\\\\\\tilde{\\\\\\\\pi}). Conventionally, BC renders a supervised learning problem, where the parameter (\\\\\\\\theta) is updated via a gradient\\\\-based law (\\\\\\\\Delta\\\\\\\\theta\\\\=\\\\-\\\\\\\\alpha\\\\_{\\\\\\\\theta}\\\\\\\\nabla\\\\_{\\\\\\\\theta}\\\\\\\\mathcal{L}*{I}), where (\\\\\\\\Delta\\\\\\\\theta) is the parameter increment, (\\\\\\\\alpha*{\\\\\\\\theta}\\\\\\\\in\\\\\\\\mathbb{R}^{\\\\+}) is the learning rate, and (\\\\\\\\nabla\\\\_{\\\\\\\\theta}\\\\\\\\mathcal{L}*{I}) is the gradient of a loss function (\\\\\\\\mathcal{L}*{I}) defined as  \\n*{I}), where (\\\\\\\\Delta\\\\\\\\theta) is the parameter increment, (\\\\\\\\alpha**{I}) is the gradient of a loss function (\\\\\\\\mathcal{L}*\\\\[\\\\\\\\mathcal{L}*{I}(\\\\\\\\mathcal{B})\\\\=\\\\\\\\sum*{j\\\\=1}^{n}\\\\\\\\mathcal{M}^{2}(a\\\\_{j}\\\\-\\\\\\\\pi\\\\_{\\\\\\\\theta} (s\\\\_{j})) \\\\\\\\tag{7}]  \\n*{I}(\\\\\\\\mathcal{B})\\\\=\\\\\\\\sum*where (\\\\\\\\mathcal{M}:\\\\\\\\mathcal{A}!\\\\\\\\rightarrow!\\\\\\\\mathbb{R}*{\\\\\\\\geq 0}) is a non\\\\-negative function that evaluates the deviation between the two policies, which is commonly the 2\\\\-norm of vectors. Such an approach is also referred to as \\\\_explicit BC (EBC)* since the loss function (7\\\\) explicitly penalizes the deviation between actions of the source and the target policies. EBC is very likely to cause overfitting since it is obsessed with the fitting of two policies. It may not be successful when the demonstration data is noisy and subject to large variance. Recent study proposes an *implicit BC (IBC)* method which suggests the following loss function \\\\[35],  \\n*{\\\\\\\\geq 0}) is a non\\\\-negative function that evaluates the deviation between the two policies, which is commonly the 2\\\\-norm of vectors. Such an approach is also referred to as \\\\_explicit BC (EBC)**implicit BC (IBC)*\\\\[\\\\\\\\mathcal{L}*{I}(\\\\\\\\mathcal{B})\\\\=\\\\\\\\sum*{j\\\\=1}^{n}E\\\\_{\\\\\\\\theta}(s\\\\_{j},a\\\\_{j}), \\\\\\\\tag{8}]  \\n*{I}(\\\\\\\\mathcal{B})\\\\=\\\\\\\\sum*where (E\\\\_{\\\\\\\\theta}:\\\\\\\\mathcal{S}!\\\\\\\\times!\\\\\\\\mathcal{A}!\\\\\\\\rightarrow!\\\\\\\\mathbb{R}*{\\\\\\\\geq 0}) is a non\\\\-negative energy function of demo data ((s*{j},a\\\\_{j})) parameterized by (\\\\\\\\theta). Thus, IBC generates the target policy through an implicit energy function, which can effectively avoid the overfitting problem. It also provides a flexible interface to combine BC with an off\\\\-policy RL agent. A technical question to be answered in this paper is how to design a proper energy function to facilitate RL with IBC.  \\n*{\\\\\\\\geq 0}) is a non\\\\-negative energy function of demo data ((s*', children=[])]),\n",
       "  ArxivPaperSection(header='h2', title='III Framework and Problem Statement', text='In this section, we first introduce the overall framework of the proposed IBC\\\\-DMP RL method. Then, we present a novel Multi\\\\-DoF DMP model for motion planning of a high degree\\\\-of\\\\-freedom (DoF) robot. Finally, based on the proposed framework, we clarify the problem to be solved in this paper.\\n\\n### *Overall Framework*\\n\\n*Overall Framework*The overall framework of IBC\\\\-DMP RL is illustrated in Fig. 2\\\\. The basic model used to generate robot trajectories is *Multi\\\\-DoF DMP*, an adapted version of the conventional DMP model introduced in Sec. II\\\\-A. It also serves as the environment in the robot motion planning problem. The human demonstration encodes a demonstration policy (\\\\\\\\tilde{\\\\\\\\mathbf{f}}) which reflects how humans behave in a motion planning task. An *IBC\\\\-DMP agent* is used to generate the desired motion planning policy (\\\\\\\\mathbf{f}) for the task. The agent is trained by a dual\\\\-buffer structure which is composed of a *demonstration buffer* and an *interaction buffer*. These two buffers store the demonstration data generated from the human demonstrations and the interaction data during the learning process, respectively.\\n\\n*Multi\\\\-DoF DMP**IBC\\\\-DMP agent**demonstration buffer**interaction buffer*### *Motion Planning Using Multi\\\\-DoF DMP*\\n\\n*Motion Planning Using Multi\\\\-DoF DMP*The conventional DMP in Sec. II\\\\-A is defined for a one\\\\-dimensional point and has been used for robot motion planning in a decoupled manner, i.e., one DMP is used to generate the trajectory for each DoF of the robot \\\\[27]. This leads to the lack of coupling among different DoFs, which restricts the flexibility of solving the optimal planning policy. To resolve this issue, in this paper, we propose the following Multi\\\\-DoF\\nDMP model for the motion planning of a robot end\\\\-effector in the Cartesian space,\\n\\n\\\\[\\\\\\\\tau\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}!\\\\=!K*{\\\\\\\\alpha}(K\\\\_{\\\\\\\\beta}(\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}!\\\\-! \\\\\\\\mathbf{x}*{t})!\\\\-!\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t})!\\\\+\\\\\\\\zeta*{t}\\\\|\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}!\\\\- !\\\\\\\\mathbf{x}*{i}\\\\|\\\\\\\\mathbf{f}(\\\\\\\\mathcal{X}\\\\_{t}), \\\\\\\\tag{9}]\\n\\n*{t}!\\\\=!K**{\\\\\\\\text{g}}!\\\\-! \\\\\\\\mathbf{x}**{t})!\\\\+\\\\\\\\zeta**{\\\\\\\\text{g}}!\\\\- !\\\\\\\\mathbf{x}*where (\\\\\\\\mathbf{x}*{t}), (\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}), (\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}\\\\\\\\in\\\\\\\\mathbb{R}^{3}) are the position, linear velocity, and acceleration of the end\\\\-effector in the Cartesian space, (\\\\\\\\mathbf{x}*{i}), (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) are the initial and goal positions of the end\\\\-effector, (K*{\\\\\\\\alpha}), (K\\\\_{\\\\\\\\beta}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3\\\\\\\\times 3}) are parametric matrices, (\\\\\\\\tau!\\\\\\\\in!\\\\\\\\mathbb{R}) and (\\\\\\\\zeta\\\\_{t}!\\\\\\\\in!\\\\\\\\mathbb{R}) are the temporal scalar and the canonical variable, the same as (1\\\\), (\\\\\\\\mathcal{X}*{t}!\\\\=!{\\\\\\\\,\\\\\\\\mathbf{x}*{t},\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t},\\\\\\\\mathbf{x}*{\\\\\\\\text {b}},\\\\\\\\zeta\\\\_{t}}) is the state vector of the Multi\\\\-DoF DMP, where (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}) is a vector that describes the configuration of the obstacle, and (\\\\\\\\mathbf{f}:\\\\\\\\mathbb{R}^{10}!\\\\\\\\rightarrow!\\\\\\\\mathbb{R}^{3}) is a multi\\\\-dimension actuation function to be determined. The initial state of model (9\\\\) is set as (\\\\\\\\mathbf{x}*{0}!\\\\=!\\\\\\\\mathbf{x}*{\\\\\\\\text{i}}) and (\\\\\\\\dot{\\\\\\\\mathbf{x}}*{0}!\\\\=!0\\\\). In this paper, we only consider a *single static obstacle* for brevity. Also, we use a vertically positioned *cylinder* model to represent the obstacle. Then, the position of the obstacle is represented as a three\\\\-dimensional constant vector (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) assigned as the Cartesian coordinate of the top surface center of the cylinder. In this sense, the DMP state can be represented as a ten\\\\-dimensional vector (\\\\\\\\mathcal{X}*{t}!\\\\=!\\\\[\\\\\\\\mathbf{x}*{t}^{\\\\\\\\top},\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}^{\\\\\\\\top}, \\\\\\\\mathbf{x}*{t}^{\\\\\\\\top}\\\\-\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}^{\\\\\\\\top},\\\\\\\\zeta\\\\_{t}]^{\\\\\\\\top}!\\\\\\\\in! \\\\\\\\mathbb{R}^{10}). Here, we encode a time\\\\-variant vector (\\\\\\\\mathbf{x}*{t}!\\\\-!\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}) into the DMP state instead of the constant vector (\\\\\\\\mathbf{x}\\\\_{\\\\\\\\text{b}}) since the former provides larger diversity to the data. The incorporation of more complicated environments with multiple dynamic obstacles is beyond the scope of this paper and will be considered in future work. The multi\\\\-DoF DMP is different from the conventional DMP (1\\\\) in three aspects.\\n\\n*{t}), (\\\\\\\\dot{\\\\\\\\mathbf{x}}**{t}\\\\\\\\in\\\\\\\\mathbb{R}^{3}) are the position, linear velocity, and acceleration of the end\\\\-effector in the Cartesian space, (\\\\\\\\mathbf{x}**{\\\\\\\\text{g}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) are the initial and goal positions of the end\\\\-effector, (K**{t}!\\\\=!{\\\\\\\\,\\\\\\\\mathbf{x}**{t},\\\\\\\\mathbf{x}**{\\\\\\\\text{b}}) is a vector that describes the configuration of the obstacle, and (\\\\\\\\mathbf{f}:\\\\\\\\mathbb{R}^{10}!\\\\\\\\rightarrow!\\\\\\\\mathbb{R}^{3}) is a multi\\\\-dimension actuation function to be determined. The initial state of model (9\\\\) is set as (\\\\\\\\mathbf{x}**{\\\\\\\\text{i}}) and (\\\\\\\\dot{\\\\\\\\mathbf{x}}**single static obstacle**cylinder**{\\\\\\\\text{b}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) assigned as the Cartesian coordinate of the top surface center of the cylinder. In this sense, the DMP state can be represented as a ten\\\\-dimensional vector (\\\\\\\\mathcal{X}**{t}^{\\\\\\\\top},\\\\\\\\dot{\\\\\\\\mathbf{x}}**{t}^{\\\\\\\\top}\\\\-\\\\\\\\mathbf{x}**{t}!\\\\-!\\\\\\\\mathbf{x}** The actuation function (\\\\\\\\mathbf{f}) does not only depend on the canonical variable (\\\\\\\\zeta\\\\_{t}) but also on the internal states of the DMP, namely (\\\\\\\\mathbf{x}*{t}) and (\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}), and the obstacle position (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}). Different from the conventional DMP models, this may change the poles or the closed\\\\-loop dynamics of the Multi\\\\-DoF DMP, which can be both an advantage and a drawback. On the one hand, the state\\\\-dependent actuation function (\\\\\\\\mathbf{f}) can be learned to automatically adjust the poles of the DMP model, such that its dynamic properties are not completely determined by the hyperparameters (K*{\\\\\\\\alpha}) and (K\\\\_{\\\\\\\\beta}). This improves the flexibility of agent training. On the other hand, the DMP model may lose its inherent stability. In this paper, we set action limits (\\\\\\\\underline{f}\\\\\\\\leq\\\\\\\\mathbf{f}(\\\\\\\\mathcal{X}\\\\_{t})\\\\\\\\leq\\\\\\\\overline{f}) to avoid this.\\n* The gain of the actuation function is the absolute distance between the initial position (\\\\\\\\mathbf{x}*{i}) and the goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}), (\\\\|\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}\\\\-\\\\\\\\mathbf{x}*{i}\\\\|), instead of the element\\\\-wise distance used by the conventional DMP (1\\\\). Such a scheme can improve the flexibility of a DMP model by fully incorporating the coupling of its different dimensions.\\n* The obstacle position (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}) is included in the state (\\\\\\\\mathcal{X}*{t}) of the DMP, instead of being incorporated as an additional virtual force term as the conventional DMP model. This provides the possibility of incorporating more complicated obstacle information into motion planning.\\n\\n- The actuation function (\\\\\\\\mathbf{f}) does not only depend on the canonical variable (\\\\\\\\zeta\\\\_{t}) but also on the internal states of the DMP, namely (\\\\\\\\mathbf{x}*{t}) and (\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}), and the obstacle position (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}). Different from the conventional DMP models, this may change the poles or the closed\\\\-loop dynamics of the Multi\\\\-DoF DMP, which can be both an advantage and a drawback. On the one hand, the state\\\\-dependent actuation function (\\\\\\\\mathbf{f}) can be learned to automatically adjust the poles of the DMP model, such that its dynamic properties are not completely determined by the hyperparameters (K*{\\\\\\\\alpha}) and (K\\\\_{\\\\\\\\beta}). This improves the flexibility of agent training. On the other hand, the DMP model may lose its inherent stability. In this paper, we set action limits (\\\\\\\\underline{f}\\\\\\\\leq\\\\\\\\mathbf{f}(\\\\\\\\mathcal{X}\\\\_{t})\\\\\\\\leq\\\\\\\\overline{f}) to avoid this.\\n*{t}) and (\\\\\\\\dot{\\\\\\\\mathbf{x}}**{\\\\\\\\text{b}}). Different from the conventional DMP models, this may change the poles or the closed\\\\-loop dynamics of the Multi\\\\-DoF DMP, which can be both an advantage and a drawback. On the one hand, the state\\\\-dependent actuation function (\\\\\\\\mathbf{f}) can be learned to automatically adjust the poles of the DMP model, such that its dynamic properties are not completely determined by the hyperparameters (K*- The gain of the actuation function is the absolute distance between the initial position (\\\\\\\\mathbf{x}*{i}) and the goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}), (\\\\|\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}\\\\-\\\\\\\\mathbf{x}*{i}\\\\|), instead of the element\\\\-wise distance used by the conventional DMP (1\\\\). Such a scheme can improve the flexibility of a DMP model by fully incorporating the coupling of its different dimensions.\\n*{i}) and the goal position (\\\\\\\\mathbf{x}**{\\\\\\\\text{g}}\\\\-\\\\\\\\mathbf{x}*- The obstacle position (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}) is included in the state (\\\\\\\\mathcal{X}*{t}) of the DMP, instead of being incorporated as an additional virtual force term as the conventional DMP model. This provides the possibility of incorporating more complicated obstacle information into motion planning.\\n*{\\\\\\\\text{b}}) is included in the state (\\\\\\\\mathcal{X}*### *Cost Function of Motion Planning*\\n\\n*Cost Function of Motion Planning*This paper solves a general robot motion planning problem. Specifically, given an initial position (\\\\\\\\mathbf{x}*{i}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) and a goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) in the Cartesian space, generate a smooth trajectory (\\\\\\\\mathbf{x}*{t}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) for (0!\\\\<!t!\\\\\\\\leq!T) using the Multi\\\\-DoF DMP model in (9\\\\), such that (\\\\\\\\mathbf{x}*{T}) is sufficiently close to (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}), where (T!\\\\\\\\in!\\\\\\\\mathbb{R}^{\\\\+}) is the predefined timing length of the trajectory. The generated trajectory should also ensure sufficient smoothness (minimal acceleration) and maintain a certain distance with a given static obstacle positioned in (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{2}). These requirements are encoded in the following cost function,\\n\\n*{i}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) and a goal position (\\\\\\\\mathbf{x}**{t}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) for (0!\\\\<!t!\\\\\\\\leq!T) using the Multi\\\\-DoF DMP model in (9\\\\), such that (\\\\\\\\mathbf{x}**{\\\\\\\\text{g}}), where (T!\\\\\\\\in!\\\\\\\\mathbb{R}^{\\\\+}) is the predefined timing length of the trajectory. The generated trajectory should also ensure sufficient smoothness (minimal acceleration) and maintain a certain distance with a given static obstacle positioned in (\\\\\\\\mathbf{x}*\\\\[\\\\\\\\mathcal{J}*{t}\\\\=\\\\\\\\left{\\\\\\\\begin{array}{ll}\\\\\\\\sum*{i\\\\=1}^{4}\\\\\\\\alpha\\\\_{i}\\\\\\\\mathcal{J} *{t}^{(i)},\\\\&0\\\\\\\\leq t\\\\<T,\\\\\\\\ \\\\\\\\alpha*{5}\\\\\\\\mathcal{J}\\\\_{t}^{(5\\\\)},\\\\&t\\\\=T,\\\\\\\\end{array}\\\\\\\\right. \\\\\\\\tag{10}]\\n\\n*{t}\\\\=\\\\\\\\left{\\\\\\\\begin{array}{ll}\\\\\\\\sum**{t}^{(i)},\\\\&0\\\\\\\\leq t\\\\<T,\\\\\\\\ \\\\\\\\alpha*where (\\\\\\\\alpha\\\\_{i}!\\\\\\\\in!\\\\\\\\mathbb{R}^{\\\\+}) are constant parameters, (i\\\\=1,2,\\\\\\\\cdots,5\\\\), and (\\\\\\\\mathcal{J}\\\\_{t}^{(i)}!\\\\\\\\in!\\\\\\\\mathbb{R}) are instant costs at time (t), defined as\\n\\n\\\\[\\\\\\\\mathcal{J}*{t}^{(1\\\\)}!\\\\=!!\\\\\\\\left\\\\|\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}\\\\\\\\right\\\\|^{2},\\\\\\\\ \\\\\\\\mathcal{J}*{t}^{(2\\\\)}!\\\\=!\\\\\\\\left\\\\|\\\\\\\\mathbf{x}*{t}!\\\\-!\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}\\\\\\\\right\\\\|^ {2},] \\\\[\\\\\\\\mathcal{J}*{t}^{(4\\\\)}!\\\\=!\\\\\\\\left{!\\\\\\\\begin{array}{ll}\\\\\\\\eta! \\\\\\\\left(!\\\\\\\\left\\\\|\\\\\\\\mathbf{x}*{t}^{(1,2\\\\)}!\\\\-!\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}^{(1,2\\\\)}! \\\\\\\\right\\\\|!\\\\-!\\\\\\\\tau\\\\_{\\\\\\\\text{b}}!,\\\\\\\\varepsilon\\\\_{0}^{\\\\\\\\text{b}}!,\\\\\\\\varepsilon\\\\_{1}^{ \\\\\\\\text{b}}\\\\\\\\right)\\\\&\\\\\\\\text{if }0!\\\\<!\\\\\\\\mathbf{x}*{t}^{(3\\\\)}!\\\\<!\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}^{(3\\\\)}\\\\\\\\ 0\\\\&\\\\\\\\text{otherwise},\\\\\\\\end{array}\\\\\\\\right.] \\\\[\\\\\\\\mathcal{J}*{t}^{(3\\\\)}\\\\=\\\\\\\\eta!\\\\\\\\left(!\\\\\\\\mathbf{x}*{t}^{(3\\\\)}, \\\\\\\\varepsilon\\\\_{0}^{\\\\\\\\text{d}},\\\\\\\\varepsilon\\\\_{1}^{\\\\\\\\text{d}}\\\\\\\\right),\\\\\\\\ \\\\\\\\mathcal{J}*{t}^{(5\\\\)}\\\\=\\\\\\\\xi(\\\\|\\\\\\\\mathbf{x}*{t}!\\\\-!\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}\\\\|, \\\\\\\\varepsilon*{T}),]\\n\\n*{t}^{(1\\\\)}!\\\\=!!\\\\\\\\left\\\\|\\\\\\\\dot{\\\\\\\\mathbf{x}}**{t}^{(2\\\\)}!\\\\=!\\\\\\\\left\\\\|\\\\\\\\mathbf{x}**{\\\\\\\\text{g}}\\\\\\\\right\\\\|^ {2},] \\\\[\\\\\\\\mathcal{J}**{t}^{(1,2\\\\)}!\\\\-!\\\\\\\\mathbf{x}**{t}^{(3\\\\)}!\\\\<!\\\\\\\\mathbf{x}**{t}^{(3\\\\)}\\\\=\\\\\\\\eta!\\\\\\\\left(!\\\\\\\\mathbf{x}**{t}^{(5\\\\)}\\\\=\\\\\\\\xi(\\\\|\\\\\\\\mathbf{x}**{\\\\\\\\text{g}}\\\\|, \\\\\\\\varepsilon*where (\\\\\\\\mathbf{x}*{t}^{(1,2\\\\)}!\\\\\\\\in!\\\\\\\\mathbb{R}^{2}), (\\\\\\\\mathbf{x}*{t}^{(3\\\\)}!\\\\\\\\in!\\\\\\\\mathbb{R}), (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}^{(1,2\\\\)}!\\\\\\\\in!\\\\\\\\mathbb{R}^{2}), (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}^{(3\\\\)}!\\\\\\\\in!\\\\\\\\mathbb{R}) are the slides of vectors (\\\\\\\\mathbf{x}*{t}) and (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}), (r\\\\_{\\\\\\\\text{b}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{\\\\+}) is the radius of the cylinder, (\\\\\\\\eta:\\\\\\\\mathbb{R}!\\\\\\\\rightarrow!\\\\\\\\mathbb{R}*{\\\\\\\\geq 0}) with constant parameters (0!\\\\<!\\\\\\\\varepsilon*{0}!\\\\<!\\\\\\\\varepsilon\\\\_{1}) is an artificial potential field function defined as\\n\\n*{t}^{(1,2\\\\)}!\\\\\\\\in!\\\\\\\\mathbb{R}^{2}), (\\\\\\\\mathbf{x}**{\\\\\\\\text{b}}^{(1,2\\\\)}!\\\\\\\\in!\\\\\\\\mathbb{R}^{2}), (\\\\\\\\mathbf{x}**{t}) and (\\\\\\\\mathbf{x}**{\\\\\\\\geq 0}) with constant parameters (0!\\\\<!\\\\\\\\varepsilon*\\\\[\\\\\\\\eta(x,\\\\\\\\varepsilon\\\\_{0},\\\\\\\\varepsilon\\\\_{1})!\\\\=!\\\\\\\\left{!\\\\\\\\begin{array}{ll}(x!\\\\- !\\\\\\\\varepsilon\\\\_{0})^{\\\\-2}!\\\\-!(\\\\\\\\varepsilon\\\\_{1}!\\\\-!\\\\\\\\varepsilon\\\\_{0})^{\\\\-2},\\\\& \\\\\\\\varepsilon\\\\_{0}!\\\\<!x!\\\\<!\\\\\\\\varepsilon\\\\_{1},\\\\\\\\ 0,\\\\&x!\\\\\\\\geq!\\\\\\\\varepsilon\\\\_{1},\\\\\\\\end{array}\\\\\\\\right. \\\\\\\\tag{11}]\\n\\n(0!\\\\<!\\\\\\\\varepsilon\\\\_{0}^{\\\\\\\\text{b}}!\\\\<!\\\\\\\\varepsilon\\\\_{1}^{\\\\\\\\text{b}}) and (0!\\\\<!\\\\\\\\varepsilon\\\\_{0}^{\\\\\\\\text{d}}!\\\\<!\\\\\\\\varepsilon\\\\_{1}^{\\\\\\\\text{d}}) are potential field parameters for the obstacle and the ground, and (\\\\\\\\xi:\\\\\\\\mathbb{R}!\\\\\\\\rightarrow!\\\\\\\\mathbb{R}\\\\_{\\\\\\\\geq 0}) is a squared dead\\\\-zone function defined as\\n\\n\\\\[\\\\\\\\xi(x,\\\\\\\\varepsilon)\\\\=\\\\\\\\left{\\\\\\\\begin{array}{ll}0,\\\\&0\\\\\\\\leq x\\\\\\\\leq\\\\\\\\varepsilon,\\\\\\\\ x\\\\-\\\\\\\\varepsilon,\\\\&x\\\\>\\\\\\\\varepsilon,\\\\\\\\end{array}\\\\\\\\right. \\\\\\\\tag{12}]\\n\\nwhere (\\\\\\\\varepsilon!\\\\\\\\in!\\\\\\\\mathbb{R}^{\\\\+}) is the dead\\\\-zone scalar. Here, (\\\\\\\\eta(x)) serves as an artificial potential field function that penalizes (x) if it gets close to (\\\\\\\\varepsilon\\\\_{0}) from the positive direction. Another parameter (\\\\\\\\varepsilon\\\\_{1}) is the upper limit of (x) such that (\\\\\\\\eta(x)) has an effect. To avoid undefinition, we assign (\\\\\\\\eta(x)) with a large number when (x!\\\\\\\\leq!\\\\\\\\varepsilon\\\\_{0}).\\n\\nIn the comprehensive cost function (10\\\\), term (\\\\\\\\mathcal{J}\\\\_{t}^{(1\\\\)}) penalizes the value of the acceleration to ensure sufficient\\n\\nFigure 2: The illustration of the IBC\\\\-DMP RL framework, where (\\\\\\\\mathcal{X}*{t}) denotes the position, velocity, and acceleration of the Multi\\\\-DoF DMP model, (\\\\\\\\tilde{\\\\\\\\mathbf{f}}(\\\\\\\\mathcal{X}*{t})) and (\\\\\\\\mathbf{f}(\\\\\\\\mathcal{X}*{t})) are the actuation functions provided by the human demonstration and the IBC\\\\-DMP agent, respectively, and (s*{t}) and (a\\\\_{t}) are the state and action data provided by human demonstration or generated by the IBC\\\\-DMP agent. Besides, the solid arrows denote the interactions, the dotted arrows indicate data storing, and the dashed arrow represents agent training.\\nsmoothness. Term (\\\\\\\\mathcal{J}*{t}^{(2\\\\)}) penalizes the distance between the current position (\\\\\\\\mathbf{x}*{t}) and the goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}), aiming at fast and straightforward goal reaching. Terms (\\\\\\\\mathcal{J}*{t}^{(3\\\\)}) and (\\\\\\\\mathcal{J}*{t}^{(4\\\\)}) attempt to keep the position (\\\\\\\\mathbf{x}*{t}) away from the ground ((z\\\\=0\\\\) plane) and the cylinder obstacle, respectively. The usage of artificial potential field functions ensures a very large cost when (\\\\\\\\mathbf{x}*{t}) runs into the cylinder obstacle or the ground. The last term (\\\\\\\\mathcal{J}*{T}^{(5\\\\)}) penalizes the distance between the ultimate position (\\\\\\\\mathbf{x}*{T}) and the goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}). A dead\\\\-zone scalar (\\\\\\\\varepsilon\\\\_{T}) is involved to prescribe the tolerable level of the ultimate error. Specifically, the ultimate cost is exerted only when the ultimate error exceeds the threshold (\\\\\\\\varepsilon\\\\_{T}). By minimizing the comprehensive cost (10\\\\) for the Multi\\\\-DoF DMP model (9\\\\), one can get a smooth trajectory (\\\\\\\\mathbf{x}*{t}) that is able to reach the goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}) at the ending time (t\\\\=T) while avoiding collision with the ground and the obstacle.\\n\\n*{t}) denotes the position, velocity, and acceleration of the Multi\\\\-DoF DMP model, (\\\\\\\\tilde{\\\\\\\\mathbf{f}}(\\\\\\\\mathcal{X}**{t})) are the actuation functions provided by the human demonstration and the IBC\\\\-DMP agent, respectively, and (s**{t}^{(2\\\\)}) penalizes the distance between the current position (\\\\\\\\mathbf{x}**{\\\\\\\\text{g}}), aiming at fast and straightforward goal reaching. Terms (\\\\\\\\mathcal{J}**{t}^{(4\\\\)}) attempt to keep the position (\\\\\\\\mathbf{x}**{t}) runs into the cylinder obstacle or the ground. The last term (\\\\\\\\mathcal{J}**{T}) and the goal position (\\\\\\\\mathbf{x}**{t}) that is able to reach the goal position (\\\\\\\\mathbf{x}*### *Decision\\\\-Making Problem Statement*\\n\\n*Decision\\\\-Making Problem Statement*Consider that the Multi\\\\-DoF DMP model (9\\\\) is discretized in time in a zero\\\\-order hold manner with a discrete sampling time (\\\\\\\\Delta t!\\\\\\\\in!\\\\\\\\mathbb{R}^{\\\\+}), as follows,\\n\\n\\\\[\\\\\\\\begin{split}\\\\\\\\mathbf{x}*{t\\\\+\\\\\\\\Delta t}!\\\\=!\\\\\\\\mathbf{x}*{t}\\\\+\\\\\\\\Delta t \\\\\\\\hat{\\\\\\\\mathbf{x}}*{t},\\\\\\\\ \\\\\\\\tilde{\\\\\\\\mathbf{x}}*{t\\\\+\\\\\\\\Delta t}!\\\\=!\\\\\\\\tilde{\\\\\\\\mathbf{x}}*{t}\\\\+\\\\\\\\Delta t\\\\\\\\tilde{ \\\\\\\\mathbf{x}}*{t},\\\\\\\\ \\\\\\\\zeta\\\\_{t\\\\+\\\\\\\\Delta t}!\\\\=!\\\\\\\\zeta\\\\_{t}\\\\-\\\\\\\\frac{\\\\\\\\Delta t}{\\\\\\\\tau}\\\\\\\\omega\\\\\\\\zeta \\\\_{t},\\\\\\\\end{split} \\\\\\\\tag{13}]\\n\\n*{t\\\\+\\\\\\\\Delta t}!\\\\=!\\\\\\\\mathbf{x}**{t},\\\\\\\\ \\\\\\\\tilde{\\\\\\\\mathbf{x}}**{t}\\\\+\\\\\\\\Delta t\\\\\\\\tilde{ \\\\\\\\mathbf{x}}*where (\\\\\\\\tilde{\\\\\\\\mathbf{x}}*{t}) is given by (9\\\\). Then, the generated trajectory (\\\\\\\\mathbf{x}*{t}) is a set of waypoints at the sampled timing points (\\\\\\\\mathbf{t}\\\\={0,\\\\\\\\Delta t,2\\\\\\\\Delta t,\\\\\\\\cdots,T}). In this sense, the objective of motion planning is to solve the optimal actuation function (\\\\\\\\mathbf{f}) in (9\\\\) such that the accumulated cost (\\\\\\\\sum\\\\_{t\\\\=0}^{T}\\\\\\\\mathcal{J}*{t}) is minimized. This problem renders decision\\\\-making over an MDP (\\\\\\\\mathcal{M}\\\\=(\\\\\\\\mathcal{S},\\\\\\\\mathcal{A},\\\\\\\\mathcal{F},\\\\\\\\mathcal{R})) defined in Sec. II\\\\-B, where (\\\\\\\\mathcal{S}\\\\={\\\\\\\\mathcal{X}*{t}\\\\|\\\\\\\\mathbf{x}*{i}\\\\\\\\in\\\\\\\\mathbb{R}^{3},\\\\\\\\,0\\\\\\\\leq t\\\\<T}) is the state space, (\\\\\\\\mathcal{A}!\\\\=!{\\\\\\\\mathbf{f}(s)\\\\\\\\,\\\\|\\\\\\\\forall\\\\\\\\,s\\\\\\\\in\\\\\\\\mathcal{S}}) is the action space, (\\\\\\\\mathcal{F}) is the state transition characterized by (13\\\\), and (\\\\\\\\mathcal{R}:\\\\-\\\\\\\\mathcal{J}*{t}) is the instant reward. In this sense, the policy (\\\\\\\\pi) can be represented as a parameterized actuation function (\\\\\\\\mathbf{f}*{\\\\\\\\theta}), modeled as a neural network and learned using an off\\\\-policy RL method introduced in Sec. II\\\\-B. Besides, BC introduced in Sec. II\\\\-C can be used to promote the training of the RL agent with the demonstration policy (\\\\\\\\mathbf{\\\\\\\\tilde{f}}), as addressed in Sec. II\\\\-C. Note that, different from the conventional imitation learning problems, this paper does not aim at using (\\\\\\\\mathbf{f}*{\\\\\\\\theta}) to approximate the demonstration policy (\\\\\\\\mathbf{\\\\\\\\tilde{f}}). Instead, (\\\\\\\\mathbf{\\\\\\\\tilde{f}}) is only used to improve the training of (\\\\\\\\mathbf{f}\\\\_{\\\\\\\\theta}).\\n\\n*{t}) is given by (9\\\\). Then, the generated trajectory (\\\\\\\\mathbf{x}**{t}) is minimized. This problem renders decision\\\\-making over an MDP (\\\\\\\\mathcal{M}\\\\=(\\\\\\\\mathcal{S},\\\\\\\\mathcal{A},\\\\\\\\mathcal{F},\\\\\\\\mathcal{R})) defined in Sec. II\\\\-B, where (\\\\\\\\mathcal{S}\\\\={\\\\\\\\mathcal{X}**{i}\\\\\\\\in\\\\\\\\mathbb{R}^{3},\\\\\\\\,0\\\\\\\\leq t\\\\<T}) is the state space, (\\\\\\\\mathcal{A}!\\\\=!{\\\\\\\\mathbf{f}(s)\\\\\\\\,\\\\|\\\\\\\\forall\\\\\\\\,s\\\\\\\\in\\\\\\\\mathcal{S}}) is the action space, (\\\\\\\\mathcal{F}) is the state transition characterized by (13\\\\), and (\\\\\\\\mathcal{R}:\\\\-\\\\\\\\mathcal{J}**{\\\\\\\\theta}), modeled as a neural network and learned using an off\\\\-policy RL method introduced in Sec. II\\\\-B. Besides, BC introduced in Sec. II\\\\-C can be used to promote the training of the RL agent with the demonstration policy (\\\\\\\\mathbf{\\\\\\\\tilde{f}}), as addressed in Sec. II\\\\-C. Note that, different from the conventional imitation learning problems, this paper does not aim at using (\\\\\\\\mathbf{f}*Now, we are ready to state the two main objectives of this paper: (1\\\\). transforming human demonstration into data that are compatible with the demonstration buffer, corresponding to the left column of Fig. 2, and (2\\\\). train the optimal policy (\\\\\\\\mathbf{f}) using the demonstration buffer, as the right column of Fig. 2\\\\. Our solutions to these two problems will be presented in Sec. IV and Sec. V, respectively.\\n\\n', children=[ArxivPaperSection(header='p', title='', text='In this section, we first introduce the overall framework of the proposed IBC\\\\-DMP RL method. Then, we present a novel Multi\\\\-DoF DMP model for motion planning of a high degree\\\\-of\\\\-freedom (DoF) robot. Finally, based on the proposed framework, we clarify the problem to be solved in this paper.', children=[]), ArxivPaperSection(header='h3', title='*Overall Framework*', text='*Overall Framework*The overall framework of IBC\\\\-DMP RL is illustrated in Fig. 2\\\\. The basic model used to generate robot trajectories is *Multi\\\\-DoF DMP*, an adapted version of the conventional DMP model introduced in Sec. II\\\\-A. It also serves as the environment in the robot motion planning problem. The human demonstration encodes a demonstration policy (\\\\\\\\tilde{\\\\\\\\mathbf{f}}) which reflects how humans behave in a motion planning task. An *IBC\\\\-DMP agent* is used to generate the desired motion planning policy (\\\\\\\\mathbf{f}) for the task. The agent is trained by a dual\\\\-buffer structure which is composed of a *demonstration buffer* and an *interaction buffer*. These two buffers store the demonstration data generated from the human demonstrations and the interaction data during the learning process, respectively.  \\n*Multi\\\\-DoF DMP**IBC\\\\-DMP agent**demonstration buffer**interaction buffer*### *Motion Planning Using Multi\\\\-DoF DMP*  \\n*Motion Planning Using Multi\\\\-DoF DMP*The conventional DMP in Sec. II\\\\-A is defined for a one\\\\-dimensional point and has been used for robot motion planning in a decoupled manner, i.e., one DMP is used to generate the trajectory for each DoF of the robot \\\\[27]. This leads to the lack of coupling among different DoFs, which restricts the flexibility of solving the optimal planning policy. To resolve this issue, in this paper, we propose the following Multi\\\\-DoF\\nDMP model for the motion planning of a robot end\\\\-effector in the Cartesian space,  \\n\\\\[\\\\\\\\tau\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}!\\\\=!K*{\\\\\\\\alpha}(K\\\\_{\\\\\\\\beta}(\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}!\\\\-! \\\\\\\\mathbf{x}*{t})!\\\\-!\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t})!\\\\+\\\\\\\\zeta*{t}\\\\|\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}!\\\\- !\\\\\\\\mathbf{x}*{i}\\\\|\\\\\\\\mathbf{f}(\\\\\\\\mathcal{X}\\\\_{t}), \\\\\\\\tag{9}]  \\n*{t}!\\\\=!K**{\\\\\\\\text{g}}!\\\\-! \\\\\\\\mathbf{x}**{t})!\\\\+\\\\\\\\zeta**{\\\\\\\\text{g}}!\\\\- !\\\\\\\\mathbf{x}*where (\\\\\\\\mathbf{x}*{t}), (\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}), (\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}\\\\\\\\in\\\\\\\\mathbb{R}^{3}) are the position, linear velocity, and acceleration of the end\\\\-effector in the Cartesian space, (\\\\\\\\mathbf{x}*{i}), (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) are the initial and goal positions of the end\\\\-effector, (K*{\\\\\\\\alpha}), (K\\\\_{\\\\\\\\beta}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3\\\\\\\\times 3}) are parametric matrices, (\\\\\\\\tau!\\\\\\\\in!\\\\\\\\mathbb{R}) and (\\\\\\\\zeta\\\\_{t}!\\\\\\\\in!\\\\\\\\mathbb{R}) are the temporal scalar and the canonical variable, the same as (1\\\\), (\\\\\\\\mathcal{X}*{t}!\\\\=!{\\\\\\\\,\\\\\\\\mathbf{x}*{t},\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t},\\\\\\\\mathbf{x}*{\\\\\\\\text {b}},\\\\\\\\zeta\\\\_{t}}) is the state vector of the Multi\\\\-DoF DMP, where (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}) is a vector that describes the configuration of the obstacle, and (\\\\\\\\mathbf{f}:\\\\\\\\mathbb{R}^{10}!\\\\\\\\rightarrow!\\\\\\\\mathbb{R}^{3}) is a multi\\\\-dimension actuation function to be determined. The initial state of model (9\\\\) is set as (\\\\\\\\mathbf{x}*{0}!\\\\=!\\\\\\\\mathbf{x}*{\\\\\\\\text{i}}) and (\\\\\\\\dot{\\\\\\\\mathbf{x}}*{0}!\\\\=!0\\\\). In this paper, we only consider a *single static obstacle* for brevity. Also, we use a vertically positioned *cylinder* model to represent the obstacle. Then, the position of the obstacle is represented as a three\\\\-dimensional constant vector (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) assigned as the Cartesian coordinate of the top surface center of the cylinder. In this sense, the DMP state can be represented as a ten\\\\-dimensional vector (\\\\\\\\mathcal{X}*{t}!\\\\=!\\\\[\\\\\\\\mathbf{x}*{t}^{\\\\\\\\top},\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}^{\\\\\\\\top}, \\\\\\\\mathbf{x}*{t}^{\\\\\\\\top}\\\\-\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}^{\\\\\\\\top},\\\\\\\\zeta\\\\_{t}]^{\\\\\\\\top}!\\\\\\\\in! \\\\\\\\mathbb{R}^{10}). Here, we encode a time\\\\-variant vector (\\\\\\\\mathbf{x}*{t}!\\\\-!\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}) into the DMP state instead of the constant vector (\\\\\\\\mathbf{x}\\\\_{\\\\\\\\text{b}}) since the former provides larger diversity to the data. The incorporation of more complicated environments with multiple dynamic obstacles is beyond the scope of this paper and will be considered in future work. The multi\\\\-DoF DMP is different from the conventional DMP (1\\\\) in three aspects.  \\n*{t}), (\\\\\\\\dot{\\\\\\\\mathbf{x}}**{t}\\\\\\\\in\\\\\\\\mathbb{R}^{3}) are the position, linear velocity, and acceleration of the end\\\\-effector in the Cartesian space, (\\\\\\\\mathbf{x}**{\\\\\\\\text{g}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) are the initial and goal positions of the end\\\\-effector, (K**{t}!\\\\=!{\\\\\\\\,\\\\\\\\mathbf{x}**{t},\\\\\\\\mathbf{x}**{\\\\\\\\text{b}}) is a vector that describes the configuration of the obstacle, and (\\\\\\\\mathbf{f}:\\\\\\\\mathbb{R}^{10}!\\\\\\\\rightarrow!\\\\\\\\mathbb{R}^{3}) is a multi\\\\-dimension actuation function to be determined. The initial state of model (9\\\\) is set as (\\\\\\\\mathbf{x}**{\\\\\\\\text{i}}) and (\\\\\\\\dot{\\\\\\\\mathbf{x}}**single static obstacle**cylinder**{\\\\\\\\text{b}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) assigned as the Cartesian coordinate of the top surface center of the cylinder. In this sense, the DMP state can be represented as a ten\\\\-dimensional vector (\\\\\\\\mathcal{X}**{t}^{\\\\\\\\top},\\\\\\\\dot{\\\\\\\\mathbf{x}}**{t}^{\\\\\\\\top}\\\\-\\\\\\\\mathbf{x}**{t}!\\\\-!\\\\\\\\mathbf{x}** The actuation function (\\\\\\\\mathbf{f}) does not only depend on the canonical variable (\\\\\\\\zeta\\\\_{t}) but also on the internal states of the DMP, namely (\\\\\\\\mathbf{x}*{t}) and (\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}), and the obstacle position (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}). Different from the conventional DMP models, this may change the poles or the closed\\\\-loop dynamics of the Multi\\\\-DoF DMP, which can be both an advantage and a drawback. On the one hand, the state\\\\-dependent actuation function (\\\\\\\\mathbf{f}) can be learned to automatically adjust the poles of the DMP model, such that its dynamic properties are not completely determined by the hyperparameters (K*{\\\\\\\\alpha}) and (K\\\\_{\\\\\\\\beta}). This improves the flexibility of agent training. On the other hand, the DMP model may lose its inherent stability. In this paper, we set action limits (\\\\\\\\underline{f}\\\\\\\\leq\\\\\\\\mathbf{f}(\\\\\\\\mathcal{X}\\\\_{t})\\\\\\\\leq\\\\\\\\overline{f}) to avoid this.\\n* The gain of the actuation function is the absolute distance between the initial position (\\\\\\\\mathbf{x}*{i}) and the goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}), (\\\\|\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}\\\\-\\\\\\\\mathbf{x}*{i}\\\\|), instead of the element\\\\-wise distance used by the conventional DMP (1\\\\). Such a scheme can improve the flexibility of a DMP model by fully incorporating the coupling of its different dimensions.\\n* The obstacle position (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}) is included in the state (\\\\\\\\mathcal{X}*{t}) of the DMP, instead of being incorporated as an additional virtual force term as the conventional DMP model. This provides the possibility of incorporating more complicated obstacle information into motion planning.  \\n- The actuation function (\\\\\\\\mathbf{f}) does not only depend on the canonical variable (\\\\\\\\zeta\\\\_{t}) but also on the internal states of the DMP, namely (\\\\\\\\mathbf{x}*{t}) and (\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}), and the obstacle position (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}). Different from the conventional DMP models, this may change the poles or the closed\\\\-loop dynamics of the Multi\\\\-DoF DMP, which can be both an advantage and a drawback. On the one hand, the state\\\\-dependent actuation function (\\\\\\\\mathbf{f}) can be learned to automatically adjust the poles of the DMP model, such that its dynamic properties are not completely determined by the hyperparameters (K*{\\\\\\\\alpha}) and (K\\\\_{\\\\\\\\beta}). This improves the flexibility of agent training. On the other hand, the DMP model may lose its inherent stability. In this paper, we set action limits (\\\\\\\\underline{f}\\\\\\\\leq\\\\\\\\mathbf{f}(\\\\\\\\mathcal{X}\\\\_{t})\\\\\\\\leq\\\\\\\\overline{f}) to avoid this.\\n*{t}) and (\\\\\\\\dot{\\\\\\\\mathbf{x}}**{\\\\\\\\text{b}}). Different from the conventional DMP models, this may change the poles or the closed\\\\-loop dynamics of the Multi\\\\-DoF DMP, which can be both an advantage and a drawback. On the one hand, the state\\\\-dependent actuation function (\\\\\\\\mathbf{f}) can be learned to automatically adjust the poles of the DMP model, such that its dynamic properties are not completely determined by the hyperparameters (K*- The gain of the actuation function is the absolute distance between the initial position (\\\\\\\\mathbf{x}*{i}) and the goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}), (\\\\|\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}\\\\-\\\\\\\\mathbf{x}*{i}\\\\|), instead of the element\\\\-wise distance used by the conventional DMP (1\\\\). Such a scheme can improve the flexibility of a DMP model by fully incorporating the coupling of its different dimensions.\\n*{i}) and the goal position (\\\\\\\\mathbf{x}**{\\\\\\\\text{g}}\\\\-\\\\\\\\mathbf{x}*- The obstacle position (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}) is included in the state (\\\\\\\\mathcal{X}*{t}) of the DMP, instead of being incorporated as an additional virtual force term as the conventional DMP model. This provides the possibility of incorporating more complicated obstacle information into motion planning.\\n*{\\\\\\\\text{b}}) is included in the state (\\\\\\\\mathcal{X}*### *Cost Function of Motion Planning*  \\n*Cost Function of Motion Planning*This paper solves a general robot motion planning problem. Specifically, given an initial position (\\\\\\\\mathbf{x}*{i}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) and a goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) in the Cartesian space, generate a smooth trajectory (\\\\\\\\mathbf{x}*{t}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) for (0!\\\\<!t!\\\\\\\\leq!T) using the Multi\\\\-DoF DMP model in (9\\\\), such that (\\\\\\\\mathbf{x}*{T}) is sufficiently close to (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}), where (T!\\\\\\\\in!\\\\\\\\mathbb{R}^{\\\\+}) is the predefined timing length of the trajectory. The generated trajectory should also ensure sufficient smoothness (minimal acceleration) and maintain a certain distance with a given static obstacle positioned in (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{2}). These requirements are encoded in the following cost function,  \\n*{i}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) and a goal position (\\\\\\\\mathbf{x}**{t}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) for (0!\\\\<!t!\\\\\\\\leq!T) using the Multi\\\\-DoF DMP model in (9\\\\), such that (\\\\\\\\mathbf{x}**{\\\\\\\\text{g}}), where (T!\\\\\\\\in!\\\\\\\\mathbb{R}^{\\\\+}) is the predefined timing length of the trajectory. The generated trajectory should also ensure sufficient smoothness (minimal acceleration) and maintain a certain distance with a given static obstacle positioned in (\\\\\\\\mathbf{x}*\\\\[\\\\\\\\mathcal{J}*{t}\\\\=\\\\\\\\left{\\\\\\\\begin{array}{ll}\\\\\\\\sum*{i\\\\=1}^{4}\\\\\\\\alpha\\\\_{i}\\\\\\\\mathcal{J} *{t}^{(i)},\\\\&0\\\\\\\\leq t\\\\<T,\\\\\\\\ \\\\\\\\alpha*{5}\\\\\\\\mathcal{J}\\\\_{t}^{(5\\\\)},\\\\&t\\\\=T,\\\\\\\\end{array}\\\\\\\\right. \\\\\\\\tag{10}]  \\n*{t}\\\\=\\\\\\\\left{\\\\\\\\begin{array}{ll}\\\\\\\\sum**{t}^{(i)},\\\\&0\\\\\\\\leq t\\\\<T,\\\\\\\\ \\\\\\\\alpha*where (\\\\\\\\alpha\\\\_{i}!\\\\\\\\in!\\\\\\\\mathbb{R}^{\\\\+}) are constant parameters, (i\\\\=1,2,\\\\\\\\cdots,5\\\\), and (\\\\\\\\mathcal{J}\\\\_{t}^{(i)}!\\\\\\\\in!\\\\\\\\mathbb{R}) are instant costs at time (t), defined as  \\n\\\\[\\\\\\\\mathcal{J}*{t}^{(1\\\\)}!\\\\=!!\\\\\\\\left\\\\|\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}\\\\\\\\right\\\\|^{2},\\\\\\\\ \\\\\\\\mathcal{J}*{t}^{(2\\\\)}!\\\\=!\\\\\\\\left\\\\|\\\\\\\\mathbf{x}*{t}!\\\\-!\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}\\\\\\\\right\\\\|^ {2},] \\\\[\\\\\\\\mathcal{J}*{t}^{(4\\\\)}!\\\\=!\\\\\\\\left{!\\\\\\\\begin{array}{ll}\\\\\\\\eta! \\\\\\\\left(!\\\\\\\\left\\\\|\\\\\\\\mathbf{x}*{t}^{(1,2\\\\)}!\\\\-!\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}^{(1,2\\\\)}! \\\\\\\\right\\\\|!\\\\-!\\\\\\\\tau\\\\_{\\\\\\\\text{b}}!,\\\\\\\\varepsilon\\\\_{0}^{\\\\\\\\text{b}}!,\\\\\\\\varepsilon\\\\_{1}^{ \\\\\\\\text{b}}\\\\\\\\right)\\\\&\\\\\\\\text{if }0!\\\\<!\\\\\\\\mathbf{x}*{t}^{(3\\\\)}!\\\\<!\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}^{(3\\\\)}\\\\\\\\ 0\\\\&\\\\\\\\text{otherwise},\\\\\\\\end{array}\\\\\\\\right.] \\\\[\\\\\\\\mathcal{J}*{t}^{(3\\\\)}\\\\=\\\\\\\\eta!\\\\\\\\left(!\\\\\\\\mathbf{x}*{t}^{(3\\\\)}, \\\\\\\\varepsilon\\\\_{0}^{\\\\\\\\text{d}},\\\\\\\\varepsilon\\\\_{1}^{\\\\\\\\text{d}}\\\\\\\\right),\\\\\\\\ \\\\\\\\mathcal{J}*{t}^{(5\\\\)}\\\\=\\\\\\\\xi(\\\\|\\\\\\\\mathbf{x}*{t}!\\\\-!\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}\\\\|, \\\\\\\\varepsilon*{T}),]  \\n*{t}^{(1\\\\)}!\\\\=!!\\\\\\\\left\\\\|\\\\\\\\dot{\\\\\\\\mathbf{x}}**{t}^{(2\\\\)}!\\\\=!\\\\\\\\left\\\\|\\\\\\\\mathbf{x}**{\\\\\\\\text{g}}\\\\\\\\right\\\\|^ {2},] \\\\[\\\\\\\\mathcal{J}**{t}^{(1,2\\\\)}!\\\\-!\\\\\\\\mathbf{x}**{t}^{(3\\\\)}!\\\\<!\\\\\\\\mathbf{x}**{t}^{(3\\\\)}\\\\=\\\\\\\\eta!\\\\\\\\left(!\\\\\\\\mathbf{x}**{t}^{(5\\\\)}\\\\=\\\\\\\\xi(\\\\|\\\\\\\\mathbf{x}**{\\\\\\\\text{g}}\\\\|, \\\\\\\\varepsilon*where (\\\\\\\\mathbf{x}*{t}^{(1,2\\\\)}!\\\\\\\\in!\\\\\\\\mathbb{R}^{2}), (\\\\\\\\mathbf{x}*{t}^{(3\\\\)}!\\\\\\\\in!\\\\\\\\mathbb{R}), (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}^{(1,2\\\\)}!\\\\\\\\in!\\\\\\\\mathbb{R}^{2}), (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}^{(3\\\\)}!\\\\\\\\in!\\\\\\\\mathbb{R}) are the slides of vectors (\\\\\\\\mathbf{x}*{t}) and (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}), (r\\\\_{\\\\\\\\text{b}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{\\\\+}) is the radius of the cylinder, (\\\\\\\\eta:\\\\\\\\mathbb{R}!\\\\\\\\rightarrow!\\\\\\\\mathbb{R}*{\\\\\\\\geq 0}) with constant parameters (0!\\\\<!\\\\\\\\varepsilon*{0}!\\\\<!\\\\\\\\varepsilon\\\\_{1}) is an artificial potential field function defined as  \\n*{t}^{(1,2\\\\)}!\\\\\\\\in!\\\\\\\\mathbb{R}^{2}), (\\\\\\\\mathbf{x}**{\\\\\\\\text{b}}^{(1,2\\\\)}!\\\\\\\\in!\\\\\\\\mathbb{R}^{2}), (\\\\\\\\mathbf{x}**{t}) and (\\\\\\\\mathbf{x}**{\\\\\\\\geq 0}) with constant parameters (0!\\\\<!\\\\\\\\varepsilon*\\\\[\\\\\\\\eta(x,\\\\\\\\varepsilon\\\\_{0},\\\\\\\\varepsilon\\\\_{1})!\\\\=!\\\\\\\\left{!\\\\\\\\begin{array}{ll}(x!\\\\- !\\\\\\\\varepsilon\\\\_{0})^{\\\\-2}!\\\\-!(\\\\\\\\varepsilon\\\\_{1}!\\\\-!\\\\\\\\varepsilon\\\\_{0})^{\\\\-2},\\\\& \\\\\\\\varepsilon\\\\_{0}!\\\\<!x!\\\\<!\\\\\\\\varepsilon\\\\_{1},\\\\\\\\ 0,\\\\&x!\\\\\\\\geq!\\\\\\\\varepsilon\\\\_{1},\\\\\\\\end{array}\\\\\\\\right. \\\\\\\\tag{11}]  \\n(0!\\\\<!\\\\\\\\varepsilon\\\\_{0}^{\\\\\\\\text{b}}!\\\\<!\\\\\\\\varepsilon\\\\_{1}^{\\\\\\\\text{b}}) and (0!\\\\<!\\\\\\\\varepsilon\\\\_{0}^{\\\\\\\\text{d}}!\\\\<!\\\\\\\\varepsilon\\\\_{1}^{\\\\\\\\text{d}}) are potential field parameters for the obstacle and the ground, and (\\\\\\\\xi:\\\\\\\\mathbb{R}!\\\\\\\\rightarrow!\\\\\\\\mathbb{R}\\\\_{\\\\\\\\geq 0}) is a squared dead\\\\-zone function defined as  \\n\\\\[\\\\\\\\xi(x,\\\\\\\\varepsilon)\\\\=\\\\\\\\left{\\\\\\\\begin{array}{ll}0,\\\\&0\\\\\\\\leq x\\\\\\\\leq\\\\\\\\varepsilon,\\\\\\\\ x\\\\-\\\\\\\\varepsilon,\\\\&x\\\\>\\\\\\\\varepsilon,\\\\\\\\end{array}\\\\\\\\right. \\\\\\\\tag{12}]  \\nwhere (\\\\\\\\varepsilon!\\\\\\\\in!\\\\\\\\mathbb{R}^{\\\\+}) is the dead\\\\-zone scalar. Here, (\\\\\\\\eta(x)) serves as an artificial potential field function that penalizes (x) if it gets close to (\\\\\\\\varepsilon\\\\_{0}) from the positive direction. Another parameter (\\\\\\\\varepsilon\\\\_{1}) is the upper limit of (x) such that (\\\\\\\\eta(x)) has an effect. To avoid undefinition, we assign (\\\\\\\\eta(x)) with a large number when (x!\\\\\\\\leq!\\\\\\\\varepsilon\\\\_{0}).  \\nIn the comprehensive cost function (10\\\\), term (\\\\\\\\mathcal{J}\\\\_{t}^{(1\\\\)}) penalizes the value of the acceleration to ensure sufficient  \\nFigure 2: The illustration of the IBC\\\\-DMP RL framework, where (\\\\\\\\mathcal{X}*{t}) denotes the position, velocity, and acceleration of the Multi\\\\-DoF DMP model, (\\\\\\\\tilde{\\\\\\\\mathbf{f}}(\\\\\\\\mathcal{X}*{t})) and (\\\\\\\\mathbf{f}(\\\\\\\\mathcal{X}*{t})) are the actuation functions provided by the human demonstration and the IBC\\\\-DMP agent, respectively, and (s*{t}) and (a\\\\_{t}) are the state and action data provided by human demonstration or generated by the IBC\\\\-DMP agent. Besides, the solid arrows denote the interactions, the dotted arrows indicate data storing, and the dashed arrow represents agent training.\\nsmoothness. Term (\\\\\\\\mathcal{J}*{t}^{(2\\\\)}) penalizes the distance between the current position (\\\\\\\\mathbf{x}*{t}) and the goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}), aiming at fast and straightforward goal reaching. Terms (\\\\\\\\mathcal{J}*{t}^{(3\\\\)}) and (\\\\\\\\mathcal{J}*{t}^{(4\\\\)}) attempt to keep the position (\\\\\\\\mathbf{x}*{t}) away from the ground ((z\\\\=0\\\\) plane) and the cylinder obstacle, respectively. The usage of artificial potential field functions ensures a very large cost when (\\\\\\\\mathbf{x}*{t}) runs into the cylinder obstacle or the ground. The last term (\\\\\\\\mathcal{J}*{T}^{(5\\\\)}) penalizes the distance between the ultimate position (\\\\\\\\mathbf{x}*{T}) and the goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}). A dead\\\\-zone scalar (\\\\\\\\varepsilon\\\\_{T}) is involved to prescribe the tolerable level of the ultimate error. Specifically, the ultimate cost is exerted only when the ultimate error exceeds the threshold (\\\\\\\\varepsilon\\\\_{T}). By minimizing the comprehensive cost (10\\\\) for the Multi\\\\-DoF DMP model (9\\\\), one can get a smooth trajectory (\\\\\\\\mathbf{x}*{t}) that is able to reach the goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}) at the ending time (t\\\\=T) while avoiding collision with the ground and the obstacle.  \\n*{t}) denotes the position, velocity, and acceleration of the Multi\\\\-DoF DMP model, (\\\\\\\\tilde{\\\\\\\\mathbf{f}}(\\\\\\\\mathcal{X}**{t})) are the actuation functions provided by the human demonstration and the IBC\\\\-DMP agent, respectively, and (s**{t}^{(2\\\\)}) penalizes the distance between the current position (\\\\\\\\mathbf{x}**{\\\\\\\\text{g}}), aiming at fast and straightforward goal reaching. Terms (\\\\\\\\mathcal{J}**{t}^{(4\\\\)}) attempt to keep the position (\\\\\\\\mathbf{x}**{t}) runs into the cylinder obstacle or the ground. The last term (\\\\\\\\mathcal{J}**{T}) and the goal position (\\\\\\\\mathbf{x}**{t}) that is able to reach the goal position (\\\\\\\\mathbf{x}*### *Decision\\\\-Making Problem Statement*  \\n*Decision\\\\-Making Problem Statement*Consider that the Multi\\\\-DoF DMP model (9\\\\) is discretized in time in a zero\\\\-order hold manner with a discrete sampling time (\\\\\\\\Delta t!\\\\\\\\in!\\\\\\\\mathbb{R}^{\\\\+}), as follows,  \\n\\\\[\\\\\\\\begin{split}\\\\\\\\mathbf{x}*{t\\\\+\\\\\\\\Delta t}!\\\\=!\\\\\\\\mathbf{x}*{t}\\\\+\\\\\\\\Delta t \\\\\\\\hat{\\\\\\\\mathbf{x}}*{t},\\\\\\\\ \\\\\\\\tilde{\\\\\\\\mathbf{x}}*{t\\\\+\\\\\\\\Delta t}!\\\\=!\\\\\\\\tilde{\\\\\\\\mathbf{x}}*{t}\\\\+\\\\\\\\Delta t\\\\\\\\tilde{ \\\\\\\\mathbf{x}}*{t},\\\\\\\\ \\\\\\\\zeta\\\\_{t\\\\+\\\\\\\\Delta t}!\\\\=!\\\\\\\\zeta\\\\_{t}\\\\-\\\\\\\\frac{\\\\\\\\Delta t}{\\\\\\\\tau}\\\\\\\\omega\\\\\\\\zeta \\\\_{t},\\\\\\\\end{split} \\\\\\\\tag{13}]  \\n*{t\\\\+\\\\\\\\Delta t}!\\\\=!\\\\\\\\mathbf{x}**{t},\\\\\\\\ \\\\\\\\tilde{\\\\\\\\mathbf{x}}**{t}\\\\+\\\\\\\\Delta t\\\\\\\\tilde{ \\\\\\\\mathbf{x}}*where (\\\\\\\\tilde{\\\\\\\\mathbf{x}}*{t}) is given by (9\\\\). Then, the generated trajectory (\\\\\\\\mathbf{x}*{t}) is a set of waypoints at the sampled timing points (\\\\\\\\mathbf{t}\\\\={0,\\\\\\\\Delta t,2\\\\\\\\Delta t,\\\\\\\\cdots,T}). In this sense, the objective of motion planning is to solve the optimal actuation function (\\\\\\\\mathbf{f}) in (9\\\\) such that the accumulated cost (\\\\\\\\sum\\\\_{t\\\\=0}^{T}\\\\\\\\mathcal{J}*{t}) is minimized. This problem renders decision\\\\-making over an MDP (\\\\\\\\mathcal{M}\\\\=(\\\\\\\\mathcal{S},\\\\\\\\mathcal{A},\\\\\\\\mathcal{F},\\\\\\\\mathcal{R})) defined in Sec. II\\\\-B, where (\\\\\\\\mathcal{S}\\\\={\\\\\\\\mathcal{X}*{t}\\\\|\\\\\\\\mathbf{x}*{i}\\\\\\\\in\\\\\\\\mathbb{R}^{3},\\\\\\\\,0\\\\\\\\leq t\\\\<T}) is the state space, (\\\\\\\\mathcal{A}!\\\\=!{\\\\\\\\mathbf{f}(s)\\\\\\\\,\\\\|\\\\\\\\forall\\\\\\\\,s\\\\\\\\in\\\\\\\\mathcal{S}}) is the action space, (\\\\\\\\mathcal{F}) is the state transition characterized by (13\\\\), and (\\\\\\\\mathcal{R}:\\\\-\\\\\\\\mathcal{J}*{t}) is the instant reward. In this sense, the policy (\\\\\\\\pi) can be represented as a parameterized actuation function (\\\\\\\\mathbf{f}*{\\\\\\\\theta}), modeled as a neural network and learned using an off\\\\-policy RL method introduced in Sec. II\\\\-B. Besides, BC introduced in Sec. II\\\\-C can be used to promote the training of the RL agent with the demonstration policy (\\\\\\\\mathbf{\\\\\\\\tilde{f}}), as addressed in Sec. II\\\\-C. Note that, different from the conventional imitation learning problems, this paper does not aim at using (\\\\\\\\mathbf{f}*{\\\\\\\\theta}) to approximate the demonstration policy (\\\\\\\\mathbf{\\\\\\\\tilde{f}}). Instead, (\\\\\\\\mathbf{\\\\\\\\tilde{f}}) is only used to improve the training of (\\\\\\\\mathbf{f}\\\\_{\\\\\\\\theta}).  \\n*{t}) is given by (9\\\\). Then, the generated trajectory (\\\\\\\\mathbf{x}**{t}) is minimized. This problem renders decision\\\\-making over an MDP (\\\\\\\\mathcal{M}\\\\=(\\\\\\\\mathcal{S},\\\\\\\\mathcal{A},\\\\\\\\mathcal{F},\\\\\\\\mathcal{R})) defined in Sec. II\\\\-B, where (\\\\\\\\mathcal{S}\\\\={\\\\\\\\mathcal{X}**{i}\\\\\\\\in\\\\\\\\mathbb{R}^{3},\\\\\\\\,0\\\\\\\\leq t\\\\<T}) is the state space, (\\\\\\\\mathcal{A}!\\\\=!{\\\\\\\\mathbf{f}(s)\\\\\\\\,\\\\|\\\\\\\\forall\\\\\\\\,s\\\\\\\\in\\\\\\\\mathcal{S}}) is the action space, (\\\\\\\\mathcal{F}) is the state transition characterized by (13\\\\), and (\\\\\\\\mathcal{R}:\\\\-\\\\\\\\mathcal{J}**{\\\\\\\\theta}), modeled as a neural network and learned using an off\\\\-policy RL method introduced in Sec. II\\\\-B. Besides, BC introduced in Sec. II\\\\-C can be used to promote the training of the RL agent with the demonstration policy (\\\\\\\\mathbf{\\\\\\\\tilde{f}}), as addressed in Sec. II\\\\-C. Note that, different from the conventional imitation learning problems, this paper does not aim at using (\\\\\\\\mathbf{f}*Now, we are ready to state the two main objectives of this paper: (1\\\\). transforming human demonstration into data that are compatible with the demonstration buffer, corresponding to the left column of Fig. 2, and (2\\\\). train the optimal policy (\\\\\\\\mathbf{f}) using the demonstration buffer, as the right column of Fig. 2\\\\. Our solutions to these two problems will be presented in Sec. IV and Sec. V, respectively.', children=[])]),\n",
       "  ArxivPaperSection(header='h2', title='IV Human Demonstration Collection', text=\"This section interprets how we collect the human demonstration data in a recording experiment and transform it into compatible data for the demonstration buffer. We have published the dataset at \\\\[59]. Note that the recorded human motion data are only for public educational purposes. No private information about the subject is disclosed. Also, the configuration of the data recording experiment does not cause any risk of harm to the subject. Therefore, this research is exempt from human\\\\-related ethic issues.\\n\\n### *Human Data Acquisition*\\n\\n*Human Data Acquisition*We use a data\\\\-recording experiment to collect human\\\\-hand motion in a point\\\\-to\\\\-point reaching (P2PR) task since it is a typical scenario of robot motion planning. The experiment is designed to capture how the human hand attempts to reach a goal position while avoiding collisions with a mid\\\\-way obstacle. An HTC VIVE tracking system, consisting of several motion trackers and a pair of virtual reality (VR) base stations is used to record the position of any object attached to the trackers. The system is able to track the Cartesian pose of the rigid body in a stable sampling frequency (100\\\\\\\\,\\\\\\\\mathrm{Hz}). The tracking precision can be as high as (1\\\\\\\\,\\\\\\\\mathrm{mm}).\\n\\nThe top view of the experiment configuration is shown in Fig. 3\\\\. A human subject sits in front of a plat desk. The desk is (0\\\\.35\\\\\\\\,\\\\\\\\mathrm{m}) vertically higher than the seat such that the subject is seated in a comfortable manner. A rectangular planar motion region is marked on the desk with height (0\\\\.5\\\\\\\\,\\\\\\\\mathrm{mm}) and width (0\\\\.6\\\\\\\\,\\\\\\\\mathrm{m}), shown as a thick\\\\-line gray box in Fig. 3\\\\. The motion region is aligned with the desk edge towards the subject. The subject's seat is placed such that the hand can be comfortably positioned in the left\\\\-bottom corner of the motion region and can naturally reach any point within the motion region without standing up. Two VR base stations are placed on the diagonal corners of the workspace for superior tracking precision, as shown as camera symbols in Fig. 3\\\\. The coordinate of the tracking system is calibrated such that its origin coincides with the left\\\\-bottom corner of the workspace and its (x)\\\\-, (y)\\\\-, and (z)\\\\-axis point to the right, front, and top of the subject, respectively. We set the initial position (\\\\\\\\mathcal{I}) as the one that the hand is comfortably placed at the origin. For a given goal (\\\\\\\\mathcal{G}) and an obstacle (\\\\\\\\mathcal{O}), the subject is required to move the hand from the initial position (\\\\\\\\mathcal{I}) to the goal (\\\\\\\\mathcal{G}) in a natural manner while avoiding the obstacle (\\\\\\\\mathcal{O}). A possible hand trajectory is illustrated as a dashed arrow in Fig. 3\\\\.\\n\\nFor the development of the IBC\\\\-DMP agent in this paper, we recorded a human demonstration dataset using the experimental setup described above. Fig. 4 illustrates the data\\n\\nFigure 3: The top view of the configuration of the data recording experiment, where (\\\\\\\\mathcal{I}), (\\\\\\\\mathcal{O}), and (\\\\\\\\mathcal{G}) are the initial, obstacle, and goal positions, respectively.\\nrecording process. As illustrated in the figure, we use three trackers to mark the positions of the obstacle, the human hand, and the goal, respectively. In the experiment, we use a cylinder bottle with a height of (66\\\\\\\\,\\\\\\\\text{mm}) and a diameter of (200\\\\\\\\,\\\\\\\\text{mm}) as the obstacle. We fix the tracker to the top of the obstacle, leading to a total height of (70\\\\\\\\,\\\\\\\\text{mm}). Besides, we tie a tracker to the wrist of the subject to track the motion of the hand. The third tracker is directly placed on the desk to serve as the goal. During the entire recording process, the subject performs (600\\\\) trials of P2PR motion repeatedly. For each trial, the goal (\\\\\\\\mathcal{G}) is placed at a random position in the motion region. It should be not too close to the initial position (\\\\\\\\mathcal{I}) to ensure the feasibility of the hand motion. The obstacle (\\\\\\\\mathcal{O}) is also positioned at a random point close to the mid\\\\-way between the initial position (\\\\\\\\mathcal{I}) and the goal (\\\\\\\\mathcal{G}). Note that the obstacle should be neither beside the goal nor too close to the initial position. Otherwise, it would be very difficult to produce a natural trajectory. The subject is asked to always perform the motion and avoid the obstacle in the most comfortable manner. Other than that, there are no restrictions from which direction the subject must avoid the obstacle. The subject is allowed to bend the torso but cannot stand up from the seat to reach a far point.\\n\\nAfter removing the invalid motions, we finally obtain (544\\\\) hand trajectories corresponding to various goal positions and obstacle positions, as illustrated in Fig. (a)a. Each trajectory contains the hand position of one P2PR motion. It can be seen that different trajectories show great variety in their shapes. Taller trajectories indicate that the hand goes over the obstacle and short ones pass around the obstacles. A common feature of these trajectories is that they all successfully avoid obstacles while maintaining the smoothness of the shape. Fig. (b)b gives a more clear perspective of how a hand trajectory reaches the target and avoids the obstacle.\\n\\n### *Data Preprocessing: Normalization*\\n\\n*Data Preprocessing: Normalization*The diversity of human hand trajectories is reflected by both their shapes and speeds. The shape of a trajectory prescribes in which direction the hand should move to avoid the obstacle, and its speed indicates how fast the hand moves. The diversity of the shapes is beneficial to improve the robustness of the trained planning agent against various obstacles and goal positions. However, the speeds of the hand trajectories do not contribute to the training robustness but bring up disturbances. Thus, a preprocessing procedure is needed to normalize the trajectory speeds while retaining the diversity of the shapes. In this paper, we are concerned with the average speed of a trajectory. Let (\\\\\\\\mathbf{\\\\\\\\tau}\\\\={0,\\\\\\\\Delta\\\\\\\\tau,2\\\\\\\\Delta\\\\\\\\tau,\\\\\\\\cdots,\\\\\\\\bar{T}}) be the time stamps of a recorded hand trajectory (\\\\\\\\mathbf{x}*{\\\\\\\\tau}^{H}\\\\\\\\in\\\\\\\\mathbb{R}^{3}), (\\\\\\\\tau\\\\\\\\in\\\\\\\\mathbf{\\\\\\\\tau}), where (\\\\\\\\Delta\\\\\\\\tau\\\\=0\\\\.01\\\\\\\\,\\\\\\\\text{s}) is the sampling time of the VIVE tracking system. Then, the average speed is defined as (\\\\\\\\bar{V}\\\\=L/\\\\\\\\bar{T}) (m/s), where (\\\\\\\\bar{T}) and (L\\\\=\\\\\\\\sum*{\\\\\\\\tau\\\\=\\\\\\\\Delta\\\\\\\\tau}^{\\\\\\\\bar{T}}\\\\|\\\\\\\\mathbf{x}*{\\\\\\\\tau}^{H}\\\\-\\\\\\\\mathbf{x}*{\\\\\\\\tau\\\\- \\\\\\\\Delta\\\\\\\\tau}^{H}\\\\|\\\\_{2}) are the total time duration and the total length of the trajectory, respectively.\\n\\n*{\\\\\\\\tau}^{H}\\\\\\\\in\\\\\\\\mathbb{R}^{3}), (\\\\\\\\tau\\\\\\\\in\\\\\\\\mathbf{\\\\\\\\tau}), where (\\\\\\\\Delta\\\\\\\\tau\\\\=0\\\\.01\\\\\\\\,\\\\\\\\text{s}) is the sampling time of the VIVE tracking system. Then, the average speed is defined as (\\\\\\\\bar{V}\\\\=L/\\\\\\\\bar{T}) (m/s), where (\\\\\\\\bar{T}) and (L\\\\=\\\\\\\\sum**{\\\\\\\\tau}^{H}\\\\-\\\\\\\\mathbf{x}*We normalize the recorded trajectories to a uniform average speed (\\\\\\\\bar{V}*{\\\\\\\\ast}\\\\=0\\\\.2\\\\\\\\,\\\\\\\\text{m/s}) by assigning each trajectory a new time series (\\\\\\\\mathbf{\\\\\\\\tau}^{\\\\\\\\prime}\\\\={0,\\\\\\\\Delta\\\\\\\\tau^{\\\\\\\\prime},2\\\\\\\\Delta\\\\\\\\tau^{\\\\\\\\prime},\\\\\\\\cdots,\\\\\\\\bar {T}^{\\\\\\\\prime}}), where (\\\\\\\\Delta\\\\\\\\tau^{\\\\\\\\prime}\\\\=\\\\\\\\frac{L}{V*{\\\\\\\\ast}T}\\\\\\\\Delta\\\\\\\\tau) and (\\\\\\\\bar{T}^{\\\\\\\\prime}\\\\=L/\\\\\\\\bar{V}*{\\\\\\\\ast}). Then, we approximate the linear velocity of the trajectory as (\\\\\\\\dot{\\\\\\\\mathbf{x}}*{\\\\\\\\tau^{\\\\\\\\prime}}^{H}\\\\=\\\\\\\\frac{\\\\\\\\mathbf{x}*{\\\\\\\\tau^{\\\\\\\\prime}}^{H}\\\\- \\\\\\\\mathbf{x}*{\\\\\\\\tau^{\\\\\\\\prime}\\\\-\\\\\\\\Delta\\\\\\\\tau}^{H}}{\\\\\\\\Delta\\\\\\\\tau^{\\\\\\\\prime}}), (\\\\\\\\tau^{\\\\\\\\prime}\\\\=\\\\\\\\Delta\\\\\\\\tau^{\\\\\\\\prime}), (2\\\\\\\\Delta\\\\\\\\tau^{\\\\\\\\prime}), (\\\\\\\\cdots), (\\\\\\\\bar{T}^{\\\\\\\\prime}), and calculate the maximal speed (\\\\\\\\dot{V}\\\\=\\\\\\\\max\\\\_{i}\\\\|\\\\\\\\dot{\\\\\\\\mathbf{x}}*{\\\\\\\\tau^{\\\\\\\\prime}}^{H}\\\\|*{2}). Similar to the average speed, the maximal speed is an important index to describe the kinematic property of a trajectory. We analyze the statistical properties of the total length, the maximal speed, and the average speed of all trajectories to investigate how the normalization changes their diversities. Their mean values (Mean), the standard deviations (Std), and the extreme deviations (Max\\\\-Min) are shown in Tab. I. It can be seen that the average speeds of the trajectories are normalized and the maximal speeds are reduced. Also, the deviation values of the maximal speeds are also reduced, which indicates less influence of the diversity of the maximal speeds on the recorded trajectories. Meanwhile, the statistical properties of the total lengths are not changed due to the consistency of the shapes of the trajectories. Therefore, the disturbance of the speeds of the trajectories is reduced while the diversity of the shapes is reserved.\\n\\n*{\\\\\\\\ast}\\\\=0\\\\.2\\\\\\\\,\\\\\\\\text{m/s}) by assigning each trajectory a new time series (\\\\\\\\mathbf{\\\\\\\\tau}^{\\\\\\\\prime}\\\\={0,\\\\\\\\Delta\\\\\\\\tau^{\\\\\\\\prime},2\\\\\\\\Delta\\\\\\\\tau^{\\\\\\\\prime},\\\\\\\\cdots,\\\\\\\\bar {T}^{\\\\\\\\prime}}), where (\\\\\\\\Delta\\\\\\\\tau^{\\\\\\\\prime}\\\\=\\\\\\\\frac{L}{V**{\\\\\\\\ast}). Then, we approximate the linear velocity of the trajectory as (\\\\\\\\dot{\\\\\\\\mathbf{x}}**{\\\\\\\\tau^{\\\\\\\\prime}}^{H}\\\\- \\\\\\\\mathbf{x}**{\\\\\\\\tau^{\\\\\\\\prime}}^{H}\\\\|*### *Buffer Generation*\\n\\n*Buffer Generation*After the normalization process, we transform the normalized hand trajectories into demonstration data that are compatible with the experience replay buffer using the Multi\\\\-DoF DMP model, as illustrated in the left column of Fig. 2\\\\. To fit the sampling rate of the DMP model (\\\\\\\\Delta t), we use the time stamp series (\\\\\\\\mathbf{t}\\\\={0,\\\\\\\\Delta t,2\\\\\\\\Delta t,\\\\\\\\cdots,T}) to interpolate the normalized trajectory (\\\\\\\\mathbf{x}*{t}^{H}), (\\\\\\\\tau\\\\\\\\in\\\\\\\\boldsymbol{\\\\\\\\tau}). The interpolated trajectory is denoted as (\\\\\\\\mathbf{x}*{t}^{H}), (t\\\\\\\\in\\\\\\\\mathbf{t}).\\n\\n*{t}^{H}), (\\\\\\\\tau\\\\\\\\in\\\\\\\\boldsymbol{\\\\\\\\tau}). The interpolated trajectory is denoted as (\\\\\\\\mathbf{x}*As addressed in Sec. II\\\\-B, the data for the experience replay buffer have the format ({\\\\\\\\,s\\\\_{t},a\\\\_{t},r\\\\_{t},s^{\\\\\\\\prime}*{t},d*{t}\\\\\\\\,}), for a certain time (t\\\\\\\\in\\\\\\\\mathbb{N}^{\\\\+}), where the state (s\\\\_{t}\\\\=\\\\\\\\mathcal{X}*{t}) is also the state of the Multi\\\\-DoF DMP model as introduced in Sec. III\\\\-B.The action (a*{t}!\\\\=!\\\\\\\\tilde{\\\\\\\\mathbf{f}}(\\\\\\\\mathcal{X}*{t})) is the output of the demonstration policy (\\\\\\\\tilde{\\\\\\\\mathbf{f}}) which allows (\\\\\\\\mathbf{x}*{t}) to fit the Multi\\\\-DoF DMP model. Then, (s^{\\\\\\\\prime}!\\\\=!\\\\\\\\mathcal{X}*{t\\\\+\\\\\\\\Delta t}) is the successive state of (s*{t}) under the action (a\\\\_{t}), (r\\\\_{t}!\\\\=!\\\\-\\\\\\\\mathcal{J}*{t}) is the instant reward, and (d*{t}) is a binary value to determine whether (\\\\\\\\mathbf{x}*{t}) reaches the goal (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{g}}).\\n\\n*{t},d**{t}) is also the state of the Multi\\\\-DoF DMP model as introduced in Sec. III\\\\-B.The action (a**{t})) is the output of the demonstration policy (\\\\\\\\tilde{\\\\\\\\mathbf{f}}) which allows (\\\\\\\\mathbf{x}**{t\\\\+\\\\\\\\Delta t}) is the successive state of (s**{t}) is the instant reward, and (d**{t}) reaches the goal (\\\\\\\\mathbf{x}*The determination of action (a\\\\_{t}) is not trivial since the demonstration policy (\\\\\\\\tilde{\\\\\\\\mathbf{f}}) is not previously known. In conventional work, each action sample (a\\\\_{t}\\\\=\\\\\\\\tilde{\\\\\\\\mathbf{f}}(\\\\\\\\mathcal{X}*{t})) for a given state sample (\\\\\\\\mathcal{X}*{t}) is directly computed by inverting the DMP model (9\\\\). This requires the acceleration (\\\\\\\\tilde{\\\\\\\\mathbf{x}}*{t}) calculated via a twice\\\\-difference operation which brings differential noises to the action samples. As a result, the variance of the samples may be increased, which may disturb the learning process. In this paper, we use a PID\\\\-based approach to generate actions for the state samples. Specifically, by representing the positions and velocities of a human hand trajectory as (\\\\\\\\mathbf{x}*{t}^{H},\\\\\\\\tilde{\\\\\\\\mathbf{x}}*{t}^{H}\\\\\\\\in\\\\\\\\mathbb{R}^{3}), we use a Multi\\\\-DoF DMP model described by (9\\\\) and (13\\\\) with the following actuation function to generate a trajectory (\\\\\\\\mathbf{x}*{t}) that fits the hand trajectory (\\\\\\\\mathbf{x}\\\\_{t}^{H}),\\n\\n*{t})) for a given state sample (\\\\\\\\mathcal{X}**{t}) calculated via a twice\\\\-difference operation which brings differential noises to the action samples. As a result, the variance of the samples may be increased, which may disturb the learning process. In this paper, we use a PID\\\\-based approach to generate actions for the state samples. Specifically, by representing the positions and velocities of a human hand trajectory as (\\\\\\\\mathbf{x}**{t}^{H}\\\\\\\\in\\\\\\\\mathbb{R}^{3}), we use a Multi\\\\-DoF DMP model described by (9\\\\) and (13\\\\) with the following actuation function to generate a trajectory (\\\\\\\\mathbf{x}*\\\\[\\\\\\\\tilde{\\\\\\\\mathbf{f}}(\\\\\\\\mathcal{X}*{t})\\\\=K*{\\\\\\\\mathrm{P}}(\\\\\\\\mathbf{x}*{t}^{H}!\\\\-! \\\\\\\\mathbf{x}*{t})\\\\+K\\\\_{\\\\\\\\mathrm{D}}(\\\\\\\\hat{\\\\\\\\mathbf{x}}*{t}^{H}!\\\\-!\\\\\\\\tilde{\\\\\\\\mathbf{x }}*{t}), \\\\\\\\tag{14}]\\n\\n*{t})\\\\=K**{t}^{H}!\\\\-! \\\\\\\\mathbf{x}**{t}^{H}!\\\\-!\\\\\\\\tilde{\\\\\\\\mathbf{x }}*with initial and goal conditions (\\\\\\\\mathbf{x}*{0}!\\\\=!\\\\\\\\mathbf{x}*{0}^{H}), (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{g}}!\\\\=!\\\\\\\\mathbf{x}*{t}^{H}), and (\\\\\\\\hat{\\\\\\\\mathbf{x}}*{0}!\\\\=!0\\\\), where (K*{\\\\\\\\mathrm{P}}!\\\\=!1500I) and (K\\\\_{\\\\\\\\mathrm{P}}!\\\\=!40I) are constant matrices, and (I!\\\\\\\\in!\\\\\\\\mathbb{R}^{3\\\\\\\\times 3}) is an identity matrix. The main advantage of the PID\\\\-based method is not requiring the acceleration (\\\\\\\\tilde{\\\\\\\\mathbf{x}}*{t}). Thus, it can reduce the noise introduced to the samples. With proper parameters (K*{\\\\\\\\mathrm{P}}), (K\\\\_{\\\\\\\\mathrm{P}}), the generated trajectory (\\\\\\\\mathbf{x}*{t}) coincides with the recorded hand trajectory (\\\\\\\\tilde{\\\\\\\\mathbf{x}}*{t}) with small errors (\\\\\\\\mathbf{x}*{t}^{H}\\\\-\\\\\\\\mathbf{x}*{t}), which ensures the efficacy of the action samples.\\n\\n*{0}!\\\\=!\\\\\\\\mathbf{x}**{\\\\\\\\mathrm{g}}!\\\\=!\\\\\\\\mathbf{x}**{0}!\\\\=!0\\\\), where (K**{t}). Thus, it can reduce the noise introduced to the samples. With proper parameters (K**{t}) coincides with the recorded hand trajectory (\\\\\\\\tilde{\\\\\\\\mathbf{x}}**{t}^{H}\\\\-\\\\\\\\mathbf{x}*Having determined the action sample (a\\\\_{t}) for the state sample (s\\\\_{t}), the successive state (s^{\\\\\\\\prime}*{t}) can also be calculated using the Multi\\\\-DoF DMP model. The reward (r*{t}) can be calculated using (\\\\-\\\\\\\\mathcal{J}*{t}) from the reward function (10\\\\). Then, the termination flag (d*{t}) is determined as follows,\\n\\n*{t}) can also be calculated using the Multi\\\\-DoF DMP model. The reward (r**{t}) from the reward function (10\\\\). Then, the termination flag (d*\\\\[d\\\\_{t}\\\\=\\\\\\\\left{\\\\\\\\begin{array}{ll}1\\\\&\\\\\\\\mathrm{if}\\\\\\\\ t\\\\=\\\\\\\\tilde{T}\\\\\\\\mathrm{or}\\\\\\\\,\\\\| \\\\\\\\mathbf{x}*{t}\\\\-\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{g}}\\\\|*{2}\\\\\\\\leq\\\\\\\\varepsilon*{T},\\\\\\\\ 0\\\\&\\\\\\\\mathrm{otherwise},\\\\\\\\end{array}\\\\\\\\right. \\\\\\\\tag{15}]\\n\\n*{t}\\\\-\\\\\\\\mathbf{x}**{2}\\\\\\\\leq\\\\\\\\varepsilon*where (\\\\\\\\tilde{T}!\\\\=!5\\\\\\\\,\\\\\\\\mathrm{s}) is the maximal time length of an episode.\\n\\nWe ultimately transform 544 recorded hand trajectories into 123171 samples. The parameters of the Multi\\\\-DoF DMP model and the cost function (\\\\\\\\mathcal{J}\\\\_{t}) are shown in Tab. II.\\n\\n\", children=[ArxivPaperSection(header='p', title='', text='This section interprets how we collect the human demonstration data in a recording experiment and transform it into compatible data for the demonstration buffer. We have published the dataset at \\\\[59]. Note that the recorded human motion data are only for public educational purposes. No private information about the subject is disclosed. Also, the configuration of the data recording experiment does not cause any risk of harm to the subject. Therefore, this research is exempt from human\\\\-related ethic issues.', children=[]), ArxivPaperSection(header='h3', title='*Human Data Acquisition*', text=\"*Human Data Acquisition*We use a data\\\\-recording experiment to collect human\\\\-hand motion in a point\\\\-to\\\\-point reaching (P2PR) task since it is a typical scenario of robot motion planning. The experiment is designed to capture how the human hand attempts to reach a goal position while avoiding collisions with a mid\\\\-way obstacle. An HTC VIVE tracking system, consisting of several motion trackers and a pair of virtual reality (VR) base stations is used to record the position of any object attached to the trackers. The system is able to track the Cartesian pose of the rigid body in a stable sampling frequency (100\\\\\\\\,\\\\\\\\mathrm{Hz}). The tracking precision can be as high as (1\\\\\\\\,\\\\\\\\mathrm{mm}).  \\nThe top view of the experiment configuration is shown in Fig. 3\\\\. A human subject sits in front of a plat desk. The desk is (0\\\\.35\\\\\\\\,\\\\\\\\mathrm{m}) vertically higher than the seat such that the subject is seated in a comfortable manner. A rectangular planar motion region is marked on the desk with height (0\\\\.5\\\\\\\\,\\\\\\\\mathrm{mm}) and width (0\\\\.6\\\\\\\\,\\\\\\\\mathrm{m}), shown as a thick\\\\-line gray box in Fig. 3\\\\. The motion region is aligned with the desk edge towards the subject. The subject's seat is placed such that the hand can be comfortably positioned in the left\\\\-bottom corner of the motion region and can naturally reach any point within the motion region without standing up. Two VR base stations are placed on the diagonal corners of the workspace for superior tracking precision, as shown as camera symbols in Fig. 3\\\\. The coordinate of the tracking system is calibrated such that its origin coincides with the left\\\\-bottom corner of the workspace and its (x)\\\\-, (y)\\\\-, and (z)\\\\-axis point to the right, front, and top of the subject, respectively. We set the initial position (\\\\\\\\mathcal{I}) as the one that the hand is comfortably placed at the origin. For a given goal (\\\\\\\\mathcal{G}) and an obstacle (\\\\\\\\mathcal{O}), the subject is required to move the hand from the initial position (\\\\\\\\mathcal{I}) to the goal (\\\\\\\\mathcal{G}) in a natural manner while avoiding the obstacle (\\\\\\\\mathcal{O}). A possible hand trajectory is illustrated as a dashed arrow in Fig. 3\\\\.  \\nFor the development of the IBC\\\\-DMP agent in this paper, we recorded a human demonstration dataset using the experimental setup described above. Fig. 4 illustrates the data  \\nFigure 3: The top view of the configuration of the data recording experiment, where (\\\\\\\\mathcal{I}), (\\\\\\\\mathcal{O}), and (\\\\\\\\mathcal{G}) are the initial, obstacle, and goal positions, respectively.\\nrecording process. As illustrated in the figure, we use three trackers to mark the positions of the obstacle, the human hand, and the goal, respectively. In the experiment, we use a cylinder bottle with a height of (66\\\\\\\\,\\\\\\\\text{mm}) and a diameter of (200\\\\\\\\,\\\\\\\\text{mm}) as the obstacle. We fix the tracker to the top of the obstacle, leading to a total height of (70\\\\\\\\,\\\\\\\\text{mm}). Besides, we tie a tracker to the wrist of the subject to track the motion of the hand. The third tracker is directly placed on the desk to serve as the goal. During the entire recording process, the subject performs (600\\\\) trials of P2PR motion repeatedly. For each trial, the goal (\\\\\\\\mathcal{G}) is placed at a random position in the motion region. It should be not too close to the initial position (\\\\\\\\mathcal{I}) to ensure the feasibility of the hand motion. The obstacle (\\\\\\\\mathcal{O}) is also positioned at a random point close to the mid\\\\-way between the initial position (\\\\\\\\mathcal{I}) and the goal (\\\\\\\\mathcal{G}). Note that the obstacle should be neither beside the goal nor too close to the initial position. Otherwise, it would be very difficult to produce a natural trajectory. The subject is asked to always perform the motion and avoid the obstacle in the most comfortable manner. Other than that, there are no restrictions from which direction the subject must avoid the obstacle. The subject is allowed to bend the torso but cannot stand up from the seat to reach a far point.  \\nAfter removing the invalid motions, we finally obtain (544\\\\) hand trajectories corresponding to various goal positions and obstacle positions, as illustrated in Fig. (a)a. Each trajectory contains the hand position of one P2PR motion. It can be seen that different trajectories show great variety in their shapes. Taller trajectories indicate that the hand goes over the obstacle and short ones pass around the obstacles. A common feature of these trajectories is that they all successfully avoid obstacles while maintaining the smoothness of the shape. Fig. (b)b gives a more clear perspective of how a hand trajectory reaches the target and avoids the obstacle.\", children=[]), ArxivPaperSection(header='h3', title='*Data Preprocessing: Normalization*', text='*Data Preprocessing: Normalization*The diversity of human hand trajectories is reflected by both their shapes and speeds. The shape of a trajectory prescribes in which direction the hand should move to avoid the obstacle, and its speed indicates how fast the hand moves. The diversity of the shapes is beneficial to improve the robustness of the trained planning agent against various obstacles and goal positions. However, the speeds of the hand trajectories do not contribute to the training robustness but bring up disturbances. Thus, a preprocessing procedure is needed to normalize the trajectory speeds while retaining the diversity of the shapes. In this paper, we are concerned with the average speed of a trajectory. Let (\\\\\\\\mathbf{\\\\\\\\tau}\\\\={0,\\\\\\\\Delta\\\\\\\\tau,2\\\\\\\\Delta\\\\\\\\tau,\\\\\\\\cdots,\\\\\\\\bar{T}}) be the time stamps of a recorded hand trajectory (\\\\\\\\mathbf{x}*{\\\\\\\\tau}^{H}\\\\\\\\in\\\\\\\\mathbb{R}^{3}), (\\\\\\\\tau\\\\\\\\in\\\\\\\\mathbf{\\\\\\\\tau}), where (\\\\\\\\Delta\\\\\\\\tau\\\\=0\\\\.01\\\\\\\\,\\\\\\\\text{s}) is the sampling time of the VIVE tracking system. Then, the average speed is defined as (\\\\\\\\bar{V}\\\\=L/\\\\\\\\bar{T}) (m/s), where (\\\\\\\\bar{T}) and (L\\\\=\\\\\\\\sum*{\\\\\\\\tau\\\\=\\\\\\\\Delta\\\\\\\\tau}^{\\\\\\\\bar{T}}\\\\|\\\\\\\\mathbf{x}*{\\\\\\\\tau}^{H}\\\\-\\\\\\\\mathbf{x}*{\\\\\\\\tau\\\\- \\\\\\\\Delta\\\\\\\\tau}^{H}\\\\|\\\\_{2}) are the total time duration and the total length of the trajectory, respectively.  \\n*{\\\\\\\\tau}^{H}\\\\\\\\in\\\\\\\\mathbb{R}^{3}), (\\\\\\\\tau\\\\\\\\in\\\\\\\\mathbf{\\\\\\\\tau}), where (\\\\\\\\Delta\\\\\\\\tau\\\\=0\\\\.01\\\\\\\\,\\\\\\\\text{s}) is the sampling time of the VIVE tracking system. Then, the average speed is defined as (\\\\\\\\bar{V}\\\\=L/\\\\\\\\bar{T}) (m/s), where (\\\\\\\\bar{T}) and (L\\\\=\\\\\\\\sum**{\\\\\\\\tau}^{H}\\\\-\\\\\\\\mathbf{x}*We normalize the recorded trajectories to a uniform average speed (\\\\\\\\bar{V}*{\\\\\\\\ast}\\\\=0\\\\.2\\\\\\\\,\\\\\\\\text{m/s}) by assigning each trajectory a new time series (\\\\\\\\mathbf{\\\\\\\\tau}^{\\\\\\\\prime}\\\\={0,\\\\\\\\Delta\\\\\\\\tau^{\\\\\\\\prime},2\\\\\\\\Delta\\\\\\\\tau^{\\\\\\\\prime},\\\\\\\\cdots,\\\\\\\\bar {T}^{\\\\\\\\prime}}), where (\\\\\\\\Delta\\\\\\\\tau^{\\\\\\\\prime}\\\\=\\\\\\\\frac{L}{V*{\\\\\\\\ast}T}\\\\\\\\Delta\\\\\\\\tau) and (\\\\\\\\bar{T}^{\\\\\\\\prime}\\\\=L/\\\\\\\\bar{V}*{\\\\\\\\ast}). Then, we approximate the linear velocity of the trajectory as (\\\\\\\\dot{\\\\\\\\mathbf{x}}*{\\\\\\\\tau^{\\\\\\\\prime}}^{H}\\\\=\\\\\\\\frac{\\\\\\\\mathbf{x}*{\\\\\\\\tau^{\\\\\\\\prime}}^{H}\\\\- \\\\\\\\mathbf{x}*{\\\\\\\\tau^{\\\\\\\\prime}\\\\-\\\\\\\\Delta\\\\\\\\tau}^{H}}{\\\\\\\\Delta\\\\\\\\tau^{\\\\\\\\prime}}), (\\\\\\\\tau^{\\\\\\\\prime}\\\\=\\\\\\\\Delta\\\\\\\\tau^{\\\\\\\\prime}), (2\\\\\\\\Delta\\\\\\\\tau^{\\\\\\\\prime}), (\\\\\\\\cdots), (\\\\\\\\bar{T}^{\\\\\\\\prime}), and calculate the maximal speed (\\\\\\\\dot{V}\\\\=\\\\\\\\max\\\\_{i}\\\\|\\\\\\\\dot{\\\\\\\\mathbf{x}}*{\\\\\\\\tau^{\\\\\\\\prime}}^{H}\\\\|*{2}). Similar to the average speed, the maximal speed is an important index to describe the kinematic property of a trajectory. We analyze the statistical properties of the total length, the maximal speed, and the average speed of all trajectories to investigate how the normalization changes their diversities. Their mean values (Mean), the standard deviations (Std), and the extreme deviations (Max\\\\-Min) are shown in Tab. I. It can be seen that the average speeds of the trajectories are normalized and the maximal speeds are reduced. Also, the deviation values of the maximal speeds are also reduced, which indicates less influence of the diversity of the maximal speeds on the recorded trajectories. Meanwhile, the statistical properties of the total lengths are not changed due to the consistency of the shapes of the trajectories. Therefore, the disturbance of the speeds of the trajectories is reduced while the diversity of the shapes is reserved.  \\n*{\\\\\\\\ast}\\\\=0\\\\.2\\\\\\\\,\\\\\\\\text{m/s}) by assigning each trajectory a new time series (\\\\\\\\mathbf{\\\\\\\\tau}^{\\\\\\\\prime}\\\\={0,\\\\\\\\Delta\\\\\\\\tau^{\\\\\\\\prime},2\\\\\\\\Delta\\\\\\\\tau^{\\\\\\\\prime},\\\\\\\\cdots,\\\\\\\\bar {T}^{\\\\\\\\prime}}), where (\\\\\\\\Delta\\\\\\\\tau^{\\\\\\\\prime}\\\\=\\\\\\\\frac{L}{V**{\\\\\\\\ast}). Then, we approximate the linear velocity of the trajectory as (\\\\\\\\dot{\\\\\\\\mathbf{x}}**{\\\\\\\\tau^{\\\\\\\\prime}}^{H}\\\\- \\\\\\\\mathbf{x}**{\\\\\\\\tau^{\\\\\\\\prime}}^{H}\\\\|*### *Buffer Generation*  \\n*Buffer Generation*After the normalization process, we transform the normalized hand trajectories into demonstration data that are compatible with the experience replay buffer using the Multi\\\\-DoF DMP model, as illustrated in the left column of Fig. 2\\\\. To fit the sampling rate of the DMP model (\\\\\\\\Delta t), we use the time stamp series (\\\\\\\\mathbf{t}\\\\={0,\\\\\\\\Delta t,2\\\\\\\\Delta t,\\\\\\\\cdots,T}) to interpolate the normalized trajectory (\\\\\\\\mathbf{x}*{t}^{H}), (\\\\\\\\tau\\\\\\\\in\\\\\\\\boldsymbol{\\\\\\\\tau}). The interpolated trajectory is denoted as (\\\\\\\\mathbf{x}*{t}^{H}), (t\\\\\\\\in\\\\\\\\mathbf{t}).  \\n*{t}^{H}), (\\\\\\\\tau\\\\\\\\in\\\\\\\\boldsymbol{\\\\\\\\tau}). The interpolated trajectory is denoted as (\\\\\\\\mathbf{x}*As addressed in Sec. II\\\\-B, the data for the experience replay buffer have the format ({\\\\\\\\,s\\\\_{t},a\\\\_{t},r\\\\_{t},s^{\\\\\\\\prime}*{t},d*{t}\\\\\\\\,}), for a certain time (t\\\\\\\\in\\\\\\\\mathbb{N}^{\\\\+}), where the state (s\\\\_{t}\\\\=\\\\\\\\mathcal{X}*{t}) is also the state of the Multi\\\\-DoF DMP model as introduced in Sec. III\\\\-B.The action (a*{t}!\\\\=!\\\\\\\\tilde{\\\\\\\\mathbf{f}}(\\\\\\\\mathcal{X}*{t})) is the output of the demonstration policy (\\\\\\\\tilde{\\\\\\\\mathbf{f}}) which allows (\\\\\\\\mathbf{x}*{t}) to fit the Multi\\\\-DoF DMP model. Then, (s^{\\\\\\\\prime}!\\\\=!\\\\\\\\mathcal{X}*{t\\\\+\\\\\\\\Delta t}) is the successive state of (s*{t}) under the action (a\\\\_{t}), (r\\\\_{t}!\\\\=!\\\\-\\\\\\\\mathcal{J}*{t}) is the instant reward, and (d*{t}) is a binary value to determine whether (\\\\\\\\mathbf{x}*{t}) reaches the goal (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{g}}).  \\n*{t},d**{t}) is also the state of the Multi\\\\-DoF DMP model as introduced in Sec. III\\\\-B.The action (a**{t})) is the output of the demonstration policy (\\\\\\\\tilde{\\\\\\\\mathbf{f}}) which allows (\\\\\\\\mathbf{x}**{t\\\\+\\\\\\\\Delta t}) is the successive state of (s**{t}) is the instant reward, and (d**{t}) reaches the goal (\\\\\\\\mathbf{x}*The determination of action (a\\\\_{t}) is not trivial since the demonstration policy (\\\\\\\\tilde{\\\\\\\\mathbf{f}}) is not previously known. In conventional work, each action sample (a\\\\_{t}\\\\=\\\\\\\\tilde{\\\\\\\\mathbf{f}}(\\\\\\\\mathcal{X}*{t})) for a given state sample (\\\\\\\\mathcal{X}*{t}) is directly computed by inverting the DMP model (9\\\\). This requires the acceleration (\\\\\\\\tilde{\\\\\\\\mathbf{x}}*{t}) calculated via a twice\\\\-difference operation which brings differential noises to the action samples. As a result, the variance of the samples may be increased, which may disturb the learning process. In this paper, we use a PID\\\\-based approach to generate actions for the state samples. Specifically, by representing the positions and velocities of a human hand trajectory as (\\\\\\\\mathbf{x}*{t}^{H},\\\\\\\\tilde{\\\\\\\\mathbf{x}}*{t}^{H}\\\\\\\\in\\\\\\\\mathbb{R}^{3}), we use a Multi\\\\-DoF DMP model described by (9\\\\) and (13\\\\) with the following actuation function to generate a trajectory (\\\\\\\\mathbf{x}*{t}) that fits the hand trajectory (\\\\\\\\mathbf{x}\\\\_{t}^{H}),  \\n*{t})) for a given state sample (\\\\\\\\mathcal{X}**{t}) calculated via a twice\\\\-difference operation which brings differential noises to the action samples. As a result, the variance of the samples may be increased, which may disturb the learning process. In this paper, we use a PID\\\\-based approach to generate actions for the state samples. Specifically, by representing the positions and velocities of a human hand trajectory as (\\\\\\\\mathbf{x}**{t}^{H}\\\\\\\\in\\\\\\\\mathbb{R}^{3}), we use a Multi\\\\-DoF DMP model described by (9\\\\) and (13\\\\) with the following actuation function to generate a trajectory (\\\\\\\\mathbf{x}*\\\\[\\\\\\\\tilde{\\\\\\\\mathbf{f}}(\\\\\\\\mathcal{X}*{t})\\\\=K*{\\\\\\\\mathrm{P}}(\\\\\\\\mathbf{x}*{t}^{H}!\\\\-! \\\\\\\\mathbf{x}*{t})\\\\+K\\\\_{\\\\\\\\mathrm{D}}(\\\\\\\\hat{\\\\\\\\mathbf{x}}*{t}^{H}!\\\\-!\\\\\\\\tilde{\\\\\\\\mathbf{x }}*{t}), \\\\\\\\tag{14}]  \\n*{t})\\\\=K**{t}^{H}!\\\\-! \\\\\\\\mathbf{x}**{t}^{H}!\\\\-!\\\\\\\\tilde{\\\\\\\\mathbf{x }}*with initial and goal conditions (\\\\\\\\mathbf{x}*{0}!\\\\=!\\\\\\\\mathbf{x}*{0}^{H}), (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{g}}!\\\\=!\\\\\\\\mathbf{x}*{t}^{H}), and (\\\\\\\\hat{\\\\\\\\mathbf{x}}*{0}!\\\\=!0\\\\), where (K*{\\\\\\\\mathrm{P}}!\\\\=!1500I) and (K\\\\_{\\\\\\\\mathrm{P}}!\\\\=!40I) are constant matrices, and (I!\\\\\\\\in!\\\\\\\\mathbb{R}^{3\\\\\\\\times 3}) is an identity matrix. The main advantage of the PID\\\\-based method is not requiring the acceleration (\\\\\\\\tilde{\\\\\\\\mathbf{x}}*{t}). Thus, it can reduce the noise introduced to the samples. With proper parameters (K*{\\\\\\\\mathrm{P}}), (K\\\\_{\\\\\\\\mathrm{P}}), the generated trajectory (\\\\\\\\mathbf{x}*{t}) coincides with the recorded hand trajectory (\\\\\\\\tilde{\\\\\\\\mathbf{x}}*{t}) with small errors (\\\\\\\\mathbf{x}*{t}^{H}\\\\-\\\\\\\\mathbf{x}*{t}), which ensures the efficacy of the action samples.  \\n*{0}!\\\\=!\\\\\\\\mathbf{x}**{\\\\\\\\mathrm{g}}!\\\\=!\\\\\\\\mathbf{x}**{0}!\\\\=!0\\\\), where (K**{t}). Thus, it can reduce the noise introduced to the samples. With proper parameters (K**{t}) coincides with the recorded hand trajectory (\\\\\\\\tilde{\\\\\\\\mathbf{x}}**{t}^{H}\\\\-\\\\\\\\mathbf{x}*Having determined the action sample (a\\\\_{t}) for the state sample (s\\\\_{t}), the successive state (s^{\\\\\\\\prime}*{t}) can also be calculated using the Multi\\\\-DoF DMP model. The reward (r*{t}) can be calculated using (\\\\-\\\\\\\\mathcal{J}*{t}) from the reward function (10\\\\). Then, the termination flag (d*{t}) is determined as follows,  \\n*{t}) can also be calculated using the Multi\\\\-DoF DMP model. The reward (r**{t}) from the reward function (10\\\\). Then, the termination flag (d*\\\\[d\\\\_{t}\\\\=\\\\\\\\left{\\\\\\\\begin{array}{ll}1\\\\&\\\\\\\\mathrm{if}\\\\\\\\ t\\\\=\\\\\\\\tilde{T}\\\\\\\\mathrm{or}\\\\\\\\,\\\\| \\\\\\\\mathbf{x}*{t}\\\\-\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{g}}\\\\|*{2}\\\\\\\\leq\\\\\\\\varepsilon*{T},\\\\\\\\ 0\\\\&\\\\\\\\mathrm{otherwise},\\\\\\\\end{array}\\\\\\\\right. \\\\\\\\tag{15}]  \\n*{t}\\\\-\\\\\\\\mathbf{x}**{2}\\\\\\\\leq\\\\\\\\varepsilon*where (\\\\\\\\tilde{T}!\\\\=!5\\\\\\\\,\\\\\\\\mathrm{s}) is the maximal time length of an episode.  \\nWe ultimately transform 544 recorded hand trajectories into 123171 samples. The parameters of the Multi\\\\-DoF DMP model and the cost function (\\\\\\\\mathcal{J}\\\\_{t}) are shown in Tab. II.', children=[])]),\n",
       "  ArxivPaperSection(header='h2', title='V Training of the IBC-DMP agent', text='Having converted the recorded hand trajectories to the demonstration data, we propose the training method of the off\\\\-policy RL agent with demonstration\\\\-based BC. We first give an overview of the training method. Then, we specifically interpret two important technical points of the proposed method, namely actor loss reshaping and critic loss refinement. Finally, we give the algorithm for agent training.\\n\\n### *Overview of the Training Method for IBC\\\\-DMP RL*\\n\\n*Overview of the Training Method for IBC\\\\-DMP RL*The training process of the IBC\\\\-DMP agent is organized as a flow chart illustrated in Fig. 6\\\\. Similar to a traditional DDPG agent, the IBC\\\\-DMP agent consists of four forward neural networks (FNN), namely a source actor network (\\\\\\\\pi\\\\_{\\\\\\\\theta^{\\\\\\\\prime}}:\\\\\\\\mathcal{S}\\\\\\\\rightarrow\\\\\\\\mathcal{A}), a target actor network (\\\\\\\\pi\\\\_{\\\\\\\\theta^{\\\\\\\\prime}}:\\\\\\\\mathcal{S}\\\\\\\\rightarrow\\\\\\\\mathcal{A}), a source critic network (Q\\\\_{w}:\\\\\\\\mathcal{S}\\\\\\\\times\\\\\\\\mathcal{A}\\\\\\\\rightarrow\\\\\\\\mathbb{R}), and a target critic network (Q\\\\_{w^{\\\\\\\\prime}}:\\\\\\\\mathcal{S}\\\\\\\\times\\\\\\\\mathcal{A}\\\\\\\\rightarrow\\\\\\\\mathbb{R}), where the subscripts (\\\\\\\\theta), (\\\\\\\\theta^{\\\\\\\\prime}), (w) and (w^{\\\\\\\\prime}) denote the parameters of these networks. The source actor and critic networks are constructed to approximate the optimal policy and the value function of the RL problem. Meanwhile, the target networks are used to improve the stability of this approximation. Besides, the IBC\\\\-DMP agent is equipped with a dual\\\\-buffer structure, which is inspired by the previous work on off\\\\-policy RL \\\\[47, 58]. The *demo buffer* is used to store the demonstration data of the human motion recorded in Sec. IV and the *replay buffer* serves as a normal experience replay storage unit. The buffers are designed with fixed depths. Both buffers import the demonstration data from the recorded demo trials before the start of the training process. They also provide the batched data to calculate the losses for the FNNs. The training process is summarized as the following five steps.\\n\\n*demo buffer**replay buffer*#### V\\\\-A1 Demonstration Data Importing\\n\\nThe human demonstration data generated in Sec. IV are imported to the demonstration buffer and then shuffled. They are also imported to the interaction buffer to boot up the training process. The importing processes are denoted as solid arrows in Fig. 6\\\\. This step is executed only once at the very beginning of each training process.\\n\\n#### V\\\\-A2 Batch Sampling\\n\\nDuring the training process, four data batches are regularly sampled from the two buffers, denoted as (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{\\\\\\\\pi}), (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{Q}), (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{\\\\\\\\pi}), and (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{J}}^{Q}), respectively. The subscript (\\\\\\\\mathcal{D}) indicates that the batches are sampled from the Demo Buffer and (\\\\\\\\mathcal{I}) means from the Replay Buffer. The superscripts (\\\\\\\\pi) and (Q) denote that the batches are used to update the neural networks. The data in the batches are used to update the parameters of the actor and critic networks. The sampling is conducted individually and independently to eliminate the dependence between the samples. The sampling process is represented as double dashed arrows in Fig. 6\\\\. The sampling frequency of the data batches is usually the same as the update rate of the networks.\\n\\n*{\\\\\\\\mathrm{D}}^{\\\\\\\\pi}), (\\\\\\\\mathcal{B}**{\\\\\\\\mathrm{I}}^{\\\\\\\\pi}), and (\\\\\\\\mathcal{B}*#### Iv\\\\-A3 Network Updates\\n\\nThe sampled data batches are used to calculate the losses of the actor network and the critic network using loss functions (\\\\\\\\mathcal{L}*{A}) and (\\\\\\\\mathcal{L}*{C}), respectively. In this paper, we propose novel loss functions for the training of the neural networks of the IBC\\\\-DMP agent, which will be introduced in Sec. II\\\\-C. With the loss functions (\\\\\\\\mathcal{L}*{A}) and (\\\\\\\\mathcal{L}*{C}), the parameters of the source networks are updated using the gradient\\\\-based law in (6\\\\). Then, the target networks are updated using (3\\\\). This process is denoted by double arrows in Fig. 6\\\\.\\n\\n*{A}) and (\\\\\\\\mathcal{L}**{A}) and (\\\\\\\\mathcal{L}*#### Iv\\\\-A4 Experience Storing and Forgetting\\n\\nSimilar to all off\\\\-policy RL agents with experience replay, the IBC\\\\-DMP agent also stores its interaction data with the environment in the interaction buffer, at every sampling instant. As shown in Fig. 6, the latest interaction data is always added to the tail of the interaction buffer, right after the demonstration data. The storing process is represented as a dashed arrow. The interaction buffer has a fixed size such that the old data is forgotten and replaced by the new data. Thus, the demonstration data are ultimately purged from the interaction buffer and lose their impacts on the data batches (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{Q}) and (\\\\\\\\mathcal{B}*{1}^{Q}) as the learning proceeds.\\n\\n*{\\\\\\\\mathrm{D}}^{Q}) and (\\\\\\\\mathcal{B}*### *Reshaped Actor Loss Based on IBC*\\n\\n*Reshaped Actor Loss Based on IBC*One of the critical technical points of the proposed IBC\\\\-DMP agent is the proper design of the loss functions to train the networks. In this paper, we propose a reshaped loss function for the actor network,\\n\\n\\\\[\\\\\\\\mathcal{L}*{A}(\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{\\\\\\\\pi},\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{\\\\\\\\pi })\\\\=\\\\\\\\hat{\\\\\\\\mathcal{L}}*{A}(\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{\\\\\\\\pi})\\\\+\\\\\\\\lambda*{\\\\\\\\mathrm{BC }}\\\\\\\\mathcal{L}*{BC}(\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{\\\\\\\\pi})\\\\\\\\,, \\\\\\\\tag{16}]\\n\\n*{A}(\\\\\\\\mathcal{B}**{\\\\\\\\mathrm{I}}^{\\\\\\\\pi })\\\\=\\\\\\\\hat{\\\\\\\\mathcal{L}}**{\\\\\\\\mathrm{I}}^{\\\\\\\\pi})\\\\+\\\\\\\\lambda**{BC}(\\\\\\\\mathcal{B}*where (\\\\\\\\hat{\\\\\\\\mathcal{L}}*{A}(\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{\\\\\\\\pi})) has the same form as a conventional actor loss function defined in (5\\\\), (\\\\\\\\mathcal{L}*{BC}(\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{\\\\\\\\pi})) is a novel IBC loss function and (\\\\\\\\lambda\\\\_{\\\\\\\\mathrm{BC}}\\\\\\\\in\\\\\\\\mathbb{R}^{\\\\+}) is the BC ratio to adjust the proportion of (\\\\\\\\lambda\\\\_{\\\\\\\\mathrm{BC}}) in the overall actor loss (\\\\\\\\mathcal{L}\\\\_{A}). For any data buffer (\\\\\\\\mathcal{B}) sized (n\\\\\\\\in\\\\\\\\mathbb{N}^{\\\\+}), the IBC loss is calculated as\\n\\n*{A}(\\\\\\\\mathcal{B}**{BC}(\\\\\\\\mathcal{B}*\\\\[\\\\\\\\mathcal{L}*{BC}(\\\\\\\\mathcal{B})!\\\\=!\\\\-!\\\\\\\\tfrac{1}{n}!\\\\\\\\sum*{j\\\\=1}^{n}E\\\\_{w}(\\\\\\\\pi\\\\_{ \\\\\\\\theta}\\\\|s\\\\_{j},a\\\\_{j}), \\\\\\\\tag{17}]\\n\\n*{BC}(\\\\\\\\mathcal{B})!\\\\=!\\\\-!\\\\\\\\tfrac{1}{n}!\\\\\\\\sum*where (E\\\\_{w}(\\\\\\\\pi\\\\_{\\\\\\\\theta}\\\\|s\\\\_{j},a\\\\_{j})) is an energy function of the current policy (\\\\\\\\pi\\\\_{\\\\\\\\theta}) defined as\\n\\n\\\\[E\\\\_{w}(\\\\\\\\pi\\\\_{\\\\\\\\theta}\\\\|s\\\\_{j},a\\\\_{j})!\\\\=!\\\\-!\\\\\\\\mathrm{ReLU}(Q\\\\_{w}(s\\\\_{j},a\\\\_{j})!\\\\-!Q *{w}(s*{j},\\\\\\\\pi\\\\_{\\\\\\\\theta}(s\\\\_{j})))\\\\\\\\,, \\\\\\\\tag{18}]\\n\\n*{w}(s*where ({s\\\\_{j},a\\\\_{j}}) are the (j)\\\\-th state and action samples of (\\\\\\\\mathcal{B}), (j\\\\=0,1,\\\\\\\\cdots,n), and (\\\\\\\\mathrm{ReLU}(x)\\\\=\\\\\\\\max(x,0\\\\)), (x\\\\\\\\in\\\\\\\\mathbb{R}), is a Rectified Linear Unit (ReLU) function.\\n\\nThe actor loss function in (16\\\\) consists of two parts. The first part (\\\\\\\\hat{\\\\\\\\mathcal{L}}*{A}) is defined on the interaction data batch (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{\\\\\\\\pi}) and has the same form as the conventional actor loss function of an off\\\\-policy RL agent. The second part (\\\\\\\\mathcal{L}*{BC}) is a novel IBC\\\\-based loss function defined on the demonstration data batch defined on the interaction data batch (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{\\\\\\\\pi}). It is used to penalize the current policy (\\\\\\\\pi\\\\_{\\\\\\\\theta}) if produces a worse action (\\\\\\\\pi\\\\_{\\\\\\\\theta}(s\\\\_{j})) than the demonstration action (a\\\\_{j}). As a result, it forces the policy (\\\\\\\\pi\\\\_{\\\\\\\\theta}) to perform better than the demonstration policy during the training process. The extent of this effect is controlled by the BC ratio (\\\\\\\\lambda\\\\_{\\\\\\\\mathrm{BC}}). In this sense, BC is seamlessly integrated into the training process of the actor network, which leads to a flexible manner of updating the policies.\\n\\n*{A}) is defined on the interaction data batch (\\\\\\\\mathcal{B}**{BC}) is a novel IBC\\\\-based loss function defined on the demonstration data batch defined on the interaction data batch (\\\\\\\\mathcal{B}*The form of the BC loss function (17\\\\) is inspired by the EBC\\\\-based RL in previous work which penalizes the deviation between the current policy (\\\\\\\\pi\\\\_{\\\\\\\\theta}) and the demonstration policy \\\\[34]. Nevertheless, (17\\\\) adopts the IBC technology which penalizes a certain energy function of the current policy \\\\[35]. In this paper, the energy function is selected as the deviation between the value functions of the current policy and the demonstration policy. Meanwhile, a ReLU operation is exerted to the deviation since no penalty is needed if the current policy performs better than the demonstration policy. The IBC\\\\-based loss function is dedicated to improving the value of the current policy instead of its similarity to the demonstration policy. For example, the penalty may not be exerted even if the two policies (a\\\\_{j}) and (\\\\\\\\pi\\\\_{\\\\\\\\theta}(s\\\\_{j})) are not the same, as long as (\\\\\\\\pi\\\\_{\\\\\\\\theta}(s\\\\_{j})) is superior to (a\\\\_{j}) according to the current value (Q\\\\_{w}). Therefore, the IBC\\\\-based loss function is expected to have a better capability of avoiding overfitting the demonstration policy compared to the EBC\\\\-based one.\\n\\n### *Refined Critic Loss*\\n\\n*Refined Critic Loss*The IBC loss in (5\\\\) depends on the parameter (w) of the critic network (Q\\\\_{w}). If the critic network (Q\\\\_{w}) is not well trained, the\\n\\nFigure 6: The flow chart of the training process of the IBC\\\\-DMP agent. The circled numbers indicate the critical steps of the training procedure which are explained in Sec. V\\\\-A.\\nBC loss (\\\\\\\\mathcal{L}*{BC}) may not be able to accurately capture the penalty that should be imposed on the current policy (\\\\\\\\pi*{\\\\\\\\theta}), leading to an invalid loss that does not help the policy update. This is especially likely to occur in the initial stage of the learning process. To solve this problem, we use a refined data batch (\\\\\\\\mathcal{B}\\\\=\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{Q}\\\\\\\\cup\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{Q}) to compose the loss function for the critic network, (\\\\\\\\mathcal{L}*{C}\\\\\\\\left(\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{Q}\\\\\\\\cup\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{Q}\\\\\\\\right)), where the form of (\\\\\\\\mathcal{L}*{C}) is defined in (4\\\\). Here, we refer to (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{Q}\\\\\\\\cup\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{Q}) as a refined data batch since it has a larger proportion of demonstration data compared to an interaction data batch (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{Q}) of the same size. The ratio between the sizes of (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{Q}) and (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{Q}) is referred to as the refining factor (\\\\\\\\lambda*{\\\\\\\\mathrm{RF}}). Here, we assume that human demonstration data are very likely to have higher values than random data. Therefore, a refined data batch with a larger (\\\\\\\\lambda\\\\_{\\\\\\\\mathrm{RF}}) is likely to better describe the true critic loss and more helpful to the booting up of the training of the critic network. However, an overlarge (\\\\\\\\lambda\\\\_{\\\\\\\\mathrm{RF}}) may lead to overfitting of the human demonstration. In this paper, we select (\\\\\\\\lambda\\\\_{\\\\\\\\mathrm{RF}}) as a constant. For better performance, the demo batch (\\\\\\\\mathcal{B}\\\\_{\\\\\\\\mathrm{D}}^{Q}) can be gradually eliminated from (\\\\\\\\mathcal{B}^{Q}) as the learning proceeds, which renders a decreasing refining factor. This can be an interesting topic for future work.\\n\\n*{BC}) may not be able to accurately capture the penalty that should be imposed on the current policy (\\\\\\\\pi**{\\\\\\\\mathrm{D}}^{Q}\\\\\\\\cup\\\\\\\\mathcal{B}**{C}\\\\\\\\left(\\\\\\\\mathcal{B}**{\\\\\\\\mathrm{I}}^{Q}\\\\\\\\right)), where the form of (\\\\\\\\mathcal{L}**{\\\\\\\\mathrm{D}}^{Q}\\\\\\\\cup\\\\\\\\mathcal{B}**{\\\\\\\\mathrm{I}}^{Q}) of the same size. The ratio between the sizes of (\\\\\\\\mathcal{B}**{\\\\\\\\mathrm{I}}^{Q}) is referred to as the refining factor (\\\\\\\\lambda*### *The Training Algorithm for An IBC\\\\-DMP Agent*\\n\\n*The Training Algorithm for An IBC\\\\-DMP Agent*The training procedure of the IBC\\\\-DMP agent is formulated as Algorithm 1, where (N) is the total number of episodes during the training process, (T) is the ending time of an episode, and (\\\\\\\\epsilon\\\\_{t}\\\\\\\\sim\\\\\\\\mathcal{N}(0,\\\\\\\\sigma^{2})) is a random variable for a certain time (t\\\\=0,1,\\\\\\\\cdots,T). Algorithm 1 inherits the following common points from the conventional off\\\\-policy RL method.\\n\\n* Initialization of the four neural networks (lines 2 and 3\\\\).\\n* The random sampling of actions (line 9\\\\).\\n* Storing interaction data (line 13\\\\).\\n* Sampling data from the interaction buffer (line 16\\\\).\\n* The update methods for the networks (lines 19 and 20\\\\).\\n\\n- Initialization of the four neural networks (lines 2 and 3\\\\).\\n- The random sampling of actions (line 9\\\\).\\n- Storing interaction data (line 13\\\\).\\n- Sampling data from the interaction buffer (line 16\\\\).\\n- The update methods for the networks (lines 19 and 20\\\\).\\nMeanwhile, Algorithm 1 is different from the conventional off\\\\-policy RL agent in the following aspects.\\n\\n* Initialization of the buffers (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}) and (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}) (line 1\\\\).\\n* A Multi\\\\-DoF DMP as the environment model (line 10\\\\).\\n* Sampling data from the demonstration buffer (line 15\\\\).\\n* The computation of loss functions (lines 17 and 18\\\\).\\n\\n- Initialization of the buffers (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}) and (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}) (line 1\\\\).\\n*{\\\\\\\\mathrm{D}}) and (\\\\\\\\mathcal{B}*- A Multi\\\\-DoF DMP as the environment model (line 10\\\\).\\n- Sampling data from the demonstration buffer (line 15\\\\).\\n- The computation of loss functions (lines 17 and 18\\\\).\\n**Input:** demonstration buffer (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}) and interaction buffer (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}})\\n\\n**Input:***{\\\\\\\\mathrm{D}}) and interaction buffer (\\\\\\\\mathcal{B}***Output:** trained policy parameter (\\\\\\\\theta)\\n\\n**Output:**\\n```\\n1:Import demonstration data to buffers \\\\(\\\\mathcal{B}_{\\\\mathrm{D}}\\\\), \\\\(\\\\mathcal{B}_{\\\\mathrm{I}}\\\\)\\n2:Randomly initialize the source networks \\\\(\\\\pi_{\\\\theta}\\\\), \\\\(Q_{w}\\\\)\\n3:Initialize the target networks \\\\(\\\\pi_{\\\\theta^{\\\\prime}}\\\\), \\\\(Q_{w^{\\\\prime}}\\\\) with \\\\(\\\\theta^{\\\\prime}\\\\!\\\\leftarrow\\\\!\\\\theta\\\\), \\\\(w^{\\\\prime}\\\\!\\\\leftarrow\\\\!w\\\\)\\n4:for\\\\(i\\\\gets 1\\\\)to\\\\(N\\\\)do\\n5: Sample the initial position \\\\(\\\\mathbf{x}_{\\\\mathrm{i}}\\\\), the obstacle position \\\\(\\\\mathbf{x}_{\\\\mathrm{b}}\\\\), and the goal position \\\\(\\\\mathbf{x}_{\\\\mathrm{g}}\\\\)\\n6: Initialize the state \\\\(s_{0}\\\\!=\\\\!\\\\mathcal{X}_{0}\\\\)\\n7:for\\\\(t\\\\gets 0\\\\)to\\\\(T\\\\)do\\n8: Observe the state \\\\(s_{t}\\\\!=\\\\!\\\\mathcal{X}_{t}\\\\)\\n9: Sample an action \\\\(a_{t}=\\\\pi_{\\\\theta}(s_{t})+\\\\epsilon_{t}\\\\)\\n10: Update the DMP model using (13)\\n11: Observe the successive state \\\\(s^{\\\\prime}_{t}\\\\!\\\\!=\\\\!\\\\mathcal{X}_{t+\\\\Delta t}\\\\)\\n12: Calculate the instant reward \\\\(r_{t}\\\\) and flag \\\\(d_{t}\\\\)\\n13: Store \\\\(\\\\{s_{t},a_{t},s^{\\\\prime}_{t},r_{t},d_{t}\\\\}\\\\) to \\\\(\\\\mathcal{B}_{\\\\mathrm{I}}\\\\)\\n14:ifUPDATE is truethen\\n15: Sample random batches \\\\(\\\\mathcal{B}_{\\\\mathrm{D}}^{\\\\pi}\\\\), \\\\(\\\\mathcal{B}_{\\\\mathrm{D}}^{Q}\\\\) from \\\\(\\\\mathcal{B}_{\\\\mathrm{D}}\\\\)\\n16: Sample random batches \\\\(\\\\mathcal{B}_{\\\\mathrm{I}}^{\\\\pi}\\\\), \\\\(\\\\mathcal{B}_{\\\\mathrm{I}}^{\\\\mathrm{D}}\\\\) from \\\\(\\\\mathcal{B}_{\\\\mathrm{I}}\\\\)\\n17: Calculate the actor loss \\\\(\\\\mathcal{L}_{A}(\\\\mathcal{B}_{\\\\mathrm{D}}^{Q},\\\\mathcal{B}_{\\\\mathrm{I}}^{\\\\pi})\\\\) using (16)\\n18: Calculate the critic loss \\\\(\\\\mathcal{L}_{B}\\\\!\\\\left(\\\\mathcal{B}_{\\\\mathrm{D}}^{Q}\\\\cup\\\\mathcal{B}_{\\\\mathrm{I}}^{Q}\\\\right)\\\\) using (4)\\n19: Update the source networks \\\\(\\\\pi_{\\\\theta}\\\\), \\\\(Q_{w}\\\\) using (6)\\n20: Update the target networks \\\\(\\\\pi_{\\\\theta^{\\\\prime}}\\\\), \\\\(Q_{w^{\\\\prime}}\\\\) using (3)\\n21:endif\\n22:\\\\(s_{t}\\\\gets s^{\\\\prime}_{t}\\\\)\\n23:endfor\\n24:endfor\\n\\n```\\n`1:Import demonstration data to buffers \\\\(\\\\mathcal{B}_{\\\\mathrm{D}}\\\\), \\\\(\\\\mathcal{B}_{\\\\mathrm{I}}\\\\)\\n2:Randomly initialize the source networks \\\\(\\\\pi_{\\\\theta}\\\\), \\\\(Q_{w}\\\\)\\n3:Initialize the target networks \\\\(\\\\pi_{\\\\theta^{\\\\prime}}\\\\), \\\\(Q_{w^{\\\\prime}}\\\\) with \\\\(\\\\theta^{\\\\prime}\\\\!\\\\leftarrow\\\\!\\\\theta\\\\), \\\\(w^{\\\\prime}\\\\!\\\\leftarrow\\\\!w\\\\)\\n4:for\\\\(i\\\\gets 1\\\\)to\\\\(N\\\\)do\\n5: Sample the initial position \\\\(\\\\mathbf{x}_{\\\\mathrm{i}}\\\\), the obstacle position \\\\(\\\\mathbf{x}_{\\\\mathrm{b}}\\\\), and the goal position \\\\(\\\\mathbf{x}_{\\\\mathrm{g}}\\\\)\\n6: Initialize the state \\\\(s_{0}\\\\!=\\\\!\\\\mathcal{X}_{0}\\\\)\\n7:for\\\\(t\\\\gets 0\\\\)to\\\\(T\\\\)do\\n8: Observe the state \\\\(s_{t}\\\\!=\\\\!\\\\mathcal{X}_{t}\\\\)\\n9: Sample an action \\\\(a_{t}=\\\\pi_{\\\\theta}(s_{t})+\\\\epsilon_{t}\\\\)\\n10: Update the DMP model using (13)\\n11: Observe the successive state \\\\(s^{\\\\prime}_{t}\\\\!\\\\!=\\\\!\\\\mathcal{X}_{t+\\\\Delta t}\\\\)\\n12: Calculate the instant reward \\\\(r_{t}\\\\) and flag \\\\(d_{t}\\\\)\\n13: Store \\\\(\\\\{s_{t},a_{t},s^{\\\\prime}_{t},r_{t},d_{t}\\\\}\\\\) to \\\\(\\\\mathcal{B}_{\\\\mathrm{I}}\\\\)\\n14:ifUPDATE is truethen\\n15: Sample random batches \\\\(\\\\mathcal{B}_{\\\\mathrm{D}}^{\\\\pi}\\\\), \\\\(\\\\mathcal{B}_{\\\\mathrm{D}}^{Q}\\\\) from \\\\(\\\\mathcal{B}_{\\\\mathrm{D}}\\\\)\\n16: Sample random batches \\\\(\\\\mathcal{B}_{\\\\mathrm{I}}^{\\\\pi}\\\\), \\\\(\\\\mathcal{B}_{\\\\mathrm{I}}^{\\\\mathrm{D}}\\\\) from \\\\(\\\\mathcal{B}_{\\\\mathrm{I}}\\\\)\\n17: Calculate the actor loss \\\\(\\\\mathcal{L}_{A}(\\\\mathcal{B}_{\\\\mathrm{D}}^{Q},\\\\mathcal{B}_{\\\\mathrm{I}}^{\\\\pi})\\\\) using (16)\\n18: Calculate the critic loss \\\\(\\\\mathcal{L}_{B}\\\\!\\\\left(\\\\mathcal{B}_{\\\\mathrm{D}}^{Q}\\\\cup\\\\mathcal{B}_{\\\\mathrm{I}}^{Q}\\\\right)\\\\) using (4)\\n19: Update the source networks \\\\(\\\\pi_{\\\\theta}\\\\), \\\\(Q_{w}\\\\) using (6)\\n20: Update the target networks \\\\(\\\\pi_{\\\\theta^{\\\\prime}}\\\\), \\\\(Q_{w^{\\\\prime}}\\\\) using (3)\\n21:endif\\n22:\\\\(s_{t}\\\\gets s^{\\\\prime}_{t}\\\\)\\n23:endfor\\n24:endfor`**Algorithm 1** The demo IBC\\\\-DMP DDPG algorithm\\n\\n**Algorithm 1**', children=[ArxivPaperSection(header='p', title='', text='Having converted the recorded hand trajectories to the demonstration data, we propose the training method of the off\\\\-policy RL agent with demonstration\\\\-based BC. We first give an overview of the training method. Then, we specifically interpret two important technical points of the proposed method, namely actor loss reshaping and critic loss refinement. Finally, we give the algorithm for agent training.', children=[]), ArxivPaperSection(header='h3', title='*Overview of the Training Method for IBC\\\\-DMP RL*', text='*Overview of the Training Method for IBC\\\\-DMP RL*The training process of the IBC\\\\-DMP agent is organized as a flow chart illustrated in Fig. 6\\\\. Similar to a traditional DDPG agent, the IBC\\\\-DMP agent consists of four forward neural networks (FNN), namely a source actor network (\\\\\\\\pi\\\\_{\\\\\\\\theta^{\\\\\\\\prime}}:\\\\\\\\mathcal{S}\\\\\\\\rightarrow\\\\\\\\mathcal{A}), a target actor network (\\\\\\\\pi\\\\_{\\\\\\\\theta^{\\\\\\\\prime}}:\\\\\\\\mathcal{S}\\\\\\\\rightarrow\\\\\\\\mathcal{A}), a source critic network (Q\\\\_{w}:\\\\\\\\mathcal{S}\\\\\\\\times\\\\\\\\mathcal{A}\\\\\\\\rightarrow\\\\\\\\mathbb{R}), and a target critic network (Q\\\\_{w^{\\\\\\\\prime}}:\\\\\\\\mathcal{S}\\\\\\\\times\\\\\\\\mathcal{A}\\\\\\\\rightarrow\\\\\\\\mathbb{R}), where the subscripts (\\\\\\\\theta), (\\\\\\\\theta^{\\\\\\\\prime}), (w) and (w^{\\\\\\\\prime}) denote the parameters of these networks. The source actor and critic networks are constructed to approximate the optimal policy and the value function of the RL problem. Meanwhile, the target networks are used to improve the stability of this approximation. Besides, the IBC\\\\-DMP agent is equipped with a dual\\\\-buffer structure, which is inspired by the previous work on off\\\\-policy RL \\\\[47, 58]. The *demo buffer* is used to store the demonstration data of the human motion recorded in Sec. IV and the *replay buffer* serves as a normal experience replay storage unit. The buffers are designed with fixed depths. Both buffers import the demonstration data from the recorded demo trials before the start of the training process. They also provide the batched data to calculate the losses for the FNNs. The training process is summarized as the following five steps.  \\n*demo buffer**replay buffer*#### V\\\\-A1 Demonstration Data Importing  \\nThe human demonstration data generated in Sec. IV are imported to the demonstration buffer and then shuffled. They are also imported to the interaction buffer to boot up the training process. The importing processes are denoted as solid arrows in Fig. 6\\\\. This step is executed only once at the very beginning of each training process.  \\n#### V\\\\-A2 Batch Sampling  \\nDuring the training process, four data batches are regularly sampled from the two buffers, denoted as (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{\\\\\\\\pi}), (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{Q}), (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{\\\\\\\\pi}), and (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{J}}^{Q}), respectively. The subscript (\\\\\\\\mathcal{D}) indicates that the batches are sampled from the Demo Buffer and (\\\\\\\\mathcal{I}) means from the Replay Buffer. The superscripts (\\\\\\\\pi) and (Q) denote that the batches are used to update the neural networks. The data in the batches are used to update the parameters of the actor and critic networks. The sampling is conducted individually and independently to eliminate the dependence between the samples. The sampling process is represented as double dashed arrows in Fig. 6\\\\. The sampling frequency of the data batches is usually the same as the update rate of the networks.  \\n*{\\\\\\\\mathrm{D}}^{\\\\\\\\pi}), (\\\\\\\\mathcal{B}**{\\\\\\\\mathrm{I}}^{\\\\\\\\pi}), and (\\\\\\\\mathcal{B}*#### Iv\\\\-A3 Network Updates  \\nThe sampled data batches are used to calculate the losses of the actor network and the critic network using loss functions (\\\\\\\\mathcal{L}*{A}) and (\\\\\\\\mathcal{L}*{C}), respectively. In this paper, we propose novel loss functions for the training of the neural networks of the IBC\\\\-DMP agent, which will be introduced in Sec. II\\\\-C. With the loss functions (\\\\\\\\mathcal{L}*{A}) and (\\\\\\\\mathcal{L}*{C}), the parameters of the source networks are updated using the gradient\\\\-based law in (6\\\\). Then, the target networks are updated using (3\\\\). This process is denoted by double arrows in Fig. 6\\\\.  \\n*{A}) and (\\\\\\\\mathcal{L}**{A}) and (\\\\\\\\mathcal{L}*#### Iv\\\\-A4 Experience Storing and Forgetting  \\nSimilar to all off\\\\-policy RL agents with experience replay, the IBC\\\\-DMP agent also stores its interaction data with the environment in the interaction buffer, at every sampling instant. As shown in Fig. 6, the latest interaction data is always added to the tail of the interaction buffer, right after the demonstration data. The storing process is represented as a dashed arrow. The interaction buffer has a fixed size such that the old data is forgotten and replaced by the new data. Thus, the demonstration data are ultimately purged from the interaction buffer and lose their impacts on the data batches (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{Q}) and (\\\\\\\\mathcal{B}*{1}^{Q}) as the learning proceeds.  \\n*{\\\\\\\\mathrm{D}}^{Q}) and (\\\\\\\\mathcal{B}*### *Reshaped Actor Loss Based on IBC*  \\n*Reshaped Actor Loss Based on IBC*One of the critical technical points of the proposed IBC\\\\-DMP agent is the proper design of the loss functions to train the networks. In this paper, we propose a reshaped loss function for the actor network,  \\n\\\\[\\\\\\\\mathcal{L}*{A}(\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{\\\\\\\\pi},\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{\\\\\\\\pi })\\\\=\\\\\\\\hat{\\\\\\\\mathcal{L}}*{A}(\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{\\\\\\\\pi})\\\\+\\\\\\\\lambda*{\\\\\\\\mathrm{BC }}\\\\\\\\mathcal{L}*{BC}(\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{\\\\\\\\pi})\\\\\\\\,, \\\\\\\\tag{16}]  \\n*{A}(\\\\\\\\mathcal{B}**{\\\\\\\\mathrm{I}}^{\\\\\\\\pi })\\\\=\\\\\\\\hat{\\\\\\\\mathcal{L}}**{\\\\\\\\mathrm{I}}^{\\\\\\\\pi})\\\\+\\\\\\\\lambda**{BC}(\\\\\\\\mathcal{B}*where (\\\\\\\\hat{\\\\\\\\mathcal{L}}*{A}(\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{\\\\\\\\pi})) has the same form as a conventional actor loss function defined in (5\\\\), (\\\\\\\\mathcal{L}*{BC}(\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{\\\\\\\\pi})) is a novel IBC loss function and (\\\\\\\\lambda\\\\_{\\\\\\\\mathrm{BC}}\\\\\\\\in\\\\\\\\mathbb{R}^{\\\\+}) is the BC ratio to adjust the proportion of (\\\\\\\\lambda\\\\_{\\\\\\\\mathrm{BC}}) in the overall actor loss (\\\\\\\\mathcal{L}\\\\_{A}). For any data buffer (\\\\\\\\mathcal{B}) sized (n\\\\\\\\in\\\\\\\\mathbb{N}^{\\\\+}), the IBC loss is calculated as  \\n*{A}(\\\\\\\\mathcal{B}**{BC}(\\\\\\\\mathcal{B}*\\\\[\\\\\\\\mathcal{L}*{BC}(\\\\\\\\mathcal{B})!\\\\=!\\\\-!\\\\\\\\tfrac{1}{n}!\\\\\\\\sum*{j\\\\=1}^{n}E\\\\_{w}(\\\\\\\\pi\\\\_{ \\\\\\\\theta}\\\\|s\\\\_{j},a\\\\_{j}), \\\\\\\\tag{17}]  \\n*{BC}(\\\\\\\\mathcal{B})!\\\\=!\\\\-!\\\\\\\\tfrac{1}{n}!\\\\\\\\sum*where (E\\\\_{w}(\\\\\\\\pi\\\\_{\\\\\\\\theta}\\\\|s\\\\_{j},a\\\\_{j})) is an energy function of the current policy (\\\\\\\\pi\\\\_{\\\\\\\\theta}) defined as  \\n\\\\[E\\\\_{w}(\\\\\\\\pi\\\\_{\\\\\\\\theta}\\\\|s\\\\_{j},a\\\\_{j})!\\\\=!\\\\-!\\\\\\\\mathrm{ReLU}(Q\\\\_{w}(s\\\\_{j},a\\\\_{j})!\\\\-!Q *{w}(s*{j},\\\\\\\\pi\\\\_{\\\\\\\\theta}(s\\\\_{j})))\\\\\\\\,, \\\\\\\\tag{18}]  \\n*{w}(s*where ({s\\\\_{j},a\\\\_{j}}) are the (j)\\\\-th state and action samples of (\\\\\\\\mathcal{B}), (j\\\\=0,1,\\\\\\\\cdots,n), and (\\\\\\\\mathrm{ReLU}(x)\\\\=\\\\\\\\max(x,0\\\\)), (x\\\\\\\\in\\\\\\\\mathbb{R}), is a Rectified Linear Unit (ReLU) function.  \\nThe actor loss function in (16\\\\) consists of two parts. The first part (\\\\\\\\hat{\\\\\\\\mathcal{L}}*{A}) is defined on the interaction data batch (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{\\\\\\\\pi}) and has the same form as the conventional actor loss function of an off\\\\-policy RL agent. The second part (\\\\\\\\mathcal{L}*{BC}) is a novel IBC\\\\-based loss function defined on the demonstration data batch defined on the interaction data batch (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{\\\\\\\\pi}). It is used to penalize the current policy (\\\\\\\\pi\\\\_{\\\\\\\\theta}) if produces a worse action (\\\\\\\\pi\\\\_{\\\\\\\\theta}(s\\\\_{j})) than the demonstration action (a\\\\_{j}). As a result, it forces the policy (\\\\\\\\pi\\\\_{\\\\\\\\theta}) to perform better than the demonstration policy during the training process. The extent of this effect is controlled by the BC ratio (\\\\\\\\lambda\\\\_{\\\\\\\\mathrm{BC}}). In this sense, BC is seamlessly integrated into the training process of the actor network, which leads to a flexible manner of updating the policies.  \\n*{A}) is defined on the interaction data batch (\\\\\\\\mathcal{B}**{BC}) is a novel IBC\\\\-based loss function defined on the demonstration data batch defined on the interaction data batch (\\\\\\\\mathcal{B}*The form of the BC loss function (17\\\\) is inspired by the EBC\\\\-based RL in previous work which penalizes the deviation between the current policy (\\\\\\\\pi\\\\_{\\\\\\\\theta}) and the demonstration policy \\\\[34]. Nevertheless, (17\\\\) adopts the IBC technology which penalizes a certain energy function of the current policy \\\\[35]. In this paper, the energy function is selected as the deviation between the value functions of the current policy and the demonstration policy. Meanwhile, a ReLU operation is exerted to the deviation since no penalty is needed if the current policy performs better than the demonstration policy. The IBC\\\\-based loss function is dedicated to improving the value of the current policy instead of its similarity to the demonstration policy. For example, the penalty may not be exerted even if the two policies (a\\\\_{j}) and (\\\\\\\\pi\\\\_{\\\\\\\\theta}(s\\\\_{j})) are not the same, as long as (\\\\\\\\pi\\\\_{\\\\\\\\theta}(s\\\\_{j})) is superior to (a\\\\_{j}) according to the current value (Q\\\\_{w}). Therefore, the IBC\\\\-based loss function is expected to have a better capability of avoiding overfitting the demonstration policy compared to the EBC\\\\-based one.', children=[]), ArxivPaperSection(header='h3', title='*Refined Critic Loss*', text='*Refined Critic Loss*The IBC loss in (5\\\\) depends on the parameter (w) of the critic network (Q\\\\_{w}). If the critic network (Q\\\\_{w}) is not well trained, the  \\nFigure 6: The flow chart of the training process of the IBC\\\\-DMP agent. The circled numbers indicate the critical steps of the training procedure which are explained in Sec. V\\\\-A.\\nBC loss (\\\\\\\\mathcal{L}*{BC}) may not be able to accurately capture the penalty that should be imposed on the current policy (\\\\\\\\pi*{\\\\\\\\theta}), leading to an invalid loss that does not help the policy update. This is especially likely to occur in the initial stage of the learning process. To solve this problem, we use a refined data batch (\\\\\\\\mathcal{B}\\\\=\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{Q}\\\\\\\\cup\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{Q}) to compose the loss function for the critic network, (\\\\\\\\mathcal{L}*{C}\\\\\\\\left(\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{Q}\\\\\\\\cup\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{Q}\\\\\\\\right)), where the form of (\\\\\\\\mathcal{L}*{C}) is defined in (4\\\\). Here, we refer to (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{Q}\\\\\\\\cup\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{Q}) as a refined data batch since it has a larger proportion of demonstration data compared to an interaction data batch (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{Q}) of the same size. The ratio between the sizes of (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{Q}) and (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{Q}) is referred to as the refining factor (\\\\\\\\lambda*{\\\\\\\\mathrm{RF}}). Here, we assume that human demonstration data are very likely to have higher values than random data. Therefore, a refined data batch with a larger (\\\\\\\\lambda\\\\_{\\\\\\\\mathrm{RF}}) is likely to better describe the true critic loss and more helpful to the booting up of the training of the critic network. However, an overlarge (\\\\\\\\lambda\\\\_{\\\\\\\\mathrm{RF}}) may lead to overfitting of the human demonstration. In this paper, we select (\\\\\\\\lambda\\\\_{\\\\\\\\mathrm{RF}}) as a constant. For better performance, the demo batch (\\\\\\\\mathcal{B}\\\\_{\\\\\\\\mathrm{D}}^{Q}) can be gradually eliminated from (\\\\\\\\mathcal{B}^{Q}) as the learning proceeds, which renders a decreasing refining factor. This can be an interesting topic for future work.  \\n*{BC}) may not be able to accurately capture the penalty that should be imposed on the current policy (\\\\\\\\pi**{\\\\\\\\mathrm{D}}^{Q}\\\\\\\\cup\\\\\\\\mathcal{B}**{C}\\\\\\\\left(\\\\\\\\mathcal{B}**{\\\\\\\\mathrm{I}}^{Q}\\\\\\\\right)), where the form of (\\\\\\\\mathcal{L}**{\\\\\\\\mathrm{D}}^{Q}\\\\\\\\cup\\\\\\\\mathcal{B}**{\\\\\\\\mathrm{I}}^{Q}) of the same size. The ratio between the sizes of (\\\\\\\\mathcal{B}**{\\\\\\\\mathrm{I}}^{Q}) is referred to as the refining factor (\\\\\\\\lambda*### *The Training Algorithm for An IBC\\\\-DMP Agent*  \\n*The Training Algorithm for An IBC\\\\-DMP Agent*The training procedure of the IBC\\\\-DMP agent is formulated as Algorithm 1, where (N) is the total number of episodes during the training process, (T) is the ending time of an episode, and (\\\\\\\\epsilon\\\\_{t}\\\\\\\\sim\\\\\\\\mathcal{N}(0,\\\\\\\\sigma^{2})) is a random variable for a certain time (t\\\\=0,1,\\\\\\\\cdots,T). Algorithm 1 inherits the following common points from the conventional off\\\\-policy RL method.  \\n* Initialization of the four neural networks (lines 2 and 3\\\\).\\n* The random sampling of actions (line 9\\\\).\\n* Storing interaction data (line 13\\\\).\\n* Sampling data from the interaction buffer (line 16\\\\).\\n* The update methods for the networks (lines 19 and 20\\\\).  \\n- Initialization of the four neural networks (lines 2 and 3\\\\).\\n- The random sampling of actions (line 9\\\\).\\n- Storing interaction data (line 13\\\\).\\n- Sampling data from the interaction buffer (line 16\\\\).\\n- The update methods for the networks (lines 19 and 20\\\\).\\nMeanwhile, Algorithm 1 is different from the conventional off\\\\-policy RL agent in the following aspects.  \\n* Initialization of the buffers (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}) and (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}) (line 1\\\\).\\n* A Multi\\\\-DoF DMP as the environment model (line 10\\\\).\\n* Sampling data from the demonstration buffer (line 15\\\\).\\n* The computation of loss functions (lines 17 and 18\\\\).  \\n- Initialization of the buffers (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}) and (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}) (line 1\\\\).\\n*{\\\\\\\\mathrm{D}}) and (\\\\\\\\mathcal{B}*- A Multi\\\\-DoF DMP as the environment model (line 10\\\\).\\n- Sampling data from the demonstration buffer (line 15\\\\).\\n- The computation of loss functions (lines 17 and 18\\\\).\\n**Input:** demonstration buffer (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}) and interaction buffer (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}})  \\n**Input:***{\\\\\\\\mathrm{D}}) and interaction buffer (\\\\\\\\mathcal{B}***Output:** trained policy parameter (\\\\\\\\theta)  \\n**Output:**\\n```\\n1:Import demonstration data to buffers \\\\(\\\\mathcal{B}_{\\\\mathrm{D}}\\\\), \\\\(\\\\mathcal{B}_{\\\\mathrm{I}}\\\\)\\n2:Randomly initialize the source networks \\\\(\\\\pi_{\\\\theta}\\\\), \\\\(Q_{w}\\\\)\\n3:Initialize the target networks \\\\(\\\\pi_{\\\\theta^{\\\\prime}}\\\\), \\\\(Q_{w^{\\\\prime}}\\\\) with \\\\(\\\\theta^{\\\\prime}\\\\!\\\\leftarrow\\\\!\\\\theta\\\\), \\\\(w^{\\\\prime}\\\\!\\\\leftarrow\\\\!w\\\\)\\n4:for\\\\(i\\\\gets 1\\\\)to\\\\(N\\\\)do\\n5: Sample the initial position \\\\(\\\\mathbf{x}_{\\\\mathrm{i}}\\\\), the obstacle position \\\\(\\\\mathbf{x}_{\\\\mathrm{b}}\\\\), and the goal position \\\\(\\\\mathbf{x}_{\\\\mathrm{g}}\\\\)\\n6: Initialize the state \\\\(s_{0}\\\\!=\\\\!\\\\mathcal{X}_{0}\\\\)\\n7:for\\\\(t\\\\gets 0\\\\)to\\\\(T\\\\)do\\n8: Observe the state \\\\(s_{t}\\\\!=\\\\!\\\\mathcal{X}_{t}\\\\)\\n9: Sample an action \\\\(a_{t}=\\\\pi_{\\\\theta}(s_{t})+\\\\epsilon_{t}\\\\)\\n10: Update the DMP model using (13)\\n11: Observe the successive state \\\\(s^{\\\\prime}_{t}\\\\!\\\\!=\\\\!\\\\mathcal{X}_{t+\\\\Delta t}\\\\)\\n12: Calculate the instant reward \\\\(r_{t}\\\\) and flag \\\\(d_{t}\\\\)\\n13: Store \\\\(\\\\{s_{t},a_{t},s^{\\\\prime}_{t},r_{t},d_{t}\\\\}\\\\) to \\\\(\\\\mathcal{B}_{\\\\mathrm{I}}\\\\)\\n14:ifUPDATE is truethen\\n15: Sample random batches \\\\(\\\\mathcal{B}_{\\\\mathrm{D}}^{\\\\pi}\\\\), \\\\(\\\\mathcal{B}_{\\\\mathrm{D}}^{Q}\\\\) from \\\\(\\\\mathcal{B}_{\\\\mathrm{D}}\\\\)\\n16: Sample random batches \\\\(\\\\mathcal{B}_{\\\\mathrm{I}}^{\\\\pi}\\\\), \\\\(\\\\mathcal{B}_{\\\\mathrm{I}}^{\\\\mathrm{D}}\\\\) from \\\\(\\\\mathcal{B}_{\\\\mathrm{I}}\\\\)\\n17: Calculate the actor loss \\\\(\\\\mathcal{L}_{A}(\\\\mathcal{B}_{\\\\mathrm{D}}^{Q},\\\\mathcal{B}_{\\\\mathrm{I}}^{\\\\pi})\\\\) using (16)\\n18: Calculate the critic loss \\\\(\\\\mathcal{L}_{B}\\\\!\\\\left(\\\\mathcal{B}_{\\\\mathrm{D}}^{Q}\\\\cup\\\\mathcal{B}_{\\\\mathrm{I}}^{Q}\\\\right)\\\\) using (4)\\n19: Update the source networks \\\\(\\\\pi_{\\\\theta}\\\\), \\\\(Q_{w}\\\\) using (6)\\n20: Update the target networks \\\\(\\\\pi_{\\\\theta^{\\\\prime}}\\\\), \\\\(Q_{w^{\\\\prime}}\\\\) using (3)\\n21:endif\\n22:\\\\(s_{t}\\\\gets s^{\\\\prime}_{t}\\\\)\\n23:endfor\\n24:endfor\\n\\n```\\n`1:Import demonstration data to buffers \\\\(\\\\mathcal{B}_{\\\\mathrm{D}}\\\\), \\\\(\\\\mathcal{B}_{\\\\mathrm{I}}\\\\)\\n2:Randomly initialize the source networks \\\\(\\\\pi_{\\\\theta}\\\\), \\\\(Q_{w}\\\\)\\n3:Initialize the target networks \\\\(\\\\pi_{\\\\theta^{\\\\prime}}\\\\), \\\\(Q_{w^{\\\\prime}}\\\\) with \\\\(\\\\theta^{\\\\prime}\\\\!\\\\leftarrow\\\\!\\\\theta\\\\), \\\\(w^{\\\\prime}\\\\!\\\\leftarrow\\\\!w\\\\)\\n4:for\\\\(i\\\\gets 1\\\\)to\\\\(N\\\\)do\\n5: Sample the initial position \\\\(\\\\mathbf{x}_{\\\\mathrm{i}}\\\\), the obstacle position \\\\(\\\\mathbf{x}_{\\\\mathrm{b}}\\\\), and the goal position \\\\(\\\\mathbf{x}_{\\\\mathrm{g}}\\\\)\\n6: Initialize the state \\\\(s_{0}\\\\!=\\\\!\\\\mathcal{X}_{0}\\\\)\\n7:for\\\\(t\\\\gets 0\\\\)to\\\\(T\\\\)do\\n8: Observe the state \\\\(s_{t}\\\\!=\\\\!\\\\mathcal{X}_{t}\\\\)\\n9: Sample an action \\\\(a_{t}=\\\\pi_{\\\\theta}(s_{t})+\\\\epsilon_{t}\\\\)\\n10: Update the DMP model using (13)\\n11: Observe the successive state \\\\(s^{\\\\prime}_{t}\\\\!\\\\!=\\\\!\\\\mathcal{X}_{t+\\\\Delta t}\\\\)\\n12: Calculate the instant reward \\\\(r_{t}\\\\) and flag \\\\(d_{t}\\\\)\\n13: Store \\\\(\\\\{s_{t},a_{t},s^{\\\\prime}_{t},r_{t},d_{t}\\\\}\\\\) to \\\\(\\\\mathcal{B}_{\\\\mathrm{I}}\\\\)\\n14:ifUPDATE is truethen\\n15: Sample random batches \\\\(\\\\mathcal{B}_{\\\\mathrm{D}}^{\\\\pi}\\\\), \\\\(\\\\mathcal{B}_{\\\\mathrm{D}}^{Q}\\\\) from \\\\(\\\\mathcal{B}_{\\\\mathrm{D}}\\\\)\\n16: Sample random batches \\\\(\\\\mathcal{B}_{\\\\mathrm{I}}^{\\\\pi}\\\\), \\\\(\\\\mathcal{B}_{\\\\mathrm{I}}^{\\\\mathrm{D}}\\\\) from \\\\(\\\\mathcal{B}_{\\\\mathrm{I}}\\\\)\\n17: Calculate the actor loss \\\\(\\\\mathcal{L}_{A}(\\\\mathcal{B}_{\\\\mathrm{D}}^{Q},\\\\mathcal{B}_{\\\\mathrm{I}}^{\\\\pi})\\\\) using (16)\\n18: Calculate the critic loss \\\\(\\\\mathcal{L}_{B}\\\\!\\\\left(\\\\mathcal{B}_{\\\\mathrm{D}}^{Q}\\\\cup\\\\mathcal{B}_{\\\\mathrm{I}}^{Q}\\\\right)\\\\) using (4)\\n19: Update the source networks \\\\(\\\\pi_{\\\\theta}\\\\), \\\\(Q_{w}\\\\) using (6)\\n20: Update the target networks \\\\(\\\\pi_{\\\\theta^{\\\\prime}}\\\\), \\\\(Q_{w^{\\\\prime}}\\\\) using (3)\\n21:endif\\n22:\\\\(s_{t}\\\\gets s^{\\\\prime}_{t}\\\\)\\n23:endfor\\n24:endfor`**Algorithm 1** The demo IBC\\\\-DMP DDPG algorithm  \\n**Algorithm 1**', children=[])]),\n",
       "  ArxivPaperSection(header='h2', title='VI Evaluation in Simulation', text='In this section, we evaluate the efficacy of the proposed IBC\\\\-DMP RL agent using a simulation study. We consider a point\\\\-to\\\\-point reaching case in the three\\\\-dimensional task space, where the end\\\\-effector of a robot is expected to move from a fixed initial position (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{i}}!\\\\=!(0,\\\\\\\\,0,\\\\\\\\,0\\\\.05\\\\)) m to an arbitrary goal position (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{g}}) while avoiding the collision with a cylinder obstacle placed at an arbitrary position (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{b}}^{(1,2\\\\)}). The radius and height of the obstacle are (r*{\\\\\\\\mathrm{b}}\\\\=0\\\\.035\\\\) m and (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{b}}^{(3\\\\)}\\\\=0\\\\.16\\\\) m which are the same as the demonstration recording experiment in Sec. IV. Note that assigning a fixed initial position (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{i}}) does not lose the generality since we can always use a coordinate transformation to transform any initial position in practice to (\\\\\\\\mathbf{x}\\\\_{\\\\\\\\mathrm{i}}). Also, in this simulation study, we treat the robot end\\\\-effector as a point and only consider its position. The experimental study in Sec. VII will show how to deploy the trained policy to the end\\\\-effector of a robot manipulator in a pick\\\\-and\\\\-place task. All length units are in meters. The code for the simulation study is implemented based on the Spinningup baseline programs \\\\[60] and written in Python. All training and test processes are performed on a Thinkpad laptop workstation with Intel(R) Core(TM) i7\\\\-10750H CPU at (2\\\\.60\\\\) GHz. The programs and data of the simulation studies are published at \\\\[61].\\n\\n*{\\\\\\\\mathrm{i}}!\\\\=!(0,\\\\\\\\,0,\\\\\\\\,0\\\\.05\\\\)) m to an arbitrary goal position (\\\\\\\\mathbf{x}**{\\\\\\\\mathrm{b}}^{(1,2\\\\)}). The radius and height of the obstacle are (r**{\\\\\\\\mathrm{b}}^{(3\\\\)}\\\\=0\\\\.16\\\\) m which are the same as the demonstration recording experiment in Sec. IV. Note that assigning a fixed initial position (\\\\\\\\mathbf{x}*### *Agent Training*\\n\\n*Agent Training*With the demonstration data collected in Sec. IV, we use Algorithm 1 to train an IBC\\\\-DMP agent for the point\\\\-to\\\\-point reaching task. The size of the demo buffer (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}) is (N*{\\\\\\\\mathrm{D}}!\\\\=!123171\\\\), which contains all eligible demonstration samples. The size of the experience replay buffer (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}) is (N*{\\\\\\\\mathrm{R}}\\\\=10^{6}) which is sufficiently large to include both the demonstration samples and the history samples during the training process. Other hyper\\\\-parameters of training are listed in Tab. III, where (N\\\\_{\\\\\\\\mathrm{D}}^{\\\\\\\\pi}), (N\\\\_{\\\\\\\\mathrm{D}}^{Q}), (N\\\\_{\\\\\\\\mathrm{I}}^{\\\\\\\\pi}), and (N\\\\_{\\\\\\\\mathrm{I}}^{Q}) are the sizes of the data batches (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{\\\\\\\\pi}), (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{Q}), (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{\\\\\\\\pi}), and (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{Q}). The configuration corresponds to a refining factor (\\\\\\\\lambda\\\\_{\\\\\\\\mathrm{RF}}\\\\=\\\\\\\\mathcal{B}). The actor and the critic are three\\\\-layer forward neural networks in which the neuron numbers are (64\\\\), (128\\\\), and (64\\\\). The actuation functions of the networks are Rectified Linear Unit (ReLU) functions. The parameters of the networks are randomly initialized.\\nEach training episode is a run of the Multi\\\\-DoF DMP model (13\\\\) from the fixed initial position (\\\\\\\\mathbf{x}*{\\\\\\\\text{i}}) to a random goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}) uniformly sampled from (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}\\\\\\\\in\\\\\\\\mathcal{P}*{\\\\\\\\text{g}}), where (\\\\\\\\mathcal{P}*{\\\\\\\\text{g}}\\\\={P*{\\\\\\\\text{g}},\\\\\\\\,\\\\\\\\Delta P\\\\_{\\\\\\\\text{g}}}) is a polyhedral region with (P\\\\_{\\\\\\\\text{g}}\\\\=(0\\\\.30\\\\\\\\,\\\\\\\\,0\\\\.35\\\\\\\\,\\\\\\\\,0\\\\.08\\\\)) m being its center coordinate and (P\\\\_{\\\\\\\\text{g}}!\\\\+!\\\\\\\\Delta P\\\\_{\\\\\\\\text{g}}) being its vertexes, where (\\\\\\\\Delta P\\\\_{\\\\\\\\text{g}}!\\\\=!(\\\\\\\\pm 0\\\\.05\\\\\\\\,\\\\\\\\,\\\\\\\\pm 0\\\\.05\\\\\\\\,\\\\\\\\,\\\\\\\\pm 0\\\\.01\\\\)) m. The purpose of uniform sampling is to improve the robustness of the trained agent against the perturbation of the actual goal positions. We also randomly sample the obstacle position by (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}!\\\\\\\\in!\\\\\\\\mathcal{P}*{\\\\\\\\text{b}}(\\\\\\\\mathbf{x}*{\\\\\\\\text{i}}, \\\\\\\\mathbf{x}*{\\\\\\\\text{g}})!) improve the robustness of the agent to obstacle positions, where (\\\\\\\\mathcal{P}*{\\\\\\\\text{b}}(\\\\\\\\mathbf{x}*{\\\\\\\\text{i}},\\\\\\\\mathbf{x}*{\\\\\\\\text{g}})!\\\\=!{P* {\\\\\\\\text{b}}(\\\\\\\\mathbf{x}*{\\\\\\\\text{i}},\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}),\\\\\\\\Delta P\\\\_{\\\\\\\\text{b}}}) is a polyhedral sampling region that depends on the initial position (\\\\\\\\mathbf{x}*{\\\\\\\\text{i}}) and the sampled goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}), with (P\\\\_{\\\\\\\\text{b}}\\\\=\\\\\\\\frac{\\\\\\\\mathbf{x}*{\\\\\\\\text{i}}\\\\+\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}}{2}) being the geometry center and (P\\\\_{\\\\\\\\text{b}}!\\\\+!\\\\\\\\Delta P\\\\_{\\\\\\\\text{b}}) being the vertexes, where (\\\\\\\\Delta P\\\\_{\\\\\\\\text{b}}!\\\\=!(\\\\\\\\pm 0\\\\.05\\\\\\\\,\\\\\\\\,\\\\\\\\pm 0\\\\.05\\\\\\\\,\\\\\\\\,\\\\\\\\pm 0\\\\.02\\\\)) m. We set the obstacle sampling space (\\\\\\\\mathcal{P}*{\\\\\\\\text{b}}) generally in the mid\\\\-way between the fixed initial position (\\\\\\\\mathbf{x}*{\\\\\\\\text{i}}) and the goal sampling space (\\\\\\\\mathcal{P}\\\\_{\\\\\\\\text{g}}) to intentionally create challenging environmental configurations for the agent training.\\n\\n*{\\\\\\\\mathrm{D}}) is (N**{\\\\\\\\mathrm{I}}) is (N**{\\\\\\\\mathrm{D}}^{\\\\\\\\pi}), (\\\\\\\\mathcal{B}**{\\\\\\\\mathrm{I}}^{\\\\\\\\pi}), and (\\\\\\\\mathcal{B}**{\\\\\\\\text{i}}) to a random goal position (\\\\\\\\mathbf{x}**{\\\\\\\\text{g}}\\\\\\\\in\\\\\\\\mathcal{P}**{\\\\\\\\text{g}}\\\\={P**{\\\\\\\\text{b}}!\\\\\\\\in!\\\\\\\\mathcal{P}**{\\\\\\\\text{i}}, \\\\\\\\mathbf{x}**{\\\\\\\\text{b}}(\\\\\\\\mathbf{x}**{\\\\\\\\text{g}})!\\\\=!{P**{\\\\\\\\text{i}},\\\\\\\\mathbf{x}**{\\\\\\\\text{i}}) and the sampled goal position (\\\\\\\\mathbf{x}**{\\\\\\\\text{i}}\\\\+\\\\\\\\mathbf{x}**{\\\\\\\\text{b}}) generally in the mid\\\\-way between the fixed initial position (\\\\\\\\mathbf{x}*We train three IBC\\\\-DMP agents with different BC ratios (\\\\\\\\lambda\\\\_{\\\\\\\\text{BC}}\\\\=1,1\\\\.5,2\\\\) to evaluate the influence of IBC on agent training. To show the advantage of the IBC\\\\-DMP agent, we also conduct a comparison study with a conventional agent without IBC. The experience replay buffer of the no\\\\-IBC agent only has one interaction buffer. For a fair comparison, the interaction buffer is filled with random samples generated from the multi\\\\-DoF DMP model (13\\\\) using random actions. Apart from this, all other training parameters of the no\\\\-IBC agent are the same as the IBC\\\\-DMP agents. Moreover, for each agent, we repeatedly train (M\\\\=10\\\\) policies with the same initial condition but with different random seeds. The evaluation of the training performance of the agents will be conducted on all (M) policies to balance the effects of randomness.\\n\\n#### V\\\\-B1 Convergence Performance\\n\\nWe first evaluate the convergence performance of the agents during the training process. For each training episode (i\\\\=1,2,\\\\\\\\cdots,N) of each randomization (j\\\\=1,2,\\\\\\\\cdots,M), we use the accumulated reward per training episode (ARPE) score defined as (R\\\\_{ij}!\\\\=!\\\\\\\\sum\\\\_{t\\\\=0}^{T}r\\\\_{t}^{ij}) to value the training performance of an episode, where (r\\\\_{t}^{ij}) is the instant reward at step (t\\\\=0,1,\\\\\\\\cdots,T) of episode (i) for random seed (j). Then, the performance of an RL agent can be evaluated by whether the ARPE converges to a high value. Nevertheless, the ARPE values among different training episodes may vary greatly. On the other hand, all ARPE values are non\\\\-positive. Therefore, we use the logarithm of ARPE (L\\\\-ARPE) defined as (L\\\\_{ij}!\\\\=!\\\\-!\\\\\\\\ln(1!\\\\-!R\\\\_{ij})) to denote the training performance, for better visualization. The lines indicating the change of the L\\\\-ARPEs of all agents as the episode increases are illustrated in Fig. 7\\\\. In each subfigure, the solid line denotes the mean values (\\\\\\\\mu(L\\\\_{i})\\\\=\\\\\\\\frac{1}{M}\\\\\\\\sum\\\\_{j\\\\=1}^{M}L\\\\_{ij}) of the L\\\\-ARPE lines over the (M) random seeds, and the shadow region represents the standard deviation (\\\\\\\\sigma(L\\\\_{i})\\\\=\\\\\\\\frac{1}{M}\\\\\\\\sum\\\\_{j\\\\=1}^{M}\\\\\\\\left(L\\\\_{ij}\\\\-\\\\\\\\mu(L\\\\_{i})\\\\\\\\right)^{2}) of the L\\\\-ARPE lines. We smooth out the lines in all subfigures with (10\\\\) episodes for clear presentation.\\n\\nFrom Fig. 7, we can see that the L\\\\-ARPE lines of all the IBC\\\\-DMP RL agents ultimately converge to steady values before (500\\\\) episodes. Among these IBC\\\\-DMP agents, the one with (\\\\\\\\lambda\\\\_{\\\\\\\\text{BC}}!\\\\=!2\\\\) has the highest ultimate average L\\\\-ARPE (\\\\\\\\mu(L\\\\_{N})) and the smallest ultimate standard deviation (\\\\\\\\sigma(L\\\\_{N})). It also shows a more clear reduction of the standard derivation (\\\\\\\\sigma(L\\\\_{i})) as (i) increases, compared to other BC ratios. Nevertheless, the no\\\\-IBC agent failed to achieve convergence. Even though it achieves a high score (\\\\\\\\mu(L\\\\_{i})) and low deviation (\\\\\\\\sigma(L\\\\_{i})) in the early stage of the training, its performance becomes worse as the training proceeds. This comparison study indicates the advantages of the IBC\\\\-DMP agents with faster convergence and higher stability to randomization than the conventional no\\\\-IBC agents.\\n\\n#### V\\\\-B2 Trained Scores\\n\\nApart from the L\\\\-ARPE lines, we also quantitatively evaluate the trained performance of the agents by analyzing their training scores. Tab. IV shows the ultimate L\\\\-ARPE scores (L\\\\_{Nj}) of all four agents for all random seeds (j\\\\=1,2,\\\\\\\\cdots,M). The averaged score (\\\\\\\\mu(L\\\\_{N})) and the standard deviation (\\\\\\\\sigma(L\\\\_{N})) are also displayed. We can observe that all IBC\\\\-DMP agents have higher ultimate average L\\\\-ARPE scores and smaller standard deviations than the no\\\\-IBC agent. Specifically, all IBC\\\\-DMP agents have training scores that are higher than or approximately equal to (\\\\-6\\\\), while the score of the no\\\\-IBC agent is lower than (\\\\-6\\\\). In this sense, we can select the L\\\\-ARPE value \\\\-6 as a standard to judge whether a trajectory is of good performance or not. The results indicate the superior training performance of the IBC\\\\-DMP agents over the conventional no\\\\-IBC agent. Tab. IV also shows that (\\\\\\\\lambda\\\\_{\\\\\\\\text{BC}}!\\\\=!2\\\\) is the best BC ratio among all configurations. The overall results indicate that IBC\\\\-DMP can improve the training\\n\\nFig. 7: The L\\\\-ARPE lines of the agents in training.\\nperformance of an RL agent given a proper BC ratio.\\n\\n### *Agent Test*\\n\\n*Agent Test*In this subsection, we evaluate the performance of the four trained agents in a test study. The Multi\\\\-DoF DMP model is required to start from a fixed initial position (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{i}}!\\\\=!(0\\\\\\\\ 0\\\\\\\\ 0\\\\.05\\\\)\\\\\\\\,)m to a random goal position (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{g}}) while avoiding a cylinder obstacle in a random position (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{b}}). For each trained policy (j\\\\=1,2,\\\\\\\\cdots,M) of each agent, we perform (R\\\\=400\\\\) test runs with different goal positions (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{g}}) and obstacle positions (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{b}}). The goal positions (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{g}}) are sampled from a polyhedral region (\\\\\\\\tilde{\\\\\\\\mathcal{P}}*{\\\\\\\\mathrm{g}}!\\\\=!{\\\\\\\\tilde{P}*{\\\\\\\\mathrm{g}},\\\\\\\\Delta\\\\\\\\tilde{P} *{\\\\\\\\mathrm{g}}}), where (\\\\\\\\tilde{P}*{\\\\\\\\mathrm{g}}!\\\\=!(0\\\\.32\\\\\\\\ 0\\\\.34\\\\\\\\ 0\\\\.09\\\\)\\\\\\\\,)m and (\\\\\\\\Delta\\\\\\\\tilde{P}*{\\\\\\\\mathrm{g}}!\\\\=!(\\\\\\\\pm 0\\\\.3\\\\\\\\ \\\\\\\\pm 0\\\\.25\\\\\\\\ \\\\\\\\pm 0\\\\.05\\\\)\\\\\\\\,)m. Note that the sampling space for test (\\\\\\\\tilde{P}*{\\\\\\\\mathrm{g}}) is larger than the one for training (P\\\\_{\\\\\\\\mathrm{g}}) as introduced in Sec. VI\\\\-A, since we intend to test the generalizability of the IBC\\\\-DMP RL agents to the goal positions that are not used for training. During the sampling of the goal positions, we also eliminate the ones that are too close to the initial positions to guarantee the feasibility of trajectory generation. For each sampled goal position (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{g}}), the obstacle position (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{b}}) is also randomly sampled from a polyhedral region (\\\\\\\\tilde{\\\\\\\\mathcal{P}}*{\\\\\\\\mathrm{b}}!\\\\=!{\\\\\\\\tilde{P}*{\\\\\\\\mathrm{b}},\\\\\\\\Delta\\\\\\\\tilde{P} *{\\\\\\\\mathrm{b}}}) of which the center (\\\\\\\\tilde{P}*{\\\\\\\\mathrm{b}}!\\\\=!\\\\\\\\dfrac{\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{i}}!\\\\+!\\\\\\\\tilde{P}*{ \\\\\\\\mathrm{g}}}{2}) is the mid\\\\-point between the initial position and the goal position, and the vertexes are (\\\\\\\\dfrac{\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{i}}\\\\+\\\\\\\\tilde{P}*{\\\\\\\\mathrm{g}}}{2}!\\\\+!\\\\\\\\Delta\\\\\\\\tilde{P} *{\\\\\\\\mathrm{b}}), where (\\\\\\\\Delta\\\\\\\\tilde{P}*{\\\\\\\\mathrm{b}}!\\\\=!(\\\\\\\\pm 0\\\\.05,\\\\\\\\ \\\\\\\\pm 0\\\\.05,\\\\\\\\ \\\\\\\\pm 0\\\\.02\\\\)\\\\\\\\,)m.\\n\\n*{\\\\\\\\mathrm{i}}!\\\\=!(0\\\\\\\\ 0\\\\\\\\ 0\\\\.05\\\\)\\\\\\\\,)m to a random goal position (\\\\\\\\mathbf{x}**{\\\\\\\\mathrm{b}}). For each trained policy (j\\\\=1,2,\\\\\\\\cdots,M) of each agent, we perform (R\\\\=400\\\\) test runs with different goal positions (\\\\\\\\mathbf{x}**{\\\\\\\\mathrm{b}}). The goal positions (\\\\\\\\mathbf{x}**{\\\\\\\\mathrm{g}}!\\\\=!{\\\\\\\\tilde{P}**{\\\\\\\\mathrm{g}}}), where (\\\\\\\\tilde{P}**{\\\\\\\\mathrm{g}}!\\\\=!(\\\\\\\\pm 0\\\\.3\\\\\\\\ \\\\\\\\pm 0\\\\.25\\\\\\\\ \\\\\\\\pm 0\\\\.05\\\\)\\\\\\\\,)m. Note that the sampling space for test (\\\\\\\\tilde{P}**{\\\\\\\\mathrm{g}}), the obstacle position (\\\\\\\\mathbf{x}**{\\\\\\\\mathrm{b}}!\\\\=!{\\\\\\\\tilde{P}**{\\\\\\\\mathrm{b}}}) of which the center (\\\\\\\\tilde{P}**{\\\\\\\\mathrm{i}}!\\\\+!\\\\\\\\tilde{P}**{\\\\\\\\mathrm{i}}\\\\+\\\\\\\\tilde{P}**{\\\\\\\\mathrm{b}}), where (\\\\\\\\Delta\\\\\\\\tilde{P}*#### Vi\\\\-B1 Test Trajectories\\n\\nFig. 8 shows the test trajectories of the trained agents for given goal and obstacle positions (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{g}}!\\\\=!(0\\\\.32\\\\\\\\ 0\\\\.34\\\\\\\\ 0\\\\.09\\\\)\\\\\\\\,)m and (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{b}}!\\\\=!(0\\\\.13\\\\\\\\ 0\\\\.14\\\\\\\\ 0\\\\.16\\\\)\\\\\\\\,)m. Each subfigure shows the trajectories of the corresponding agent generated by (M) trained policies for the fixed (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{g}}) and (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{b}}), where the red trajectories are those colliding with the obstacle or with the ground. Here, we refer to a trajectory as *with collisions* if it intersects with the obstacle or with the ground, i.e., if it has at least (1\\\\) discrete\\\\-time samples that are within the cylinder obstacle domain or under the ground. It is noticed that (5\\\\) out of (10\\\\) test trajectories of the conventional no\\\\-IBC agent are with collisions. On the contrary, each IBC\\\\-DMP agent only has at most (1\\\\) colliding trajectory. This indicates the superior test performance of IBC\\\\-DMP agents in terms of collision avoidance, compared to the no\\\\-IBC agent.\\n\\n*{\\\\\\\\mathrm{g}}!\\\\=!(0\\\\.32\\\\\\\\ 0\\\\.34\\\\\\\\ 0\\\\.09\\\\)\\\\\\\\,)m and (\\\\\\\\mathbf{x}**{\\\\\\\\mathrm{g}}) and (\\\\\\\\mathbf{x}**with collisions*#### Vi\\\\-B2 Test Scores\\n\\nWe also use test scores to quantify the test performance of the agents. For each agent, we use the L\\\\-ARPE score (L\\\\_{rj}) defined in Sec. VI\\\\-A1 to describe the test performance of a trajectory generated by policy (j\\\\=1,2,\\\\\\\\cdots,M) and position sample (r\\\\=1,2,\\\\\\\\cdots,R). Then, the averaged score (L\\\\_{rj}!\\\\=!\\\\\\\\frac{1}{R}\\\\\\\\sum\\\\_{r\\\\=1}^{H}L\\\\_{rj}) can be used to evaluate the general test performance of policy (j). The test scores (L\\\\_{rj}) for all policies (j) and all trained agents are listed in Tab. V, where the mean values and standard deviations of the test scores, defined as (\\\\\\\\mu(L\\\\_{r})!\\\\=!\\\\\\\\frac{1}{M}\\\\\\\\sum\\\\_{j\\\\=1}^{M}L\\\\_{rj}) and (\\\\\\\\sigma(L\\\\_{r})!\\\\=!\\\\\\\\frac{1}{M}\\\\\\\\sum\\\\_{j\\\\=1}^{M}\\\\\\\\left(L\\\\_{rj}\\\\-\\\\\\\\mu(L\\\\_{r})\\\\\\\\right)^{2}) are also presented.\\n\\nTab. V clearly shows that all IBC\\\\-DMP agents have much higher average score (\\\\\\\\mu(L\\\\_{r})) and smaller standard deviation (\\\\\\\\sigma(L\\\\_{r})) than the no\\\\-IBC agent. Specifically, all IBC\\\\-DMP agents are higher than or approximately equal to the performance standard \\\\-6, while the no\\\\-IBC agent has a lower test score. This indicates the advantage of the IBC\\\\-DMP agents in test performance. Since we have far larger sampling spaces for goal and obstacle positions in the test than that in training, the superior test performance of the IBC\\\\-DMP agents also reflects their better generalizability to various goal and obstacle positions than the no\\\\-IBC agents. Also, among the IBC\\\\-DMP agents, the one with the largest BC ratio (\\\\\\\\left\\\\\\\\langle\\\\\\\\lambda\\\\_{\\\\\\\\mathrm{BC}}\\\\\\\\right\\\\\\\\rangle) performs the best with the highest average score (\\\\\\\\mu(L\\\\_{r})!\\\\=!)\\\\-5\\\\.886 and the smallest standard deviation (\\\\\\\\sigma(L\\\\_{r})!\\\\=!)0\\\\.241\\\\. This indicates that it does not only have the best overall test performance but also has the best stability to the changes of different environmental conditions. The overall results validate the efficacy of the IBC\\\\-DMP RL framework in improving the generalizability and stability of RL agents.\\n\\n#### Vi\\\\-B3 Collision Rates\\n\\nOne of the main concerns of robot motion planning is the avoidance of collision with obstacles\\n\\nFig. 8: The test trajectories of the agents for fixed goal and obstacle positions, where the blue dot is the initial position, the purple star denotes the goal position, and the yellow cylinder represents the obstacle. The trajectories that collide with the obstacle or with the ground are marked as red, while those not are in blue.\\nin the environment. To evaluate the performance of the agents with respect to collision avoidance in the test study, we calculate the collision rates (R\\\\_{cj}\\\\=N\\\\_{\\\\\\\\rm cls}^{j}/N\\\\_{\\\\\\\\rm cll}^{j}) for all policies (j\\\\=1\\\\), (2\\\\), (\\\\\\\\cdots), (10\\\\), where (N\\\\_{\\\\\\\\rm tll}^{j}\\\\=400\\\\) is the total number of test trajectories of policy (j) and (N\\\\_{\\\\\\\\rm cls}^{j}) is the number of trajectories with collisions. Tab. VI lists the collision rates of all (10\\\\) policies of the four agents. The overall average value and the standard deviation of the collision rates of each agent are also presented. It is clearly shown that the IBC\\\\-DMP agents have far smaller collision rates than the no\\\\-IBC agent. They also have smaller standard deviation values. The agent with (\\\\\\\\lambda\\\\_{\\\\\\\\rm BC}\\\\=2\\\\) can be recognized as the safest and the most reliable one among the four agents with the lowest average collision rates and the smallest standard deviation. The overall results indicate that IBC\\\\-DMP agents have superior collision avoidance performance over the conventional no\\\\-IBC agent.\\n\\n', children=[ArxivPaperSection(header='p', title='', text='In this section, we evaluate the efficacy of the proposed IBC\\\\-DMP RL agent using a simulation study. We consider a point\\\\-to\\\\-point reaching case in the three\\\\-dimensional task space, where the end\\\\-effector of a robot is expected to move from a fixed initial position (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{i}}!\\\\=!(0,\\\\\\\\,0,\\\\\\\\,0\\\\.05\\\\)) m to an arbitrary goal position (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{g}}) while avoiding the collision with a cylinder obstacle placed at an arbitrary position (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{b}}^{(1,2\\\\)}). The radius and height of the obstacle are (r*{\\\\\\\\mathrm{b}}\\\\=0\\\\.035\\\\) m and (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{b}}^{(3\\\\)}\\\\=0\\\\.16\\\\) m which are the same as the demonstration recording experiment in Sec. IV. Note that assigning a fixed initial position (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{i}}) does not lose the generality since we can always use a coordinate transformation to transform any initial position in practice to (\\\\\\\\mathbf{x}\\\\_{\\\\\\\\mathrm{i}}). Also, in this simulation study, we treat the robot end\\\\-effector as a point and only consider its position. The experimental study in Sec. VII will show how to deploy the trained policy to the end\\\\-effector of a robot manipulator in a pick\\\\-and\\\\-place task. All length units are in meters. The code for the simulation study is implemented based on the Spinningup baseline programs \\\\[60] and written in Python. All training and test processes are performed on a Thinkpad laptop workstation with Intel(R) Core(TM) i7\\\\-10750H CPU at (2\\\\.60\\\\) GHz. The programs and data of the simulation studies are published at \\\\[61].  \\n*{\\\\\\\\mathrm{i}}!\\\\=!(0,\\\\\\\\,0,\\\\\\\\,0\\\\.05\\\\)) m to an arbitrary goal position (\\\\\\\\mathbf{x}**{\\\\\\\\mathrm{b}}^{(1,2\\\\)}). The radius and height of the obstacle are (r**{\\\\\\\\mathrm{b}}^{(3\\\\)}\\\\=0\\\\.16\\\\) m which are the same as the demonstration recording experiment in Sec. IV. Note that assigning a fixed initial position (\\\\\\\\mathbf{x}*### *Agent Training*  \\n*Agent Training*With the demonstration data collected in Sec. IV, we use Algorithm 1 to train an IBC\\\\-DMP agent for the point\\\\-to\\\\-point reaching task. The size of the demo buffer (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}) is (N*{\\\\\\\\mathrm{D}}!\\\\=!123171\\\\), which contains all eligible demonstration samples. The size of the experience replay buffer (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}) is (N*{\\\\\\\\mathrm{R}}\\\\=10^{6}) which is sufficiently large to include both the demonstration samples and the history samples during the training process. Other hyper\\\\-parameters of training are listed in Tab. III, where (N\\\\_{\\\\\\\\mathrm{D}}^{\\\\\\\\pi}), (N\\\\_{\\\\\\\\mathrm{D}}^{Q}), (N\\\\_{\\\\\\\\mathrm{I}}^{\\\\\\\\pi}), and (N\\\\_{\\\\\\\\mathrm{I}}^{Q}) are the sizes of the data batches (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{\\\\\\\\pi}), (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{D}}^{Q}), (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{\\\\\\\\pi}), and (\\\\\\\\mathcal{B}*{\\\\\\\\mathrm{I}}^{Q}). The configuration corresponds to a refining factor (\\\\\\\\lambda\\\\_{\\\\\\\\mathrm{RF}}\\\\=\\\\\\\\mathcal{B}). The actor and the critic are three\\\\-layer forward neural networks in which the neuron numbers are (64\\\\), (128\\\\), and (64\\\\). The actuation functions of the networks are Rectified Linear Unit (ReLU) functions. The parameters of the networks are randomly initialized.\\nEach training episode is a run of the Multi\\\\-DoF DMP model (13\\\\) from the fixed initial position (\\\\\\\\mathbf{x}*{\\\\\\\\text{i}}) to a random goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}) uniformly sampled from (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}\\\\\\\\in\\\\\\\\mathcal{P}*{\\\\\\\\text{g}}), where (\\\\\\\\mathcal{P}*{\\\\\\\\text{g}}\\\\={P*{\\\\\\\\text{g}},\\\\\\\\,\\\\\\\\Delta P\\\\_{\\\\\\\\text{g}}}) is a polyhedral region with (P\\\\_{\\\\\\\\text{g}}\\\\=(0\\\\.30\\\\\\\\,\\\\\\\\,0\\\\.35\\\\\\\\,\\\\\\\\,0\\\\.08\\\\)) m being its center coordinate and (P\\\\_{\\\\\\\\text{g}}!\\\\+!\\\\\\\\Delta P\\\\_{\\\\\\\\text{g}}) being its vertexes, where (\\\\\\\\Delta P\\\\_{\\\\\\\\text{g}}!\\\\=!(\\\\\\\\pm 0\\\\.05\\\\\\\\,\\\\\\\\,\\\\\\\\pm 0\\\\.05\\\\\\\\,\\\\\\\\,\\\\\\\\pm 0\\\\.01\\\\)) m. The purpose of uniform sampling is to improve the robustness of the trained agent against the perturbation of the actual goal positions. We also randomly sample the obstacle position by (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}!\\\\\\\\in!\\\\\\\\mathcal{P}*{\\\\\\\\text{b}}(\\\\\\\\mathbf{x}*{\\\\\\\\text{i}}, \\\\\\\\mathbf{x}*{\\\\\\\\text{g}})!) improve the robustness of the agent to obstacle positions, where (\\\\\\\\mathcal{P}*{\\\\\\\\text{b}}(\\\\\\\\mathbf{x}*{\\\\\\\\text{i}},\\\\\\\\mathbf{x}*{\\\\\\\\text{g}})!\\\\=!{P* {\\\\\\\\text{b}}(\\\\\\\\mathbf{x}*{\\\\\\\\text{i}},\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}),\\\\\\\\Delta P\\\\_{\\\\\\\\text{b}}}) is a polyhedral sampling region that depends on the initial position (\\\\\\\\mathbf{x}*{\\\\\\\\text{i}}) and the sampled goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}), with (P\\\\_{\\\\\\\\text{b}}\\\\=\\\\\\\\frac{\\\\\\\\mathbf{x}*{\\\\\\\\text{i}}\\\\+\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}}{2}) being the geometry center and (P\\\\_{\\\\\\\\text{b}}!\\\\+!\\\\\\\\Delta P\\\\_{\\\\\\\\text{b}}) being the vertexes, where (\\\\\\\\Delta P\\\\_{\\\\\\\\text{b}}!\\\\=!(\\\\\\\\pm 0\\\\.05\\\\\\\\,\\\\\\\\,\\\\\\\\pm 0\\\\.05\\\\\\\\,\\\\\\\\,\\\\\\\\pm 0\\\\.02\\\\)) m. We set the obstacle sampling space (\\\\\\\\mathcal{P}*{\\\\\\\\text{b}}) generally in the mid\\\\-way between the fixed initial position (\\\\\\\\mathbf{x}*{\\\\\\\\text{i}}) and the goal sampling space (\\\\\\\\mathcal{P}\\\\_{\\\\\\\\text{g}}) to intentionally create challenging environmental configurations for the agent training.  \\n*{\\\\\\\\mathrm{D}}) is (N**{\\\\\\\\mathrm{I}}) is (N**{\\\\\\\\mathrm{D}}^{\\\\\\\\pi}), (\\\\\\\\mathcal{B}**{\\\\\\\\mathrm{I}}^{\\\\\\\\pi}), and (\\\\\\\\mathcal{B}**{\\\\\\\\text{i}}) to a random goal position (\\\\\\\\mathbf{x}**{\\\\\\\\text{g}}\\\\\\\\in\\\\\\\\mathcal{P}**{\\\\\\\\text{g}}\\\\={P**{\\\\\\\\text{b}}!\\\\\\\\in!\\\\\\\\mathcal{P}**{\\\\\\\\text{i}}, \\\\\\\\mathbf{x}**{\\\\\\\\text{b}}(\\\\\\\\mathbf{x}**{\\\\\\\\text{g}})!\\\\=!{P**{\\\\\\\\text{i}},\\\\\\\\mathbf{x}**{\\\\\\\\text{i}}) and the sampled goal position (\\\\\\\\mathbf{x}**{\\\\\\\\text{i}}\\\\+\\\\\\\\mathbf{x}**{\\\\\\\\text{b}}) generally in the mid\\\\-way between the fixed initial position (\\\\\\\\mathbf{x}*We train three IBC\\\\-DMP agents with different BC ratios (\\\\\\\\lambda\\\\_{\\\\\\\\text{BC}}\\\\=1,1\\\\.5,2\\\\) to evaluate the influence of IBC on agent training. To show the advantage of the IBC\\\\-DMP agent, we also conduct a comparison study with a conventional agent without IBC. The experience replay buffer of the no\\\\-IBC agent only has one interaction buffer. For a fair comparison, the interaction buffer is filled with random samples generated from the multi\\\\-DoF DMP model (13\\\\) using random actions. Apart from this, all other training parameters of the no\\\\-IBC agent are the same as the IBC\\\\-DMP agents. Moreover, for each agent, we repeatedly train (M\\\\=10\\\\) policies with the same initial condition but with different random seeds. The evaluation of the training performance of the agents will be conducted on all (M) policies to balance the effects of randomness.  \\n#### V\\\\-B1 Convergence Performance  \\nWe first evaluate the convergence performance of the agents during the training process. For each training episode (i\\\\=1,2,\\\\\\\\cdots,N) of each randomization (j\\\\=1,2,\\\\\\\\cdots,M), we use the accumulated reward per training episode (ARPE) score defined as (R\\\\_{ij}!\\\\=!\\\\\\\\sum\\\\_{t\\\\=0}^{T}r\\\\_{t}^{ij}) to value the training performance of an episode, where (r\\\\_{t}^{ij}) is the instant reward at step (t\\\\=0,1,\\\\\\\\cdots,T) of episode (i) for random seed (j). Then, the performance of an RL agent can be evaluated by whether the ARPE converges to a high value. Nevertheless, the ARPE values among different training episodes may vary greatly. On the other hand, all ARPE values are non\\\\-positive. Therefore, we use the logarithm of ARPE (L\\\\-ARPE) defined as (L\\\\_{ij}!\\\\=!\\\\-!\\\\\\\\ln(1!\\\\-!R\\\\_{ij})) to denote the training performance, for better visualization. The lines indicating the change of the L\\\\-ARPEs of all agents as the episode increases are illustrated in Fig. 7\\\\. In each subfigure, the solid line denotes the mean values (\\\\\\\\mu(L\\\\_{i})\\\\=\\\\\\\\frac{1}{M}\\\\\\\\sum\\\\_{j\\\\=1}^{M}L\\\\_{ij}) of the L\\\\-ARPE lines over the (M) random seeds, and the shadow region represents the standard deviation (\\\\\\\\sigma(L\\\\_{i})\\\\=\\\\\\\\frac{1}{M}\\\\\\\\sum\\\\_{j\\\\=1}^{M}\\\\\\\\left(L\\\\_{ij}\\\\-\\\\\\\\mu(L\\\\_{i})\\\\\\\\right)^{2}) of the L\\\\-ARPE lines. We smooth out the lines in all subfigures with (10\\\\) episodes for clear presentation.  \\nFrom Fig. 7, we can see that the L\\\\-ARPE lines of all the IBC\\\\-DMP RL agents ultimately converge to steady values before (500\\\\) episodes. Among these IBC\\\\-DMP agents, the one with (\\\\\\\\lambda\\\\_{\\\\\\\\text{BC}}!\\\\=!2\\\\) has the highest ultimate average L\\\\-ARPE (\\\\\\\\mu(L\\\\_{N})) and the smallest ultimate standard deviation (\\\\\\\\sigma(L\\\\_{N})). It also shows a more clear reduction of the standard derivation (\\\\\\\\sigma(L\\\\_{i})) as (i) increases, compared to other BC ratios. Nevertheless, the no\\\\-IBC agent failed to achieve convergence. Even though it achieves a high score (\\\\\\\\mu(L\\\\_{i})) and low deviation (\\\\\\\\sigma(L\\\\_{i})) in the early stage of the training, its performance becomes worse as the training proceeds. This comparison study indicates the advantages of the IBC\\\\-DMP agents with faster convergence and higher stability to randomization than the conventional no\\\\-IBC agents.  \\n#### V\\\\-B2 Trained Scores  \\nApart from the L\\\\-ARPE lines, we also quantitatively evaluate the trained performance of the agents by analyzing their training scores. Tab. IV shows the ultimate L\\\\-ARPE scores (L\\\\_{Nj}) of all four agents for all random seeds (j\\\\=1,2,\\\\\\\\cdots,M). The averaged score (\\\\\\\\mu(L\\\\_{N})) and the standard deviation (\\\\\\\\sigma(L\\\\_{N})) are also displayed. We can observe that all IBC\\\\-DMP agents have higher ultimate average L\\\\-ARPE scores and smaller standard deviations than the no\\\\-IBC agent. Specifically, all IBC\\\\-DMP agents have training scores that are higher than or approximately equal to (\\\\-6\\\\), while the score of the no\\\\-IBC agent is lower than (\\\\-6\\\\). In this sense, we can select the L\\\\-ARPE value \\\\-6 as a standard to judge whether a trajectory is of good performance or not. The results indicate the superior training performance of the IBC\\\\-DMP agents over the conventional no\\\\-IBC agent. Tab. IV also shows that (\\\\\\\\lambda\\\\_{\\\\\\\\text{BC}}!\\\\=!2\\\\) is the best BC ratio among all configurations. The overall results indicate that IBC\\\\-DMP can improve the training  \\nFig. 7: The L\\\\-ARPE lines of the agents in training.\\nperformance of an RL agent given a proper BC ratio.', children=[]), ArxivPaperSection(header='h3', title='*Agent Test*', text='*Agent Test*In this subsection, we evaluate the performance of the four trained agents in a test study. The Multi\\\\-DoF DMP model is required to start from a fixed initial position (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{i}}!\\\\=!(0\\\\\\\\ 0\\\\\\\\ 0\\\\.05\\\\)\\\\\\\\,)m to a random goal position (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{g}}) while avoiding a cylinder obstacle in a random position (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{b}}). For each trained policy (j\\\\=1,2,\\\\\\\\cdots,M) of each agent, we perform (R\\\\=400\\\\) test runs with different goal positions (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{g}}) and obstacle positions (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{b}}). The goal positions (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{g}}) are sampled from a polyhedral region (\\\\\\\\tilde{\\\\\\\\mathcal{P}}*{\\\\\\\\mathrm{g}}!\\\\=!{\\\\\\\\tilde{P}*{\\\\\\\\mathrm{g}},\\\\\\\\Delta\\\\\\\\tilde{P} *{\\\\\\\\mathrm{g}}}), where (\\\\\\\\tilde{P}*{\\\\\\\\mathrm{g}}!\\\\=!(0\\\\.32\\\\\\\\ 0\\\\.34\\\\\\\\ 0\\\\.09\\\\)\\\\\\\\,)m and (\\\\\\\\Delta\\\\\\\\tilde{P}*{\\\\\\\\mathrm{g}}!\\\\=!(\\\\\\\\pm 0\\\\.3\\\\\\\\ \\\\\\\\pm 0\\\\.25\\\\\\\\ \\\\\\\\pm 0\\\\.05\\\\)\\\\\\\\,)m. Note that the sampling space for test (\\\\\\\\tilde{P}*{\\\\\\\\mathrm{g}}) is larger than the one for training (P\\\\_{\\\\\\\\mathrm{g}}) as introduced in Sec. VI\\\\-A, since we intend to test the generalizability of the IBC\\\\-DMP RL agents to the goal positions that are not used for training. During the sampling of the goal positions, we also eliminate the ones that are too close to the initial positions to guarantee the feasibility of trajectory generation. For each sampled goal position (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{g}}), the obstacle position (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{b}}) is also randomly sampled from a polyhedral region (\\\\\\\\tilde{\\\\\\\\mathcal{P}}*{\\\\\\\\mathrm{b}}!\\\\=!{\\\\\\\\tilde{P}*{\\\\\\\\mathrm{b}},\\\\\\\\Delta\\\\\\\\tilde{P} *{\\\\\\\\mathrm{b}}}) of which the center (\\\\\\\\tilde{P}*{\\\\\\\\mathrm{b}}!\\\\=!\\\\\\\\dfrac{\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{i}}!\\\\+!\\\\\\\\tilde{P}*{ \\\\\\\\mathrm{g}}}{2}) is the mid\\\\-point between the initial position and the goal position, and the vertexes are (\\\\\\\\dfrac{\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{i}}\\\\+\\\\\\\\tilde{P}*{\\\\\\\\mathrm{g}}}{2}!\\\\+!\\\\\\\\Delta\\\\\\\\tilde{P} *{\\\\\\\\mathrm{b}}), where (\\\\\\\\Delta\\\\\\\\tilde{P}*{\\\\\\\\mathrm{b}}!\\\\=!(\\\\\\\\pm 0\\\\.05,\\\\\\\\ \\\\\\\\pm 0\\\\.05,\\\\\\\\ \\\\\\\\pm 0\\\\.02\\\\)\\\\\\\\,)m.  \\n*{\\\\\\\\mathrm{i}}!\\\\=!(0\\\\\\\\ 0\\\\\\\\ 0\\\\.05\\\\)\\\\\\\\,)m to a random goal position (\\\\\\\\mathbf{x}**{\\\\\\\\mathrm{b}}). For each trained policy (j\\\\=1,2,\\\\\\\\cdots,M) of each agent, we perform (R\\\\=400\\\\) test runs with different goal positions (\\\\\\\\mathbf{x}**{\\\\\\\\mathrm{b}}). The goal positions (\\\\\\\\mathbf{x}**{\\\\\\\\mathrm{g}}!\\\\=!{\\\\\\\\tilde{P}**{\\\\\\\\mathrm{g}}}), where (\\\\\\\\tilde{P}**{\\\\\\\\mathrm{g}}!\\\\=!(\\\\\\\\pm 0\\\\.3\\\\\\\\ \\\\\\\\pm 0\\\\.25\\\\\\\\ \\\\\\\\pm 0\\\\.05\\\\)\\\\\\\\,)m. Note that the sampling space for test (\\\\\\\\tilde{P}**{\\\\\\\\mathrm{g}}), the obstacle position (\\\\\\\\mathbf{x}**{\\\\\\\\mathrm{b}}!\\\\=!{\\\\\\\\tilde{P}**{\\\\\\\\mathrm{b}}}) of which the center (\\\\\\\\tilde{P}**{\\\\\\\\mathrm{i}}!\\\\+!\\\\\\\\tilde{P}**{\\\\\\\\mathrm{i}}\\\\+\\\\\\\\tilde{P}**{\\\\\\\\mathrm{b}}), where (\\\\\\\\Delta\\\\\\\\tilde{P}*#### Vi\\\\-B1 Test Trajectories  \\nFig. 8 shows the test trajectories of the trained agents for given goal and obstacle positions (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{g}}!\\\\=!(0\\\\.32\\\\\\\\ 0\\\\.34\\\\\\\\ 0\\\\.09\\\\)\\\\\\\\,)m and (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{b}}!\\\\=!(0\\\\.13\\\\\\\\ 0\\\\.14\\\\\\\\ 0\\\\.16\\\\)\\\\\\\\,)m. Each subfigure shows the trajectories of the corresponding agent generated by (M) trained policies for the fixed (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{g}}) and (\\\\\\\\mathbf{x}*{\\\\\\\\mathrm{b}}), where the red trajectories are those colliding with the obstacle or with the ground. Here, we refer to a trajectory as *with collisions* if it intersects with the obstacle or with the ground, i.e., if it has at least (1\\\\) discrete\\\\-time samples that are within the cylinder obstacle domain or under the ground. It is noticed that (5\\\\) out of (10\\\\) test trajectories of the conventional no\\\\-IBC agent are with collisions. On the contrary, each IBC\\\\-DMP agent only has at most (1\\\\) colliding trajectory. This indicates the superior test performance of IBC\\\\-DMP agents in terms of collision avoidance, compared to the no\\\\-IBC agent.  \\n*{\\\\\\\\mathrm{g}}!\\\\=!(0\\\\.32\\\\\\\\ 0\\\\.34\\\\\\\\ 0\\\\.09\\\\)\\\\\\\\,)m and (\\\\\\\\mathbf{x}**{\\\\\\\\mathrm{g}}) and (\\\\\\\\mathbf{x}**with collisions*#### Vi\\\\-B2 Test Scores  \\nWe also use test scores to quantify the test performance of the agents. For each agent, we use the L\\\\-ARPE score (L\\\\_{rj}) defined in Sec. VI\\\\-A1 to describe the test performance of a trajectory generated by policy (j\\\\=1,2,\\\\\\\\cdots,M) and position sample (r\\\\=1,2,\\\\\\\\cdots,R). Then, the averaged score (L\\\\_{rj}!\\\\=!\\\\\\\\frac{1}{R}\\\\\\\\sum\\\\_{r\\\\=1}^{H}L\\\\_{rj}) can be used to evaluate the general test performance of policy (j). The test scores (L\\\\_{rj}) for all policies (j) and all trained agents are listed in Tab. V, where the mean values and standard deviations of the test scores, defined as (\\\\\\\\mu(L\\\\_{r})!\\\\=!\\\\\\\\frac{1}{M}\\\\\\\\sum\\\\_{j\\\\=1}^{M}L\\\\_{rj}) and (\\\\\\\\sigma(L\\\\_{r})!\\\\=!\\\\\\\\frac{1}{M}\\\\\\\\sum\\\\_{j\\\\=1}^{M}\\\\\\\\left(L\\\\_{rj}\\\\-\\\\\\\\mu(L\\\\_{r})\\\\\\\\right)^{2}) are also presented.  \\nTab. V clearly shows that all IBC\\\\-DMP agents have much higher average score (\\\\\\\\mu(L\\\\_{r})) and smaller standard deviation (\\\\\\\\sigma(L\\\\_{r})) than the no\\\\-IBC agent. Specifically, all IBC\\\\-DMP agents are higher than or approximately equal to the performance standard \\\\-6, while the no\\\\-IBC agent has a lower test score. This indicates the advantage of the IBC\\\\-DMP agents in test performance. Since we have far larger sampling spaces for goal and obstacle positions in the test than that in training, the superior test performance of the IBC\\\\-DMP agents also reflects their better generalizability to various goal and obstacle positions than the no\\\\-IBC agents. Also, among the IBC\\\\-DMP agents, the one with the largest BC ratio (\\\\\\\\left\\\\\\\\langle\\\\\\\\lambda\\\\_{\\\\\\\\mathrm{BC}}\\\\\\\\right\\\\\\\\rangle) performs the best with the highest average score (\\\\\\\\mu(L\\\\_{r})!\\\\=!)\\\\-5\\\\.886 and the smallest standard deviation (\\\\\\\\sigma(L\\\\_{r})!\\\\=!)0\\\\.241\\\\. This indicates that it does not only have the best overall test performance but also has the best stability to the changes of different environmental conditions. The overall results validate the efficacy of the IBC\\\\-DMP RL framework in improving the generalizability and stability of RL agents.  \\n#### Vi\\\\-B3 Collision Rates  \\nOne of the main concerns of robot motion planning is the avoidance of collision with obstacles  \\nFig. 8: The test trajectories of the agents for fixed goal and obstacle positions, where the blue dot is the initial position, the purple star denotes the goal position, and the yellow cylinder represents the obstacle. The trajectories that collide with the obstacle or with the ground are marked as red, while those not are in blue.\\nin the environment. To evaluate the performance of the agents with respect to collision avoidance in the test study, we calculate the collision rates (R\\\\_{cj}\\\\=N\\\\_{\\\\\\\\rm cls}^{j}/N\\\\_{\\\\\\\\rm cll}^{j}) for all policies (j\\\\=1\\\\), (2\\\\), (\\\\\\\\cdots), (10\\\\), where (N\\\\_{\\\\\\\\rm tll}^{j}\\\\=400\\\\) is the total number of test trajectories of policy (j) and (N\\\\_{\\\\\\\\rm cls}^{j}) is the number of trajectories with collisions. Tab. VI lists the collision rates of all (10\\\\) policies of the four agents. The overall average value and the standard deviation of the collision rates of each agent are also presented. It is clearly shown that the IBC\\\\-DMP agents have far smaller collision rates than the no\\\\-IBC agent. They also have smaller standard deviation values. The agent with (\\\\\\\\lambda\\\\_{\\\\\\\\rm BC}\\\\=2\\\\) can be recognized as the safest and the most reliable one among the four agents with the lowest average collision rates and the smallest standard deviation. The overall results indicate that IBC\\\\-DMP agents have superior collision avoidance performance over the conventional no\\\\-IBC agent.', children=[])]),\n",
       "  ArxivPaperSection(header='h2', title='VII Experimental Validation', text=\"In this section, we use an experimental case study to evaluate the performance of the proposed IBC\\\\-DMP RL method. A Rubik's cube\\\\-stacking case is used to demonstrate how to use an IBC\\\\-DMP agent to accomplish a general pick\\\\-and\\\\-place\\\\-based assembly task. The robot used in this study is a 6\\\\-DoF Kinova(r) Gen 3 manipulator with a two\\\\-finger Roboti(r) 2F\\\\-85 gripper and an Omnivision OV5640 on\\\\-gripper camera as shown in Fig. (a)a. The robot is connected to a desktop workstation which is equipped with an AMD(r) Ryzen9 3950X CPU and an Intel(r) RTX3090 GPU. The operating system of the desktop is Ubuntu 18\\\\.04\\\\. The robot is controlled by the Kinova(r) Kortex API written in Python \\\\[62]. Another RGB camera is deployed in the front of the robot to provide vision from the third\\\\-person point of view, as shown in Fig. (b)b.\\n\\n### *Experiment Configuration*\\n\\n*Experiment Configuration*As shown in Fig. (b)b, the workspace of the cube\\\\-stacking scenario is marked as a rectangular region on the table in front of the Gen3 robot arm. We set the origin of the task coordinate at the robot base. The axes of the coordinate are shown as colored arrows in Fig. (b)b. The coordinates of the vertexes of the workspace are ((0\\\\.25,0\\\\.45,0\\\\)) m, ((0\\\\.25,)\\\\-(0\\\\.2,0\\\\)) m, ((0\\\\.6,0\\\\.45,0\\\\)) m, and ((0\\\\.6,)\\\\-(0\\\\.2,0\\\\)) m. The size of the workspace is determined to cover the largest view of the on\\\\-gripper camera within the reach of the robot gripper. Three Rubik's cubes with different sizes (two cubes with a 57 mm edge and one cube with a 45 mm edge) and colors are randomly placed within the rectangular region manually. A paper cup with a radius of 90 mm and a height of 110 mm is also manually placed at a random position as an obstacle. The robot is commanded to start from the HOME position at (0\\\\.356, 0\\\\.106, 0\\\\.277\\\\) m, as shown in Fig. (b)b, pick up the Rubik's cubes, and place them at the GOAL position (marked as a green light point) one by one from the largest to the smallest (red, orange, and then blue), until they are piled up in a column. The piling\\\\-up process can be recognized as a simple assembly task, during which the robot gripper should not collide with the paper cup. The vision of the Rubik's cubes and the paper cup obstacle is captured using the on\\\\-gripper camera and their positions are calculated using the pre\\\\-built object detection libraries in YOLOv8 \\\\[63] and OpenCV \\\\[64].\\n\\nFor each experimental trial, we represent the initial positions of the three cubes with red, orange, and blue top surfaces as (P\\\\_{\\\\\\\\rm red}), (P\\\\_{\\\\\\\\rm org}), and (P\\\\_{\\\\\\\\rm blu}), respectively. We also set three vai\\\\-points (P\\\\_{\\\\\\\\rm red}), (P\\\\_{\\\\\\\\rm org}), and (P\\\\_{\\\\\\\\rm blu}) 5 cm right above them to ease the grasping of the cubes. In this sense, six trajectories need to be generated for the cube\\\\-stacking task, including HOME (\\\\-\\\\\\\\tilde{P}*{\\\\\\\\rm red}\\\\-G*{\\\\\\\\rm red}\\\\-\\\\\\\\tilde{P}*{\\\\\\\\rm org}\\\\-G*{\\\\\\\\rm org}\\\\-\\\\\\\\tilde{P}*{ \\\\\\\\rm blu}\\\\-G*{\\\\\\\\rm blu}), where (G\\\\_{\\\\\\\\rm red}), (G\\\\_{\\\\\\\\rm org}), and (G\\\\_{\\\\\\\\rm blu}) are the stacking positions of the cubes in the ultimate stacking\\n\\n*{\\\\\\\\rm red}\\\\-G**{\\\\\\\\rm org}\\\\-G**{ \\\\\\\\rm blu}\\\\-G*\\\\\\\\begin{table}\\n\\\\\\\\begin{tabular}{c\\\\|c\\\\|c\\\\|c\\\\|c} \\\\\\\\hline \\\\\\\\hline Stages \\\\& no\\\\-IBC \\\\& (\\\\\\\\lambda\\\\_{\\\\\\\\rm BC}\\\\=1\\\\) \\\\& (\\\\\\\\lambda\\\\_{\\\\\\\\rm BC}\\\\=1\\\\) \\\\& blue\\\\-\\\\=0 \\\\\\\\ \\\\\\\\hline (L\\\\_{P1}) \\\\& \\\\-6\\\\.1022 \\\\& \\\\-6\\\\.8036 \\\\& \\\\-6\\\\.2833 \\\\& \\\\-6\\\\.0177 \\\\\\\\ (L\\\\_{P2}) \\\\& \\\\-9\\\\.7703 \\\\& \\\\-5\\\\.9426 \\\\& \\\\-5\\\\.1831 \\\\& \\\\-5\\\\.6973 \\\\\\\\ (L\\\\_{P3}) \\\\& \\\\-6\\\\.3339 \\\\& \\\\-5\\\\.9161 \\\\& \\\\-6\\\\.0908 \\\\& \\\\-5\\\\.8809 \\\\\\\\ (L\\\\_{P4}) \\\\& \\\\-5\\\\.6509 \\\\& \\\\-5\\\\.9897 \\\\& \\\\-5\\\\.9751 \\\\& \\\\-6\\\\.0104 \\\\\\\\ (L\\\\_{P5}) \\\\& \\\\-5\\\\.7340 \\\\& \\\\-5\\\\.9492 \\\\& \\\\-6\\\\.0357 \\\\& \\\\-5\\\\.5467 \\\\\\\\ (L\\\\_{P6}) \\\\& \\\\-5\\\\.6169 \\\\& \\\\-5\\\\.6672 \\\\& \\\\-6\\\\.8701 \\\\& \\\\-6\\\\.1443 \\\\\\\\ (L\\\\_{P7}) \\\\& \\\\-7\\\\.3525 \\\\& \\\\-6\\\\.5007 \\\\& \\\\-5\\\\.7318 \\\\& \\\\-5\\\\.5285 \\\\\\\\ (L\\\\_{P8}) \\\\& \\\\-8\\\\.8828 \\\\& \\\\-6\\\\.2070 \\\\& \\\\-5\\\\.8197 \\\\& \\\\-6\\\\.2114 \\\\\\\\ (L\\\\_{P9}) \\\\& \\\\-6\\\\.4230 \\\\& \\\\-5\\\\.9020 \\\\& \\\\-6\\\\.2411 \\\\& \\\\-5\\\\.8731 \\\\\\\\ (L\\\\_{P10}) \\\\& \\\\-5\\\\.6641 \\\\& \\\\-5\\\\.7333 \\\\& \\\\-6\\\\.2062 \\\\& \\\\-6\\\\.2656 \\\\\\\\ \\\\\\\\hline (\\\\\\\\mu(L\\\\_{r})) \\\\& **\\\\-6\\\\.7530** \\\\& **\\\\-6\\\\.0625** \\\\& **\\\\-6\\\\.0437** \\\\& **\\\\-5\\\\.9176** \\\\\\\\ \\\\\\\\hline (\\\\\\\\mathbf{\\\\\\\\sigma}(\\\\\\\\mathbf{L\\\\_{r}})) \\\\& **1\\\\.3942** \\\\& **0\\\\.3333** \\\\& **0\\\\.4124** \\\\& **0\\\\.2487** \\\\\\\\ \\\\\\\\hline \\\\\\\\end{tabular}\\n\\\\\\\\end{table}\\nTable V: The Quantitative Scores of Agent Test\\n\\n**\\\\-6\\\\.7530****\\\\-6\\\\.0625****\\\\-6\\\\.0437****\\\\-5\\\\.9176****1\\\\.3942****0\\\\.3333****0\\\\.4124****0\\\\.2487**Figure 9: The Kinova® Gen3 robot (b) and the assembly scenario (b).\\ncolumn. The procedure of the stacking task is described in Algorithm 2, where *DMP* is a function used to generate the desired trajectories using the multi\\\\-DoF DMP model (9\\\\) with given initial, goal, and obstacle positions (\\\\\\\\mathbf{x}*{i}), (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}), and (\\\\\\\\mathbf{x}\\\\_{\\\\\\\\text{b}}), respectively, and the trained policy (\\\\\\\\mathbf{f}). In this experiment, we select policy \\\\#5 We are not involving additional task\\\\-level complexities in this task since the main goal of this paper is to evaluate how an IBC\\\\-DMP agent efficiently produces safe trajectories. Also, for brevity, the multi\\\\-DoF DMP model is only used to generate the translational positions of the gripper. The orientations of the gripper are commanded in a very intuitive and simple manner with the following principles.\\n\\n*DMP**{i}), (\\\\\\\\mathbf{x}** The gripper always points down to the table.\\n* The orientations about (x)\\\\- and (y)\\\\-axis are fixed for all generated trajectories.\\n* The orientations about (z)\\\\-axis are linearly changed during the motion, in order to grasp the cube or place it with the proper orientation in the line 5 of Algorithm 2\\\\.\\n\\n- The gripper always points down to the table.\\n- The orientations about (x)\\\\- and (y)\\\\-axis are fixed for all generated trajectories.\\n- The orientations about (z)\\\\-axis are linearly changed during the motion, in order to grasp the cube or place it with the proper orientation in the line 5 of Algorithm 2\\\\.\\nSuch a design ensures the successful execution of the cube\\\\-stacking task without causing singular configurations. All trajectories are designed in the Cartesian space and are mapped to the joint space of the robot using the pre\\\\-built inverse kinematics (IK) library of the Kinova(r) Gen 3 robot.\\n\\n\\n```\\n1:Initialize robot gripper at HOME position\\n2:Assign \\\\(\\\\mathbf{x}_{\\\\text{b}}\\\\) with the paper cup position\\n3:for\\\\(j\\\\)in \\\\(\\\\{\\\\text{red},\\\\text{org},\\\\text{blu}\\\\}\\\\)do\\n4:\\\\(\\\\mathbf{x}_{i}\\\\leftarrow\\\\mathbf{x}_{t}\\\\), \\\\(\\\\mathbf{x}_{\\\\text{g}}\\\\leftarrow\\\\bar{P}_{j}\\\\)\\n5:Generate trajectory \\\\((\\\\mathbf{x}_{t},\\\\hat{\\\\mathbf{x}}_{t})\\\\!=\\\\!D\\\\text{MP}(\\\\mathbf{x}_{i},\\\\,\\\\mathbf{x }_{\\\\text{g}},\\\\,\\\\mathbf{x}_{\\\\text{b}},\\\\,\\\\mathbf{f})\\\\)\\n6:while\\\\(\\\\|\\\\mathbf{x}_{t}\\\\!-\\\\!\\\\mathbf{x}_{\\\\text{g}}\\\\|\\\\!>\\\\!\\\\varepsilon_{T}\\\\)do\\n7: Follow trajectory \\\\((\\\\mathbf{x}_{t},\\\\hat{\\\\mathbf{x}}_{t})\\\\)\\n8:endwhile\\n9: Grasp cube at \\\\(P_{j}\\\\)\\n10: Determine stacking position \\\\(\\\\mathbf{x}_{i}\\\\leftarrow\\\\mathbf{x}_{t}\\\\), \\\\(\\\\mathbf{x}_{\\\\text{g}}\\\\gets G_{j}\\\\)\\n11: Repeat line 5 to line 8\\n12: Release cube\\n13:endfor\\n\\n```\\n`1:Initialize robot gripper at HOME position\\n2:Assign \\\\(\\\\mathbf{x}_{\\\\text{b}}\\\\) with the paper cup position\\n3:for\\\\(j\\\\)in \\\\(\\\\{\\\\text{red},\\\\text{org},\\\\text{blu}\\\\}\\\\)do\\n4:\\\\(\\\\mathbf{x}_{i}\\\\leftarrow\\\\mathbf{x}_{t}\\\\), \\\\(\\\\mathbf{x}_{\\\\text{g}}\\\\leftarrow\\\\bar{P}_{j}\\\\)\\n5:Generate trajectory \\\\((\\\\mathbf{x}_{t},\\\\hat{\\\\mathbf{x}}_{t})\\\\!=\\\\!D\\\\text{MP}(\\\\mathbf{x}_{i},\\\\,\\\\mathbf{x }_{\\\\text{g}},\\\\,\\\\mathbf{x}_{\\\\text{b}},\\\\,\\\\mathbf{f})\\\\)\\n6:while\\\\(\\\\|\\\\mathbf{x}_{t}\\\\!-\\\\!\\\\mathbf{x}_{\\\\text{g}}\\\\|\\\\!>\\\\!\\\\varepsilon_{T}\\\\)do\\n7: Follow trajectory \\\\((\\\\mathbf{x}_{t},\\\\hat{\\\\mathbf{x}}_{t})\\\\)\\n8:endwhile\\n9: Grasp cube at \\\\(P_{j}\\\\)\\n10: Determine stacking position \\\\(\\\\mathbf{x}_{i}\\\\leftarrow\\\\mathbf{x}_{t}\\\\), \\\\(\\\\mathbf{x}_{\\\\text{g}}\\\\gets G_{j}\\\\)\\n11: Repeat line 5 to line 8\\n12: Release cube\\n13:endfor`**Algorithm 2** Cube stacking task procedure\\n\\n**Algorithm 2**### *Experiment Results*\\n\\n*Experiment Results*To evaluate the overall efficacy of the IBC\\\\-DMP RL framework, we select policy \\\\#6 of (\\\\\\\\lambda\\\\_{\\\\\\\\text{BC}}!\\\\=!1\\\\.5\\\\), which performs the worst in the test study as shown in Tab. V, for the experimental validation. If the selected policy shows a decent performance, we can claim the general validity of the IBC\\\\-DMP framework in practical applications. We perform 22 experimental trials to incorporate the influence of randomness. For each trial, the initial positions of the cubes and the cup are randomly placed by hand. Also, we intentionally create challenging situations where the robot gripper has a large chance to go around the paper cup. One example trial is shown in Fig. 10, where the paper cup is in the mid\\\\-way between the red and the orange cubes, and the goal position. In this situation, the gripper must go around the cup to avoid collision with it, leading to nontrivial trajectories. Fig. 10 shows that the IBC\\\\-DMP policy can successfully generate collision\\\\-free trajectories for the robot gripper, which indicates the efficacy of the IBC\\\\-DMP motion planning framework.\\n\\nSimilar to Sec. VI\\\\-B, we also use the L\\\\-ARPE scores to quantitatively evaluate the performance of the IBC\\\\-DMP policy in the experimental study. We use the internal position sensor of the Gen 3 robot to record the actual executed paths of the gripper during the cube\\\\-stacking task. Fig. 11 is the box plot of the L\\\\-ARPE scores of these paths which are grouped according to trials. Each box denotes the distribution of the L\\\\-ARPE scores of the 6 trajectories of an experimental trial. The red bar in each box represents the median value of the L\\\\-ARPE scores which quantifies the overall performance of the policy in the corresponding trial. It is noticed that all red bars are higher than \\\\-7, which shows its advantage compared to the test scores of the no\\\\-IBC agent in Tab. V. Also, the scores of 16 out of 22 trials are higher than the performance standard \\\\-6, which indicates the decent overall performance of the selected policy with respect to L\\\\-ARPE scores.\\n\\nThe distribution of the ultimate reaching errors is displayed in the box plot in Fig. 12\\\\. It is noticed that all reaching errors are strictly restricted under 0\\\\.01 m due to the selection of the error threshold (\\\\\\\\varepsilon\\\\_{T}). Meanwhile, the overall collision rate is (13\\\\.64\\\\\\\\%) which is higher than the testing collision rates of the IBC\\\\-DMP agents due to the uncertainties in the real experiment. However, it is far lower than the testing collision rate of the conventional no\\\\-IBC agent. Therefore, the experimental results indicate that the selected policy achieves the ideal ultimate reaching errors as prescribed by the error threshold and has advantages over the conventional no\\\\-IBC agent in terms of collision avoidance. Since the selected policy is of inferior test performance among other IBC\\\\-DMP policies, its decent performance implies the efficacy of the proposed IBC\\\\-DMP RL framework in resolving practical robot tasks.\\n\\n\", children=[ArxivPaperSection(header='p', title='', text=\"In this section, we use an experimental case study to evaluate the performance of the proposed IBC\\\\-DMP RL method. A Rubik's cube\\\\-stacking case is used to demonstrate how to use an IBC\\\\-DMP agent to accomplish a general pick\\\\-and\\\\-place\\\\-based assembly task. The robot used in this study is a 6\\\\-DoF Kinova(r) Gen 3 manipulator with a two\\\\-finger Roboti(r) 2F\\\\-85 gripper and an Omnivision OV5640 on\\\\-gripper camera as shown in Fig. (a)a. The robot is connected to a desktop workstation which is equipped with an AMD(r) Ryzen9 3950X CPU and an Intel(r) RTX3090 GPU. The operating system of the desktop is Ubuntu 18\\\\.04\\\\. The robot is controlled by the Kinova(r) Kortex API written in Python \\\\[62]. Another RGB camera is deployed in the front of the robot to provide vision from the third\\\\-person point of view, as shown in Fig. (b)b.\", children=[]), ArxivPaperSection(header='h3', title='*Experiment Configuration*', text=\"*Experiment Configuration*As shown in Fig. (b)b, the workspace of the cube\\\\-stacking scenario is marked as a rectangular region on the table in front of the Gen3 robot arm. We set the origin of the task coordinate at the robot base. The axes of the coordinate are shown as colored arrows in Fig. (b)b. The coordinates of the vertexes of the workspace are ((0\\\\.25,0\\\\.45,0\\\\)) m, ((0\\\\.25,)\\\\-(0\\\\.2,0\\\\)) m, ((0\\\\.6,0\\\\.45,0\\\\)) m, and ((0\\\\.6,)\\\\-(0\\\\.2,0\\\\)) m. The size of the workspace is determined to cover the largest view of the on\\\\-gripper camera within the reach of the robot gripper. Three Rubik's cubes with different sizes (two cubes with a 57 mm edge and one cube with a 45 mm edge) and colors are randomly placed within the rectangular region manually. A paper cup with a radius of 90 mm and a height of 110 mm is also manually placed at a random position as an obstacle. The robot is commanded to start from the HOME position at (0\\\\.356, 0\\\\.106, 0\\\\.277\\\\) m, as shown in Fig. (b)b, pick up the Rubik's cubes, and place them at the GOAL position (marked as a green light point) one by one from the largest to the smallest (red, orange, and then blue), until they are piled up in a column. The piling\\\\-up process can be recognized as a simple assembly task, during which the robot gripper should not collide with the paper cup. The vision of the Rubik's cubes and the paper cup obstacle is captured using the on\\\\-gripper camera and their positions are calculated using the pre\\\\-built object detection libraries in YOLOv8 \\\\[63] and OpenCV \\\\[64].  \\nFor each experimental trial, we represent the initial positions of the three cubes with red, orange, and blue top surfaces as (P\\\\_{\\\\\\\\rm red}), (P\\\\_{\\\\\\\\rm org}), and (P\\\\_{\\\\\\\\rm blu}), respectively. We also set three vai\\\\-points (P\\\\_{\\\\\\\\rm red}), (P\\\\_{\\\\\\\\rm org}), and (P\\\\_{\\\\\\\\rm blu}) 5 cm right above them to ease the grasping of the cubes. In this sense, six trajectories need to be generated for the cube\\\\-stacking task, including HOME (\\\\-\\\\\\\\tilde{P}*{\\\\\\\\rm red}\\\\-G*{\\\\\\\\rm red}\\\\-\\\\\\\\tilde{P}*{\\\\\\\\rm org}\\\\-G*{\\\\\\\\rm org}\\\\-\\\\\\\\tilde{P}*{ \\\\\\\\rm blu}\\\\-G*{\\\\\\\\rm blu}), where (G\\\\_{\\\\\\\\rm red}), (G\\\\_{\\\\\\\\rm org}), and (G\\\\_{\\\\\\\\rm blu}) are the stacking positions of the cubes in the ultimate stacking  \\n*{\\\\\\\\rm red}\\\\-G**{\\\\\\\\rm org}\\\\-G**{ \\\\\\\\rm blu}\\\\-G*\\\\\\\\begin{table}\\n\\\\\\\\begin{tabular}{c\\\\|c\\\\|c\\\\|c\\\\|c} \\\\\\\\hline \\\\\\\\hline Stages \\\\& no\\\\-IBC \\\\& (\\\\\\\\lambda\\\\_{\\\\\\\\rm BC}\\\\=1\\\\) \\\\& (\\\\\\\\lambda\\\\_{\\\\\\\\rm BC}\\\\=1\\\\) \\\\& blue\\\\-\\\\=0 \\\\\\\\ \\\\\\\\hline (L\\\\_{P1}) \\\\& \\\\-6\\\\.1022 \\\\& \\\\-6\\\\.8036 \\\\& \\\\-6\\\\.2833 \\\\& \\\\-6\\\\.0177 \\\\\\\\ (L\\\\_{P2}) \\\\& \\\\-9\\\\.7703 \\\\& \\\\-5\\\\.9426 \\\\& \\\\-5\\\\.1831 \\\\& \\\\-5\\\\.6973 \\\\\\\\ (L\\\\_{P3}) \\\\& \\\\-6\\\\.3339 \\\\& \\\\-5\\\\.9161 \\\\& \\\\-6\\\\.0908 \\\\& \\\\-5\\\\.8809 \\\\\\\\ (L\\\\_{P4}) \\\\& \\\\-5\\\\.6509 \\\\& \\\\-5\\\\.9897 \\\\& \\\\-5\\\\.9751 \\\\& \\\\-6\\\\.0104 \\\\\\\\ (L\\\\_{P5}) \\\\& \\\\-5\\\\.7340 \\\\& \\\\-5\\\\.9492 \\\\& \\\\-6\\\\.0357 \\\\& \\\\-5\\\\.5467 \\\\\\\\ (L\\\\_{P6}) \\\\& \\\\-5\\\\.6169 \\\\& \\\\-5\\\\.6672 \\\\& \\\\-6\\\\.8701 \\\\& \\\\-6\\\\.1443 \\\\\\\\ (L\\\\_{P7}) \\\\& \\\\-7\\\\.3525 \\\\& \\\\-6\\\\.5007 \\\\& \\\\-5\\\\.7318 \\\\& \\\\-5\\\\.5285 \\\\\\\\ (L\\\\_{P8}) \\\\& \\\\-8\\\\.8828 \\\\& \\\\-6\\\\.2070 \\\\& \\\\-5\\\\.8197 \\\\& \\\\-6\\\\.2114 \\\\\\\\ (L\\\\_{P9}) \\\\& \\\\-6\\\\.4230 \\\\& \\\\-5\\\\.9020 \\\\& \\\\-6\\\\.2411 \\\\& \\\\-5\\\\.8731 \\\\\\\\ (L\\\\_{P10}) \\\\& \\\\-5\\\\.6641 \\\\& \\\\-5\\\\.7333 \\\\& \\\\-6\\\\.2062 \\\\& \\\\-6\\\\.2656 \\\\\\\\ \\\\\\\\hline (\\\\\\\\mu(L\\\\_{r})) \\\\& **\\\\-6\\\\.7530** \\\\& **\\\\-6\\\\.0625** \\\\& **\\\\-6\\\\.0437** \\\\& **\\\\-5\\\\.9176** \\\\\\\\ \\\\\\\\hline (\\\\\\\\mathbf{\\\\\\\\sigma}(\\\\\\\\mathbf{L\\\\_{r}})) \\\\& **1\\\\.3942** \\\\& **0\\\\.3333** \\\\& **0\\\\.4124** \\\\& **0\\\\.2487** \\\\\\\\ \\\\\\\\hline \\\\\\\\end{tabular}\\n\\\\\\\\end{table}\\nTable V: The Quantitative Scores of Agent Test  \\n**\\\\-6\\\\.7530****\\\\-6\\\\.0625****\\\\-6\\\\.0437****\\\\-5\\\\.9176****1\\\\.3942****0\\\\.3333****0\\\\.4124****0\\\\.2487**Figure 9: The Kinova® Gen3 robot (b) and the assembly scenario (b).\\ncolumn. The procedure of the stacking task is described in Algorithm 2, where *DMP* is a function used to generate the desired trajectories using the multi\\\\-DoF DMP model (9\\\\) with given initial, goal, and obstacle positions (\\\\\\\\mathbf{x}*{i}), (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}), and (\\\\\\\\mathbf{x}\\\\_{\\\\\\\\text{b}}), respectively, and the trained policy (\\\\\\\\mathbf{f}). In this experiment, we select policy \\\\#5 We are not involving additional task\\\\-level complexities in this task since the main goal of this paper is to evaluate how an IBC\\\\-DMP agent efficiently produces safe trajectories. Also, for brevity, the multi\\\\-DoF DMP model is only used to generate the translational positions of the gripper. The orientations of the gripper are commanded in a very intuitive and simple manner with the following principles.  \\n*DMP**{i}), (\\\\\\\\mathbf{x}** The gripper always points down to the table.\\n* The orientations about (x)\\\\- and (y)\\\\-axis are fixed for all generated trajectories.\\n* The orientations about (z)\\\\-axis are linearly changed during the motion, in order to grasp the cube or place it with the proper orientation in the line 5 of Algorithm 2\\\\.  \\n- The gripper always points down to the table.\\n- The orientations about (x)\\\\- and (y)\\\\-axis are fixed for all generated trajectories.\\n- The orientations about (z)\\\\-axis are linearly changed during the motion, in order to grasp the cube or place it with the proper orientation in the line 5 of Algorithm 2\\\\.\\nSuch a design ensures the successful execution of the cube\\\\-stacking task without causing singular configurations. All trajectories are designed in the Cartesian space and are mapped to the joint space of the robot using the pre\\\\-built inverse kinematics (IK) library of the Kinova(r) Gen 3 robot.  \\n```\\n1:Initialize robot gripper at HOME position\\n2:Assign \\\\(\\\\mathbf{x}_{\\\\text{b}}\\\\) with the paper cup position\\n3:for\\\\(j\\\\)in \\\\(\\\\{\\\\text{red},\\\\text{org},\\\\text{blu}\\\\}\\\\)do\\n4:\\\\(\\\\mathbf{x}_{i}\\\\leftarrow\\\\mathbf{x}_{t}\\\\), \\\\(\\\\mathbf{x}_{\\\\text{g}}\\\\leftarrow\\\\bar{P}_{j}\\\\)\\n5:Generate trajectory \\\\((\\\\mathbf{x}_{t},\\\\hat{\\\\mathbf{x}}_{t})\\\\!=\\\\!D\\\\text{MP}(\\\\mathbf{x}_{i},\\\\,\\\\mathbf{x }_{\\\\text{g}},\\\\,\\\\mathbf{x}_{\\\\text{b}},\\\\,\\\\mathbf{f})\\\\)\\n6:while\\\\(\\\\|\\\\mathbf{x}_{t}\\\\!-\\\\!\\\\mathbf{x}_{\\\\text{g}}\\\\|\\\\!>\\\\!\\\\varepsilon_{T}\\\\)do\\n7: Follow trajectory \\\\((\\\\mathbf{x}_{t},\\\\hat{\\\\mathbf{x}}_{t})\\\\)\\n8:endwhile\\n9: Grasp cube at \\\\(P_{j}\\\\)\\n10: Determine stacking position \\\\(\\\\mathbf{x}_{i}\\\\leftarrow\\\\mathbf{x}_{t}\\\\), \\\\(\\\\mathbf{x}_{\\\\text{g}}\\\\gets G_{j}\\\\)\\n11: Repeat line 5 to line 8\\n12: Release cube\\n13:endfor\\n\\n```\\n`1:Initialize robot gripper at HOME position\\n2:Assign \\\\(\\\\mathbf{x}_{\\\\text{b}}\\\\) with the paper cup position\\n3:for\\\\(j\\\\)in \\\\(\\\\{\\\\text{red},\\\\text{org},\\\\text{blu}\\\\}\\\\)do\\n4:\\\\(\\\\mathbf{x}_{i}\\\\leftarrow\\\\mathbf{x}_{t}\\\\), \\\\(\\\\mathbf{x}_{\\\\text{g}}\\\\leftarrow\\\\bar{P}_{j}\\\\)\\n5:Generate trajectory \\\\((\\\\mathbf{x}_{t},\\\\hat{\\\\mathbf{x}}_{t})\\\\!=\\\\!D\\\\text{MP}(\\\\mathbf{x}_{i},\\\\,\\\\mathbf{x }_{\\\\text{g}},\\\\,\\\\mathbf{x}_{\\\\text{b}},\\\\,\\\\mathbf{f})\\\\)\\n6:while\\\\(\\\\|\\\\mathbf{x}_{t}\\\\!-\\\\!\\\\mathbf{x}_{\\\\text{g}}\\\\|\\\\!>\\\\!\\\\varepsilon_{T}\\\\)do\\n7: Follow trajectory \\\\((\\\\mathbf{x}_{t},\\\\hat{\\\\mathbf{x}}_{t})\\\\)\\n8:endwhile\\n9: Grasp cube at \\\\(P_{j}\\\\)\\n10: Determine stacking position \\\\(\\\\mathbf{x}_{i}\\\\leftarrow\\\\mathbf{x}_{t}\\\\), \\\\(\\\\mathbf{x}_{\\\\text{g}}\\\\gets G_{j}\\\\)\\n11: Repeat line 5 to line 8\\n12: Release cube\\n13:endfor`**Algorithm 2** Cube stacking task procedure  \\n**Algorithm 2**### *Experiment Results*  \\n*Experiment Results*To evaluate the overall efficacy of the IBC\\\\-DMP RL framework, we select policy \\\\#6 of (\\\\\\\\lambda\\\\_{\\\\\\\\text{BC}}!\\\\=!1\\\\.5\\\\), which performs the worst in the test study as shown in Tab. V, for the experimental validation. If the selected policy shows a decent performance, we can claim the general validity of the IBC\\\\-DMP framework in practical applications. We perform 22 experimental trials to incorporate the influence of randomness. For each trial, the initial positions of the cubes and the cup are randomly placed by hand. Also, we intentionally create challenging situations where the robot gripper has a large chance to go around the paper cup. One example trial is shown in Fig. 10, where the paper cup is in the mid\\\\-way between the red and the orange cubes, and the goal position. In this situation, the gripper must go around the cup to avoid collision with it, leading to nontrivial trajectories. Fig. 10 shows that the IBC\\\\-DMP policy can successfully generate collision\\\\-free trajectories for the robot gripper, which indicates the efficacy of the IBC\\\\-DMP motion planning framework.  \\nSimilar to Sec. VI\\\\-B, we also use the L\\\\-ARPE scores to quantitatively evaluate the performance of the IBC\\\\-DMP policy in the experimental study. We use the internal position sensor of the Gen 3 robot to record the actual executed paths of the gripper during the cube\\\\-stacking task. Fig. 11 is the box plot of the L\\\\-ARPE scores of these paths which are grouped according to trials. Each box denotes the distribution of the L\\\\-ARPE scores of the 6 trajectories of an experimental trial. The red bar in each box represents the median value of the L\\\\-ARPE scores which quantifies the overall performance of the policy in the corresponding trial. It is noticed that all red bars are higher than \\\\-7, which shows its advantage compared to the test scores of the no\\\\-IBC agent in Tab. V. Also, the scores of 16 out of 22 trials are higher than the performance standard \\\\-6, which indicates the decent overall performance of the selected policy with respect to L\\\\-ARPE scores.  \\nThe distribution of the ultimate reaching errors is displayed in the box plot in Fig. 12\\\\. It is noticed that all reaching errors are strictly restricted under 0\\\\.01 m due to the selection of the error threshold (\\\\\\\\varepsilon\\\\_{T}). Meanwhile, the overall collision rate is (13\\\\.64\\\\\\\\%) which is higher than the testing collision rates of the IBC\\\\-DMP agents due to the uncertainties in the real experiment. However, it is far lower than the testing collision rate of the conventional no\\\\-IBC agent. Therefore, the experimental results indicate that the selected policy achieves the ideal ultimate reaching errors as prescribed by the error threshold and has advantages over the conventional no\\\\-IBC agent in terms of collision avoidance. Since the selected policy is of inferior test performance among other IBC\\\\-DMP policies, its decent performance implies the efficacy of the proposed IBC\\\\-DMP RL framework in resolving practical robot tasks.\", children=[])]),\n",
       "  ArxivPaperSection(header='h2', title='VIII Discussion', text=\"In this paper, the two important technologies used to facilitate off\\\\-policy RL for robot motion planning are multi\\\\-DoF DMP and IBC. They both can be recognized as the proper encoding of expert knowledge. The multi\\\\-DoF DMP is adapted from conventional motion primitives of which the effectiveness has been verified by many experts and peers. From a mathematical point of view, DMP models can be seen as a class of heuristic models dedicated to reducing the dimensionality of a planning problem. The dynamic structure of a DMP model reflects how expert knowledge is used to restrict the smoothness and inherent stability of generated trajectories. IBC, as an adapted version of BC for imitation learning, provides a different manner to encode expert knowledge by cloning the expert's behaviors in the task. Note that these two different sources of expert knowledge might not always be beneficial to the given robot task. They may even conflict with each other in certain circumstances. A very important technical point of this paper is to flexibly combine the two types of expert knowledge using an off\\\\-policy RL framework, such that the resulting motion planning policy is not overfitting to any source of expert knowledge. Furthermore, the advantages of both types of knowledge are fully exploited to improve the\\ntraining speed, generalizability, and reliability of the RL agent. The decent results of the simulation and experimental studies provide evidence that the performance of RL can be improved by properly encoding expert knowledge.\\n\\nAlthough the general efficacy of the IBC\\\\-DMP framework is validated, it still has limitations. The multi\\\\-DoF DMP model has a higher flexibility than the conventional DMP since its actuation function is dependent on the state of the model. However, this may sacrifice the stability of the DMP model, possibly leading to odd\\\\-shaped trajectories. Besides, an IBC\\\\-DMP may be over\\\\-trained if the number of training episodes is too large, where the training score of the policy decreases or becomes unsteady as the training proceeds longer. This may also be due to the sacrifice of the inherent stability of the Multi\\\\-DoF DMP. Another limitation of IBC\\\\-DMP is that its performance tends to be sensitive to the exploring noise added to actions during the training stage. Further investigations on the correlation between the performance of IBC\\\\-DMP and the action noise are needed in future work.\\n\\n\", children=[ArxivPaperSection(header='p', title='', text=\"In this paper, the two important technologies used to facilitate off\\\\-policy RL for robot motion planning are multi\\\\-DoF DMP and IBC. They both can be recognized as the proper encoding of expert knowledge. The multi\\\\-DoF DMP is adapted from conventional motion primitives of which the effectiveness has been verified by many experts and peers. From a mathematical point of view, DMP models can be seen as a class of heuristic models dedicated to reducing the dimensionality of a planning problem. The dynamic structure of a DMP model reflects how expert knowledge is used to restrict the smoothness and inherent stability of generated trajectories. IBC, as an adapted version of BC for imitation learning, provides a different manner to encode expert knowledge by cloning the expert's behaviors in the task. Note that these two different sources of expert knowledge might not always be beneficial to the given robot task. They may even conflict with each other in certain circumstances. A very important technical point of this paper is to flexibly combine the two types of expert knowledge using an off\\\\-policy RL framework, such that the resulting motion planning policy is not overfitting to any source of expert knowledge. Furthermore, the advantages of both types of knowledge are fully exploited to improve the\\ntraining speed, generalizability, and reliability of the RL agent. The decent results of the simulation and experimental studies provide evidence that the performance of RL can be improved by properly encoding expert knowledge.  \\nAlthough the general efficacy of the IBC\\\\-DMP framework is validated, it still has limitations. The multi\\\\-DoF DMP model has a higher flexibility than the conventional DMP since its actuation function is dependent on the state of the model. However, this may sacrifice the stability of the DMP model, possibly leading to odd\\\\-shaped trajectories. Besides, an IBC\\\\-DMP may be over\\\\-trained if the number of training episodes is too large, where the training score of the policy decreases or becomes unsteady as the training proceeds longer. This may also be due to the sacrifice of the inherent stability of the Multi\\\\-DoF DMP. Another limitation of IBC\\\\-DMP is that its performance tends to be sensitive to the exploring noise added to actions during the training stage. Further investigations on the correlation between the performance of IBC\\\\-DMP and the action noise are needed in future work.\", children=[])]),\n",
       "  ArxivPaperSection(header='h2', title='IX Conclusion', text='In this paper, we propose a novel framework for developing RL agents for robot motion\\\\-planning tasks based on two promoted methods, namely multi\\\\-DoF DMP and IBC. An off\\\\-policy RL agent serves as a bridge to flexibly combine the expert knowledge encoded by the two methods, resulting in an advantageous agent with improved training speed, generalizability, and reliability. Its efficacy and advantage over the conventional no\\\\-IBC agent are validated by simulation and experimental studies. In future work, we will focus on improving the stability of the Multi\\\\-DoF DMP model and the sensitivity of IBC\\\\-DMP agents to exploring action noise.\\n\\n', children=[ArxivPaperSection(header='p', title='', text='In this paper, we propose a novel framework for developing RL agents for robot motion\\\\-planning tasks based on two promoted methods, namely multi\\\\-DoF DMP and IBC. An off\\\\-policy RL agent serves as a bridge to flexibly combine the expert knowledge encoded by the two methods, resulting in an advantageous agent with improved training speed, generalizability, and reliability. Its efficacy and advantage over the conventional no\\\\-IBC agent are validated by simulation and experimental studies. In future work, we will focus on improving the stability of the Multi\\\\-DoF DMP model and the sensitivity of IBC\\\\-DMP agents to exploring action noise.', children=[])])])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ArxivPaperSection(header='p', title='', text='In this section, we first introduce the overall framework of the proposed IBC\\\\-DMP RL method. Then, we present a novel Multi\\\\-DoF DMP model for motion planning of a high degree\\\\-of\\\\-freedom (DoF) robot. Finally, based on the proposed framework, we clarify the problem to be solved in this paper.', children=[]),\n",
       " ArxivPaperSection(header='h3', title='*Overall Framework*', text='*Overall Framework*The overall framework of IBC\\\\-DMP RL is illustrated in Fig. 2\\\\. The basic model used to generate robot trajectories is *Multi\\\\-DoF DMP*, an adapted version of the conventional DMP model introduced in Sec. II\\\\-A. It also serves as the environment in the robot motion planning problem. The human demonstration encodes a demonstration policy (\\\\\\\\tilde{\\\\\\\\mathbf{f}}) which reflects how humans behave in a motion planning task. An *IBC\\\\-DMP agent* is used to generate the desired motion planning policy (\\\\\\\\mathbf{f}) for the task. The agent is trained by a dual\\\\-buffer structure which is composed of a *demonstration buffer* and an *interaction buffer*. These two buffers store the demonstration data generated from the human demonstrations and the interaction data during the learning process, respectively.  \\n*Multi\\\\-DoF DMP**IBC\\\\-DMP agent**demonstration buffer**interaction buffer*### *Motion Planning Using Multi\\\\-DoF DMP*  \\n*Motion Planning Using Multi\\\\-DoF DMP*The conventional DMP in Sec. II\\\\-A is defined for a one\\\\-dimensional point and has been used for robot motion planning in a decoupled manner, i.e., one DMP is used to generate the trajectory for each DoF of the robot \\\\[27]. This leads to the lack of coupling among different DoFs, which restricts the flexibility of solving the optimal planning policy. To resolve this issue, in this paper, we propose the following Multi\\\\-DoF\\nDMP model for the motion planning of a robot end\\\\-effector in the Cartesian space,  \\n\\\\[\\\\\\\\tau\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}!\\\\=!K*{\\\\\\\\alpha}(K\\\\_{\\\\\\\\beta}(\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}!\\\\-! \\\\\\\\mathbf{x}*{t})!\\\\-!\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t})!\\\\+\\\\\\\\zeta*{t}\\\\|\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}!\\\\- !\\\\\\\\mathbf{x}*{i}\\\\|\\\\\\\\mathbf{f}(\\\\\\\\mathcal{X}\\\\_{t}), \\\\\\\\tag{9}]  \\n*{t}!\\\\=!K**{\\\\\\\\text{g}}!\\\\-! \\\\\\\\mathbf{x}**{t})!\\\\+\\\\\\\\zeta**{\\\\\\\\text{g}}!\\\\- !\\\\\\\\mathbf{x}*where (\\\\\\\\mathbf{x}*{t}), (\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}), (\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}\\\\\\\\in\\\\\\\\mathbb{R}^{3}) are the position, linear velocity, and acceleration of the end\\\\-effector in the Cartesian space, (\\\\\\\\mathbf{x}*{i}), (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) are the initial and goal positions of the end\\\\-effector, (K*{\\\\\\\\alpha}), (K\\\\_{\\\\\\\\beta}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3\\\\\\\\times 3}) are parametric matrices, (\\\\\\\\tau!\\\\\\\\in!\\\\\\\\mathbb{R}) and (\\\\\\\\zeta\\\\_{t}!\\\\\\\\in!\\\\\\\\mathbb{R}) are the temporal scalar and the canonical variable, the same as (1\\\\), (\\\\\\\\mathcal{X}*{t}!\\\\=!{\\\\\\\\,\\\\\\\\mathbf{x}*{t},\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t},\\\\\\\\mathbf{x}*{\\\\\\\\text {b}},\\\\\\\\zeta\\\\_{t}}) is the state vector of the Multi\\\\-DoF DMP, where (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}) is a vector that describes the configuration of the obstacle, and (\\\\\\\\mathbf{f}:\\\\\\\\mathbb{R}^{10}!\\\\\\\\rightarrow!\\\\\\\\mathbb{R}^{3}) is a multi\\\\-dimension actuation function to be determined. The initial state of model (9\\\\) is set as (\\\\\\\\mathbf{x}*{0}!\\\\=!\\\\\\\\mathbf{x}*{\\\\\\\\text{i}}) and (\\\\\\\\dot{\\\\\\\\mathbf{x}}*{0}!\\\\=!0\\\\). In this paper, we only consider a *single static obstacle* for brevity. Also, we use a vertically positioned *cylinder* model to represent the obstacle. Then, the position of the obstacle is represented as a three\\\\-dimensional constant vector (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) assigned as the Cartesian coordinate of the top surface center of the cylinder. In this sense, the DMP state can be represented as a ten\\\\-dimensional vector (\\\\\\\\mathcal{X}*{t}!\\\\=!\\\\[\\\\\\\\mathbf{x}*{t}^{\\\\\\\\top},\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}^{\\\\\\\\top}, \\\\\\\\mathbf{x}*{t}^{\\\\\\\\top}\\\\-\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}^{\\\\\\\\top},\\\\\\\\zeta\\\\_{t}]^{\\\\\\\\top}!\\\\\\\\in! \\\\\\\\mathbb{R}^{10}). Here, we encode a time\\\\-variant vector (\\\\\\\\mathbf{x}*{t}!\\\\-!\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}) into the DMP state instead of the constant vector (\\\\\\\\mathbf{x}\\\\_{\\\\\\\\text{b}}) since the former provides larger diversity to the data. The incorporation of more complicated environments with multiple dynamic obstacles is beyond the scope of this paper and will be considered in future work. The multi\\\\-DoF DMP is different from the conventional DMP (1\\\\) in three aspects.  \\n*{t}), (\\\\\\\\dot{\\\\\\\\mathbf{x}}**{t}\\\\\\\\in\\\\\\\\mathbb{R}^{3}) are the position, linear velocity, and acceleration of the end\\\\-effector in the Cartesian space, (\\\\\\\\mathbf{x}**{\\\\\\\\text{g}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) are the initial and goal positions of the end\\\\-effector, (K**{t}!\\\\=!{\\\\\\\\,\\\\\\\\mathbf{x}**{t},\\\\\\\\mathbf{x}**{\\\\\\\\text{b}}) is a vector that describes the configuration of the obstacle, and (\\\\\\\\mathbf{f}:\\\\\\\\mathbb{R}^{10}!\\\\\\\\rightarrow!\\\\\\\\mathbb{R}^{3}) is a multi\\\\-dimension actuation function to be determined. The initial state of model (9\\\\) is set as (\\\\\\\\mathbf{x}**{\\\\\\\\text{i}}) and (\\\\\\\\dot{\\\\\\\\mathbf{x}}**single static obstacle**cylinder**{\\\\\\\\text{b}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) assigned as the Cartesian coordinate of the top surface center of the cylinder. In this sense, the DMP state can be represented as a ten\\\\-dimensional vector (\\\\\\\\mathcal{X}**{t}^{\\\\\\\\top},\\\\\\\\dot{\\\\\\\\mathbf{x}}**{t}^{\\\\\\\\top}\\\\-\\\\\\\\mathbf{x}**{t}!\\\\-!\\\\\\\\mathbf{x}** The actuation function (\\\\\\\\mathbf{f}) does not only depend on the canonical variable (\\\\\\\\zeta\\\\_{t}) but also on the internal states of the DMP, namely (\\\\\\\\mathbf{x}*{t}) and (\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}), and the obstacle position (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}). Different from the conventional DMP models, this may change the poles or the closed\\\\-loop dynamics of the Multi\\\\-DoF DMP, which can be both an advantage and a drawback. On the one hand, the state\\\\-dependent actuation function (\\\\\\\\mathbf{f}) can be learned to automatically adjust the poles of the DMP model, such that its dynamic properties are not completely determined by the hyperparameters (K*{\\\\\\\\alpha}) and (K\\\\_{\\\\\\\\beta}). This improves the flexibility of agent training. On the other hand, the DMP model may lose its inherent stability. In this paper, we set action limits (\\\\\\\\underline{f}\\\\\\\\leq\\\\\\\\mathbf{f}(\\\\\\\\mathcal{X}\\\\_{t})\\\\\\\\leq\\\\\\\\overline{f}) to avoid this.\\n* The gain of the actuation function is the absolute distance between the initial position (\\\\\\\\mathbf{x}*{i}) and the goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}), (\\\\|\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}\\\\-\\\\\\\\mathbf{x}*{i}\\\\|), instead of the element\\\\-wise distance used by the conventional DMP (1\\\\). Such a scheme can improve the flexibility of a DMP model by fully incorporating the coupling of its different dimensions.\\n* The obstacle position (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}) is included in the state (\\\\\\\\mathcal{X}*{t}) of the DMP, instead of being incorporated as an additional virtual force term as the conventional DMP model. This provides the possibility of incorporating more complicated obstacle information into motion planning.  \\n- The actuation function (\\\\\\\\mathbf{f}) does not only depend on the canonical variable (\\\\\\\\zeta\\\\_{t}) but also on the internal states of the DMP, namely (\\\\\\\\mathbf{x}*{t}) and (\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}), and the obstacle position (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}). Different from the conventional DMP models, this may change the poles or the closed\\\\-loop dynamics of the Multi\\\\-DoF DMP, which can be both an advantage and a drawback. On the one hand, the state\\\\-dependent actuation function (\\\\\\\\mathbf{f}) can be learned to automatically adjust the poles of the DMP model, such that its dynamic properties are not completely determined by the hyperparameters (K*{\\\\\\\\alpha}) and (K\\\\_{\\\\\\\\beta}). This improves the flexibility of agent training. On the other hand, the DMP model may lose its inherent stability. In this paper, we set action limits (\\\\\\\\underline{f}\\\\\\\\leq\\\\\\\\mathbf{f}(\\\\\\\\mathcal{X}\\\\_{t})\\\\\\\\leq\\\\\\\\overline{f}) to avoid this.\\n*{t}) and (\\\\\\\\dot{\\\\\\\\mathbf{x}}**{\\\\\\\\text{b}}). Different from the conventional DMP models, this may change the poles or the closed\\\\-loop dynamics of the Multi\\\\-DoF DMP, which can be both an advantage and a drawback. On the one hand, the state\\\\-dependent actuation function (\\\\\\\\mathbf{f}) can be learned to automatically adjust the poles of the DMP model, such that its dynamic properties are not completely determined by the hyperparameters (K*- The gain of the actuation function is the absolute distance between the initial position (\\\\\\\\mathbf{x}*{i}) and the goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}), (\\\\|\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}\\\\-\\\\\\\\mathbf{x}*{i}\\\\|), instead of the element\\\\-wise distance used by the conventional DMP (1\\\\). Such a scheme can improve the flexibility of a DMP model by fully incorporating the coupling of its different dimensions.\\n*{i}) and the goal position (\\\\\\\\mathbf{x}**{\\\\\\\\text{g}}\\\\-\\\\\\\\mathbf{x}*- The obstacle position (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}) is included in the state (\\\\\\\\mathcal{X}*{t}) of the DMP, instead of being incorporated as an additional virtual force term as the conventional DMP model. This provides the possibility of incorporating more complicated obstacle information into motion planning.\\n*{\\\\\\\\text{b}}) is included in the state (\\\\\\\\mathcal{X}*### *Cost Function of Motion Planning*  \\n*Cost Function of Motion Planning*This paper solves a general robot motion planning problem. Specifically, given an initial position (\\\\\\\\mathbf{x}*{i}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) and a goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) in the Cartesian space, generate a smooth trajectory (\\\\\\\\mathbf{x}*{t}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) for (0!\\\\<!t!\\\\\\\\leq!T) using the Multi\\\\-DoF DMP model in (9\\\\), such that (\\\\\\\\mathbf{x}*{T}) is sufficiently close to (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}), where (T!\\\\\\\\in!\\\\\\\\mathbb{R}^{\\\\+}) is the predefined timing length of the trajectory. The generated trajectory should also ensure sufficient smoothness (minimal acceleration) and maintain a certain distance with a given static obstacle positioned in (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{2}). These requirements are encoded in the following cost function,  \\n*{i}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) and a goal position (\\\\\\\\mathbf{x}**{t}!\\\\\\\\in!\\\\\\\\mathbb{R}^{3}) for (0!\\\\<!t!\\\\\\\\leq!T) using the Multi\\\\-DoF DMP model in (9\\\\), such that (\\\\\\\\mathbf{x}**{\\\\\\\\text{g}}), where (T!\\\\\\\\in!\\\\\\\\mathbb{R}^{\\\\+}) is the predefined timing length of the trajectory. The generated trajectory should also ensure sufficient smoothness (minimal acceleration) and maintain a certain distance with a given static obstacle positioned in (\\\\\\\\mathbf{x}*\\\\[\\\\\\\\mathcal{J}*{t}\\\\=\\\\\\\\left{\\\\\\\\begin{array}{ll}\\\\\\\\sum*{i\\\\=1}^{4}\\\\\\\\alpha\\\\_{i}\\\\\\\\mathcal{J} *{t}^{(i)},\\\\&0\\\\\\\\leq t\\\\<T,\\\\\\\\ \\\\\\\\alpha*{5}\\\\\\\\mathcal{J}\\\\_{t}^{(5\\\\)},\\\\&t\\\\=T,\\\\\\\\end{array}\\\\\\\\right. \\\\\\\\tag{10}]  \\n*{t}\\\\=\\\\\\\\left{\\\\\\\\begin{array}{ll}\\\\\\\\sum**{t}^{(i)},\\\\&0\\\\\\\\leq t\\\\<T,\\\\\\\\ \\\\\\\\alpha*where (\\\\\\\\alpha\\\\_{i}!\\\\\\\\in!\\\\\\\\mathbb{R}^{\\\\+}) are constant parameters, (i\\\\=1,2,\\\\\\\\cdots,5\\\\), and (\\\\\\\\mathcal{J}\\\\_{t}^{(i)}!\\\\\\\\in!\\\\\\\\mathbb{R}) are instant costs at time (t), defined as  \\n\\\\[\\\\\\\\mathcal{J}*{t}^{(1\\\\)}!\\\\=!!\\\\\\\\left\\\\|\\\\\\\\dot{\\\\\\\\mathbf{x}}*{t}\\\\\\\\right\\\\|^{2},\\\\\\\\ \\\\\\\\mathcal{J}*{t}^{(2\\\\)}!\\\\=!\\\\\\\\left\\\\|\\\\\\\\mathbf{x}*{t}!\\\\-!\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}\\\\\\\\right\\\\|^ {2},] \\\\[\\\\\\\\mathcal{J}*{t}^{(4\\\\)}!\\\\=!\\\\\\\\left{!\\\\\\\\begin{array}{ll}\\\\\\\\eta! \\\\\\\\left(!\\\\\\\\left\\\\|\\\\\\\\mathbf{x}*{t}^{(1,2\\\\)}!\\\\-!\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}^{(1,2\\\\)}! \\\\\\\\right\\\\|!\\\\-!\\\\\\\\tau\\\\_{\\\\\\\\text{b}}!,\\\\\\\\varepsilon\\\\_{0}^{\\\\\\\\text{b}}!,\\\\\\\\varepsilon\\\\_{1}^{ \\\\\\\\text{b}}\\\\\\\\right)\\\\&\\\\\\\\text{if }0!\\\\<!\\\\\\\\mathbf{x}*{t}^{(3\\\\)}!\\\\<!\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}^{(3\\\\)}\\\\\\\\ 0\\\\&\\\\\\\\text{otherwise},\\\\\\\\end{array}\\\\\\\\right.] \\\\[\\\\\\\\mathcal{J}*{t}^{(3\\\\)}\\\\=\\\\\\\\eta!\\\\\\\\left(!\\\\\\\\mathbf{x}*{t}^{(3\\\\)}, \\\\\\\\varepsilon\\\\_{0}^{\\\\\\\\text{d}},\\\\\\\\varepsilon\\\\_{1}^{\\\\\\\\text{d}}\\\\\\\\right),\\\\\\\\ \\\\\\\\mathcal{J}*{t}^{(5\\\\)}\\\\=\\\\\\\\xi(\\\\|\\\\\\\\mathbf{x}*{t}!\\\\-!\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}\\\\|, \\\\\\\\varepsilon*{T}),]  \\n*{t}^{(1\\\\)}!\\\\=!!\\\\\\\\left\\\\|\\\\\\\\dot{\\\\\\\\mathbf{x}}**{t}^{(2\\\\)}!\\\\=!\\\\\\\\left\\\\|\\\\\\\\mathbf{x}**{\\\\\\\\text{g}}\\\\\\\\right\\\\|^ {2},] \\\\[\\\\\\\\mathcal{J}**{t}^{(1,2\\\\)}!\\\\-!\\\\\\\\mathbf{x}**{t}^{(3\\\\)}!\\\\<!\\\\\\\\mathbf{x}**{t}^{(3\\\\)}\\\\=\\\\\\\\eta!\\\\\\\\left(!\\\\\\\\mathbf{x}**{t}^{(5\\\\)}\\\\=\\\\\\\\xi(\\\\|\\\\\\\\mathbf{x}**{\\\\\\\\text{g}}\\\\|, \\\\\\\\varepsilon*where (\\\\\\\\mathbf{x}*{t}^{(1,2\\\\)}!\\\\\\\\in!\\\\\\\\mathbb{R}^{2}), (\\\\\\\\mathbf{x}*{t}^{(3\\\\)}!\\\\\\\\in!\\\\\\\\mathbb{R}), (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}^{(1,2\\\\)}!\\\\\\\\in!\\\\\\\\mathbb{R}^{2}), (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}^{(3\\\\)}!\\\\\\\\in!\\\\\\\\mathbb{R}) are the slides of vectors (\\\\\\\\mathbf{x}*{t}) and (\\\\\\\\mathbf{x}*{\\\\\\\\text{b}}), (r\\\\_{\\\\\\\\text{b}}!\\\\\\\\in!\\\\\\\\mathbb{R}^{\\\\+}) is the radius of the cylinder, (\\\\\\\\eta:\\\\\\\\mathbb{R}!\\\\\\\\rightarrow!\\\\\\\\mathbb{R}*{\\\\\\\\geq 0}) with constant parameters (0!\\\\<!\\\\\\\\varepsilon*{0}!\\\\<!\\\\\\\\varepsilon\\\\_{1}) is an artificial potential field function defined as  \\n*{t}^{(1,2\\\\)}!\\\\\\\\in!\\\\\\\\mathbb{R}^{2}), (\\\\\\\\mathbf{x}**{\\\\\\\\text{b}}^{(1,2\\\\)}!\\\\\\\\in!\\\\\\\\mathbb{R}^{2}), (\\\\\\\\mathbf{x}**{t}) and (\\\\\\\\mathbf{x}**{\\\\\\\\geq 0}) with constant parameters (0!\\\\<!\\\\\\\\varepsilon*\\\\[\\\\\\\\eta(x,\\\\\\\\varepsilon\\\\_{0},\\\\\\\\varepsilon\\\\_{1})!\\\\=!\\\\\\\\left{!\\\\\\\\begin{array}{ll}(x!\\\\- !\\\\\\\\varepsilon\\\\_{0})^{\\\\-2}!\\\\-!(\\\\\\\\varepsilon\\\\_{1}!\\\\-!\\\\\\\\varepsilon\\\\_{0})^{\\\\-2},\\\\& \\\\\\\\varepsilon\\\\_{0}!\\\\<!x!\\\\<!\\\\\\\\varepsilon\\\\_{1},\\\\\\\\ 0,\\\\&x!\\\\\\\\geq!\\\\\\\\varepsilon\\\\_{1},\\\\\\\\end{array}\\\\\\\\right. \\\\\\\\tag{11}]  \\n(0!\\\\<!\\\\\\\\varepsilon\\\\_{0}^{\\\\\\\\text{b}}!\\\\<!\\\\\\\\varepsilon\\\\_{1}^{\\\\\\\\text{b}}) and (0!\\\\<!\\\\\\\\varepsilon\\\\_{0}^{\\\\\\\\text{d}}!\\\\<!\\\\\\\\varepsilon\\\\_{1}^{\\\\\\\\text{d}}) are potential field parameters for the obstacle and the ground, and (\\\\\\\\xi:\\\\\\\\mathbb{R}!\\\\\\\\rightarrow!\\\\\\\\mathbb{R}\\\\_{\\\\\\\\geq 0}) is a squared dead\\\\-zone function defined as  \\n\\\\[\\\\\\\\xi(x,\\\\\\\\varepsilon)\\\\=\\\\\\\\left{\\\\\\\\begin{array}{ll}0,\\\\&0\\\\\\\\leq x\\\\\\\\leq\\\\\\\\varepsilon,\\\\\\\\ x\\\\-\\\\\\\\varepsilon,\\\\&x\\\\>\\\\\\\\varepsilon,\\\\\\\\end{array}\\\\\\\\right. \\\\\\\\tag{12}]  \\nwhere (\\\\\\\\varepsilon!\\\\\\\\in!\\\\\\\\mathbb{R}^{\\\\+}) is the dead\\\\-zone scalar. Here, (\\\\\\\\eta(x)) serves as an artificial potential field function that penalizes (x) if it gets close to (\\\\\\\\varepsilon\\\\_{0}) from the positive direction. Another parameter (\\\\\\\\varepsilon\\\\_{1}) is the upper limit of (x) such that (\\\\\\\\eta(x)) has an effect. To avoid undefinition, we assign (\\\\\\\\eta(x)) with a large number when (x!\\\\\\\\leq!\\\\\\\\varepsilon\\\\_{0}).  \\nIn the comprehensive cost function (10\\\\), term (\\\\\\\\mathcal{J}\\\\_{t}^{(1\\\\)}) penalizes the value of the acceleration to ensure sufficient  \\nFigure 2: The illustration of the IBC\\\\-DMP RL framework, where (\\\\\\\\mathcal{X}*{t}) denotes the position, velocity, and acceleration of the Multi\\\\-DoF DMP model, (\\\\\\\\tilde{\\\\\\\\mathbf{f}}(\\\\\\\\mathcal{X}*{t})) and (\\\\\\\\mathbf{f}(\\\\\\\\mathcal{X}*{t})) are the actuation functions provided by the human demonstration and the IBC\\\\-DMP agent, respectively, and (s*{t}) and (a\\\\_{t}) are the state and action data provided by human demonstration or generated by the IBC\\\\-DMP agent. Besides, the solid arrows denote the interactions, the dotted arrows indicate data storing, and the dashed arrow represents agent training.\\nsmoothness. Term (\\\\\\\\mathcal{J}*{t}^{(2\\\\)}) penalizes the distance between the current position (\\\\\\\\mathbf{x}*{t}) and the goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}), aiming at fast and straightforward goal reaching. Terms (\\\\\\\\mathcal{J}*{t}^{(3\\\\)}) and (\\\\\\\\mathcal{J}*{t}^{(4\\\\)}) attempt to keep the position (\\\\\\\\mathbf{x}*{t}) away from the ground ((z\\\\=0\\\\) plane) and the cylinder obstacle, respectively. The usage of artificial potential field functions ensures a very large cost when (\\\\\\\\mathbf{x}*{t}) runs into the cylinder obstacle or the ground. The last term (\\\\\\\\mathcal{J}*{T}^{(5\\\\)}) penalizes the distance between the ultimate position (\\\\\\\\mathbf{x}*{T}) and the goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}). A dead\\\\-zone scalar (\\\\\\\\varepsilon\\\\_{T}) is involved to prescribe the tolerable level of the ultimate error. Specifically, the ultimate cost is exerted only when the ultimate error exceeds the threshold (\\\\\\\\varepsilon\\\\_{T}). By minimizing the comprehensive cost (10\\\\) for the Multi\\\\-DoF DMP model (9\\\\), one can get a smooth trajectory (\\\\\\\\mathbf{x}*{t}) that is able to reach the goal position (\\\\\\\\mathbf{x}*{\\\\\\\\text{g}}) at the ending time (t\\\\=T) while avoiding collision with the ground and the obstacle.  \\n*{t}) denotes the position, velocity, and acceleration of the Multi\\\\-DoF DMP model, (\\\\\\\\tilde{\\\\\\\\mathbf{f}}(\\\\\\\\mathcal{X}**{t})) are the actuation functions provided by the human demonstration and the IBC\\\\-DMP agent, respectively, and (s**{t}^{(2\\\\)}) penalizes the distance between the current position (\\\\\\\\mathbf{x}**{\\\\\\\\text{g}}), aiming at fast and straightforward goal reaching. Terms (\\\\\\\\mathcal{J}**{t}^{(4\\\\)}) attempt to keep the position (\\\\\\\\mathbf{x}**{t}) runs into the cylinder obstacle or the ground. The last term (\\\\\\\\mathcal{J}**{T}) and the goal position (\\\\\\\\mathbf{x}**{t}) that is able to reach the goal position (\\\\\\\\mathbf{x}*### *Decision\\\\-Making Problem Statement*  \\n*Decision\\\\-Making Problem Statement*Consider that the Multi\\\\-DoF DMP model (9\\\\) is discretized in time in a zero\\\\-order hold manner with a discrete sampling time (\\\\\\\\Delta t!\\\\\\\\in!\\\\\\\\mathbb{R}^{\\\\+}), as follows,  \\n\\\\[\\\\\\\\begin{split}\\\\\\\\mathbf{x}*{t\\\\+\\\\\\\\Delta t}!\\\\=!\\\\\\\\mathbf{x}*{t}\\\\+\\\\\\\\Delta t \\\\\\\\hat{\\\\\\\\mathbf{x}}*{t},\\\\\\\\ \\\\\\\\tilde{\\\\\\\\mathbf{x}}*{t\\\\+\\\\\\\\Delta t}!\\\\=!\\\\\\\\tilde{\\\\\\\\mathbf{x}}*{t}\\\\+\\\\\\\\Delta t\\\\\\\\tilde{ \\\\\\\\mathbf{x}}*{t},\\\\\\\\ \\\\\\\\zeta\\\\_{t\\\\+\\\\\\\\Delta t}!\\\\=!\\\\\\\\zeta\\\\_{t}\\\\-\\\\\\\\frac{\\\\\\\\Delta t}{\\\\\\\\tau}\\\\\\\\omega\\\\\\\\zeta \\\\_{t},\\\\\\\\end{split} \\\\\\\\tag{13}]  \\n*{t\\\\+\\\\\\\\Delta t}!\\\\=!\\\\\\\\mathbf{x}**{t},\\\\\\\\ \\\\\\\\tilde{\\\\\\\\mathbf{x}}**{t}\\\\+\\\\\\\\Delta t\\\\\\\\tilde{ \\\\\\\\mathbf{x}}*where (\\\\\\\\tilde{\\\\\\\\mathbf{x}}*{t}) is given by (9\\\\). Then, the generated trajectory (\\\\\\\\mathbf{x}*{t}) is a set of waypoints at the sampled timing points (\\\\\\\\mathbf{t}\\\\={0,\\\\\\\\Delta t,2\\\\\\\\Delta t,\\\\\\\\cdots,T}). In this sense, the objective of motion planning is to solve the optimal actuation function (\\\\\\\\mathbf{f}) in (9\\\\) such that the accumulated cost (\\\\\\\\sum\\\\_{t\\\\=0}^{T}\\\\\\\\mathcal{J}*{t}) is minimized. This problem renders decision\\\\-making over an MDP (\\\\\\\\mathcal{M}\\\\=(\\\\\\\\mathcal{S},\\\\\\\\mathcal{A},\\\\\\\\mathcal{F},\\\\\\\\mathcal{R})) defined in Sec. II\\\\-B, where (\\\\\\\\mathcal{S}\\\\={\\\\\\\\mathcal{X}*{t}\\\\|\\\\\\\\mathbf{x}*{i}\\\\\\\\in\\\\\\\\mathbb{R}^{3},\\\\\\\\,0\\\\\\\\leq t\\\\<T}) is the state space, (\\\\\\\\mathcal{A}!\\\\=!{\\\\\\\\mathbf{f}(s)\\\\\\\\,\\\\|\\\\\\\\forall\\\\\\\\,s\\\\\\\\in\\\\\\\\mathcal{S}}) is the action space, (\\\\\\\\mathcal{F}) is the state transition characterized by (13\\\\), and (\\\\\\\\mathcal{R}:\\\\-\\\\\\\\mathcal{J}*{t}) is the instant reward. In this sense, the policy (\\\\\\\\pi) can be represented as a parameterized actuation function (\\\\\\\\mathbf{f}*{\\\\\\\\theta}), modeled as a neural network and learned using an off\\\\-policy RL method introduced in Sec. II\\\\-B. Besides, BC introduced in Sec. II\\\\-C can be used to promote the training of the RL agent with the demonstration policy (\\\\\\\\mathbf{\\\\\\\\tilde{f}}), as addressed in Sec. II\\\\-C. Note that, different from the conventional imitation learning problems, this paper does not aim at using (\\\\\\\\mathbf{f}*{\\\\\\\\theta}) to approximate the demonstration policy (\\\\\\\\mathbf{\\\\\\\\tilde{f}}). Instead, (\\\\\\\\mathbf{\\\\\\\\tilde{f}}) is only used to improve the training of (\\\\\\\\mathbf{f}\\\\_{\\\\\\\\theta}).  \\n*{t}) is given by (9\\\\). Then, the generated trajectory (\\\\\\\\mathbf{x}**{t}) is minimized. This problem renders decision\\\\-making over an MDP (\\\\\\\\mathcal{M}\\\\=(\\\\\\\\mathcal{S},\\\\\\\\mathcal{A},\\\\\\\\mathcal{F},\\\\\\\\mathcal{R})) defined in Sec. II\\\\-B, where (\\\\\\\\mathcal{S}\\\\={\\\\\\\\mathcal{X}**{i}\\\\\\\\in\\\\\\\\mathbb{R}^{3},\\\\\\\\,0\\\\\\\\leq t\\\\<T}) is the state space, (\\\\\\\\mathcal{A}!\\\\=!{\\\\\\\\mathbf{f}(s)\\\\\\\\,\\\\|\\\\\\\\forall\\\\\\\\,s\\\\\\\\in\\\\\\\\mathcal{S}}) is the action space, (\\\\\\\\mathcal{F}) is the state transition characterized by (13\\\\), and (\\\\\\\\mathcal{R}:\\\\-\\\\\\\\mathcal{J}**{\\\\\\\\theta}), modeled as a neural network and learned using an off\\\\-policy RL method introduced in Sec. II\\\\-B. Besides, BC introduced in Sec. II\\\\-C can be used to promote the training of the RL agent with the demonstration policy (\\\\\\\\mathbf{\\\\\\\\tilde{f}}), as addressed in Sec. II\\\\-C. Note that, different from the conventional imitation learning problems, this paper does not aim at using (\\\\\\\\mathbf{f}*Now, we are ready to state the two main objectives of this paper: (1\\\\). transforming human demonstration into data that are compatible with the demonstration buffer, corresponding to the left column of Fig. 2, and (2\\\\). train the optimal policy (\\\\\\\\mathbf{f}) using the demonstration buffer, as the right column of Fig. 2\\\\. Our solutions to these two problems will be presented in Sec. IV and Sec. V, respectively.', children=[])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][1][2].children"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
