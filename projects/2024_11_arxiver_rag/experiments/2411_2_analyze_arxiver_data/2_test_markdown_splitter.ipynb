{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.config import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 7) Index(['id', 'title', 'abstract', 'authors', 'published_date', 'link',\n",
      "       'markdown'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(os.path.join(settings.data_dir, \"arxiver/data/train.parquet\"))\n",
    "## Sample 10k\n",
    "df = df.sample(10000)\n",
    "print(df.shape, df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Splitter\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "]\n",
    "splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D-Mol: A Novel Contrastive Learning Framework for Molecular Property Prediction with 3D Information\n",
      "\n",
      "###### Abstract\n",
      "\n",
      "Molecular property prediction offers an effective and efficient approach for early screening and optimization of drug candidates. Although deep learning based methods have made notable progress, most existing works still do not fully utilize 3D spatial information. This can lead to a single molecular representation representing multiple actual molecules. To address these issues, we propose a novel 3D structure-based molecular modeling method named 3D-Mol. In order to accurately represent complete spatial structure, we design a novel encoder to extract 3D features by deconstructing the molecules into three geometric graphs. In addition, we use 20M unlabeled data to pretrain our model by contrastive learning. We consider conformations with the same topological structure as positive pairs and the opposites as negative pairs, while the weight is determined by the dissimilarity between the conformations. We compare 3D-Mol with various state-of-the-art baselines on 7 benchmarks and demonstrate our outstanding performance in 5 benchmarks.\n",
      "\n",
      "Deep Learning Molecular representation Molecular property prediction Graph neural network Self-supervised learning Contrastive learning\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Molecular property prediction accelerates drug candidate identification, saving time and resources. It helps researchers prioritize promising compounds, streamlining drug development and increasing success rates. Moreover, it aids in understanding structure-activity relationships, revealing how specific features impact properties, interactions, and biological effects. Although deep learning has achieved success in molecular property prediction, its potential is significantly constrained by the scarcity of labeled data because labeled data for molecular properties usually requires expensive and time-consuming experiments[1].\n",
      "\n",
      "Self-supervised learning uses large amounts of unlabeled data to pretrain models to leverage unlabeled data to learn rich feature representations. Many works[2, 3, 4] improve their model performance through self-supervised learning. Early deep learning methods for molecular property prediction[5, 6, 7, 8, 9] effectively utilized NLP-based self-supervised learning methods to handle data represented by the SMILES molecular formula[10]. However, SMILES can't fully reflect the topological relationships of a molecule. Therefore, many self-supervised methods based on molecular graphs have emerged, such as PretrainGNN[11], N-Gram-graph[12], MolCLR[13], GROVER[14]. They designed unique pretraining methods based on molecular graph. Besides, lots of work use graph neural networks to capture the topological information of molecules, such as MPNN[15], AttentiveFP[16] and D-MPNN[17]. However, these methods have not dealt with the 3D spatial information of molecules, which is critical for predicting molecular properties. As shown in Figure 1, Thalidomide is divided into two forms, R-Thalidomide and S-Thalidomide, due to different 3D structures. The former can be used to treat skin diseases, while the latter has been implicated in teratogenesis. Recently, some integrating geometric information into graph structures has attracted research attention in some molecular property estimation tasks[18, 19, 20, 21, 22, 23, 24, 25, 22], they either fail to fully extract the 3D information of molecules or only use masking methods for data augmentation in self-supervised learning, and only use the stablest conformation but ignore the others.\n",
      "To address these issues, we propose a novel method, 3D-Mol, for molecular property prediction. Firstly, we use atom-bond graph, bond-angle graph and plane-angle graph to represent the spatial structural information of molecules, extracting the 3D spatial structure representation of molecules through the information transfer within these three graphs and their interactions. In pretraining stage, we design a unique weighted contrastive learning method, which uses the different 3D conformations of the same SMILES as weighted positive pair SMILES, and the weight is dependent on the difference between those conformations. We also employ the geometry pretraining task following GearNet[26]. We learn 3D structural feature of molecular representations from a large volume of unlabeled data, and then finetune the well-pretrained model according to downstream tasks and data to predict molecular properties. We compared our approach with several state-of-the-art baselines on 7 molecular property prediction benchmarks[27], where our method achieved the best results on 5 benchmarks. In summary, our main contributions are as follows:\n",
      "\n",
      "\\(\\bullet\\) We propose a novel molecular representation method and design a corresponding model to fully extract the **3D spatial structural features** of molecules.\n",
      "\n",
      "\\(\\bullet\\) We design a unique weighted contrastive learning task, using the **different 3D conformations from the same SMILES as weighted positive pair**, and the weight is dependent on the difference between the conformations, thereby deeply learning the microscopic characteristics of molecular 3D space.\n",
      "\n",
      "\\(\\bullet\\) We have conducted thorough evaluations of the 3D-Mol model on various molecular property prediction datasets. Experimental results show that **3D-Mol significantly outperforms existing competitive models** in multiple benchmark tests.\n",
      "\n",
      "## 2 Related Work\n",
      "\n",
      "In general, there are two strategies to improve molecular property prediction. One is to design a novel molecular encoder based on molecular information for effective latent vector extraction, the other is to design a novel pretraining method to pretrain the molecular encoder by using a large amount of unlabeled data. The following are the related works of each.\n",
      "\n",
      "### Molecular Representation and Encoder\n",
      "\n",
      "Proposing a novel molecular representation and encoder method is usually the first option for researchers to improve the accuracy of molecular property prediction. Some early works learn representation from chemical fingerprints, such as ECFP[21] and MACCS[28], frequently used in early machine learning[21, 29, 30]. The other learns representation from molecular descriptors, such as SMILES[10]. Inspired by mature NLP models, SMILES-BERT[31] applies the BERT[2] strategy to pretrain on SMILES to extract molecular representations. However, These methods depend on feature engineering, failing to capture the complete topological structure of molecules.\n",
      "\n",
      "Recently, many works use molecular graph as molecular representation because the natural representation of molecule is molecular graph and it represents the topology information. GG-NN[15], DMPNN[17], and DeepAtomicCharge[32] employed a message passing scheme for molecular property prediction. AttentiveFP[16] uses\n",
      "\n",
      "Figure 1: Thalidomide exists in two distinct 3D stereoisomeric forms, known as R-Thalidomide and S-Thalidomide. The former is recognized for its therapeutic properties in the treatment of various skin conditions, but the latter has been implicated in teratogenesis. It shows that despite having identical 2D molecular topology, the properties of two molecules vary significantly due to their distinct 3D structures.\n",
      "a graph attention network to aggregate and update node information. The MP-GNN[33] merges specific-scale Graph Neural Networks and element-specific Graph Neural Networks, capturing various atomic interactions of multiphysical representations at different scales. MGCN[34] designed a GCN to capture multilevel quantum interactions from the conformation and spatial information of molecules. But these works focuses on 2D molecular representation, and extracting only 2D topological information from molecules is insufficient.\n",
      "\n",
      "Many works[35] show the necessity of using 3D spatial information of molecules. Recently, some research has also begun modeling 3D molecules to address this issue. SGCN[25] applies different weights according to atomic distances during the GCN-based message passing process. SchNet[18] models complex atomic interactions using Gaussian radial basis functions for potential energy surface predictionor to accelerate the exploration of chemical space. DimeNet[22] proposes directional message passing to fully utilize directional information within molecules. [23] develops a novel geometrically-enhanced molecular representation learning method, and employs a specifically designed geometric-based graph neural network structure. However, these methods do not fully exploit the 3D structural information of molecules and lack the ability to learn the representations of 3D conformations with the same molecular topology.\n",
      "\n",
      "### Self-supervised Learning on Molecules\n",
      "\n",
      "Self-supervised learning has achieved enormous success in BERT[2] and GPT[36]. Inspired by these, numerous works for molecular property prediction use this approch to effectively utilize large amounts of unlabeled data for pretraining.\n",
      "\n",
      "For one-dimensional data, SMILES is frequently used to extract molecular feature in pretraining stage. ChemBERTa[9] followed RoBERTa[37] by employing masked language modeling (MLM) as a pretraining task, predicting masked tokens to restore the original sentence, aiding pretraining models to understand sequence semantics. SMILES Transformer[38] used a SMILES string as input to produce a temporary embedding, which is then restored to the original input SMILES by a decoder.\n",
      "\n",
      "As the topological information of molecular graphs is gaining more attention, many pretraining methods aimed at graph data have been proposed. Shengchao Liu et al[12] used the n-gram method in NLP to extract and represent features of molecules. PretrainGNN[11] proposed a new pretraining strategy, include node-level and graph-level self-supervised pretraining tasks. GraphCL[39], MOCL[40] and MolCLR[13] performed molecular contrastive learning via graph neural networks, proposed new molecule augmentation methods. MPG[41] and GROVER[14] focused on node level and graph level representation and corresponding pretraining tasks for node level and graph level. iMolCLR[42], Sugar[43] and ReLMole[44] focused on the substructure of molecule, and designed the substructure pretraining task by using substructure information. However, the aforementioned pretraining strategies are only targeted at learning the topology information of the molecule.\n",
      "\n",
      "With the 3D information of molecules proven to aid in the prediction of molecular representations, recent works have focused on pretraining task for the 3D structure information of molecules. 3DGCN[45] introduced a relative position matrix that includes 3D positions between atoms to ensure translational invariance during convolution. GraphMVP[46] proposed a SSL method involving contrastive learning and generative learning between 3D and 2D molecular views. [23] proposed a self-supervised framework using molecular geometric information. They constructed a new bond angle graph, where the chemical bonds within a molecule are considered as nodes instead of edges, and the angle formed between two bonds is considered as the edge between them. Uni-Mol[47] employed the transformer to extract molecular representation by predicting atom distance. Although these works have used the spatial information of molecules, but they have not fully utilized the spatial information of molecules, nor have they enabled the model to learn the representation of geometric isomers. To address these issues, we use RDKit to generate multiple conformations from same topology structure as positive pairs and design a weighted contrastive learning task for self-supervised training.\n",
      "\n",
      "## 3 Method\n",
      "\n",
      "### Molecular Representation\n",
      "\n",
      "We deconstruct molecular conformation into three graph, denoted as \\(Mol=\\{G_{a-b},G_{b-a},G_{p-a}\\}\\). In most databases, molecular raw data is represented by SMILES. To extract topological and spatial structure information from molecules, we need to use RDKit to transform the SMILES representation into molecular conformations. In our method, we decompose molecular conformation into three graphs. The first graph, named atom-bond graph, is the commonly used as 2D molecular graph, and is represented as \\(G_{a-b}=\\{V,E,P_{atom},P_{bond}\\}\\), where \\(V\\) is the set of atoms and \\(E\\) is the set of bonds. \\(P_{atom}\\in R^{|V|*d_{atom}}\\), and are the attributes of atoms, and \\(d_{atom}\\) is the number of atom attributes. \\(P_{bond}\\in R^{|E|*d_{bond}}\\), and is the attributes of bonds, and \\(d_{bond}\\) is the number of bond attributes. The second graph, named bond-angle graph, is represented as \\(G_{b-a}=\\{E,P,Ang_{\\theta}\\}\\), where \\(P\\) is a set of the plane that is comprised w\n",
      "3 connected atoms. \\(Ang_{\\theta}\\) is the set of corresponding bond angle \\(\\theta\\). The third graph, named the plane-angle graph, is represented as \\(G_{p-a}=\\{P,C,Ang_{\\phi}\\}\\). \\(C\\) represents the set of two connected planes, which connect with a bond. \\(Ang_{\\phi}\\) represents the corresponding dihedral angle \\(\\phi\\). The first graph allows the model to learn the topological information of molecules, and the second and third graphs allow the model to learn the spatial structure information of molecules.\n",
      "\n",
      "### Attribute Embedding Layer\n",
      "\n",
      "The 3D information of the molecule, such as the length of bonds and the angle between bonds, carries key chemical information. Firstly, we convert float numbers, like angle and bond length, to latent vectors. Referring to the previous work (Shui and Karypis 2020)[24], we employed several RBF layers to encode different geometric factors:\n",
      "\n",
      "\\[F_{l}^{k}=exp(-\\beta_{l}^{k}(exp(-l)-\\mu_{l}^{k})^{2})*W_{l}^{k} \\tag{1}\\]\n",
      "\n",
      "where \\(F_{l}^{k}\\) is the k-dimensional feature of bond length \\(l\\), and \\(\\mu_{l}^{k}\\) and \\(\\beta_{l}^{k}\\) are the center and width of \\(l\\) respectively. \\(\\mu_{l}^{k}\\) is 0.1\\(k\\) and \\(\\beta_{l}^{k}\\) is 10. Similarly, the k-dimensional feature of \\(F_{\\theta}^{k}\\) and \\(F_{\\phi}^{k}\\) of x is computed as:\n",
      "\n",
      "\\[F_{\\theta}^{k}=exp(-\\beta_{\\theta}^{k}(-\\theta-\\mu_{\\theta}^{k})^{2})*W_{ \\theta}^{k} \\tag{2}\\]\n",
      "\n",
      "\\[F_{\\phi}^{k}=exp(\\beta_{\\phi}^{k}(-\\phi-\\mu_{\\phi}^{k})^{2})*W_{\\phi}^{k} \\tag{3}\\]\n",
      "\n",
      "where \\(\\mu_{\\theta}^{k}\\) and \\(\\beta_{\\theta}^{k}\\) are the center and width of \\(\\theta\\), and \\(\\mu_{\\phi}^{k}\\) and \\(\\beta_{\\phi}^{k}\\) are the center and width of \\(\\phi\\). Centers of bond angle and dihedral angle are represented as \\(\\mu_{\\phi}^{k}\\) and \\(\\mu_{\\theta}^{k}\\) respectively, and the numerical value of them are \\(\\pi\\)/K, where K is the number of feature dimensions.\n",
      "\n",
      "For the other property of atom and bond, we represent them with \\(P_{atom}\\) and \\(P_{bond}\\) respectively. Inspired by NLP, we embed them with the word embedding function. The initial features of atom and bond are represented as \\(F_{atom}^{0}\\) and \\(F_{bond}^{0}\\) respectively.\n",
      "\n",
      "Figure 2: **The overview of the 3D-Mol model framework.** a) In the pretraining stage, we employ weighted contrastive learning to effectively pretrain our model. In addition to using the mask strategy for graph data augmentation, we consider conformations stemming from the same topological structure as positive pairs, with their weight determined by the dissimilarity between the conformations. Conversely, distinct topological structures are treated as negative pairs, and we further utilize fingerprint differences to compute the weight of negative pairs. b) In the finetuning stage, we refine the well-pretrained encoder using diverse downstream datasets, followed by supervised learning.\n",
      "### 3D-Mol Layer\n",
      "\n",
      "Inspired by GEM[23], we use message passing strategy to let node send and receive messages with edge in \\(\\{G_{a-b}^{i},G_{b-a}^{i},G_{p-a}^{i}\\}\\). For the \\(i_{th}\\) layer in 3D-Mol, the information of \\(\\{G_{a-b}^{i},G_{b-a}^{i},G_{p-a}^{i}\\}\\) will be updated by message passing neural network orderly. The message passing of the latter graph need the information of the former graph. The overview is shown in figure 3, and the details are as follow:\n",
      "\n",
      "First, we use \\(GNN_{a-b}^{i}\\) to aggregate the message and update the atom and bond latent vectors in \\(G_{a-b}^{i}\\). Given an atom v, its representation vector \\(F_{v}^{i}\\) is formalized by:\n",
      "\n",
      "\\[a_{v}^{i,a-b}=Agg_{a-b}^{(i)}(F_{v}^{i-1},F_{u}^{i-1},F_{uv}^{i-1}|u\\in N(v)) \\tag{4}\\]\n",
      "\n",
      "\\[F_{v}^{i}=Comb_{a-b,n}^{(k)}(F_{v}^{i-1},a_{v}^{i}) \\tag{5}\\]\n",
      "\n",
      "\\[F_{uv}^{i,temp}=Comb_{a-b,e}^{(k)}(F_{uv}^{i-1},F_{u}^{i-1},F_{v}^{i-1}) \\tag{6}\\]\n",
      "\n",
      "where \\(N(v)\\) is the set of neighbors of atom v in \\(G_{a-b}^{i}\\), and \\(Agg_{a-b}^{(i)}\\) is the aggregation function for aggregating messages from an atom's neighborhood in \\(G_{a-b}^{i}\\). \\(Comb_{a-b,n}^{(k)}\\) is the update function for updating the atom latent vectors in \\(G_{a-b}^{i}\\), and \\(Comb_{a-b,e}^{(k)}\\) is the update function for updating the bond latent vectors in \\(G_{a-b}^{i}\\). \\(a_{v}^{i(a-b)}\\) is the information from the neighboring atom and the corresponding bond after be aggregated in \\(G_{a-b}^{i}\\). \\(F_{uv}^{i,temp}\\) is the temporary bond latent vectors of bond \\(uv\\) in \\(i_{th}\\) layer. Processing \\(G_{a-b}^{i}\\) with \\(GNN_{a-b}^{i}\\) make features of atom and bond can be updated by information from their neighbor, and the model will learn the topology information of the molecule. \\(F_{uv}^{i,temp}\\) is part of bond feature in \\(G_{b-a}^{i}\\).\n",
      "\n",
      "Then, we use \\(GNN_{b-a}^{i}\\) to aggregate the message and update the bond and plane vector in \\(G_{b-a}^{i}\\). Given a bond \\(uv\\), its latent vector \\(F_{uv}^{i}\\) is formalized by:\n",
      "\n",
      "\\[a_{uv}^{i,b-a}=Agg_{b-a}^{(i)}(\\{F_{uv}^{i-1},F_{vw}^{i-1},F_{uvw}^{i-1}|u\\in N (v)\\cap\\in N(v)\\cap u\\neq w\\}) \\tag{7}\\]\n",
      "\n",
      "\\[F_{uv}^{i}=Comb_{b-a,n}^{(k)}(F_{uv}^{i-1},F_{uv}^{i,temp},a_{uv}^{i}) \\tag{8}\\]\n",
      "\n",
      "\\[F_{uvw}^{i-1,temp}=Comb_{b-a,e}^{(k)}(F_{uvw}^{i-1},F_{uv}^{i-1},F_{vw}^{i-1}) \\tag{9}\\]\n",
      "\n",
      "Figure 3: **Overview of the 3D-Mol encoder layer.** The 3D-Mol encoder layer comprises three steps. Firstly, employing a message passing strategy, nodes in each graph exchange messages with their connected edges, leading to the updating of edge and node latent vectors. Secondly, the edge latent vector from the lower-level graph is transmitted to the higher-level graph as part of the node latent vector. Finally, the iteration is performed n times to derive the \\(n_{th}\\) node latent vector, from which we extract the molecular latent vectors.\n",
      "where \\(Agg^{(i)}_{b-a}\\) is the aggregation function for aggregating messages from a bond's neighborhood in \\(G^{i}_{b-a}\\), and \\(Comb^{(k)}_{b-a,n}\\) is the update function for updating the bond latent vector in \\(G^{i}_{b-a}\\), and \\(Comb^{(k)}_{b-a,e}\\) is the update function for updating the plane latent vector in \\(G^{i}_{b-a}\\). \\(a^{i,b-a}_{vx}\\) is the information from the neighboring bond and the corresponding bond angle after being aggregated. Processing \\(G^{i}_{b-a}\\) with \\(GNN^{i}_{b-a}\\) make feature of bond and bond angle can be update based on information from their neighbor, and the model will learn the 3D information of the molecular. \\(F^{i-1,temp}_{uvw}\\) is part of plane feature in \\(G^{i}_{p-a}\\).\n",
      "\n",
      "After process the \\(G^{i}_{b-a}\\), we use \\(GNN^{i}_{p-a}\\) to aggregate the message and update the plane latent vector in \\(G^{i}_{p-a}\\). Given a plane which is constructed by node u, v, w and bond \\(uv\\), \\(vw\\), its latent vector \\(F^{i}_{uvw}\\) is formalized by:\n",
      "\n",
      "\\[a^{i,p-a}_{uvw}=Agg^{(i)}_{p-a}(\\{F^{i-1}_{uvw},F^{i-1}_{vvh},F^{i-1}_{uvw}|u \\in N(v)\\cap v\\in N(w)\\cap v\\in N(h)\\cap u\\neq v\\neq w\\neq h\\}) \\tag{10}\\]\n",
      "\n",
      "\\[F^{i}_{uvw}=Comb^{(k)}_{p-a,n}(F^{i-1}_{uv},F^{i-1}_{uv},F^{i-1}_{vw}) \\tag{11}\\]\n",
      "\n",
      "\\[F^{i-1}_{uvw}=Comb^{(k)}_{b-a,e}(F^{i-1}_{uvw},F^{i-1}_{uv},F^{i-1}_{vw}) \\tag{12}\\]\n",
      "\n",
      "where \\(agg^{(i)}_{p-a}\\) is the aggregation function for aggregating messages from a plane's neighborhood in \\(G^{i}_{p-a}\\), and \\(Comb^{(k)}_{p-a,n}\\) is the update function for updating the plane latent vector in \\(G^{i}_{p-a}\\), and \\(Comb^{(k)}_{p-a,e}\\) is the update function for updating the dihedral angle latent vector in \\(G^{i}_{p-a}\\). \\(a^{i}_{uv}\\) is the information from the neighboring plane and the corresponding dihedral angle after be aggregated. Processing \\(G^{i}_{p-a}\\) with \\(GNN^{i}_{p-a}\\) makes feature of plane and dihedral angle can be updated by information from their neighbor, and the model will learn the 3D information of the molecular, and model the bond interaction.\n",
      "\n",
      "The representation vectors of the atoms at the final iteration are integrated to gain the molecular representation vector \\(F_{mol}\\) by the Readout function, which is formalized as:\n",
      "\n",
      "\\[F_{mol}=Readout(F^{n}_{u}|u\\in N(v)) \\tag{13}\\]\n",
      "\n",
      "where \\(F^{n}\\) is the last 3D-Mol layer output. The molecular latent vector \\(F_{mol}\\) is used to predict molecular properties. We extract the \\(F_{mol}\\) from all atom latent vector in the last layer.\n",
      "\n",
      "### Pretrain Strategy\n",
      "\n",
      "To improve the performance of 3D-Mol encoder, we employ the constructive learning as pretraining method, using different conformations of the same topological structure as positive pair. We also combine our pretraining method with geometry task in [26] to pretrain 3D-Mol with a large amount of unlabeled data. The overview of our pretraining method is shown in figure 3, and the following are the details of our pretraining method.\n",
      "\n",
      "#### 3.4.1 Weighted contrastive learning task\n",
      "\n",
      "Our objective is to facilitate the learning of the consistency and difference between the most stable molecular conformation, denoted as \\(Mconf_{i}\\), and another randomly selected conformation, denoted as \\(Mconf_{j}\\). To accomplish this, we employ weighted contrastive learning using a batch of molecular representations, with the loss function defined as follows:\n",
      "\n",
      "\\[L^{conf}_{i,j}=-log\\frac{exp(w^{conf}_{i,j}sim(F_{i},F^{mk}_{j})/\\tau)}{\\Sigma^ {2N}_{k=1}\\{k\\neq i\\}exp(w^{fp}_{i,k}sim(F_{i},F_{k})/\\tau)} \\tag{14}\\]\n",
      "\n",
      "\\[w^{conf}_{i,j}=1-\\lambda_{conf}*ConfSim(Mconf_{i},Mconf_{j}) \\tag{15}\\]\n",
      "\n",
      "\\[w^{fp}_{i,k}=1-\\lambda_{fp}*FPSim(Mconf_{i},Mconf_{k}) \\tag{16}\\]\n",
      "\n",
      "where \\(F_{i}\\) is the latent vector extracted from \\(Mconf_{i}\\), and \\(sim(F_{i},F_{j})\\) is the similarity between two latent vectors \\((F_{i}\\), \\(F_{j})\\), and penalized by a weight coefficient \\(w^{conf}_{i,j}\\). \\(w^{conf}_{i,j}\\) is computed by \\(ConfSim(Mconf_{i},Mconf_{j})\\), the difference between \\(Mconf_{i}\\) and \\(Mconf_{j}\\), which can be computed by RDKit. \\(\\lambda_{conf}\\in[0,1]\\) is the hyperparameter that determines the scale of penalty for the difference between two conformation. Except using different conformations as the positive pair, we also use node and subgraph masking as the molecular data augmentation strategy. We mask \\(Mconf_{i}\\) 15\\(\\%\\) nodes and corresponding edges, and the mask latent vector conformation is denoted as \\(F^{mk}_{j}\\). Following iMolCLR[42], the similarity measurement between two latent vectors \\(F_{i}\\), \\(F_{k}\\) from a negative molecule pair (\\(Mconf_{i},Mconf_{j}\\)) is penalized by a weight coefficient \\(w^{fp}_{i,k}\\), which computed by the Fingerprint similarity between \\(Mconf_{i}\\) and \\(Mconf_{k}\\). \\(FPSim(Mconf_{i},Mconf_{j})\\) evaluates the fingerprint similarity of the given two molecules \\(Mconf_{i},Mconf_{j}\\), and \\(\\lambda_{fp}\\in[0,1]\\) is the hyperparameter that determines the scale of penalty for faulty negatives.\n",
      "#### 3.4.2 Geometry task\n",
      "\n",
      "3D information have been shown to be important features[22], so we employ geometry task as pretraining method. For bond angle and dihedral angle prediction, we sample adjacent atoms to better capture local structural information. Since angular values are more sensitive to errors in protein structures than distances, we use discretized values for prediction.\n",
      "\n",
      "\\[L_{i,j}^{dist}=(f_{dist}(Fn_{n,i}^{mk},Fn_{n,j}^{mk})-dist_{i,j})^{2} \\tag{17}\\]\n",
      "\n",
      "\\[L_{i,j}^{l}=(f_{l}(Fn_{n,i}^{mk},Fn_{n,j}^{mk})-l_{i,j})^{2} \\tag{18}\\]\n",
      "\n",
      "\\[L_{i,j,k}^{\\theta}=CE(f_{\\theta}(Fn_{n,i}^{mk},Fn_{n,j}^{mk},Fn_{n,k}^{mk}), bin(\\theta_{i,j,k})) \\tag{19}\\]\n",
      "\n",
      "\\[L_{i,j,k,p}^{\\phi}=CE(f_{\\phi}(Fn_{n,i}^{mk},Fn_{n,j}^{mk},Fn_{n,k}^{mk},Fn_{ n,p}^{mk}),bin(\\phi_{i,j,k,p})) \\tag{20}\\]\n",
      "\n",
      "where \\(f_{\\phi}(.)\\), \\(f_{\\theta}(.)\\), \\(f_{dist}(.)\\) and \\(f_{l}\\) are the MLPs for each task, and \\(L_{i,j}^{dist}\\), \\(L_{i,j}^{l}\\), \\(L_{i,j,k}^{\\theta}\\), \\(L_{i,j,k,p}^{\\phi}\\) and \\(L_{i}^{FP}\\) are the loss functions for each task. \\(CE(.)\\) is cross entropy loss, and \\(bin()\\) is used to discretize the bond angle and dihedral angle. \\(Fn_{n,i}^{mk}\\) is the latent vector of node i after masking the corresponding sampled items in each task.\n",
      "\n",
      "In addition to the aforementioned pretraining tasks to capture global molecular information, we leverage masked molecular latent vectors for fingerprint (FP) prediction, effectively incorporating latent representations to enrich the predictive capability.\n",
      "\n",
      "\\[L_{i}^{FP}=BCE(f_{FP}(Fm^{mk}),FP_{i}) \\tag{21}\\]\n",
      "\n",
      "where \\(f_{FP}\\) is the MLPs for global geometric task, and \\(L_{i}^{FP}\\) is the loss function. \\(BCE(.)\\) is binary cross entropy loss. \\(Fm^{mk}\\) is the latent vector of the masking molecule.\n",
      "\n",
      "## 4 Experiment\n",
      "\n",
      "In this section, we conduct experiments on 7 benchmark datasets in MoleculeNet to demonstrate the effectiveness of 3D-Mol for molecular property prediction. Firstly we use a large amount of unlabeled data and our pretraining method to pretrain the 3D-Mol model, then we use the downstream task to finetune well-pretrained model and predict the molecular property. We compared it with a variety of state-of-the-art methods. Also we conduct several ablation studies to confirm the 3D-Mol model and our pretraining method is useful.\n",
      "\n",
      "### Datasets and Setup\n",
      "\n",
      "#### 4.1.1 Pretraining stage\n",
      "\n",
      "We use 20 million unlabeled molecules to pretrain 3D-Mol. The unlabeled data is extracted from ZINC20 and PubChem, both of which are publicly accessible databases containing drug-like compounds. To ensure consistency with prior research[23], we randomly selected 90\\(\\%\\) of these molecules for training purposes, while the remaining \\(\\%\\) was set aside for evaluation. The raw data obtained from ZINC20 and PubChem was provided in the SMILES format. In order to convert the SMILES representations into molecular conformations, we employed RDKit and applied the ETKDG method. For our model, we use Adam optimizer with a learning rate of 1e-3. The batch size is set to 256 for pretraining and 32 for finetuning. The hidden size of all models is unspecified. The geometric embedding dimension K is 64, and the number of angle domains is 8. The hyperparameters \\(\\lambda_{conf}\\) and \\(\\lambda_{fp}\\) are both set to 0.5.\n",
      "\n",
      "\\begin{table}\n",
      "\\begin{tabular}{c c c c} \\hline \\hline Dataset & \\(\\#\\) Tasks & Task Type & \\(\\#\\) Molecules \\\\ \\hline BACE & 1 & Classifcation & 1513 \\\\ Sider & 27 & Classifcation & 1427 \\\\ Tox21 & 12 & Classifcation & 7831 \\\\ ToxCast & 617 & Classifcation & 8597 \\\\ ESOL & 1 & Regression & 1128 \\\\ FreeSolv & 1 & Regression & 643 \\\\ Lipophilicity & 1 & Regression & 4200 \\\\ \\hline \\hline \\end{tabular}\n",
      "\\end{table}\n",
      "Table 1: Statistics information of datasets\n",
      "#### 4.1.2 Finetuning stage\n",
      "\n",
      "We use 7 molecular datasets obtained from MoleculeNet to demonstrate the effectiveness of 3D-Mol. These datasets encompass a range of biophysics datasets such as BACE, physical chemistry datasets like ESOL, and physiology datasets like Tox21. In the fine-tuning stage, we employed nine molecular datasets obtained from MoleculeNet. These datasets encompass a range of biophysics datasets such as BACE, physical chemistry datasets like ESOL, and physiology datasets like Tox21. Table 1 provides a summary of the statistical information for these datasets, while the remaining details are outlined below:\n",
      "\n",
      "\\(\\bullet\\) BACE. The BACE dataset provides both quantitative (IC50) and qualitative (binary label) binding results for a set of inhibitors targeting human \\(\\beta\\)-secretase 1 (BACE-1).\n",
      "\n",
      "\\(\\bullet\\) Tox21. The Tox21 initiative aims to advance toxicology practices in the 21st century and has created a public database containing qualitative toxicity measurements for 12 biological targets, including nuclear receptors and stress response pathways.\n",
      "\n",
      "\\(\\bullet\\) Toxcast. ToxCast, an initiative related to Tox21, offers a comprehensive collection of toxicology data obtained through in vitro high-throughput screening. It includes information from over 600 experiments and covers a large library of compounds.\n",
      "\n",
      "\\(\\bullet\\) SIDER. The SIDER database is a compilation of marketed drugs and their associated adverse drug reactions (ADRs), categorized into 27 system organ classes.\n",
      "\n",
      "\\(\\bullet\\) ESOL. The ESOL dataset is a smaller collection of water solubility data, specifically providing information on the log solubility in mols per liter for common organic small molecules.\n",
      "\n",
      "FreeSolv. The FreeSolv database offers experimental and calculated hydration-free energy values for small molecules dissolved in water.\n",
      "\n",
      "\\(\\bullet\\) Lipo. Lipophilicity is a crucial characteristic of drug molecules that affects their membrane permeability and solubility. The Lipo dataset contains experimental data on the octanol/water distribution coefficient (logD at pH 7.4).\n",
      "\n",
      "Following the previous works[23], We partitioned the datasets into train/validation/test sets in an 80/10/10 ratio for downstream tasks, and We use scaffold splitting and report the mean and standard deviation by the results of 3 random seeds.\n",
      "\n",
      "### Metric\n",
      "\n",
      "Consistent with prior studies, we adopt the average ROC-AUC as the evaluation metric for the classification datasets (BACE, SIDER, Tox21 and ToxCast), which is a widely used metric for assessing the performance of binary classification tasks. For the regression datasets (ESOL, FreeSolv and Lipophilicity), we utilize the RMSE as the evaluation metric.\n",
      "\n",
      "### Result\n",
      "\n",
      "a) **To validate the efficacy of our proposed method, we compare it with several baseline methods.** The baseline methods are as follows: N-Gram[12] generates a graph representation by constructing node embeddings\n",
      "\n",
      "\\begin{table}\n",
      "\\begin{tabular}{|c|c|c|c|c|c|c|c|} \\hline \\multirow{2}{*}{model} & \\multicolumn{3}{c|}{Classification (ROC-AUC \\(\\%\\), higher is better \\(\\uparrow\\) )} & \\multicolumn{3}{c|}{Regression (RMSE, lower is better \\(\\downarrow\\))} \\\\ \\cline{2-7}  & BACE & SIDER & Tox21 & ToxCast & ESOL & FreeSolv & Lipophilicity \\\\ \\hline N-Gram\\({}_{\\text{RF}}\\) & \\(0.779_{0.015}\\) & \\(0.668_{0.007}\\) & \\(0.743_{0.004}\\) & \\(-\\) & \\(1.074_{0.107}\\) & \\(2.688_{0.085}\\) & \\(0.812_{0.028}\\) \\\\ N-Gram\\({}_{\\text{XGB}}\\) & \\(0.791_{0.013}\\) & \\(0.655_{0.007}\\) & \\(0.758_{0.009}\\) & \\(-\\) & \\(1.083_{0.107}\\) & \\(5.061_{0.744}\\) & \\(2.072_{0.030}\\) \\\\ PretrainGNN & \\(0.845_{0.007}\\) & \\(0.627_{0.008}\\) & \\(0.781_{0.006}\\) & \\(0.657_{0.006}\\) & \\(1.100_{0.006}\\) & \\(2.764_{0.002}\\) & \\(0.739_{0.003}\\) \\\\ GROVER\\({}_{\\text{base}}\\) & \\(0.826_{0.007}\\) & \\(0.648_{0.006}\\) & \\(0.743_{0.001}\\) & \\(0.654_{0.004}\\) & \\(0.983_{0.090}\\) & \\(2.176_{0.052}\\) & \\(0.817_{0.008}\\) \\\\ GROVER\\({}_{\\text{large}}\\) & \\(0.810_{0.014}\\) & \\(0.654_{0.001}\\) & \\(0.735_{0.001}\\) & \\(0.653_{0.005}\\) & \\(0.895_{0.017}\\) & \\(2.272_{0.051}\\) & \\(0.823_{0.010}\\) \\\\ MolCLR & \\(0.824_{0.009}\\) & \\(0.589_{0.014}\\) & \\(0.750_{0.002}\\) & \\(-\\) & \\(1.271_{0.040}\\) & \\(2.594_{0.249}\\) & \\(0.691_{0.004}\\) \\\\ \\hline\n",
      "3DInfomax & \\(0.797_{0.015}\\) & \\(0.606_{0.008}\\) & \\(0.644_{0.011}\\) & \\(0.745_{0.007}\\) & \\(0.894_{0.028}\\) & \\(2.337_{0.107}\\) & \\(0.695_{0.012}\\) \\\\ GraphMVP & \\(0.812_{0.009}\\) & \\(0.639_{0.012}\\) & \\(0.759_{0.005}\\) & \\(0.631_{0.004}\\) & \\(1.029_{0.033}\\) & \\(-\\) & \\(0.681_{0.010}\\) \\\\ GEM & \\(0.856_{0.011}\\) & \\(\\mathbf{0.672}_{0.004}\\) & \\(0.781_{0.005}\\) & \\(0.692_{0.004}\\) & \\(0.798_{0.029}\\) & \\(1.877_{0.094}\\) & \\(0.660_{0.008}\\) \\\\ Uni-Mol & \\(0.857_{0.005}\\) & \\(0.659_{0.013}\\) & \\(\\mathbf{0.796}_{0.006}\\) & \\(0.696_{0.001}\\) & \\(0.788_{0.029}\\) & \\(1.620_{0.035}\\) & \\(0.603_{0.010}\\) \\\\ \\hline\n",
      "3D-Mol\\({}_{\\text{wcl}}\\) & \\(\\mathbf{0.875}_{0.004}\\) & \\(0.656_{0.002}\\) & \\(0.786_{0.003}\\) & \\(\\mathbf{0.697}_{0.003}\\) & \\(\\mathbf{0.783}_{0.009}\\) & \\(\\mathbf{1.617}_{0.050}\\) & \\(\\mathbf{0.598}_{0.018}\\) \\\\ \\hline \\end{tabular}\n",
      "\\end{table}\n",
      "Table 2: Comparison of performance on the 7 molecular property prediction tasks and the methods below are pretraining method. We mark the best results in bold and underline the second best.\n",
      "based on short walks. PretrainGNN[11] implements several types of self-supervised learning tasks. 3D Infomax[48] maximizes the mutual information between learned 3D summary vectors and the representations of a graph neural network. MolCLR[13] is a 2D-2D view contrastive learning model that involves atom masking, bond deletion, and subgraph removal. GraphMVP[46] introduces 2D-3D view contrastive learning approaches. GROVER[14] focused on node level and graph level representation and corresponding pretraining tasks for node level and graph level. GEM[23] employs predictive geometry self-supervised learning schemes that leverage 3D molecular information. Uni-Mol[47] enlarge the application scope and representation ability of molecular representation learning by using transformer. As result in Table 2, Our method gets the best result in 5 datasets and gets the second best result in 1 dataset. Furthermore, our method achieves overwhelming performance on BACE by a large margin. That shows that our method is better at extracting the molecular information. To do the ablation study, We compare the results of 3D-Mol and 3D-Mol without pretraining. It shows that the former achieve overwhelming performance and our pretraining method can improve the 3D-Mol model performance.\n",
      "\n",
      "**b) To validate the efficacy of our proposed model 3D-Mol encoder, we compare it with several baseline molecular encoder without pretraining.** The baseline molecular encoders are as follows: DMPNN[17] employed a message passing scheme for molecular property prediction. AttentiveFP[16] is an attention-based GNN that incorporates graph-level information. MGCN[34] designed hierarchical graph neural network to directly extract features from the conformation and spatial information followed by the multilevel interactions. HMGNN[24] leverages global molecular representations through an attention mechanism. SGCN[25] applies different weights according to atomic distances during the GCN-based message passing process. DimeNet[22] proposes directional message passing to fully utilize directional information within molecules. GEM[23] employs message passing strategy to extract 3D molecular information. We present the experimental result to show the efficiency of our 3D-Mol model, as can be seen in Table 3. From Table 4, 3D-Mol encoder significantly outperforms all the baselines on both types of tasks and improves the performance over the best baselines with \\(2\\%\\) and \\(11\\%\\) for classification and regression tasks respectively, since 3D-Mol incorporates geometrical parameters.\n",
      "\n",
      "c) **To validate the efficacy of our proposed pretraining task, we compare the performance of no pretraining 3DGNN, pretraining by geometrical tasks and pretraining by geometrical tasks and weighted contrastive loss, and the result shows in Table 4. The result shows that geometry tasks signifcantly improve the performance of 3DGNN. Compared with the pretraining method combined with weighted contrastive learning. In general, The combined pretraining method is better to improve the 3DGNN performance.\n",
      "\n",
      "\\begin{table}\n",
      "\\begin{tabular}{|c|c|c|c|c|c|c|c|} \\hline \\multirow{2}{*}{model} & \\multicolumn{3}{c|}{Classification (ROC-AUC \\(\\%\\), higher is better \\(\\uparrow\\) )} & \\multicolumn{3}{c|}{Regression (RMSE, lower is better \\(\\downarrow\\))} \\\\ \\cline{2-7}  & BACE & SIDER & Tox21 & ToxCast & ESOL & FreeSolv & Lipophilicity \\\\ \\hline \\(3\\mathrm{DGNN}\\) & **0.875\\({}_{0.004}\\)** & 0.656\\({}_{0.002}\\) & 0.786\\({}_{0.003}\\) & **0.697\\({}_{0.003}\\)** & **0.783\\({}_{0.009}\\)** & 1.617\\({}_{0.050}\\) & **0.598\\({}_{0.018}\\)** \\\\ \\(3\\mathrm{DGNN}_{\\mathrm{wo.pre}}\\) & 0.832\\({}_{0.005}\\) & 0.624\\({}_{0.013}\\) & 0.780\\({}_{0.004}\\) & 0.682\\({}_{0.007}\\) & 0.794\\({}_{0.027}\\) & 1.769\\({}_{0.039}\\) & 0.674\\({}_{0.007}\\) \\\\ \\(3\\mathrm{DGNN}_{\\mathrm{wo.cl_{weighted}}}\\) & 0.874\\({}_{0.006}\\) & **0.661\\({}_{0.005}\\)** & **0.790\\({}_{0.003}\\)** & 0.693\\({}_{0.005}\\) & 0.795\\({}_{0.014}\\) & **1.557\\({}_{0.003}\\)** & 0.607\\({}_{0.006}\\) \\\\ \\hline \\end{tabular}\n",
      "\\end{table}\n",
      "Table 4: Ablation study. We compare the performance of no pretraining 3DGNN, pretraining by geometrical tasks and pretraining by geometrical tasks and weighted contrastive loss, and mark the best results in bold and underline the second best.\n",
      "\n",
      "\\begin{table}\n",
      "\\begin{tabular}{|c|c|c|c|c|c|c|c|} \\hline \\multirow{2}{*}{model} & \\multicolumn{3}{c|}{Classification (ROC-AUC \\(\\%\\), higher is better \\(\\uparrow\\) )} & \\multicolumn{3}{c|}{Regression (RMSE, lower is better \\(\\downarrow\\))} \\\\ \\cline{2-7}  & BACE & SIDER & Tox21 & ToxCast & ESOL & FreeSolv & Lipophilicity \\\\ \\hline DMPNN & 0.809\\({}_{0.006}\\) & 0.570\\({}_{0.007}\\) & 0.759\\({}_{0.007}\\) & 0.655\\({}_{0.3}\\) & 1.050\\({}_{0.008}\\) & 2.082\\({}_{0.082}\\) & 0.683\\({}_{0.016}\\) \\\\ AttentiveFP & 0.784\\({}_{0.000}\\) & 0.606\\({}_{0.032}\\) & 0.761\\({}_{0.005}\\) & 0.637\\({}_{0.002}\\) & 0.877\\({}_{0.029}\\) & 2.073\\({}_{0.183}\\) & 0.721\\({}_{0.001}\\) \\\\ MGCN & 0.734\\({}_{0.008}\\) & 0.587\\({}_{0.019}\\) & 0.741\\({}_{0.006}\\) & \\(-\\) & \\(-\\) & \\(-\\) & \\(-\\) \\\\ \\hline SGCN & \\(-\\) & 0.559\\({}_{0.005}\\) & 0.766\\({}_{0.002}\\) & 0.657\\({}_{0.003}\\) & 1.629\\({}_{0.001}\\) & 2.363\\({}_{0.050}\\) & 1.021\\({}_{0.013}\\) \\\\ HMGNN & \\(-\\) & 0.615\\({}_{0.005}\\) & 0.768\\({}_{0.002}\\) & 0.672\\({}_{0.001}\\) & 1.390\\({}_{0.073}\\) & 2.123\\({}_{0.179}\\) & 2.116\\({}_{0.473}\\) \\\\ DimeNet & \\(-\\) & 0.612\\({}_{0.004}\\) & 0.774\\({}_{0.006}\\) & 0.637\\({}_{0.004}\\) & 0.878\\({}_{0.023}\\) & 2.094\\({}_{0.118}\\) & 0.727\\({}_{0.019}\\) \\\\ GEM & 0.828\\({}_{0.012}\\) & 0.606\\({}_{0.010}\\) & 0.773\\({}_{0.007}\\) & 0.675\\({}_{0.005}\\) & 0.832\\({}_{0.010}\\) & 1.857\\({}_{0.071}\\) & 0.666\\({}_{0.015}\\) \\\\ \\hline \\(3\\mathrm{D-Molw}_{\\mathrm{wo.pre}}\\) & **0.832\\({}_{0.005}\\)** & **0.624\\({}_{0.013}\\)** & **0.780\\({}_{0.004}\\)** & **0.682\\({}_{0.007}\\)** & **0.794\\({}_{0.027}\\)** & **1.769\\({}_{0.039}\\)** & **0.674\\({}_{0.007}\\)** \\\\ \\hline \\end{tabular}\n",
      "\\end{table}\n",
      "Table 3: Comparison of performance on the 9 molecular property prediction tasks, and the methods below are no pretraining. We mark the best results in bold and underline the second best.\n",
      "## 5 Conclusion\n",
      "\n",
      "In this paper, we innovatively propose a novel 3D molecular model framework, 3D-Mol, to extract 3D molecular features. Furthermore, to effectively utilize a large number of unlabeled molecules and molecular conformations for feature extraction, we have designed a new self-supervised pretraining strategy. Our approach has been validated through numerous experiments and compared with multiple competitive benchmarks, demonstrating superior performance over other methods across various benchmarks.\n",
      "\n",
      "## Acknowledgment\n",
      "\n",
      "The research was supported by the PengCheng Laboratory and by PengCheng Laboratory Cloud-Brain.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Sample\n",
    "'''\n",
    "Abstract is usually defined as\n",
    "'###### Abstract'\n",
    "'''\n",
    "idx = 0\n",
    "# idx = 15\n",
    "sample = df.iloc[idx]['markdown']\n",
    "# print(len(sample), sample[:200])\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='3D-Mol: A Novel Contrastive Learning Framework for Molecular Property Prediction with 3D Information  \\n###### Abstract  \\nMolecular property prediction offers an effective and efficient approach for early screening and optimization of drug candidates. Although deep learning based methods have made notable progress, most existing works still do not fully utilize 3D spatial information. This can lead to a single molecular representation representing multiple actual molecules. To address these issues, we propose a novel 3D structure-based molecular modeling method named 3D-Mol. In order to accurately represent complete spatial structure, we design a novel encoder to extract 3D features by deconstructing the molecules into three geometric graphs. In addition, we use 20M unlabeled data to pretrain our model by contrastive learning. We consider conformations with the same topological structure as positive pairs and the opposites as negative pairs, while the weight is determined by the dissimilarity between the conformations. We compare 3D-Mol with various state-of-the-art baselines on 7 benchmarks and demonstrate our outstanding performance in 5 benchmarks.  \\nDeep Learning Molecular representation Molecular property prediction Graph neural network Self-supervised learning Contrastive learning'),\n",
       " Document(metadata={'Header 2': '1 Introduction'}, page_content=\"Molecular property prediction accelerates drug candidate identification, saving time and resources. It helps researchers prioritize promising compounds, streamlining drug development and increasing success rates. Moreover, it aids in understanding structure-activity relationships, revealing how specific features impact properties, interactions, and biological effects. Although deep learning has achieved success in molecular property prediction, its potential is significantly constrained by the scarcity of labeled data because labeled data for molecular properties usually requires expensive and time-consuming experiments[1].  \\nSelf-supervised learning uses large amounts of unlabeled data to pretrain models to leverage unlabeled data to learn rich feature representations. Many works[2, 3, 4] improve their model performance through self-supervised learning. Early deep learning methods for molecular property prediction[5, 6, 7, 8, 9] effectively utilized NLP-based self-supervised learning methods to handle data represented by the SMILES molecular formula[10]. However, SMILES can't fully reflect the topological relationships of a molecule. Therefore, many self-supervised methods based on molecular graphs have emerged, such as PretrainGNN[11], N-Gram-graph[12], MolCLR[13], GROVER[14]. They designed unique pretraining methods based on molecular graph. Besides, lots of work use graph neural networks to capture the topological information of molecules, such as MPNN[15], AttentiveFP[16] and D-MPNN[17]. However, these methods have not dealt with the 3D spatial information of molecules, which is critical for predicting molecular properties. As shown in Figure 1, Thalidomide is divided into two forms, R-Thalidomide and S-Thalidomide, due to different 3D structures. The former can be used to treat skin diseases, while the latter has been implicated in teratogenesis. Recently, some integrating geometric information into graph structures has attracted research attention in some molecular property estimation tasks[18, 19, 20, 21, 22, 23, 24, 25, 22], they either fail to fully extract the 3D information of molecules or only use masking methods for data augmentation in self-supervised learning, and only use the stablest conformation but ignore the others.\\nTo address these issues, we propose a novel method, 3D-Mol, for molecular property prediction. Firstly, we use atom-bond graph, bond-angle graph and plane-angle graph to represent the spatial structural information of molecules, extracting the 3D spatial structure representation of molecules through the information transfer within these three graphs and their interactions. In pretraining stage, we design a unique weighted contrastive learning method, which uses the different 3D conformations of the same SMILES as weighted positive pair SMILES, and the weight is dependent on the difference between those conformations. We also employ the geometry pretraining task following GearNet[26]. We learn 3D structural feature of molecular representations from a large volume of unlabeled data, and then finetune the well-pretrained model according to downstream tasks and data to predict molecular properties. We compared our approach with several state-of-the-art baselines on 7 molecular property prediction benchmarks[27], where our method achieved the best results on 5 benchmarks. In summary, our main contributions are as follows:  \\n\\\\(\\\\bullet\\\\) We propose a novel molecular representation method and design a corresponding model to fully extract the **3D spatial structural features** of molecules.  \\n\\\\(\\\\bullet\\\\) We design a unique weighted contrastive learning task, using the **different 3D conformations from the same SMILES as weighted positive pair**, and the weight is dependent on the difference between the conformations, thereby deeply learning the microscopic characteristics of molecular 3D space.  \\n\\\\(\\\\bullet\\\\) We have conducted thorough evaluations of the 3D-Mol model on various molecular property prediction datasets. Experimental results show that **3D-Mol significantly outperforms existing competitive models** in multiple benchmark tests.\"),\n",
       " Document(metadata={'Header 2': '2 Related Work'}, page_content='In general, there are two strategies to improve molecular property prediction. One is to design a novel molecular encoder based on molecular information for effective latent vector extraction, the other is to design a novel pretraining method to pretrain the molecular encoder by using a large amount of unlabeled data. The following are the related works of each.  \\n### Molecular Representation and Encoder  \\nProposing a novel molecular representation and encoder method is usually the first option for researchers to improve the accuracy of molecular property prediction. Some early works learn representation from chemical fingerprints, such as ECFP[21] and MACCS[28], frequently used in early machine learning[21, 29, 30]. The other learns representation from molecular descriptors, such as SMILES[10]. Inspired by mature NLP models, SMILES-BERT[31] applies the BERT[2] strategy to pretrain on SMILES to extract molecular representations. However, These methods depend on feature engineering, failing to capture the complete topological structure of molecules.  \\nRecently, many works use molecular graph as molecular representation because the natural representation of molecule is molecular graph and it represents the topology information. GG-NN[15], DMPNN[17], and DeepAtomicCharge[32] employed a message passing scheme for molecular property prediction. AttentiveFP[16] uses  \\nFigure 1: Thalidomide exists in two distinct 3D stereoisomeric forms, known as R-Thalidomide and S-Thalidomide. The former is recognized for its therapeutic properties in the treatment of various skin conditions, but the latter has been implicated in teratogenesis. It shows that despite having identical 2D molecular topology, the properties of two molecules vary significantly due to their distinct 3D structures.\\na graph attention network to aggregate and update node information. The MP-GNN[33] merges specific-scale Graph Neural Networks and element-specific Graph Neural Networks, capturing various atomic interactions of multiphysical representations at different scales. MGCN[34] designed a GCN to capture multilevel quantum interactions from the conformation and spatial information of molecules. But these works focuses on 2D molecular representation, and extracting only 2D topological information from molecules is insufficient.  \\nMany works[35] show the necessity of using 3D spatial information of molecules. Recently, some research has also begun modeling 3D molecules to address this issue. SGCN[25] applies different weights according to atomic distances during the GCN-based message passing process. SchNet[18] models complex atomic interactions using Gaussian radial basis functions for potential energy surface predictionor to accelerate the exploration of chemical space. DimeNet[22] proposes directional message passing to fully utilize directional information within molecules. [23] develops a novel geometrically-enhanced molecular representation learning method, and employs a specifically designed geometric-based graph neural network structure. However, these methods do not fully exploit the 3D structural information of molecules and lack the ability to learn the representations of 3D conformations with the same molecular topology.  \\n### Self-supervised Learning on Molecules  \\nSelf-supervised learning has achieved enormous success in BERT[2] and GPT[36]. Inspired by these, numerous works for molecular property prediction use this approch to effectively utilize large amounts of unlabeled data for pretraining.  \\nFor one-dimensional data, SMILES is frequently used to extract molecular feature in pretraining stage. ChemBERTa[9] followed RoBERTa[37] by employing masked language modeling (MLM) as a pretraining task, predicting masked tokens to restore the original sentence, aiding pretraining models to understand sequence semantics. SMILES Transformer[38] used a SMILES string as input to produce a temporary embedding, which is then restored to the original input SMILES by a decoder.  \\nAs the topological information of molecular graphs is gaining more attention, many pretraining methods aimed at graph data have been proposed. Shengchao Liu et al[12] used the n-gram method in NLP to extract and represent features of molecules. PretrainGNN[11] proposed a new pretraining strategy, include node-level and graph-level self-supervised pretraining tasks. GraphCL[39], MOCL[40] and MolCLR[13] performed molecular contrastive learning via graph neural networks, proposed new molecule augmentation methods. MPG[41] and GROVER[14] focused on node level and graph level representation and corresponding pretraining tasks for node level and graph level. iMolCLR[42], Sugar[43] and ReLMole[44] focused on the substructure of molecule, and designed the substructure pretraining task by using substructure information. However, the aforementioned pretraining strategies are only targeted at learning the topology information of the molecule.  \\nWith the 3D information of molecules proven to aid in the prediction of molecular representations, recent works have focused on pretraining task for the 3D structure information of molecules. 3DGCN[45] introduced a relative position matrix that includes 3D positions between atoms to ensure translational invariance during convolution. GraphMVP[46] proposed a SSL method involving contrastive learning and generative learning between 3D and 2D molecular views. [23] proposed a self-supervised framework using molecular geometric information. They constructed a new bond angle graph, where the chemical bonds within a molecule are considered as nodes instead of edges, and the angle formed between two bonds is considered as the edge between them. Uni-Mol[47] employed the transformer to extract molecular representation by predicting atom distance. Although these works have used the spatial information of molecules, but they have not fully utilized the spatial information of molecules, nor have they enabled the model to learn the representation of geometric isomers. To address these issues, we use RDKit to generate multiple conformations from same topology structure as positive pairs and design a weighted contrastive learning task for self-supervised training.'),\n",
       " Document(metadata={'Header 2': '3 Method'}, page_content=\"### Molecular Representation  \\nWe deconstruct molecular conformation into three graph, denoted as \\\\(Mol=\\\\{G_{a-b},G_{b-a},G_{p-a}\\\\}\\\\). In most databases, molecular raw data is represented by SMILES. To extract topological and spatial structure information from molecules, we need to use RDKit to transform the SMILES representation into molecular conformations. In our method, we decompose molecular conformation into three graphs. The first graph, named atom-bond graph, is the commonly used as 2D molecular graph, and is represented as \\\\(G_{a-b}=\\\\{V,E,P_{atom},P_{bond}\\\\}\\\\), where \\\\(V\\\\) is the set of atoms and \\\\(E\\\\) is the set of bonds. \\\\(P_{atom}\\\\in R^{|V|*d_{atom}}\\\\), and are the attributes of atoms, and \\\\(d_{atom}\\\\) is the number of atom attributes. \\\\(P_{bond}\\\\in R^{|E|*d_{bond}}\\\\), and is the attributes of bonds, and \\\\(d_{bond}\\\\) is the number of bond attributes. The second graph, named bond-angle graph, is represented as \\\\(G_{b-a}=\\\\{E,P,Ang_{\\\\theta}\\\\}\\\\), where \\\\(P\\\\) is a set of the plane that is comprised w\\n3 connected atoms. \\\\(Ang_{\\\\theta}\\\\) is the set of corresponding bond angle \\\\(\\\\theta\\\\). The third graph, named the plane-angle graph, is represented as \\\\(G_{p-a}=\\\\{P,C,Ang_{\\\\phi}\\\\}\\\\). \\\\(C\\\\) represents the set of two connected planes, which connect with a bond. \\\\(Ang_{\\\\phi}\\\\) represents the corresponding dihedral angle \\\\(\\\\phi\\\\). The first graph allows the model to learn the topological information of molecules, and the second and third graphs allow the model to learn the spatial structure information of molecules.  \\n### Attribute Embedding Layer  \\nThe 3D information of the molecule, such as the length of bonds and the angle between bonds, carries key chemical information. Firstly, we convert float numbers, like angle and bond length, to latent vectors. Referring to the previous work (Shui and Karypis 2020)[24], we employed several RBF layers to encode different geometric factors:  \\n\\\\[F_{l}^{k}=exp(-\\\\beta_{l}^{k}(exp(-l)-\\\\mu_{l}^{k})^{2})*W_{l}^{k} \\\\tag{1}\\\\]  \\nwhere \\\\(F_{l}^{k}\\\\) is the k-dimensional feature of bond length \\\\(l\\\\), and \\\\(\\\\mu_{l}^{k}\\\\) and \\\\(\\\\beta_{l}^{k}\\\\) are the center and width of \\\\(l\\\\) respectively. \\\\(\\\\mu_{l}^{k}\\\\) is 0.1\\\\(k\\\\) and \\\\(\\\\beta_{l}^{k}\\\\) is 10. Similarly, the k-dimensional feature of \\\\(F_{\\\\theta}^{k}\\\\) and \\\\(F_{\\\\phi}^{k}\\\\) of x is computed as:  \\n\\\\[F_{\\\\theta}^{k}=exp(-\\\\beta_{\\\\theta}^{k}(-\\\\theta-\\\\mu_{\\\\theta}^{k})^{2})*W_{ \\\\theta}^{k} \\\\tag{2}\\\\]  \\n\\\\[F_{\\\\phi}^{k}=exp(\\\\beta_{\\\\phi}^{k}(-\\\\phi-\\\\mu_{\\\\phi}^{k})^{2})*W_{\\\\phi}^{k} \\\\tag{3}\\\\]  \\nwhere \\\\(\\\\mu_{\\\\theta}^{k}\\\\) and \\\\(\\\\beta_{\\\\theta}^{k}\\\\) are the center and width of \\\\(\\\\theta\\\\), and \\\\(\\\\mu_{\\\\phi}^{k}\\\\) and \\\\(\\\\beta_{\\\\phi}^{k}\\\\) are the center and width of \\\\(\\\\phi\\\\). Centers of bond angle and dihedral angle are represented as \\\\(\\\\mu_{\\\\phi}^{k}\\\\) and \\\\(\\\\mu_{\\\\theta}^{k}\\\\) respectively, and the numerical value of them are \\\\(\\\\pi\\\\)/K, where K is the number of feature dimensions.  \\nFor the other property of atom and bond, we represent them with \\\\(P_{atom}\\\\) and \\\\(P_{bond}\\\\) respectively. Inspired by NLP, we embed them with the word embedding function. The initial features of atom and bond are represented as \\\\(F_{atom}^{0}\\\\) and \\\\(F_{bond}^{0}\\\\) respectively.  \\nFigure 2: **The overview of the 3D-Mol model framework.** a) In the pretraining stage, we employ weighted contrastive learning to effectively pretrain our model. In addition to using the mask strategy for graph data augmentation, we consider conformations stemming from the same topological structure as positive pairs, with their weight determined by the dissimilarity between the conformations. Conversely, distinct topological structures are treated as negative pairs, and we further utilize fingerprint differences to compute the weight of negative pairs. b) In the finetuning stage, we refine the well-pretrained encoder using diverse downstream datasets, followed by supervised learning.\\n### 3D-Mol Layer  \\nInspired by GEM[23], we use message passing strategy to let node send and receive messages with edge in \\\\(\\\\{G_{a-b}^{i},G_{b-a}^{i},G_{p-a}^{i}\\\\}\\\\). For the \\\\(i_{th}\\\\) layer in 3D-Mol, the information of \\\\(\\\\{G_{a-b}^{i},G_{b-a}^{i},G_{p-a}^{i}\\\\}\\\\) will be updated by message passing neural network orderly. The message passing of the latter graph need the information of the former graph. The overview is shown in figure 3, and the details are as follow:  \\nFirst, we use \\\\(GNN_{a-b}^{i}\\\\) to aggregate the message and update the atom and bond latent vectors in \\\\(G_{a-b}^{i}\\\\). Given an atom v, its representation vector \\\\(F_{v}^{i}\\\\) is formalized by:  \\n\\\\[a_{v}^{i,a-b}=Agg_{a-b}^{(i)}(F_{v}^{i-1},F_{u}^{i-1},F_{uv}^{i-1}|u\\\\in N(v)) \\\\tag{4}\\\\]  \\n\\\\[F_{v}^{i}=Comb_{a-b,n}^{(k)}(F_{v}^{i-1},a_{v}^{i}) \\\\tag{5}\\\\]  \\n\\\\[F_{uv}^{i,temp}=Comb_{a-b,e}^{(k)}(F_{uv}^{i-1},F_{u}^{i-1},F_{v}^{i-1}) \\\\tag{6}\\\\]  \\nwhere \\\\(N(v)\\\\) is the set of neighbors of atom v in \\\\(G_{a-b}^{i}\\\\), and \\\\(Agg_{a-b}^{(i)}\\\\) is the aggregation function for aggregating messages from an atom's neighborhood in \\\\(G_{a-b}^{i}\\\\). \\\\(Comb_{a-b,n}^{(k)}\\\\) is the update function for updating the atom latent vectors in \\\\(G_{a-b}^{i}\\\\), and \\\\(Comb_{a-b,e}^{(k)}\\\\) is the update function for updating the bond latent vectors in \\\\(G_{a-b}^{i}\\\\). \\\\(a_{v}^{i(a-b)}\\\\) is the information from the neighboring atom and the corresponding bond after be aggregated in \\\\(G_{a-b}^{i}\\\\). \\\\(F_{uv}^{i,temp}\\\\) is the temporary bond latent vectors of bond \\\\(uv\\\\) in \\\\(i_{th}\\\\) layer. Processing \\\\(G_{a-b}^{i}\\\\) with \\\\(GNN_{a-b}^{i}\\\\) make features of atom and bond can be updated by information from their neighbor, and the model will learn the topology information of the molecule. \\\\(F_{uv}^{i,temp}\\\\) is part of bond feature in \\\\(G_{b-a}^{i}\\\\).  \\nThen, we use \\\\(GNN_{b-a}^{i}\\\\) to aggregate the message and update the bond and plane vector in \\\\(G_{b-a}^{i}\\\\). Given a bond \\\\(uv\\\\), its latent vector \\\\(F_{uv}^{i}\\\\) is formalized by:  \\n\\\\[a_{uv}^{i,b-a}=Agg_{b-a}^{(i)}(\\\\{F_{uv}^{i-1},F_{vw}^{i-1},F_{uvw}^{i-1}|u\\\\in N (v)\\\\cap\\\\in N(v)\\\\cap u\\\\neq w\\\\}) \\\\tag{7}\\\\]  \\n\\\\[F_{uv}^{i}=Comb_{b-a,n}^{(k)}(F_{uv}^{i-1},F_{uv}^{i,temp},a_{uv}^{i}) \\\\tag{8}\\\\]  \\n\\\\[F_{uvw}^{i-1,temp}=Comb_{b-a,e}^{(k)}(F_{uvw}^{i-1},F_{uv}^{i-1},F_{vw}^{i-1}) \\\\tag{9}\\\\]  \\nFigure 3: **Overview of the 3D-Mol encoder layer.** The 3D-Mol encoder layer comprises three steps. Firstly, employing a message passing strategy, nodes in each graph exchange messages with their connected edges, leading to the updating of edge and node latent vectors. Secondly, the edge latent vector from the lower-level graph is transmitted to the higher-level graph as part of the node latent vector. Finally, the iteration is performed n times to derive the \\\\(n_{th}\\\\) node latent vector, from which we extract the molecular latent vectors.\\nwhere \\\\(Agg^{(i)}_{b-a}\\\\) is the aggregation function for aggregating messages from a bond's neighborhood in \\\\(G^{i}_{b-a}\\\\), and \\\\(Comb^{(k)}_{b-a,n}\\\\) is the update function for updating the bond latent vector in \\\\(G^{i}_{b-a}\\\\), and \\\\(Comb^{(k)}_{b-a,e}\\\\) is the update function for updating the plane latent vector in \\\\(G^{i}_{b-a}\\\\). \\\\(a^{i,b-a}_{vx}\\\\) is the information from the neighboring bond and the corresponding bond angle after being aggregated. Processing \\\\(G^{i}_{b-a}\\\\) with \\\\(GNN^{i}_{b-a}\\\\) make feature of bond and bond angle can be update based on information from their neighbor, and the model will learn the 3D information of the molecular. \\\\(F^{i-1,temp}_{uvw}\\\\) is part of plane feature in \\\\(G^{i}_{p-a}\\\\).  \\nAfter process the \\\\(G^{i}_{b-a}\\\\), we use \\\\(GNN^{i}_{p-a}\\\\) to aggregate the message and update the plane latent vector in \\\\(G^{i}_{p-a}\\\\). Given a plane which is constructed by node u, v, w and bond \\\\(uv\\\\), \\\\(vw\\\\), its latent vector \\\\(F^{i}_{uvw}\\\\) is formalized by:  \\n\\\\[a^{i,p-a}_{uvw}=Agg^{(i)}_{p-a}(\\\\{F^{i-1}_{uvw},F^{i-1}_{vvh},F^{i-1}_{uvw}|u \\\\in N(v)\\\\cap v\\\\in N(w)\\\\cap v\\\\in N(h)\\\\cap u\\\\neq v\\\\neq w\\\\neq h\\\\}) \\\\tag{10}\\\\]  \\n\\\\[F^{i}_{uvw}=Comb^{(k)}_{p-a,n}(F^{i-1}_{uv},F^{i-1}_{uv},F^{i-1}_{vw}) \\\\tag{11}\\\\]  \\n\\\\[F^{i-1}_{uvw}=Comb^{(k)}_{b-a,e}(F^{i-1}_{uvw},F^{i-1}_{uv},F^{i-1}_{vw}) \\\\tag{12}\\\\]  \\nwhere \\\\(agg^{(i)}_{p-a}\\\\) is the aggregation function for aggregating messages from a plane's neighborhood in \\\\(G^{i}_{p-a}\\\\), and \\\\(Comb^{(k)}_{p-a,n}\\\\) is the update function for updating the plane latent vector in \\\\(G^{i}_{p-a}\\\\), and \\\\(Comb^{(k)}_{p-a,e}\\\\) is the update function for updating the dihedral angle latent vector in \\\\(G^{i}_{p-a}\\\\). \\\\(a^{i}_{uv}\\\\) is the information from the neighboring plane and the corresponding dihedral angle after be aggregated. Processing \\\\(G^{i}_{p-a}\\\\) with \\\\(GNN^{i}_{p-a}\\\\) makes feature of plane and dihedral angle can be updated by information from their neighbor, and the model will learn the 3D information of the molecular, and model the bond interaction.  \\nThe representation vectors of the atoms at the final iteration are integrated to gain the molecular representation vector \\\\(F_{mol}\\\\) by the Readout function, which is formalized as:  \\n\\\\[F_{mol}=Readout(F^{n}_{u}|u\\\\in N(v)) \\\\tag{13}\\\\]  \\nwhere \\\\(F^{n}\\\\) is the last 3D-Mol layer output. The molecular latent vector \\\\(F_{mol}\\\\) is used to predict molecular properties. We extract the \\\\(F_{mol}\\\\) from all atom latent vector in the last layer.  \\n### Pretrain Strategy  \\nTo improve the performance of 3D-Mol encoder, we employ the constructive learning as pretraining method, using different conformations of the same topological structure as positive pair. We also combine our pretraining method with geometry task in [26] to pretrain 3D-Mol with a large amount of unlabeled data. The overview of our pretraining method is shown in figure 3, and the following are the details of our pretraining method.  \\n#### 3.4.1 Weighted contrastive learning task  \\nOur objective is to facilitate the learning of the consistency and difference between the most stable molecular conformation, denoted as \\\\(Mconf_{i}\\\\), and another randomly selected conformation, denoted as \\\\(Mconf_{j}\\\\). To accomplish this, we employ weighted contrastive learning using a batch of molecular representations, with the loss function defined as follows:  \\n\\\\[L^{conf}_{i,j}=-log\\\\frac{exp(w^{conf}_{i,j}sim(F_{i},F^{mk}_{j})/\\\\tau)}{\\\\Sigma^ {2N}_{k=1}\\\\{k\\\\neq i\\\\}exp(w^{fp}_{i,k}sim(F_{i},F_{k})/\\\\tau)} \\\\tag{14}\\\\]  \\n\\\\[w^{conf}_{i,j}=1-\\\\lambda_{conf}*ConfSim(Mconf_{i},Mconf_{j}) \\\\tag{15}\\\\]  \\n\\\\[w^{fp}_{i,k}=1-\\\\lambda_{fp}*FPSim(Mconf_{i},Mconf_{k}) \\\\tag{16}\\\\]  \\nwhere \\\\(F_{i}\\\\) is the latent vector extracted from \\\\(Mconf_{i}\\\\), and \\\\(sim(F_{i},F_{j})\\\\) is the similarity between two latent vectors \\\\((F_{i}\\\\), \\\\(F_{j})\\\\), and penalized by a weight coefficient \\\\(w^{conf}_{i,j}\\\\). \\\\(w^{conf}_{i,j}\\\\) is computed by \\\\(ConfSim(Mconf_{i},Mconf_{j})\\\\), the difference between \\\\(Mconf_{i}\\\\) and \\\\(Mconf_{j}\\\\), which can be computed by RDKit. \\\\(\\\\lambda_{conf}\\\\in[0,1]\\\\) is the hyperparameter that determines the scale of penalty for the difference between two conformation. Except using different conformations as the positive pair, we also use node and subgraph masking as the molecular data augmentation strategy. We mask \\\\(Mconf_{i}\\\\) 15\\\\(\\\\%\\\\) nodes and corresponding edges, and the mask latent vector conformation is denoted as \\\\(F^{mk}_{j}\\\\). Following iMolCLR[42], the similarity measurement between two latent vectors \\\\(F_{i}\\\\), \\\\(F_{k}\\\\) from a negative molecule pair (\\\\(Mconf_{i},Mconf_{j}\\\\)) is penalized by a weight coefficient \\\\(w^{fp}_{i,k}\\\\), which computed by the Fingerprint similarity between \\\\(Mconf_{i}\\\\) and \\\\(Mconf_{k}\\\\). \\\\(FPSim(Mconf_{i},Mconf_{j})\\\\) evaluates the fingerprint similarity of the given two molecules \\\\(Mconf_{i},Mconf_{j}\\\\), and \\\\(\\\\lambda_{fp}\\\\in[0,1]\\\\) is the hyperparameter that determines the scale of penalty for faulty negatives.\\n#### 3.4.2 Geometry task  \\n3D information have been shown to be important features[22], so we employ geometry task as pretraining method. For bond angle and dihedral angle prediction, we sample adjacent atoms to better capture local structural information. Since angular values are more sensitive to errors in protein structures than distances, we use discretized values for prediction.  \\n\\\\[L_{i,j}^{dist}=(f_{dist}(Fn_{n,i}^{mk},Fn_{n,j}^{mk})-dist_{i,j})^{2} \\\\tag{17}\\\\]  \\n\\\\[L_{i,j}^{l}=(f_{l}(Fn_{n,i}^{mk},Fn_{n,j}^{mk})-l_{i,j})^{2} \\\\tag{18}\\\\]  \\n\\\\[L_{i,j,k}^{\\\\theta}=CE(f_{\\\\theta}(Fn_{n,i}^{mk},Fn_{n,j}^{mk},Fn_{n,k}^{mk}), bin(\\\\theta_{i,j,k})) \\\\tag{19}\\\\]  \\n\\\\[L_{i,j,k,p}^{\\\\phi}=CE(f_{\\\\phi}(Fn_{n,i}^{mk},Fn_{n,j}^{mk},Fn_{n,k}^{mk},Fn_{ n,p}^{mk}),bin(\\\\phi_{i,j,k,p})) \\\\tag{20}\\\\]  \\nwhere \\\\(f_{\\\\phi}(.)\\\\), \\\\(f_{\\\\theta}(.)\\\\), \\\\(f_{dist}(.)\\\\) and \\\\(f_{l}\\\\) are the MLPs for each task, and \\\\(L_{i,j}^{dist}\\\\), \\\\(L_{i,j}^{l}\\\\), \\\\(L_{i,j,k}^{\\\\theta}\\\\), \\\\(L_{i,j,k,p}^{\\\\phi}\\\\) and \\\\(L_{i}^{FP}\\\\) are the loss functions for each task. \\\\(CE(.)\\\\) is cross entropy loss, and \\\\(bin()\\\\) is used to discretize the bond angle and dihedral angle. \\\\(Fn_{n,i}^{mk}\\\\) is the latent vector of node i after masking the corresponding sampled items in each task.  \\nIn addition to the aforementioned pretraining tasks to capture global molecular information, we leverage masked molecular latent vectors for fingerprint (FP) prediction, effectively incorporating latent representations to enrich the predictive capability.  \\n\\\\[L_{i}^{FP}=BCE(f_{FP}(Fm^{mk}),FP_{i}) \\\\tag{21}\\\\]  \\nwhere \\\\(f_{FP}\\\\) is the MLPs for global geometric task, and \\\\(L_{i}^{FP}\\\\) is the loss function. \\\\(BCE(.)\\\\) is binary cross entropy loss. \\\\(Fm^{mk}\\\\) is the latent vector of the masking molecule.\"),\n",
       " Document(metadata={'Header 2': '4 Experiment'}, page_content='In this section, we conduct experiments on 7 benchmark datasets in MoleculeNet to demonstrate the effectiveness of 3D-Mol for molecular property prediction. Firstly we use a large amount of unlabeled data and our pretraining method to pretrain the 3D-Mol model, then we use the downstream task to finetune well-pretrained model and predict the molecular property. We compared it with a variety of state-of-the-art methods. Also we conduct several ablation studies to confirm the 3D-Mol model and our pretraining method is useful.  \\n### Datasets and Setup  \\n#### 4.1.1 Pretraining stage  \\nWe use 20 million unlabeled molecules to pretrain 3D-Mol. The unlabeled data is extracted from ZINC20 and PubChem, both of which are publicly accessible databases containing drug-like compounds. To ensure consistency with prior research[23], we randomly selected 90\\\\(\\\\%\\\\) of these molecules for training purposes, while the remaining \\\\(\\\\%\\\\) was set aside for evaluation. The raw data obtained from ZINC20 and PubChem was provided in the SMILES format. In order to convert the SMILES representations into molecular conformations, we employed RDKit and applied the ETKDG method. For our model, we use Adam optimizer with a learning rate of 1e-3. The batch size is set to 256 for pretraining and 32 for finetuning. The hidden size of all models is unspecified. The geometric embedding dimension K is 64, and the number of angle domains is 8. The hyperparameters \\\\(\\\\lambda_{conf}\\\\) and \\\\(\\\\lambda_{fp}\\\\) are both set to 0.5.  \\n\\\\begin{table}\\n\\\\begin{tabular}{c c c c} \\\\hline \\\\hline Dataset & \\\\(\\\\#\\\\) Tasks & Task Type & \\\\(\\\\#\\\\) Molecules \\\\\\\\ \\\\hline BACE & 1 & Classifcation & 1513 \\\\\\\\ Sider & 27 & Classifcation & 1427 \\\\\\\\ Tox21 & 12 & Classifcation & 7831 \\\\\\\\ ToxCast & 617 & Classifcation & 8597 \\\\\\\\ ESOL & 1 & Regression & 1128 \\\\\\\\ FreeSolv & 1 & Regression & 643 \\\\\\\\ Lipophilicity & 1 & Regression & 4200 \\\\\\\\ \\\\hline \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 1: Statistics information of datasets\\n#### 4.1.2 Finetuning stage  \\nWe use 7 molecular datasets obtained from MoleculeNet to demonstrate the effectiveness of 3D-Mol. These datasets encompass a range of biophysics datasets such as BACE, physical chemistry datasets like ESOL, and physiology datasets like Tox21. In the fine-tuning stage, we employed nine molecular datasets obtained from MoleculeNet. These datasets encompass a range of biophysics datasets such as BACE, physical chemistry datasets like ESOL, and physiology datasets like Tox21. Table 1 provides a summary of the statistical information for these datasets, while the remaining details are outlined below:  \\n\\\\(\\\\bullet\\\\) BACE. The BACE dataset provides both quantitative (IC50) and qualitative (binary label) binding results for a set of inhibitors targeting human \\\\(\\\\beta\\\\)-secretase 1 (BACE-1).  \\n\\\\(\\\\bullet\\\\) Tox21. The Tox21 initiative aims to advance toxicology practices in the 21st century and has created a public database containing qualitative toxicity measurements for 12 biological targets, including nuclear receptors and stress response pathways.  \\n\\\\(\\\\bullet\\\\) Toxcast. ToxCast, an initiative related to Tox21, offers a comprehensive collection of toxicology data obtained through in vitro high-throughput screening. It includes information from over 600 experiments and covers a large library of compounds.  \\n\\\\(\\\\bullet\\\\) SIDER. The SIDER database is a compilation of marketed drugs and their associated adverse drug reactions (ADRs), categorized into 27 system organ classes.  \\n\\\\(\\\\bullet\\\\) ESOL. The ESOL dataset is a smaller collection of water solubility data, specifically providing information on the log solubility in mols per liter for common organic small molecules.  \\nFreeSolv. The FreeSolv database offers experimental and calculated hydration-free energy values for small molecules dissolved in water.  \\n\\\\(\\\\bullet\\\\) Lipo. Lipophilicity is a crucial characteristic of drug molecules that affects their membrane permeability and solubility. The Lipo dataset contains experimental data on the octanol/water distribution coefficient (logD at pH 7.4).  \\nFollowing the previous works[23], We partitioned the datasets into train/validation/test sets in an 80/10/10 ratio for downstream tasks, and We use scaffold splitting and report the mean and standard deviation by the results of 3 random seeds.  \\n### Metric  \\nConsistent with prior studies, we adopt the average ROC-AUC as the evaluation metric for the classification datasets (BACE, SIDER, Tox21 and ToxCast), which is a widely used metric for assessing the performance of binary classification tasks. For the regression datasets (ESOL, FreeSolv and Lipophilicity), we utilize the RMSE as the evaluation metric.  \\n### Result  \\na) **To validate the efficacy of our proposed method, we compare it with several baseline methods.** The baseline methods are as follows: N-Gram[12] generates a graph representation by constructing node embeddings  \\n\\\\begin{table}\\n\\\\begin{tabular}{|c|c|c|c|c|c|c|c|} \\\\hline \\\\multirow{2}{*}{model} & \\\\multicolumn{3}{c|}{Classification (ROC-AUC \\\\(\\\\%\\\\), higher is better \\\\(\\\\uparrow\\\\) )} & \\\\multicolumn{3}{c|}{Regression (RMSE, lower is better \\\\(\\\\downarrow\\\\))} \\\\\\\\ \\\\cline{2-7}  & BACE & SIDER & Tox21 & ToxCast & ESOL & FreeSolv & Lipophilicity \\\\\\\\ \\\\hline N-Gram\\\\({}_{\\\\text{RF}}\\\\) & \\\\(0.779_{0.015}\\\\) & \\\\(0.668_{0.007}\\\\) & \\\\(0.743_{0.004}\\\\) & \\\\(-\\\\) & \\\\(1.074_{0.107}\\\\) & \\\\(2.688_{0.085}\\\\) & \\\\(0.812_{0.028}\\\\) \\\\\\\\ N-Gram\\\\({}_{\\\\text{XGB}}\\\\) & \\\\(0.791_{0.013}\\\\) & \\\\(0.655_{0.007}\\\\) & \\\\(0.758_{0.009}\\\\) & \\\\(-\\\\) & \\\\(1.083_{0.107}\\\\) & \\\\(5.061_{0.744}\\\\) & \\\\(2.072_{0.030}\\\\) \\\\\\\\ PretrainGNN & \\\\(0.845_{0.007}\\\\) & \\\\(0.627_{0.008}\\\\) & \\\\(0.781_{0.006}\\\\) & \\\\(0.657_{0.006}\\\\) & \\\\(1.100_{0.006}\\\\) & \\\\(2.764_{0.002}\\\\) & \\\\(0.739_{0.003}\\\\) \\\\\\\\ GROVER\\\\({}_{\\\\text{base}}\\\\) & \\\\(0.826_{0.007}\\\\) & \\\\(0.648_{0.006}\\\\) & \\\\(0.743_{0.001}\\\\) & \\\\(0.654_{0.004}\\\\) & \\\\(0.983_{0.090}\\\\) & \\\\(2.176_{0.052}\\\\) & \\\\(0.817_{0.008}\\\\) \\\\\\\\ GROVER\\\\({}_{\\\\text{large}}\\\\) & \\\\(0.810_{0.014}\\\\) & \\\\(0.654_{0.001}\\\\) & \\\\(0.735_{0.001}\\\\) & \\\\(0.653_{0.005}\\\\) & \\\\(0.895_{0.017}\\\\) & \\\\(2.272_{0.051}\\\\) & \\\\(0.823_{0.010}\\\\) \\\\\\\\ MolCLR & \\\\(0.824_{0.009}\\\\) & \\\\(0.589_{0.014}\\\\) & \\\\(0.750_{0.002}\\\\) & \\\\(-\\\\) & \\\\(1.271_{0.040}\\\\) & \\\\(2.594_{0.249}\\\\) & \\\\(0.691_{0.004}\\\\) \\\\\\\\ \\\\hline\\n3DInfomax & \\\\(0.797_{0.015}\\\\) & \\\\(0.606_{0.008}\\\\) & \\\\(0.644_{0.011}\\\\) & \\\\(0.745_{0.007}\\\\) & \\\\(0.894_{0.028}\\\\) & \\\\(2.337_{0.107}\\\\) & \\\\(0.695_{0.012}\\\\) \\\\\\\\ GraphMVP & \\\\(0.812_{0.009}\\\\) & \\\\(0.639_{0.012}\\\\) & \\\\(0.759_{0.005}\\\\) & \\\\(0.631_{0.004}\\\\) & \\\\(1.029_{0.033}\\\\) & \\\\(-\\\\) & \\\\(0.681_{0.010}\\\\) \\\\\\\\ GEM & \\\\(0.856_{0.011}\\\\) & \\\\(\\\\mathbf{0.672}_{0.004}\\\\) & \\\\(0.781_{0.005}\\\\) & \\\\(0.692_{0.004}\\\\) & \\\\(0.798_{0.029}\\\\) & \\\\(1.877_{0.094}\\\\) & \\\\(0.660_{0.008}\\\\) \\\\\\\\ Uni-Mol & \\\\(0.857_{0.005}\\\\) & \\\\(0.659_{0.013}\\\\) & \\\\(\\\\mathbf{0.796}_{0.006}\\\\) & \\\\(0.696_{0.001}\\\\) & \\\\(0.788_{0.029}\\\\) & \\\\(1.620_{0.035}\\\\) & \\\\(0.603_{0.010}\\\\) \\\\\\\\ \\\\hline\\n3D-Mol\\\\({}_{\\\\text{wcl}}\\\\) & \\\\(\\\\mathbf{0.875}_{0.004}\\\\) & \\\\(0.656_{0.002}\\\\) & \\\\(0.786_{0.003}\\\\) & \\\\(\\\\mathbf{0.697}_{0.003}\\\\) & \\\\(\\\\mathbf{0.783}_{0.009}\\\\) & \\\\(\\\\mathbf{1.617}_{0.050}\\\\) & \\\\(\\\\mathbf{0.598}_{0.018}\\\\) \\\\\\\\ \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 2: Comparison of performance on the 7 molecular property prediction tasks and the methods below are pretraining method. We mark the best results in bold and underline the second best.\\nbased on short walks. PretrainGNN[11] implements several types of self-supervised learning tasks. 3D Infomax[48] maximizes the mutual information between learned 3D summary vectors and the representations of a graph neural network. MolCLR[13] is a 2D-2D view contrastive learning model that involves atom masking, bond deletion, and subgraph removal. GraphMVP[46] introduces 2D-3D view contrastive learning approaches. GROVER[14] focused on node level and graph level representation and corresponding pretraining tasks for node level and graph level. GEM[23] employs predictive geometry self-supervised learning schemes that leverage 3D molecular information. Uni-Mol[47] enlarge the application scope and representation ability of molecular representation learning by using transformer. As result in Table 2, Our method gets the best result in 5 datasets and gets the second best result in 1 dataset. Furthermore, our method achieves overwhelming performance on BACE by a large margin. That shows that our method is better at extracting the molecular information. To do the ablation study, We compare the results of 3D-Mol and 3D-Mol without pretraining. It shows that the former achieve overwhelming performance and our pretraining method can improve the 3D-Mol model performance.  \\n**b) To validate the efficacy of our proposed model 3D-Mol encoder, we compare it with several baseline molecular encoder without pretraining.** The baseline molecular encoders are as follows: DMPNN[17] employed a message passing scheme for molecular property prediction. AttentiveFP[16] is an attention-based GNN that incorporates graph-level information. MGCN[34] designed hierarchical graph neural network to directly extract features from the conformation and spatial information followed by the multilevel interactions. HMGNN[24] leverages global molecular representations through an attention mechanism. SGCN[25] applies different weights according to atomic distances during the GCN-based message passing process. DimeNet[22] proposes directional message passing to fully utilize directional information within molecules. GEM[23] employs message passing strategy to extract 3D molecular information. We present the experimental result to show the efficiency of our 3D-Mol model, as can be seen in Table 3. From Table 4, 3D-Mol encoder significantly outperforms all the baselines on both types of tasks and improves the performance over the best baselines with \\\\(2\\\\%\\\\) and \\\\(11\\\\%\\\\) for classification and regression tasks respectively, since 3D-Mol incorporates geometrical parameters.  \\nc) **To validate the efficacy of our proposed pretraining task, we compare the performance of no pretraining 3DGNN, pretraining by geometrical tasks and pretraining by geometrical tasks and weighted contrastive loss, and the result shows in Table 4. The result shows that geometry tasks signifcantly improve the performance of 3DGNN. Compared with the pretraining method combined with weighted contrastive learning. In general, The combined pretraining method is better to improve the 3DGNN performance.  \\n\\\\begin{table}\\n\\\\begin{tabular}{|c|c|c|c|c|c|c|c|} \\\\hline \\\\multirow{2}{*}{model} & \\\\multicolumn{3}{c|}{Classification (ROC-AUC \\\\(\\\\%\\\\), higher is better \\\\(\\\\uparrow\\\\) )} & \\\\multicolumn{3}{c|}{Regression (RMSE, lower is better \\\\(\\\\downarrow\\\\))} \\\\\\\\ \\\\cline{2-7}  & BACE & SIDER & Tox21 & ToxCast & ESOL & FreeSolv & Lipophilicity \\\\\\\\ \\\\hline \\\\(3\\\\mathrm{DGNN}\\\\) & **0.875\\\\({}_{0.004}\\\\)** & 0.656\\\\({}_{0.002}\\\\) & 0.786\\\\({}_{0.003}\\\\) & **0.697\\\\({}_{0.003}\\\\)** & **0.783\\\\({}_{0.009}\\\\)** & 1.617\\\\({}_{0.050}\\\\) & **0.598\\\\({}_{0.018}\\\\)** \\\\\\\\ \\\\(3\\\\mathrm{DGNN}_{\\\\mathrm{wo.pre}}\\\\) & 0.832\\\\({}_{0.005}\\\\) & 0.624\\\\({}_{0.013}\\\\) & 0.780\\\\({}_{0.004}\\\\) & 0.682\\\\({}_{0.007}\\\\) & 0.794\\\\({}_{0.027}\\\\) & 1.769\\\\({}_{0.039}\\\\) & 0.674\\\\({}_{0.007}\\\\) \\\\\\\\ \\\\(3\\\\mathrm{DGNN}_{\\\\mathrm{wo.cl_{weighted}}}\\\\) & 0.874\\\\({}_{0.006}\\\\) & **0.661\\\\({}_{0.005}\\\\)** & **0.790\\\\({}_{0.003}\\\\)** & 0.693\\\\({}_{0.005}\\\\) & 0.795\\\\({}_{0.014}\\\\) & **1.557\\\\({}_{0.003}\\\\)** & 0.607\\\\({}_{0.006}\\\\) \\\\\\\\ \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 4: Ablation study. We compare the performance of no pretraining 3DGNN, pretraining by geometrical tasks and pretraining by geometrical tasks and weighted contrastive loss, and mark the best results in bold and underline the second best.  \\n\\\\begin{table}\\n\\\\begin{tabular}{|c|c|c|c|c|c|c|c|} \\\\hline \\\\multirow{2}{*}{model} & \\\\multicolumn{3}{c|}{Classification (ROC-AUC \\\\(\\\\%\\\\), higher is better \\\\(\\\\uparrow\\\\) )} & \\\\multicolumn{3}{c|}{Regression (RMSE, lower is better \\\\(\\\\downarrow\\\\))} \\\\\\\\ \\\\cline{2-7}  & BACE & SIDER & Tox21 & ToxCast & ESOL & FreeSolv & Lipophilicity \\\\\\\\ \\\\hline DMPNN & 0.809\\\\({}_{0.006}\\\\) & 0.570\\\\({}_{0.007}\\\\) & 0.759\\\\({}_{0.007}\\\\) & 0.655\\\\({}_{0.3}\\\\) & 1.050\\\\({}_{0.008}\\\\) & 2.082\\\\({}_{0.082}\\\\) & 0.683\\\\({}_{0.016}\\\\) \\\\\\\\ AttentiveFP & 0.784\\\\({}_{0.000}\\\\) & 0.606\\\\({}_{0.032}\\\\) & 0.761\\\\({}_{0.005}\\\\) & 0.637\\\\({}_{0.002}\\\\) & 0.877\\\\({}_{0.029}\\\\) & 2.073\\\\({}_{0.183}\\\\) & 0.721\\\\({}_{0.001}\\\\) \\\\\\\\ MGCN & 0.734\\\\({}_{0.008}\\\\) & 0.587\\\\({}_{0.019}\\\\) & 0.741\\\\({}_{0.006}\\\\) & \\\\(-\\\\) & \\\\(-\\\\) & \\\\(-\\\\) & \\\\(-\\\\) \\\\\\\\ \\\\hline SGCN & \\\\(-\\\\) & 0.559\\\\({}_{0.005}\\\\) & 0.766\\\\({}_{0.002}\\\\) & 0.657\\\\({}_{0.003}\\\\) & 1.629\\\\({}_{0.001}\\\\) & 2.363\\\\({}_{0.050}\\\\) & 1.021\\\\({}_{0.013}\\\\) \\\\\\\\ HMGNN & \\\\(-\\\\) & 0.615\\\\({}_{0.005}\\\\) & 0.768\\\\({}_{0.002}\\\\) & 0.672\\\\({}_{0.001}\\\\) & 1.390\\\\({}_{0.073}\\\\) & 2.123\\\\({}_{0.179}\\\\) & 2.116\\\\({}_{0.473}\\\\) \\\\\\\\ DimeNet & \\\\(-\\\\) & 0.612\\\\({}_{0.004}\\\\) & 0.774\\\\({}_{0.006}\\\\) & 0.637\\\\({}_{0.004}\\\\) & 0.878\\\\({}_{0.023}\\\\) & 2.094\\\\({}_{0.118}\\\\) & 0.727\\\\({}_{0.019}\\\\) \\\\\\\\ GEM & 0.828\\\\({}_{0.012}\\\\) & 0.606\\\\({}_{0.010}\\\\) & 0.773\\\\({}_{0.007}\\\\) & 0.675\\\\({}_{0.005}\\\\) & 0.832\\\\({}_{0.010}\\\\) & 1.857\\\\({}_{0.071}\\\\) & 0.666\\\\({}_{0.015}\\\\) \\\\\\\\ \\\\hline \\\\(3\\\\mathrm{D-Molw}_{\\\\mathrm{wo.pre}}\\\\) & **0.832\\\\({}_{0.005}\\\\)** & **0.624\\\\({}_{0.013}\\\\)** & **0.780\\\\({}_{0.004}\\\\)** & **0.682\\\\({}_{0.007}\\\\)** & **0.794\\\\({}_{0.027}\\\\)** & **1.769\\\\({}_{0.039}\\\\)** & **0.674\\\\({}_{0.007}\\\\)** \\\\\\\\ \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 3: Comparison of performance on the 9 molecular property prediction tasks, and the methods below are no pretraining. We mark the best results in bold and underline the second best.'),\n",
       " Document(metadata={'Header 2': '5 Conclusion'}, page_content='In this paper, we innovatively propose a novel 3D molecular model framework, 3D-Mol, to extract 3D molecular features. Furthermore, to effectively utilize a large number of unlabeled molecules and molecular conformations for feature extraction, we have designed a new self-supervised pretraining strategy. Our approach has been validated through numerous experiments and compared with multiple competitive benchmarks, demonstrating superior performance over other methods across various benchmarks.'),\n",
       " Document(metadata={'Header 2': 'Acknowledgment'}, page_content='The research was supported by the PengCheng Laboratory and by PengCheng Laboratory Cloud-Brain.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_splits = splitter.split_text(sample)\n",
    "print(len(sample_splits))\n",
    "sample_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_markdown_hierarchy(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.strip().split('\\n')\n",
    "    # Define a pattern to match headers (from # to ######)\n",
    "    header_pattern = re.compile(r'^(#{1,6})\\s*(.*)$')\n",
    "\n",
    "    # Initialize the root of the hierarchy\n",
    "    root = {'children': []}\n",
    "    # Stack to keep track of the current hierarchy levels\n",
    "    stack = [{'level': 0, 'node': root}]\n",
    "    # Accumulate text for the current node\n",
    "    current_text = []\n",
    "\n",
    "    for line in lines:\n",
    "        header_match = header_pattern.match(line)\n",
    "        if header_match:\n",
    "            # If we have accumulated text, add it to the current node\n",
    "            if current_text:\n",
    "                # Join accumulated text and add to the last node's 'text'\n",
    "                stack[-1]['node'].setdefault('text', '')\n",
    "                if stack[-1]['node']['text']:\n",
    "                    stack[-1]['node']['text'] += '\\n'\n",
    "                stack[-1]['node']['text'] += '\\n'.join(current_text).strip()\n",
    "                current_text = []\n",
    "            # Extract header level and text\n",
    "            header_marks, header_text = header_match.groups()\n",
    "            level = len(header_marks)\n",
    "            # Pop the stack to find the correct parent level\n",
    "            while stack and stack[-1]['level'] >= level:\n",
    "                stack.pop()\n",
    "            # Create a new node for the header\n",
    "            node = {\n",
    "                'header': f'h{level}',\n",
    "                'value': header_text.strip(),\n",
    "                'children': []\n",
    "            }\n",
    "            # Add the new node to its parent's 'children'\n",
    "            stack[-1]['node']['children'].append(node) ## parent\n",
    "            # Push the new node onto the stack\n",
    "            stack.append({'level': level, 'node': node}) ## for accumulating potential children\n",
    "        else:\n",
    "            # Accumulate non-header lines\n",
    "            current_text.append(line)\n",
    "    # After processing all lines, add any remaining text\n",
    "    if current_text:\n",
    "        stack[-1]['node'].setdefault('text', '')\n",
    "        if stack[-1]['node']['text']:\n",
    "            stack[-1]['node']['text'] += '\\n'\n",
    "        stack[-1]['node']['text'] += '\\n'.join(current_text).strip()\n",
    "    # Return the hierarchy starting from the root's children\n",
    "    return root['children']\n",
    "hierarchical_structure = parse_markdown_hierarchy(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'children': [],\n",
      "  'header': 'h6',\n",
      "  'text': 'Molecular property prediction offers an effective and efficient approach for early '\n",
      "          'screening and optimization of drug candidates. Although deep learning based methods '\n",
      "          'have made notable progress, most existing works still do not fully utilize 3D spatial '\n",
      "          'information. This can lead to a single molecular representation representing multiple '\n",
      "          'actual molecules. To address these issues, we propose a novel 3D structure-based '\n",
      "          'molecular modeling method named 3D-Mol. In order to accurately represent complete '\n",
      "          'spatial structure, we design a novel encoder to extract 3D features by deconstructing '\n",
      "          'the molecules into three geometric graphs. In addition, we use 20M unlabeled data to '\n",
      "          'pretrain our model by contrastive learning. We consider conformations with the same '\n",
      "          'topological structure as positive pairs and the opposites as negative pairs, while the '\n",
      "          'weight is determined by the dissimilarity between the conformations. We compare 3D-Mol '\n",
      "          'with various state-of-the-art baselines on 7 benchmarks and demonstrate our outstanding '\n",
      "          'performance in 5 benchmarks.\\n'\n",
      "          '\\n'\n",
      "          'Deep Learning Molecular representation Molecular property prediction Graph neural '\n",
      "          'network Self-supervised learning Contrastive learning',\n",
      "  'value': 'Abstract'},\n",
      " {'children': [],\n",
      "  'header': 'h2',\n",
      "  'text': 'Molecular property prediction accelerates drug candidate identification, saving time '\n",
      "          'and resources. It helps researchers prioritize promising compounds, streamlining drug '\n",
      "          'development and increasing success rates. Moreover, it aids in understanding '\n",
      "          'structure-activity relationships, revealing how specific features impact properties, '\n",
      "          'interactions, and biological effects. Although deep learning has achieved success in '\n",
      "          'molecular property prediction, its potential is significantly constrained by the '\n",
      "          'scarcity of labeled data because labeled data for molecular properties usually requires '\n",
      "          'expensive and time-consuming experiments[1].\\n'\n",
      "          '\\n'\n",
      "          'Self-supervised learning uses large amounts of unlabeled data to pretrain models to '\n",
      "          'leverage unlabeled data to learn rich feature representations. Many works[2, 3, 4] '\n",
      "          'improve their model performance through self-supervised learning. Early deep learning '\n",
      "          'methods for molecular property prediction[5, 6, 7, 8, 9] effectively utilized NLP-based '\n",
      "          'self-supervised learning methods to handle data represented by the SMILES molecular '\n",
      "          \"formula[10]. However, SMILES can't fully reflect the topological relationships of a \"\n",
      "          'molecule. Therefore, many self-supervised methods based on molecular graphs have '\n",
      "          'emerged, such as PretrainGNN[11], N-Gram-graph[12], MolCLR[13], GROVER[14]. They '\n",
      "          'designed unique pretraining methods based on molecular graph. Besides, lots of work use '\n",
      "          'graph neural networks to capture the topological information of molecules, such as '\n",
      "          'MPNN[15], AttentiveFP[16] and D-MPNN[17]. However, these methods have not dealt with '\n",
      "          'the 3D spatial information of molecules, which is critical for predicting molecular '\n",
      "          'properties. As shown in Figure 1, Thalidomide is divided into two forms, R-Thalidomide '\n",
      "          'and S-Thalidomide, due to different 3D structures. The former can be used to treat skin '\n",
      "          'diseases, while the latter has been implicated in teratogenesis. Recently, some '\n",
      "          'integrating geometric information into graph structures has attracted research '\n",
      "          'attention in some molecular property estimation tasks[18, 19, 20, 21, 22, 23, 24, 25, '\n",
      "          '22], they either fail to fully extract the 3D information of molecules or only use '\n",
      "          'masking methods for data augmentation in self-supervised learning, and only use the '\n",
      "          'stablest conformation but ignore the others.\\n'\n",
      "          'To address these issues, we propose a novel method, 3D-Mol, for molecular property '\n",
      "          'prediction. Firstly, we use atom-bond graph, bond-angle graph and plane-angle graph to '\n",
      "          'represent the spatial structural information of molecules, extracting the 3D spatial '\n",
      "          'structure representation of molecules through the information transfer within these '\n",
      "          'three graphs and their interactions. In pretraining stage, we design a unique weighted '\n",
      "          'contrastive learning method, which uses the different 3D conformations of the same '\n",
      "          'SMILES as weighted positive pair SMILES, and the weight is dependent on the difference '\n",
      "          'between those conformations. We also employ the geometry pretraining task following '\n",
      "          'GearNet[26]. We learn 3D structural feature of molecular representations from a large '\n",
      "          'volume of unlabeled data, and then finetune the well-pretrained model according to '\n",
      "          'downstream tasks and data to predict molecular properties. We compared our approach '\n",
      "          'with several state-of-the-art baselines on 7 molecular property prediction '\n",
      "          'benchmarks[27], where our method achieved the best results on 5 benchmarks. In summary, '\n",
      "          'our main contributions are as follows:\\n'\n",
      "          '\\n'\n",
      "          '\\\\(\\\\bullet\\\\) We propose a novel molecular representation method and design a '\n",
      "          'corresponding model to fully extract the **3D spatial structural features** of '\n",
      "          'molecules.\\n'\n",
      "          '\\n'\n",
      "          '\\\\(\\\\bullet\\\\) We design a unique weighted contrastive learning task, using the '\n",
      "          '**different 3D conformations from the same SMILES as weighted positive pair**, and the '\n",
      "          'weight is dependent on the difference between the conformations, thereby deeply '\n",
      "          'learning the microscopic characteristics of molecular 3D space.\\n'\n",
      "          '\\n'\n",
      "          '\\\\(\\\\bullet\\\\) We have conducted thorough evaluations of the 3D-Mol model on various '\n",
      "          'molecular property prediction datasets. Experimental results show that **3D-Mol '\n",
      "          'significantly outperforms existing competitive models** in multiple benchmark tests.',\n",
      "  'value': '1 Introduction'},\n",
      " {'children': [{'children': [],\n",
      "                'header': 'h3',\n",
      "                'text': 'Proposing a novel molecular representation and encoder method is usually '\n",
      "                        'the first option for researchers to improve the accuracy of molecular '\n",
      "                        'property prediction. Some early works learn representation from chemical '\n",
      "                        'fingerprints, such as ECFP[21] and MACCS[28], frequently used in early '\n",
      "                        'machine learning[21, 29, 30]. The other learns representation from '\n",
      "                        'molecular descriptors, such as SMILES[10]. Inspired by mature NLP models, '\n",
      "                        'SMILES-BERT[31] applies the BERT[2] strategy to pretrain on SMILES to '\n",
      "                        'extract molecular representations. However, These methods depend on '\n",
      "                        'feature engineering, failing to capture the complete topological '\n",
      "                        'structure of molecules.\\n'\n",
      "                        '\\n'\n",
      "                        'Recently, many works use molecular graph as molecular representation '\n",
      "                        'because the natural representation of molecule is molecular graph and it '\n",
      "                        'represents the topology information. GG-NN[15], DMPNN[17], and '\n",
      "                        'DeepAtomicCharge[32] employed a message passing scheme for molecular '\n",
      "                        'property prediction. AttentiveFP[16] uses\\n'\n",
      "                        '\\n'\n",
      "                        'Figure 1: Thalidomide exists in two distinct 3D stereoisomeric forms, '\n",
      "                        'known as R-Thalidomide and S-Thalidomide. The former is recognized for '\n",
      "                        'its therapeutic properties in the treatment of various skin conditions, '\n",
      "                        'but the latter has been implicated in teratogenesis. It shows that '\n",
      "                        'despite having identical 2D molecular topology, the properties of two '\n",
      "                        'molecules vary significantly due to their distinct 3D structures.\\n'\n",
      "                        'a graph attention network to aggregate and update node information. The '\n",
      "                        'MP-GNN[33] merges specific-scale Graph Neural Networks and '\n",
      "                        'element-specific Graph Neural Networks, capturing various atomic '\n",
      "                        'interactions of multiphysical representations at different scales. '\n",
      "                        'MGCN[34] designed a GCN to capture multilevel quantum interactions from '\n",
      "                        'the conformation and spatial information of molecules. But these works '\n",
      "                        'focuses on 2D molecular representation, and extracting only 2D '\n",
      "                        'topological information from molecules is insufficient.\\n'\n",
      "                        '\\n'\n",
      "                        'Many works[35] show the necessity of using 3D spatial information of '\n",
      "                        'molecules. Recently, some research has also begun modeling 3D molecules '\n",
      "                        'to address this issue. SGCN[25] applies different weights according to '\n",
      "                        'atomic distances during the GCN-based message passing process. SchNet[18] '\n",
      "                        'models complex atomic interactions using Gaussian radial basis functions '\n",
      "                        'for potential energy surface predictionor to accelerate the exploration '\n",
      "                        'of chemical space. DimeNet[22] proposes directional message passing to '\n",
      "                        'fully utilize directional information within molecules. [23] develops a '\n",
      "                        'novel geometrically-enhanced molecular representation learning method, '\n",
      "                        'and employs a specifically designed geometric-based graph neural network '\n",
      "                        'structure. However, these methods do not fully exploit the 3D structural '\n",
      "                        'information of molecules and lack the ability to learn the '\n",
      "                        'representations of 3D conformations with the same molecular topology.',\n",
      "                'value': 'Molecular Representation and Encoder'},\n",
      "               {'children': [],\n",
      "                'header': 'h3',\n",
      "                'text': 'Self-supervised learning has achieved enormous success in BERT[2] and '\n",
      "                        'GPT[36]. Inspired by these, numerous works for molecular property '\n",
      "                        'prediction use this approch to effectively utilize large amounts of '\n",
      "                        'unlabeled data for pretraining.\\n'\n",
      "                        '\\n'\n",
      "                        'For one-dimensional data, SMILES is frequently used to extract molecular '\n",
      "                        'feature in pretraining stage. ChemBERTa[9] followed RoBERTa[37] by '\n",
      "                        'employing masked language modeling (MLM) as a pretraining task, '\n",
      "                        'predicting masked tokens to restore the original sentence, aiding '\n",
      "                        'pretraining models to understand sequence semantics. SMILES '\n",
      "                        'Transformer[38] used a SMILES string as input to produce a temporary '\n",
      "                        'embedding, which is then restored to the original input SMILES by a '\n",
      "                        'decoder.\\n'\n",
      "                        '\\n'\n",
      "                        'As the topological information of molecular graphs is gaining more '\n",
      "                        'attention, many pretraining methods aimed at graph data have been '\n",
      "                        'proposed. Shengchao Liu et al[12] used the n-gram method in NLP to '\n",
      "                        'extract and represent features of molecules. PretrainGNN[11] proposed a '\n",
      "                        'new pretraining strategy, include node-level and graph-level '\n",
      "                        'self-supervised pretraining tasks. GraphCL[39], MOCL[40] and MolCLR[13] '\n",
      "                        'performed molecular contrastive learning via graph neural networks, '\n",
      "                        'proposed new molecule augmentation methods. MPG[41] and GROVER[14] '\n",
      "                        'focused on node level and graph level representation and corresponding '\n",
      "                        'pretraining tasks for node level and graph level. iMolCLR[42], Sugar[43] '\n",
      "                        'and ReLMole[44] focused on the substructure of molecule, and designed the '\n",
      "                        'substructure pretraining task by using substructure information. However, '\n",
      "                        'the aforementioned pretraining strategies are only targeted at learning '\n",
      "                        'the topology information of the molecule.\\n'\n",
      "                        '\\n'\n",
      "                        'With the 3D information of molecules proven to aid in the prediction of '\n",
      "                        'molecular representations, recent works have focused on pretraining task '\n",
      "                        'for the 3D structure information of molecules. 3DGCN[45] introduced a '\n",
      "                        'relative position matrix that includes 3D positions between atoms to '\n",
      "                        'ensure translational invariance during convolution. GraphMVP[46] proposed '\n",
      "                        'a SSL method involving contrastive learning and generative learning '\n",
      "                        'between 3D and 2D molecular views. [23] proposed a self-supervised '\n",
      "                        'framework using molecular geometric information. They constructed a new '\n",
      "                        'bond angle graph, where the chemical bonds within a molecule are '\n",
      "                        'considered as nodes instead of edges, and the angle formed between two '\n",
      "                        'bonds is considered as the edge between them. Uni-Mol[47] employed the '\n",
      "                        'transformer to extract molecular representation by predicting atom '\n",
      "                        'distance. Although these works have used the spatial information of '\n",
      "                        'molecules, but they have not fully utilized the spatial information of '\n",
      "                        'molecules, nor have they enabled the model to learn the representation of '\n",
      "                        'geometric isomers. To address these issues, we use RDKit to generate '\n",
      "                        'multiple conformations from same topology structure as positive pairs and '\n",
      "                        'design a weighted contrastive learning task for self-supervised training.',\n",
      "                'value': 'Self-supervised Learning on Molecules'}],\n",
      "  'header': 'h2',\n",
      "  'text': 'In general, there are two strategies to improve molecular property prediction. One is '\n",
      "          'to design a novel molecular encoder based on molecular information for effective latent '\n",
      "          'vector extraction, the other is to design a novel pretraining method to pretrain the '\n",
      "          'molecular encoder by using a large amount of unlabeled data. The following are the '\n",
      "          'related works of each.',\n",
      "  'value': '2 Related Work'},\n",
      " {'children': [{'children': [],\n",
      "                'header': 'h3',\n",
      "                'text': 'We deconstruct molecular conformation into three graph, denoted as '\n",
      "                        '\\\\(Mol=\\\\{G_{a-b},G_{b-a},G_{p-a}\\\\}\\\\). In most databases, molecular raw '\n",
      "                        'data is represented by SMILES. To extract topological and spatial '\n",
      "                        'structure information from molecules, we need to use RDKit to transform '\n",
      "                        'the SMILES representation into molecular conformations. In our method, we '\n",
      "                        'decompose molecular conformation into three graphs. The first graph, '\n",
      "                        'named atom-bond graph, is the commonly used as 2D molecular graph, and is '\n",
      "                        'represented as \\\\(G_{a-b}=\\\\{V,E,P_{atom},P_{bond}\\\\}\\\\), where \\\\(V\\\\) '\n",
      "                        'is the set of atoms and \\\\(E\\\\) is the set of bonds. \\\\(P_{atom}\\\\in '\n",
      "                        'R^{|V|*d_{atom}}\\\\), and are the attributes of atoms, and \\\\(d_{atom}\\\\) '\n",
      "                        'is the number of atom attributes. \\\\(P_{bond}\\\\in R^{|E|*d_{bond}}\\\\), '\n",
      "                        'and is the attributes of bonds, and \\\\(d_{bond}\\\\) is the number of bond '\n",
      "                        'attributes. The second graph, named bond-angle graph, is represented as '\n",
      "                        '\\\\(G_{b-a}=\\\\{E,P,Ang_{\\\\theta}\\\\}\\\\), where \\\\(P\\\\) is a set of the '\n",
      "                        'plane that is comprised w\\n'\n",
      "                        '3 connected atoms. \\\\(Ang_{\\\\theta}\\\\) is the set of corresponding bond '\n",
      "                        'angle \\\\(\\\\theta\\\\). The third graph, named the plane-angle graph, is '\n",
      "                        'represented as \\\\(G_{p-a}=\\\\{P,C,Ang_{\\\\phi}\\\\}\\\\). \\\\(C\\\\) represents '\n",
      "                        'the set of two connected planes, which connect with a bond. '\n",
      "                        '\\\\(Ang_{\\\\phi}\\\\) represents the corresponding dihedral angle '\n",
      "                        '\\\\(\\\\phi\\\\). The first graph allows the model to learn the topological '\n",
      "                        'information of molecules, and the second and third graphs allow the model '\n",
      "                        'to learn the spatial structure information of molecules.',\n",
      "                'value': 'Molecular Representation'},\n",
      "               {'children': [],\n",
      "                'header': 'h3',\n",
      "                'text': 'The 3D information of the molecule, such as the length of bonds and the '\n",
      "                        'angle between bonds, carries key chemical information. Firstly, we '\n",
      "                        'convert float numbers, like angle and bond length, to latent vectors. '\n",
      "                        'Referring to the previous work (Shui and Karypis 2020)[24], we employed '\n",
      "                        'several RBF layers to encode different geometric factors:\\n'\n",
      "                        '\\n'\n",
      "                        '\\\\[F_{l}^{k}=exp(-\\\\beta_{l}^{k}(exp(-l)-\\\\mu_{l}^{k})^{2})*W_{l}^{k} '\n",
      "                        '\\\\tag{1}\\\\]\\n'\n",
      "                        '\\n'\n",
      "                        'where \\\\(F_{l}^{k}\\\\) is the k-dimensional feature of bond length '\n",
      "                        '\\\\(l\\\\), and \\\\(\\\\mu_{l}^{k}\\\\) and \\\\(\\\\beta_{l}^{k}\\\\) are the center '\n",
      "                        'and width of \\\\(l\\\\) respectively. \\\\(\\\\mu_{l}^{k}\\\\) is 0.1\\\\(k\\\\) and '\n",
      "                        '\\\\(\\\\beta_{l}^{k}\\\\) is 10. Similarly, the k-dimensional feature of '\n",
      "                        '\\\\(F_{\\\\theta}^{k}\\\\) and \\\\(F_{\\\\phi}^{k}\\\\) of x is computed as:\\n'\n",
      "                        '\\n'\n",
      "                        '\\\\[F_{\\\\theta}^{k}=exp(-\\\\beta_{\\\\theta}^{k}(-\\\\theta-\\\\mu_{\\\\theta}^{k})^{2})*W_{ '\n",
      "                        '\\\\theta}^{k} \\\\tag{2}\\\\]\\n'\n",
      "                        '\\n'\n",
      "                        '\\\\[F_{\\\\phi}^{k}=exp(\\\\beta_{\\\\phi}^{k}(-\\\\phi-\\\\mu_{\\\\phi}^{k})^{2})*W_{\\\\phi}^{k} '\n",
      "                        '\\\\tag{3}\\\\]\\n'\n",
      "                        '\\n'\n",
      "                        'where \\\\(\\\\mu_{\\\\theta}^{k}\\\\) and \\\\(\\\\beta_{\\\\theta}^{k}\\\\) are the '\n",
      "                        'center and width of \\\\(\\\\theta\\\\), and \\\\(\\\\mu_{\\\\phi}^{k}\\\\) and '\n",
      "                        '\\\\(\\\\beta_{\\\\phi}^{k}\\\\) are the center and width of \\\\(\\\\phi\\\\). Centers '\n",
      "                        'of bond angle and dihedral angle are represented as '\n",
      "                        '\\\\(\\\\mu_{\\\\phi}^{k}\\\\) and \\\\(\\\\mu_{\\\\theta}^{k}\\\\) respectively, and the '\n",
      "                        'numerical value of them are \\\\(\\\\pi\\\\)/K, where K is the number of '\n",
      "                        'feature dimensions.\\n'\n",
      "                        '\\n'\n",
      "                        'For the other property of atom and bond, we represent them with '\n",
      "                        '\\\\(P_{atom}\\\\) and \\\\(P_{bond}\\\\) respectively. Inspired by NLP, we embed '\n",
      "                        'them with the word embedding function. The initial features of atom and '\n",
      "                        'bond are represented as \\\\(F_{atom}^{0}\\\\) and \\\\(F_{bond}^{0}\\\\) '\n",
      "                        'respectively.\\n'\n",
      "                        '\\n'\n",
      "                        'Figure 2: **The overview of the 3D-Mol model framework.** a) In the '\n",
      "                        'pretraining stage, we employ weighted contrastive learning to effectively '\n",
      "                        'pretrain our model. In addition to using the mask strategy for graph data '\n",
      "                        'augmentation, we consider conformations stemming from the same '\n",
      "                        'topological structure as positive pairs, with their weight determined by '\n",
      "                        'the dissimilarity between the conformations. Conversely, distinct '\n",
      "                        'topological structures are treated as negative pairs, and we further '\n",
      "                        'utilize fingerprint differences to compute the weight of negative pairs. '\n",
      "                        'b) In the finetuning stage, we refine the well-pretrained encoder using '\n",
      "                        'diverse downstream datasets, followed by supervised learning.',\n",
      "                'value': 'Attribute Embedding Layer'},\n",
      "               {'children': [],\n",
      "                'header': 'h3',\n",
      "                'text': 'Inspired by GEM[23], we use message passing strategy to let node send and '\n",
      "                        'receive messages with edge in '\n",
      "                        '\\\\(\\\\{G_{a-b}^{i},G_{b-a}^{i},G_{p-a}^{i}\\\\}\\\\). For the \\\\(i_{th}\\\\) '\n",
      "                        'layer in 3D-Mol, the information of '\n",
      "                        '\\\\(\\\\{G_{a-b}^{i},G_{b-a}^{i},G_{p-a}^{i}\\\\}\\\\) will be updated by '\n",
      "                        'message passing neural network orderly. The message passing of the latter '\n",
      "                        'graph need the information of the former graph. The overview is shown in '\n",
      "                        'figure 3, and the details are as follow:\\n'\n",
      "                        '\\n'\n",
      "                        'First, we use \\\\(GNN_{a-b}^{i}\\\\) to aggregate the message and update the '\n",
      "                        'atom and bond latent vectors in \\\\(G_{a-b}^{i}\\\\). Given an atom v, its '\n",
      "                        'representation vector \\\\(F_{v}^{i}\\\\) is formalized by:\\n'\n",
      "                        '\\n'\n",
      "                        '\\\\[a_{v}^{i,a-b}=Agg_{a-b}^{(i)}(F_{v}^{i-1},F_{u}^{i-1},F_{uv}^{i-1}|u\\\\in '\n",
      "                        'N(v)) \\\\tag{4}\\\\]\\n'\n",
      "                        '\\n'\n",
      "                        '\\\\[F_{v}^{i}=Comb_{a-b,n}^{(k)}(F_{v}^{i-1},a_{v}^{i}) \\\\tag{5}\\\\]\\n'\n",
      "                        '\\n'\n",
      "                        '\\\\[F_{uv}^{i,temp}=Comb_{a-b,e}^{(k)}(F_{uv}^{i-1},F_{u}^{i-1},F_{v}^{i-1}) '\n",
      "                        '\\\\tag{6}\\\\]\\n'\n",
      "                        '\\n'\n",
      "                        'where \\\\(N(v)\\\\) is the set of neighbors of atom v in \\\\(G_{a-b}^{i}\\\\), '\n",
      "                        'and \\\\(Agg_{a-b}^{(i)}\\\\) is the aggregation function for aggregating '\n",
      "                        \"messages from an atom's neighborhood in \\\\(G_{a-b}^{i}\\\\). \"\n",
      "                        '\\\\(Comb_{a-b,n}^{(k)}\\\\) is the update function for updating the atom '\n",
      "                        'latent vectors in \\\\(G_{a-b}^{i}\\\\), and \\\\(Comb_{a-b,e}^{(k)}\\\\) is the '\n",
      "                        'update function for updating the bond latent vectors in '\n",
      "                        '\\\\(G_{a-b}^{i}\\\\). \\\\(a_{v}^{i(a-b)}\\\\) is the information from the '\n",
      "                        'neighboring atom and the corresponding bond after be aggregated in '\n",
      "                        '\\\\(G_{a-b}^{i}\\\\). \\\\(F_{uv}^{i,temp}\\\\) is the temporary bond latent '\n",
      "                        'vectors of bond \\\\(uv\\\\) in \\\\(i_{th}\\\\) layer. Processing '\n",
      "                        '\\\\(G_{a-b}^{i}\\\\) with \\\\(GNN_{a-b}^{i}\\\\) make features of atom and bond '\n",
      "                        'can be updated by information from their neighbor, and the model will '\n",
      "                        'learn the topology information of the molecule. \\\\(F_{uv}^{i,temp}\\\\) is '\n",
      "                        'part of bond feature in \\\\(G_{b-a}^{i}\\\\).\\n'\n",
      "                        '\\n'\n",
      "                        'Then, we use \\\\(GNN_{b-a}^{i}\\\\) to aggregate the message and update the '\n",
      "                        'bond and plane vector in \\\\(G_{b-a}^{i}\\\\). Given a bond \\\\(uv\\\\), its '\n",
      "                        'latent vector \\\\(F_{uv}^{i}\\\\) is formalized by:\\n'\n",
      "                        '\\n'\n",
      "                        '\\\\[a_{uv}^{i,b-a}=Agg_{b-a}^{(i)}(\\\\{F_{uv}^{i-1},F_{vw}^{i-1},F_{uvw}^{i-1}|u\\\\in '\n",
      "                        'N (v)\\\\cap\\\\in N(v)\\\\cap u\\\\neq w\\\\}) \\\\tag{7}\\\\]\\n'\n",
      "                        '\\n'\n",
      "                        '\\\\[F_{uv}^{i}=Comb_{b-a,n}^{(k)}(F_{uv}^{i-1},F_{uv}^{i,temp},a_{uv}^{i}) '\n",
      "                        '\\\\tag{8}\\\\]\\n'\n",
      "                        '\\n'\n",
      "                        '\\\\[F_{uvw}^{i-1,temp}=Comb_{b-a,e}^{(k)}(F_{uvw}^{i-1},F_{uv}^{i-1},F_{vw}^{i-1}) '\n",
      "                        '\\\\tag{9}\\\\]\\n'\n",
      "                        '\\n'\n",
      "                        'Figure 3: **Overview of the 3D-Mol encoder layer.** The 3D-Mol encoder '\n",
      "                        'layer comprises three steps. Firstly, employing a message passing '\n",
      "                        'strategy, nodes in each graph exchange messages with their connected '\n",
      "                        'edges, leading to the updating of edge and node latent vectors. Secondly, '\n",
      "                        'the edge latent vector from the lower-level graph is transmitted to the '\n",
      "                        'higher-level graph as part of the node latent vector. Finally, the '\n",
      "                        'iteration is performed n times to derive the \\\\(n_{th}\\\\) node latent '\n",
      "                        'vector, from which we extract the molecular latent vectors.\\n'\n",
      "                        'where \\\\(Agg^{(i)}_{b-a}\\\\) is the aggregation function for aggregating '\n",
      "                        \"messages from a bond's neighborhood in \\\\(G^{i}_{b-a}\\\\), and \"\n",
      "                        '\\\\(Comb^{(k)}_{b-a,n}\\\\) is the update function for updating the bond '\n",
      "                        'latent vector in \\\\(G^{i}_{b-a}\\\\), and \\\\(Comb^{(k)}_{b-a,e}\\\\) is the '\n",
      "                        'update function for updating the plane latent vector in '\n",
      "                        '\\\\(G^{i}_{b-a}\\\\). \\\\(a^{i,b-a}_{vx}\\\\) is the information from the '\n",
      "                        'neighboring bond and the corresponding bond angle after being aggregated. '\n",
      "                        'Processing \\\\(G^{i}_{b-a}\\\\) with \\\\(GNN^{i}_{b-a}\\\\) make feature of '\n",
      "                        'bond and bond angle can be update based on information from their '\n",
      "                        'neighbor, and the model will learn the 3D information of the molecular. '\n",
      "                        '\\\\(F^{i-1,temp}_{uvw}\\\\) is part of plane feature in \\\\(G^{i}_{p-a}\\\\).\\n'\n",
      "                        '\\n'\n",
      "                        'After process the \\\\(G^{i}_{b-a}\\\\), we use \\\\(GNN^{i}_{p-a}\\\\) to '\n",
      "                        'aggregate the message and update the plane latent vector in '\n",
      "                        '\\\\(G^{i}_{p-a}\\\\). Given a plane which is constructed by node u, v, w and '\n",
      "                        'bond \\\\(uv\\\\), \\\\(vw\\\\), its latent vector \\\\(F^{i}_{uvw}\\\\) is '\n",
      "                        'formalized by:\\n'\n",
      "                        '\\n'\n",
      "                        '\\\\[a^{i,p-a}_{uvw}=Agg^{(i)}_{p-a}(\\\\{F^{i-1}_{uvw},F^{i-1}_{vvh},F^{i-1}_{uvw}|u '\n",
      "                        '\\\\in N(v)\\\\cap v\\\\in N(w)\\\\cap v\\\\in N(h)\\\\cap u\\\\neq v\\\\neq w\\\\neq h\\\\}) '\n",
      "                        '\\\\tag{10}\\\\]\\n'\n",
      "                        '\\n'\n",
      "                        '\\\\[F^{i}_{uvw}=Comb^{(k)}_{p-a,n}(F^{i-1}_{uv},F^{i-1}_{uv},F^{i-1}_{vw}) '\n",
      "                        '\\\\tag{11}\\\\]\\n'\n",
      "                        '\\n'\n",
      "                        '\\\\[F^{i-1}_{uvw}=Comb^{(k)}_{b-a,e}(F^{i-1}_{uvw},F^{i-1}_{uv},F^{i-1}_{vw}) '\n",
      "                        '\\\\tag{12}\\\\]\\n'\n",
      "                        '\\n'\n",
      "                        'where \\\\(agg^{(i)}_{p-a}\\\\) is the aggregation function for aggregating '\n",
      "                        \"messages from a plane's neighborhood in \\\\(G^{i}_{p-a}\\\\), and \"\n",
      "                        '\\\\(Comb^{(k)}_{p-a,n}\\\\) is the update function for updating the plane '\n",
      "                        'latent vector in \\\\(G^{i}_{p-a}\\\\), and \\\\(Comb^{(k)}_{p-a,e}\\\\) is the '\n",
      "                        'update function for updating the dihedral angle latent vector in '\n",
      "                        '\\\\(G^{i}_{p-a}\\\\). \\\\(a^{i}_{uv}\\\\) is the information from the '\n",
      "                        'neighboring plane and the corresponding dihedral angle after be '\n",
      "                        'aggregated. Processing \\\\(G^{i}_{p-a}\\\\) with \\\\(GNN^{i}_{p-a}\\\\) makes '\n",
      "                        'feature of plane and dihedral angle can be updated by information from '\n",
      "                        'their neighbor, and the model will learn the 3D information of the '\n",
      "                        'molecular, and model the bond interaction.\\n'\n",
      "                        '\\n'\n",
      "                        'The representation vectors of the atoms at the final iteration are '\n",
      "                        'integrated to gain the molecular representation vector \\\\(F_{mol}\\\\) by '\n",
      "                        'the Readout function, which is formalized as:\\n'\n",
      "                        '\\n'\n",
      "                        '\\\\[F_{mol}=Readout(F^{n}_{u}|u\\\\in N(v)) \\\\tag{13}\\\\]\\n'\n",
      "                        '\\n'\n",
      "                        'where \\\\(F^{n}\\\\) is the last 3D-Mol layer output. The molecular latent '\n",
      "                        'vector \\\\(F_{mol}\\\\) is used to predict molecular properties. We extract '\n",
      "                        'the \\\\(F_{mol}\\\\) from all atom latent vector in the last layer.',\n",
      "                'value': '3D-Mol Layer'},\n",
      "               {'children': [{'children': [],\n",
      "                              'header': 'h4',\n",
      "                              'text': 'Our objective is to facilitate the learning of the '\n",
      "                                      'consistency and difference between the most stable '\n",
      "                                      'molecular conformation, denoted as \\\\(Mconf_{i}\\\\), and '\n",
      "                                      'another randomly selected conformation, denoted as '\n",
      "                                      '\\\\(Mconf_{j}\\\\). To accomplish this, we employ weighted '\n",
      "                                      'contrastive learning using a batch of molecular '\n",
      "                                      'representations, with the loss function defined as '\n",
      "                                      'follows:\\n'\n",
      "                                      '\\n'\n",
      "                                      '\\\\[L^{conf}_{i,j}=-log\\\\frac{exp(w^{conf}_{i,j}sim(F_{i},F^{mk}_{j})/\\\\tau)}{\\\\Sigma^ '\n",
      "                                      '{2N}_{k=1}\\\\{k\\\\neq '\n",
      "                                      'i\\\\}exp(w^{fp}_{i,k}sim(F_{i},F_{k})/\\\\tau)} \\\\tag{14}\\\\]\\n'\n",
      "                                      '\\n'\n",
      "                                      '\\\\[w^{conf}_{i,j}=1-\\\\lambda_{conf}*ConfSim(Mconf_{i},Mconf_{j}) '\n",
      "                                      '\\\\tag{15}\\\\]\\n'\n",
      "                                      '\\n'\n",
      "                                      '\\\\[w^{fp}_{i,k}=1-\\\\lambda_{fp}*FPSim(Mconf_{i},Mconf_{k}) '\n",
      "                                      '\\\\tag{16}\\\\]\\n'\n",
      "                                      '\\n'\n",
      "                                      'where \\\\(F_{i}\\\\) is the latent vector extracted from '\n",
      "                                      '\\\\(Mconf_{i}\\\\), and \\\\(sim(F_{i},F_{j})\\\\) is the '\n",
      "                                      'similarity between two latent vectors \\\\((F_{i}\\\\), '\n",
      "                                      '\\\\(F_{j})\\\\), and penalized by a weight coefficient '\n",
      "                                      '\\\\(w^{conf}_{i,j}\\\\). \\\\(w^{conf}_{i,j}\\\\) is computed by '\n",
      "                                      '\\\\(ConfSim(Mconf_{i},Mconf_{j})\\\\), the difference between '\n",
      "                                      '\\\\(Mconf_{i}\\\\) and \\\\(Mconf_{j}\\\\), which can be computed '\n",
      "                                      'by RDKit. \\\\(\\\\lambda_{conf}\\\\in[0,1]\\\\) is the '\n",
      "                                      'hyperparameter that determines the scale of penalty for the '\n",
      "                                      'difference between two conformation. Except using different '\n",
      "                                      'conformations as the positive pair, we also use node and '\n",
      "                                      'subgraph masking as the molecular data augmentation '\n",
      "                                      'strategy. We mask \\\\(Mconf_{i}\\\\) 15\\\\(\\\\%\\\\) nodes and '\n",
      "                                      'corresponding edges, and the mask latent vector '\n",
      "                                      'conformation is denoted as \\\\(F^{mk}_{j}\\\\). Following '\n",
      "                                      'iMolCLR[42], the similarity measurement between two latent '\n",
      "                                      'vectors \\\\(F_{i}\\\\), \\\\(F_{k}\\\\) from a negative molecule '\n",
      "                                      'pair (\\\\(Mconf_{i},Mconf_{j}\\\\)) is penalized by a weight '\n",
      "                                      'coefficient \\\\(w^{fp}_{i,k}\\\\), which computed by the '\n",
      "                                      'Fingerprint similarity between \\\\(Mconf_{i}\\\\) and '\n",
      "                                      '\\\\(Mconf_{k}\\\\). \\\\(FPSim(Mconf_{i},Mconf_{j})\\\\) evaluates '\n",
      "                                      'the fingerprint similarity of the given two molecules '\n",
      "                                      '\\\\(Mconf_{i},Mconf_{j}\\\\), and \\\\(\\\\lambda_{fp}\\\\in[0,1]\\\\) '\n",
      "                                      'is the hyperparameter that determines the scale of penalty '\n",
      "                                      'for faulty negatives.',\n",
      "                              'value': '3.4.1 Weighted contrastive learning task'},\n",
      "                             {'children': [],\n",
      "                              'header': 'h4',\n",
      "                              'text': '3D information have been shown to be important '\n",
      "                                      'features[22], so we employ geometry task as pretraining '\n",
      "                                      'method. For bond angle and dihedral angle prediction, we '\n",
      "                                      'sample adjacent atoms to better capture local structural '\n",
      "                                      'information. Since angular values are more sensitive to '\n",
      "                                      'errors in protein structures than distances, we use '\n",
      "                                      'discretized values for prediction.\\n'\n",
      "                                      '\\n'\n",
      "                                      '\\\\[L_{i,j}^{dist}=(f_{dist}(Fn_{n,i}^{mk},Fn_{n,j}^{mk})-dist_{i,j})^{2} '\n",
      "                                      '\\\\tag{17}\\\\]\\n'\n",
      "                                      '\\n'\n",
      "                                      '\\\\[L_{i,j}^{l}=(f_{l}(Fn_{n,i}^{mk},Fn_{n,j}^{mk})-l_{i,j})^{2} '\n",
      "                                      '\\\\tag{18}\\\\]\\n'\n",
      "                                      '\\n'\n",
      "                                      '\\\\[L_{i,j,k}^{\\\\theta}=CE(f_{\\\\theta}(Fn_{n,i}^{mk},Fn_{n,j}^{mk},Fn_{n,k}^{mk}), '\n",
      "                                      'bin(\\\\theta_{i,j,k})) \\\\tag{19}\\\\]\\n'\n",
      "                                      '\\n'\n",
      "                                      '\\\\[L_{i,j,k,p}^{\\\\phi}=CE(f_{\\\\phi}(Fn_{n,i}^{mk},Fn_{n,j}^{mk},Fn_{n,k}^{mk},Fn_{ '\n",
      "                                      'n,p}^{mk}),bin(\\\\phi_{i,j,k,p})) \\\\tag{20}\\\\]\\n'\n",
      "                                      '\\n'\n",
      "                                      'where \\\\(f_{\\\\phi}(.)\\\\), \\\\(f_{\\\\theta}(.)\\\\), '\n",
      "                                      '\\\\(f_{dist}(.)\\\\) and \\\\(f_{l}\\\\) are the MLPs for each '\n",
      "                                      'task, and \\\\(L_{i,j}^{dist}\\\\), \\\\(L_{i,j}^{l}\\\\), '\n",
      "                                      '\\\\(L_{i,j,k}^{\\\\theta}\\\\), \\\\(L_{i,j,k,p}^{\\\\phi}\\\\) and '\n",
      "                                      '\\\\(L_{i}^{FP}\\\\) are the loss functions for each task. '\n",
      "                                      '\\\\(CE(.)\\\\) is cross entropy loss, and \\\\(bin()\\\\) is used '\n",
      "                                      'to discretize the bond angle and dihedral angle. '\n",
      "                                      '\\\\(Fn_{n,i}^{mk}\\\\) is the latent vector of node i after '\n",
      "                                      'masking the corresponding sampled items in each task.\\n'\n",
      "                                      '\\n'\n",
      "                                      'In addition to the aforementioned pretraining tasks to '\n",
      "                                      'capture global molecular information, we leverage masked '\n",
      "                                      'molecular latent vectors for fingerprint (FP) prediction, '\n",
      "                                      'effectively incorporating latent representations to enrich '\n",
      "                                      'the predictive capability.\\n'\n",
      "                                      '\\n'\n",
      "                                      '\\\\[L_{i}^{FP}=BCE(f_{FP}(Fm^{mk}),FP_{i}) \\\\tag{21}\\\\]\\n'\n",
      "                                      '\\n'\n",
      "                                      'where \\\\(f_{FP}\\\\) is the MLPs for global geometric task, '\n",
      "                                      'and \\\\(L_{i}^{FP}\\\\) is the loss function. \\\\(BCE(.)\\\\) is '\n",
      "                                      'binary cross entropy loss. \\\\(Fm^{mk}\\\\) is the latent '\n",
      "                                      'vector of the masking molecule.',\n",
      "                              'value': '3.4.2 Geometry task'}],\n",
      "                'header': 'h3',\n",
      "                'text': 'To improve the performance of 3D-Mol encoder, we employ the constructive '\n",
      "                        'learning as pretraining method, using different conformations of the same '\n",
      "                        'topological structure as positive pair. We also combine our pretraining '\n",
      "                        'method with geometry task in [26] to pretrain 3D-Mol with a large amount '\n",
      "                        'of unlabeled data. The overview of our pretraining method is shown in '\n",
      "                        'figure 3, and the following are the details of our pretraining method.',\n",
      "                'value': 'Pretrain Strategy'}],\n",
      "  'header': 'h2',\n",
      "  'text': '',\n",
      "  'value': '3 Method'},\n",
      " {'children': [{'children': [{'children': [],\n",
      "                              'header': 'h4',\n",
      "                              'text': 'We use 20 million unlabeled molecules to pretrain 3D-Mol. '\n",
      "                                      'The unlabeled data is extracted from ZINC20 and PubChem, '\n",
      "                                      'both of which are publicly accessible databases containing '\n",
      "                                      'drug-like compounds. To ensure consistency with prior '\n",
      "                                      'research[23], we randomly selected 90\\\\(\\\\%\\\\) of these '\n",
      "                                      'molecules for training purposes, while the remaining '\n",
      "                                      '\\\\(\\\\%\\\\) was set aside for evaluation. The raw data '\n",
      "                                      'obtained from ZINC20 and PubChem was provided in the SMILES '\n",
      "                                      'format. In order to convert the SMILES representations into '\n",
      "                                      'molecular conformations, we employed RDKit and applied the '\n",
      "                                      'ETKDG method. For our model, we use Adam optimizer with a '\n",
      "                                      'learning rate of 1e-3. The batch size is set to 256 for '\n",
      "                                      'pretraining and 32 for finetuning. The hidden size of all '\n",
      "                                      'models is unspecified. The geometric embedding dimension K '\n",
      "                                      'is 64, and the number of angle domains is 8. The '\n",
      "                                      'hyperparameters \\\\(\\\\lambda_{conf}\\\\) and '\n",
      "                                      '\\\\(\\\\lambda_{fp}\\\\) are both set to 0.5.\\n'\n",
      "                                      '\\n'\n",
      "                                      '\\\\begin{table}\\n'\n",
      "                                      '\\\\begin{tabular}{c c c c} \\\\hline \\\\hline Dataset & '\n",
      "                                      '\\\\(\\\\#\\\\) Tasks & Task Type & \\\\(\\\\#\\\\) Molecules \\\\\\\\ '\n",
      "                                      '\\\\hline BACE & 1 & Classifcation & 1513 \\\\\\\\ Sider & 27 & '\n",
      "                                      'Classifcation & 1427 \\\\\\\\ Tox21 & 12 & Classifcation & 7831 '\n",
      "                                      '\\\\\\\\ ToxCast & 617 & Classifcation & 8597 \\\\\\\\ ESOL & 1 & '\n",
      "                                      'Regression & 1128 \\\\\\\\ FreeSolv & 1 & Regression & 643 \\\\\\\\ '\n",
      "                                      'Lipophilicity & 1 & Regression & 4200 \\\\\\\\ \\\\hline \\\\hline '\n",
      "                                      '\\\\end{tabular}\\n'\n",
      "                                      '\\\\end{table}\\n'\n",
      "                                      'Table 1: Statistics information of datasets',\n",
      "                              'value': '4.1.1 Pretraining stage'},\n",
      "                             {'children': [],\n",
      "                              'header': 'h4',\n",
      "                              'text': 'We use 7 molecular datasets obtained from MoleculeNet to '\n",
      "                                      'demonstrate the effectiveness of 3D-Mol. These datasets '\n",
      "                                      'encompass a range of biophysics datasets such as BACE, '\n",
      "                                      'physical chemistry datasets like ESOL, and physiology '\n",
      "                                      'datasets like Tox21. In the fine-tuning stage, we employed '\n",
      "                                      'nine molecular datasets obtained from MoleculeNet. These '\n",
      "                                      'datasets encompass a range of biophysics datasets such as '\n",
      "                                      'BACE, physical chemistry datasets like ESOL, and physiology '\n",
      "                                      'datasets like Tox21. Table 1 provides a summary of the '\n",
      "                                      'statistical information for these datasets, while the '\n",
      "                                      'remaining details are outlined below:\\n'\n",
      "                                      '\\n'\n",
      "                                      '\\\\(\\\\bullet\\\\) BACE. The BACE dataset provides both '\n",
      "                                      'quantitative (IC50) and qualitative (binary label) binding '\n",
      "                                      'results for a set of inhibitors targeting human '\n",
      "                                      '\\\\(\\\\beta\\\\)-secretase 1 (BACE-1).\\n'\n",
      "                                      '\\n'\n",
      "                                      '\\\\(\\\\bullet\\\\) Tox21. The Tox21 initiative aims to advance '\n",
      "                                      'toxicology practices in the 21st century and has created a '\n",
      "                                      'public database containing qualitative toxicity '\n",
      "                                      'measurements for 12 biological targets, including nuclear '\n",
      "                                      'receptors and stress response pathways.\\n'\n",
      "                                      '\\n'\n",
      "                                      '\\\\(\\\\bullet\\\\) Toxcast. ToxCast, an initiative related to '\n",
      "                                      'Tox21, offers a comprehensive collection of toxicology data '\n",
      "                                      'obtained through in vitro high-throughput screening. It '\n",
      "                                      'includes information from over 600 experiments and covers a '\n",
      "                                      'large library of compounds.\\n'\n",
      "                                      '\\n'\n",
      "                                      '\\\\(\\\\bullet\\\\) SIDER. The SIDER database is a compilation '\n",
      "                                      'of marketed drugs and their associated adverse drug '\n",
      "                                      'reactions (ADRs), categorized into 27 system organ '\n",
      "                                      'classes.\\n'\n",
      "                                      '\\n'\n",
      "                                      '\\\\(\\\\bullet\\\\) ESOL. The ESOL dataset is a smaller '\n",
      "                                      'collection of water solubility data, specifically providing '\n",
      "                                      'information on the log solubility in mols per liter for '\n",
      "                                      'common organic small molecules.\\n'\n",
      "                                      '\\n'\n",
      "                                      'FreeSolv. The FreeSolv database offers experimental and '\n",
      "                                      'calculated hydration-free energy values for small molecules '\n",
      "                                      'dissolved in water.\\n'\n",
      "                                      '\\n'\n",
      "                                      '\\\\(\\\\bullet\\\\) Lipo. Lipophilicity is a crucial '\n",
      "                                      'characteristic of drug molecules that affects their '\n",
      "                                      'membrane permeability and solubility. The Lipo dataset '\n",
      "                                      'contains experimental data on the octanol/water '\n",
      "                                      'distribution coefficient (logD at pH 7.4).\\n'\n",
      "                                      '\\n'\n",
      "                                      'Following the previous works[23], We partitioned the '\n",
      "                                      'datasets into train/validation/test sets in an 80/10/10 '\n",
      "                                      'ratio for downstream tasks, and We use scaffold splitting '\n",
      "                                      'and report the mean and standard deviation by the results '\n",
      "                                      'of 3 random seeds.',\n",
      "                              'value': '4.1.2 Finetuning stage'}],\n",
      "                'header': 'h3',\n",
      "                'text': '',\n",
      "                'value': 'Datasets and Setup'},\n",
      "               {'children': [],\n",
      "                'header': 'h3',\n",
      "                'text': 'Consistent with prior studies, we adopt the average ROC-AUC as the '\n",
      "                        'evaluation metric for the classification datasets (BACE, SIDER, Tox21 and '\n",
      "                        'ToxCast), which is a widely used metric for assessing the performance of '\n",
      "                        'binary classification tasks. For the regression datasets (ESOL, FreeSolv '\n",
      "                        'and Lipophilicity), we utilize the RMSE as the evaluation metric.',\n",
      "                'value': 'Metric'},\n",
      "               {'children': [],\n",
      "                'header': 'h3',\n",
      "                'text': 'a) **To validate the efficacy of our proposed method, we compare it with '\n",
      "                        'several baseline methods.** The baseline methods are as follows: '\n",
      "                        'N-Gram[12] generates a graph representation by constructing node '\n",
      "                        'embeddings\\n'\n",
      "                        '\\n'\n",
      "                        '\\\\begin{table}\\n'\n",
      "                        '\\\\begin{tabular}{|c|c|c|c|c|c|c|c|} \\\\hline \\\\multirow{2}{*}{model} & '\n",
      "                        '\\\\multicolumn{3}{c|}{Classification (ROC-AUC \\\\(\\\\%\\\\), higher is better '\n",
      "                        '\\\\(\\\\uparrow\\\\) )} & \\\\multicolumn{3}{c|}{Regression (RMSE, lower is '\n",
      "                        'better \\\\(\\\\downarrow\\\\))} \\\\\\\\ \\\\cline{2-7}  & BACE & SIDER & Tox21 & '\n",
      "                        'ToxCast & ESOL & FreeSolv & Lipophilicity \\\\\\\\ \\\\hline '\n",
      "                        'N-Gram\\\\({}_{\\\\text{RF}}\\\\) & \\\\(0.779_{0.015}\\\\) & \\\\(0.668_{0.007}\\\\) & '\n",
      "                        '\\\\(0.743_{0.004}\\\\) & \\\\(-\\\\) & \\\\(1.074_{0.107}\\\\) & \\\\(2.688_{0.085}\\\\) '\n",
      "                        '& \\\\(0.812_{0.028}\\\\) \\\\\\\\ N-Gram\\\\({}_{\\\\text{XGB}}\\\\) & '\n",
      "                        '\\\\(0.791_{0.013}\\\\) & \\\\(0.655_{0.007}\\\\) & \\\\(0.758_{0.009}\\\\) & \\\\(-\\\\) '\n",
      "                        '& \\\\(1.083_{0.107}\\\\) & \\\\(5.061_{0.744}\\\\) & \\\\(2.072_{0.030}\\\\) \\\\\\\\ '\n",
      "                        'PretrainGNN & \\\\(0.845_{0.007}\\\\) & \\\\(0.627_{0.008}\\\\) & '\n",
      "                        '\\\\(0.781_{0.006}\\\\) & \\\\(0.657_{0.006}\\\\) & \\\\(1.100_{0.006}\\\\) & '\n",
      "                        '\\\\(2.764_{0.002}\\\\) & \\\\(0.739_{0.003}\\\\) \\\\\\\\ '\n",
      "                        'GROVER\\\\({}_{\\\\text{base}}\\\\) & \\\\(0.826_{0.007}\\\\) & \\\\(0.648_{0.006}\\\\) '\n",
      "                        '& \\\\(0.743_{0.001}\\\\) & \\\\(0.654_{0.004}\\\\) & \\\\(0.983_{0.090}\\\\) & '\n",
      "                        '\\\\(2.176_{0.052}\\\\) & \\\\(0.817_{0.008}\\\\) \\\\\\\\ '\n",
      "                        'GROVER\\\\({}_{\\\\text{large}}\\\\) & \\\\(0.810_{0.014}\\\\) & '\n",
      "                        '\\\\(0.654_{0.001}\\\\) & \\\\(0.735_{0.001}\\\\) & \\\\(0.653_{0.005}\\\\) & '\n",
      "                        '\\\\(0.895_{0.017}\\\\) & \\\\(2.272_{0.051}\\\\) & \\\\(0.823_{0.010}\\\\) \\\\\\\\ '\n",
      "                        'MolCLR & \\\\(0.824_{0.009}\\\\) & \\\\(0.589_{0.014}\\\\) & \\\\(0.750_{0.002}\\\\) '\n",
      "                        '& \\\\(-\\\\) & \\\\(1.271_{0.040}\\\\) & \\\\(2.594_{0.249}\\\\) & '\n",
      "                        '\\\\(0.691_{0.004}\\\\) \\\\\\\\ \\\\hline\\n'\n",
      "                        '3DInfomax & \\\\(0.797_{0.015}\\\\) & \\\\(0.606_{0.008}\\\\) & '\n",
      "                        '\\\\(0.644_{0.011}\\\\) & \\\\(0.745_{0.007}\\\\) & \\\\(0.894_{0.028}\\\\) & '\n",
      "                        '\\\\(2.337_{0.107}\\\\) & \\\\(0.695_{0.012}\\\\) \\\\\\\\ GraphMVP & '\n",
      "                        '\\\\(0.812_{0.009}\\\\) & \\\\(0.639_{0.012}\\\\) & \\\\(0.759_{0.005}\\\\) & '\n",
      "                        '\\\\(0.631_{0.004}\\\\) & \\\\(1.029_{0.033}\\\\) & \\\\(-\\\\) & \\\\(0.681_{0.010}\\\\) '\n",
      "                        '\\\\\\\\ GEM & \\\\(0.856_{0.011}\\\\) & \\\\(\\\\mathbf{0.672}_{0.004}\\\\) & '\n",
      "                        '\\\\(0.781_{0.005}\\\\) & \\\\(0.692_{0.004}\\\\) & \\\\(0.798_{0.029}\\\\) & '\n",
      "                        '\\\\(1.877_{0.094}\\\\) & \\\\(0.660_{0.008}\\\\) \\\\\\\\ Uni-Mol & '\n",
      "                        '\\\\(0.857_{0.005}\\\\) & \\\\(0.659_{0.013}\\\\) & \\\\(\\\\mathbf{0.796}_{0.006}\\\\) '\n",
      "                        '& \\\\(0.696_{0.001}\\\\) & \\\\(0.788_{0.029}\\\\) & \\\\(1.620_{0.035}\\\\) & '\n",
      "                        '\\\\(0.603_{0.010}\\\\) \\\\\\\\ \\\\hline\\n'\n",
      "                        '3D-Mol\\\\({}_{\\\\text{wcl}}\\\\) & \\\\(\\\\mathbf{0.875}_{0.004}\\\\) & '\n",
      "                        '\\\\(0.656_{0.002}\\\\) & \\\\(0.786_{0.003}\\\\) & \\\\(\\\\mathbf{0.697}_{0.003}\\\\) '\n",
      "                        '& \\\\(\\\\mathbf{0.783}_{0.009}\\\\) & \\\\(\\\\mathbf{1.617}_{0.050}\\\\) & '\n",
      "                        '\\\\(\\\\mathbf{0.598}_{0.018}\\\\) \\\\\\\\ \\\\hline \\\\end{tabular}\\n'\n",
      "                        '\\\\end{table}\\n'\n",
      "                        'Table 2: Comparison of performance on the 7 molecular property prediction '\n",
      "                        'tasks and the methods below are pretraining method. We mark the best '\n",
      "                        'results in bold and underline the second best.\\n'\n",
      "                        'based on short walks. PretrainGNN[11] implements several types of '\n",
      "                        'self-supervised learning tasks. 3D Infomax[48] maximizes the mutual '\n",
      "                        'information between learned 3D summary vectors and the representations of '\n",
      "                        'a graph neural network. MolCLR[13] is a 2D-2D view contrastive learning '\n",
      "                        'model that involves atom masking, bond deletion, and subgraph removal. '\n",
      "                        'GraphMVP[46] introduces 2D-3D view contrastive learning approaches. '\n",
      "                        'GROVER[14] focused on node level and graph level representation and '\n",
      "                        'corresponding pretraining tasks for node level and graph level. GEM[23] '\n",
      "                        'employs predictive geometry self-supervised learning schemes that '\n",
      "                        'leverage 3D molecular information. Uni-Mol[47] enlarge the application '\n",
      "                        'scope and representation ability of molecular representation learning by '\n",
      "                        'using transformer. As result in Table 2, Our method gets the best result '\n",
      "                        'in 5 datasets and gets the second best result in 1 dataset. Furthermore, '\n",
      "                        'our method achieves overwhelming performance on BACE by a large margin. '\n",
      "                        'That shows that our method is better at extracting the molecular '\n",
      "                        'information. To do the ablation study, We compare the results of 3D-Mol '\n",
      "                        'and 3D-Mol without pretraining. It shows that the former achieve '\n",
      "                        'overwhelming performance and our pretraining method can improve the '\n",
      "                        '3D-Mol model performance.\\n'\n",
      "                        '\\n'\n",
      "                        '**b) To validate the efficacy of our proposed model 3D-Mol encoder, we '\n",
      "                        'compare it with several baseline molecular encoder without pretraining.** '\n",
      "                        'The baseline molecular encoders are as follows: DMPNN[17] employed a '\n",
      "                        'message passing scheme for molecular property prediction. AttentiveFP[16] '\n",
      "                        'is an attention-based GNN that incorporates graph-level information. '\n",
      "                        'MGCN[34] designed hierarchical graph neural network to directly extract '\n",
      "                        'features from the conformation and spatial information followed by the '\n",
      "                        'multilevel interactions. HMGNN[24] leverages global molecular '\n",
      "                        'representations through an attention mechanism. SGCN[25] applies '\n",
      "                        'different weights according to atomic distances during the GCN-based '\n",
      "                        'message passing process. DimeNet[22] proposes directional message passing '\n",
      "                        'to fully utilize directional information within molecules. GEM[23] '\n",
      "                        'employs message passing strategy to extract 3D molecular information. We '\n",
      "                        'present the experimental result to show the efficiency of our 3D-Mol '\n",
      "                        'model, as can be seen in Table 3. From Table 4, 3D-Mol encoder '\n",
      "                        'significantly outperforms all the baselines on both types of tasks and '\n",
      "                        'improves the performance over the best baselines with \\\\(2\\\\%\\\\) and '\n",
      "                        '\\\\(11\\\\%\\\\) for classification and regression tasks respectively, since '\n",
      "                        '3D-Mol incorporates geometrical parameters.\\n'\n",
      "                        '\\n'\n",
      "                        'c) **To validate the efficacy of our proposed pretraining task, we '\n",
      "                        'compare the performance of no pretraining 3DGNN, pretraining by '\n",
      "                        'geometrical tasks and pretraining by geometrical tasks and weighted '\n",
      "                        'contrastive loss, and the result shows in Table 4. The result shows that '\n",
      "                        'geometry tasks signifcantly improve the performance of 3DGNN. Compared '\n",
      "                        'with the pretraining method combined with weighted contrastive learning. '\n",
      "                        'In general, The combined pretraining method is better to improve the '\n",
      "                        '3DGNN performance.\\n'\n",
      "                        '\\n'\n",
      "                        '\\\\begin{table}\\n'\n",
      "                        '\\\\begin{tabular}{|c|c|c|c|c|c|c|c|} \\\\hline \\\\multirow{2}{*}{model} & '\n",
      "                        '\\\\multicolumn{3}{c|}{Classification (ROC-AUC \\\\(\\\\%\\\\), higher is better '\n",
      "                        '\\\\(\\\\uparrow\\\\) )} & \\\\multicolumn{3}{c|}{Regression (RMSE, lower is '\n",
      "                        'better \\\\(\\\\downarrow\\\\))} \\\\\\\\ \\\\cline{2-7}  & BACE & SIDER & Tox21 & '\n",
      "                        'ToxCast & ESOL & FreeSolv & Lipophilicity \\\\\\\\ \\\\hline '\n",
      "                        '\\\\(3\\\\mathrm{DGNN}\\\\) & **0.875\\\\({}_{0.004}\\\\)** & 0.656\\\\({}_{0.002}\\\\) '\n",
      "                        '& 0.786\\\\({}_{0.003}\\\\) & **0.697\\\\({}_{0.003}\\\\)** & '\n",
      "                        '**0.783\\\\({}_{0.009}\\\\)** & 1.617\\\\({}_{0.050}\\\\) & '\n",
      "                        '**0.598\\\\({}_{0.018}\\\\)** \\\\\\\\ \\\\(3\\\\mathrm{DGNN}_{\\\\mathrm{wo.pre}}\\\\) & '\n",
      "                        '0.832\\\\({}_{0.005}\\\\) & 0.624\\\\({}_{0.013}\\\\) & 0.780\\\\({}_{0.004}\\\\) & '\n",
      "                        '0.682\\\\({}_{0.007}\\\\) & 0.794\\\\({}_{0.027}\\\\) & 1.769\\\\({}_{0.039}\\\\) & '\n",
      "                        '0.674\\\\({}_{0.007}\\\\) \\\\\\\\ '\n",
      "                        '\\\\(3\\\\mathrm{DGNN}_{\\\\mathrm{wo.cl_{weighted}}}\\\\) & '\n",
      "                        '0.874\\\\({}_{0.006}\\\\) & **0.661\\\\({}_{0.005}\\\\)** & '\n",
      "                        '**0.790\\\\({}_{0.003}\\\\)** & 0.693\\\\({}_{0.005}\\\\) & 0.795\\\\({}_{0.014}\\\\) '\n",
      "                        '& **1.557\\\\({}_{0.003}\\\\)** & 0.607\\\\({}_{0.006}\\\\) \\\\\\\\ \\\\hline '\n",
      "                        '\\\\end{tabular}\\n'\n",
      "                        '\\\\end{table}\\n'\n",
      "                        'Table 4: Ablation study. We compare the performance of no pretraining '\n",
      "                        '3DGNN, pretraining by geometrical tasks and pretraining by geometrical '\n",
      "                        'tasks and weighted contrastive loss, and mark the best results in bold '\n",
      "                        'and underline the second best.\\n'\n",
      "                        '\\n'\n",
      "                        '\\\\begin{table}\\n'\n",
      "                        '\\\\begin{tabular}{|c|c|c|c|c|c|c|c|} \\\\hline \\\\multirow{2}{*}{model} & '\n",
      "                        '\\\\multicolumn{3}{c|}{Classification (ROC-AUC \\\\(\\\\%\\\\), higher is better '\n",
      "                        '\\\\(\\\\uparrow\\\\) )} & \\\\multicolumn{3}{c|}{Regression (RMSE, lower is '\n",
      "                        'better \\\\(\\\\downarrow\\\\))} \\\\\\\\ \\\\cline{2-7}  & BACE & SIDER & Tox21 & '\n",
      "                        'ToxCast & ESOL & FreeSolv & Lipophilicity \\\\\\\\ \\\\hline DMPNN & '\n",
      "                        '0.809\\\\({}_{0.006}\\\\) & 0.570\\\\({}_{0.007}\\\\) & 0.759\\\\({}_{0.007}\\\\) & '\n",
      "                        '0.655\\\\({}_{0.3}\\\\) & 1.050\\\\({}_{0.008}\\\\) & 2.082\\\\({}_{0.082}\\\\) & '\n",
      "                        '0.683\\\\({}_{0.016}\\\\) \\\\\\\\ AttentiveFP & 0.784\\\\({}_{0.000}\\\\) & '\n",
      "                        '0.606\\\\({}_{0.032}\\\\) & 0.761\\\\({}_{0.005}\\\\) & 0.637\\\\({}_{0.002}\\\\) & '\n",
      "                        '0.877\\\\({}_{0.029}\\\\) & 2.073\\\\({}_{0.183}\\\\) & 0.721\\\\({}_{0.001}\\\\) '\n",
      "                        '\\\\\\\\ MGCN & 0.734\\\\({}_{0.008}\\\\) & 0.587\\\\({}_{0.019}\\\\) & '\n",
      "                        '0.741\\\\({}_{0.006}\\\\) & \\\\(-\\\\) & \\\\(-\\\\) & \\\\(-\\\\) & \\\\(-\\\\) \\\\\\\\ '\n",
      "                        '\\\\hline SGCN & \\\\(-\\\\) & 0.559\\\\({}_{0.005}\\\\) & 0.766\\\\({}_{0.002}\\\\) & '\n",
      "                        '0.657\\\\({}_{0.003}\\\\) & 1.629\\\\({}_{0.001}\\\\) & 2.363\\\\({}_{0.050}\\\\) & '\n",
      "                        '1.021\\\\({}_{0.013}\\\\) \\\\\\\\ HMGNN & \\\\(-\\\\) & 0.615\\\\({}_{0.005}\\\\) & '\n",
      "                        '0.768\\\\({}_{0.002}\\\\) & 0.672\\\\({}_{0.001}\\\\) & 1.390\\\\({}_{0.073}\\\\) & '\n",
      "                        '2.123\\\\({}_{0.179}\\\\) & 2.116\\\\({}_{0.473}\\\\) \\\\\\\\ DimeNet & \\\\(-\\\\) & '\n",
      "                        '0.612\\\\({}_{0.004}\\\\) & 0.774\\\\({}_{0.006}\\\\) & 0.637\\\\({}_{0.004}\\\\) & '\n",
      "                        '0.878\\\\({}_{0.023}\\\\) & 2.094\\\\({}_{0.118}\\\\) & 0.727\\\\({}_{0.019}\\\\) '\n",
      "                        '\\\\\\\\ GEM & 0.828\\\\({}_{0.012}\\\\) & 0.606\\\\({}_{0.010}\\\\) & '\n",
      "                        '0.773\\\\({}_{0.007}\\\\) & 0.675\\\\({}_{0.005}\\\\) & 0.832\\\\({}_{0.010}\\\\) & '\n",
      "                        '1.857\\\\({}_{0.071}\\\\) & 0.666\\\\({}_{0.015}\\\\) \\\\\\\\ \\\\hline '\n",
      "                        '\\\\(3\\\\mathrm{D-Molw}_{\\\\mathrm{wo.pre}}\\\\) & **0.832\\\\({}_{0.005}\\\\)** & '\n",
      "                        '**0.624\\\\({}_{0.013}\\\\)** & **0.780\\\\({}_{0.004}\\\\)** & '\n",
      "                        '**0.682\\\\({}_{0.007}\\\\)** & **0.794\\\\({}_{0.027}\\\\)** & '\n",
      "                        '**1.769\\\\({}_{0.039}\\\\)** & **0.674\\\\({}_{0.007}\\\\)** \\\\\\\\ \\\\hline '\n",
      "                        '\\\\end{tabular}\\n'\n",
      "                        '\\\\end{table}\\n'\n",
      "                        'Table 3: Comparison of performance on the 9 molecular property prediction '\n",
      "                        'tasks, and the methods below are no pretraining. We mark the best results '\n",
      "                        'in bold and underline the second best.',\n",
      "                'value': 'Result'}],\n",
      "  'header': 'h2',\n",
      "  'text': 'In this section, we conduct experiments on 7 benchmark datasets in MoleculeNet to '\n",
      "          'demonstrate the effectiveness of 3D-Mol for molecular property prediction. Firstly we '\n",
      "          'use a large amount of unlabeled data and our pretraining method to pretrain the 3D-Mol '\n",
      "          'model, then we use the downstream task to finetune well-pretrained model and predict '\n",
      "          'the molecular property. We compared it with a variety of state-of-the-art methods. Also '\n",
      "          'we conduct several ablation studies to confirm the 3D-Mol model and our pretraining '\n",
      "          'method is useful.',\n",
      "  'value': '4 Experiment'},\n",
      " {'children': [],\n",
      "  'header': 'h2',\n",
      "  'text': 'In this paper, we innovatively propose a novel 3D molecular model framework, 3D-Mol, to '\n",
      "          'extract 3D molecular features. Furthermore, to effectively utilize a large number of '\n",
      "          'unlabeled molecules and molecular conformations for feature extraction, we have '\n",
      "          'designed a new self-supervised pretraining strategy. Our approach has been validated '\n",
      "          'through numerous experiments and compared with multiple competitive benchmarks, '\n",
      "          'demonstrating superior performance over other methods across various benchmarks.',\n",
      "  'value': '5 Conclusion'},\n",
      " {'children': [],\n",
      "  'header': 'h2',\n",
      "  'text': 'The research was supported by the PengCheng Laboratory and by PengCheng Laboratory '\n",
      "          'Cloud-Brain.',\n",
      "  'value': 'Acknowledgment'}]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(hierarchical_structure, width=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
