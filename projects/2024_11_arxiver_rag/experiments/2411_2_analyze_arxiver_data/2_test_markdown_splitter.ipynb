{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.config import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 7) Index(['id', 'title', 'abstract', 'authors', 'published_date', 'link',\n",
      "       'markdown'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(os.path.join(settings.data_dir, \"arxiver/data/train.parquet\"))\n",
    "## Sample 10k\n",
    "df = df.sample(10000)\n",
    "print(df.shape, df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Splitter\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "]\n",
    "splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Enhanced Low-Dimensional Sensing Mapless Navigation\n",
      "\n",
      "###### Abstract\n",
      "\n",
      "In this study, we present two distinct approaches of Deep Reinforcement Learning (Deep-RL) algorithms for a mobile robot. The research methodology primarily involves a comparative analysis between a Deep-RL strategy grounded in the foundational Deep Q-Network (DQN) algorithm, and the Double Deep Q-Network (DDQN) algorithm. The agents in these approaches leverage 24 measurements from laser range sampling, coupled with the agent's positional differentials and orientation relative to the target. This amalgamation of data influences the agents' determinations regarding navigation, ultimately dictating the robot's velocities. By embracing this parsimonious sensory framework as proposed, we successfully showcase the training of an agent for proficiently executing navigation tasks and adeptly circumventing obstacles. Notably, this accomplishment is attained without a dependency on intricate sensory inputs like those inherent to image-centric methodologies. The proposed methodology is evaluated in three different real environments, revealing that Double Deep structures significantly enhance the navigation capabilities of mobile robots compared to simple Q structures.\n",
      "\n",
      "## Supplementary Material\n",
      "\n",
      "The experimental demonstrations are available at [https://youtu.be/A29er5CGygw](https://youtu.be/A29er5CGygw). Released code, Docker image, and pre-trained models at [https://github.com/LindaMoraes/turtlebot-project](https://github.com/LindaMoraes/turtlebot-project).\n",
      "\n",
      "## I Introduction\n",
      "\n",
      "Reinforcement Learning (RL) provides a promising approach for a multitude of challenges in robotics.These approaches have showcased leading-edge effectiveness in addressing diverse challenges in robot learning scenarios, attributed to the progress made in the field of deep learning neural networks (Deep ANN). The portrayal of the agent through a Deep ANN has notably bolstered its competence in maneuvering intricate settings and accomplishing a spectrum of tasks. Nevertheless, this advancement has also ushered in novel complexities, particularly in the realm of learning from data sets with elevated dimensions. These intricacies arise due to the limitations set by ANN, including those linked to the learning gradient. To overcome this constraint, dedicated Deep-RL approaches such as Contrastive Learning have been utilized to mitigate the issue and streamline agent learning. Particularly noteworthy is the discovery that impressive results can be attained in navigation tasks by relying on basic sensory data. This fact has been validated in the context of ground-based mobile robots, aerial robots, underwater robots, and even hybrid robots. The principle of mapless navigation underpins various challenges in mobile robotics, leading to the successful application of numerous Deep-RL algorithms.\n",
      "\n",
      "Considering this perspective, this study aims to exhibit and assess the efficacy of two Deep-RL methodologies in tasks concerning the purpose-driven navigation of a ground-based mobile robot. A real-world evaluation of the DQN and DDQN algorithms is undertaken for comparative analysis. These strategies are rooted in the concept of straightforward sensing, wherein the design encompasses 26 state samples. This compilation comprises 24 readings from laser sensors, coupled with measurements of the mobile robot's distance and orientation relative to the target. Moreover, our emphasis extends to showcasing the performance distinction between Double Q architectures and conventional Q architectures. The architecture we propose for the learning process is illustrated in Figure 1.\n",
      "\n",
      "Overall, this paper brings forth the subsequent contributions:\n",
      "\n",
      "* The effectiveness of Double Q approaches in achieving mapless navigation for terrestrial mobile robots is demonstrated through real-world evaluations.\n",
      "* The integration of Double Q methodologies and a simplified sensing approach is demonstrated to be highly effective in tackling crucial obstacles within the domain of Deep-RL. This achievement encompasses pivotal challenges like the convergence intricacies of gradient descent and the persistent concern of catastrophic forgetting, providing a consistent and reliable means of mitigation.\n",
      "* A comprehensive framework is presented to facilitate future testing and exploration of Deep-RL approaches for mobile robots.\n",
      "\n",
      "This manuscript comprises seven distinct sections. Commencing with a concise introduction, the subsequent segment (Section II) delves into the studies conducted by fellow researchers in the domain, which have notably influenced and\n",
      "guided this present work. The subsequent portion (Section III) establishes the theoretical foundation for the algorithms that were implemented in the experimental phase. Subsequently, in Section IV, the suite of tools, software components, and environments harnessed for the study are elaborated upon. The instructional approach adopted to train the agent in achieving target objectives is elucidated in Section V, accompanied by a comprehensive explication of the network architecture and the intricacies of the reward function employed. Ultimately, the outcomes and findings attained through this study are expounded upon in Section VI. Lastly, the concluding section delves into the major achievements garnered and the potential applications of Deep-RL.\n",
      "\n",
      "## II Related Work\n",
      "\n",
      "Deep Reinforcement Learning has been applied previously to mapless navigation with mobile robots, [1, 2]. Mnih _et al._[3] computed a value function for future reward using an algorithm called deep Q-network (DQN) for the Atari games, [3, 4]. It is important to emphasize that the DQN restricts itself to discrete actions when applied to a problem such as robot control. In order to extend the DQN to continuous control, Lillicrap _et al._[5] proposes the deep deterministic policy gradients (DDPG) algorithm. This innovation cleared the path for using Deep-RL in mobile robot navigation.\n",
      "\n",
      "Tai _et al._[6] pioneered a mapless motion planning solution for a mobile robot, utilizing sensor-derived range data and target position as system input to generate continuous steering commands. Their approach initially employed discrete steering commands [7]. Their study showcased the potential of training an agent, through asynchronous Deep-RL techniques, to achieve a predetermined target using this mapless motion planner.\n",
      "\n",
      "Similarly inspired by Tai _et al._ in [6] and related works, the present paper centers on the creation of a mapless motion planning system based on low-dimensional range readings. Diverging from previous approaches, our study uses a deterministic approach based on Double Deep Reinforcement Learning for solving navigation-related problems for a mobile robot and including a dynamic target for the terrestrial mobile robot in environments with no asynchronous training. Overall, we demonstrate that low dimensional sensing data and simple Deep-RL approaches, such as the DDQN, can be used to excel at navigation-related tasks for terrestrial mobile robots.\n",
      "\n",
      "Through this, we show that typical Deep-RL issues, such as the convergence of the gradient descent and the forgetting problems, can be effectively mitigated.\n",
      "\n",
      "## III Theoretical Background\n",
      "\n",
      "### _Deep Reinforcement Learning_\n",
      "\n",
      "According to [8, 9], and [7], with the advancements in deep learning, these techniques started being applied to methods that were previously inefficient. One of these methods was reinforcement learning, which could only be used for problems with limited sample sizes and relied on a linear function approximator. The capability of deep learning techniques to handle large volumes of data and inputs with high dimensionality aligns seamlessly with reinforcement learning methods, resulting in what is now known as deep reinforcement learning (Deep-RL) methods.\n",
      "\n",
      "### _Deep-Q Network - DQN_\n",
      "\n",
      "Leading the recent breakthroughs in Deep-RL, we find the method known as Deep-Q Network (DQN) [3], which was developed by Mnih _et al._ The DQN method incorporates key principles of reinforcement learning, including the utilization of the _Bellman equation_, as described by:\n",
      "\n",
      "\\[Q^{*}(s,a)=\\mathbb{E}_{s^{\\prime}\\sim_{e}}[r+\\gamma\\underset{a^{\\prime}}{max} Q^{*}(s^{\\prime},a^{\\prime})|s,a] \\tag{1}\\]\n",
      "\n",
      "An optimal state-action pair is given by the Bellman equation. By utilizing a neural network with weights \\(\\theta\\) that generates a function that calculates the action-value function, it is found \\(Q(s,a,\\theta)\\approx Q^{*}(s,a)\\), ensuring convergence towards the optimal value. The training of this neural network implies minimizing the following equation:\n",
      "\n",
      "\\[\\mathcal{L}(\\theta)=[(y-Q(s,a,\\theta))^{2}]. \\tag{2}\\]\n",
      "\n",
      "With \\(L(\\theta)\\) named as loss function and \\(y=\\mathbb{E}_{s^{\\prime}\\sim_{e}}[r+\\gamma\\underset{a^{\\prime}}{max}Q(s^{ \\prime},a^{\\prime};\\theta_{t})|s,a]\\) being a target function derived from a network weights.\n",
      "\n",
      "The neural network inputs are sampled state-action pairs derived from an experience replay buffer. The experience replay buffer stores each transition obtained by the agent. The agent has a \\(\\epsilon\\)-greedy policy.\n",
      "\n",
      "### _Double Deep-Q Network - DDQN_\n",
      "\n",
      "Hasselt [10] proposed a solution known as Double Q-Learning. This algorithm calculates the next state value using a double estimator. Building upon the Double Q-Learning strategy in conjunction with DQN, Van Hasselt _et al._[4]. DDQN calculates the target function with the equation\n",
      "\n",
      "\\[y_{t}^{DoubleDQN}=r_{t+1}+\\gamma Q(S_{t+1},\\underset{a}{argmax}Q(s_{t+1},a; \\theta_{t}),\\theta_{t}^{-}), \\tag{3}\\]\n",
      "\n",
      "Fig. 1: The Turtlebot3 Burger operates amidst obstacles in a specific scenario (on the left), while the model architecture, encompassing inputs and outputs of our techniques, namely DQN and Double DQN, is depicted on the right.\n",
      "## IV Experimental Setup\n",
      "\n",
      "This research involved conducting laboratory experiments using real robots, the main tools and experimental setup will be discussed.\n",
      "\n",
      "### _PyTorch_\n",
      "\n",
      "The algorithms used in this work were written in Python and the library PyTorch.It is highly regarded for its user-friendly nature, simplicity, and integration of familiar Python concepts such as classes, structures, and conditional loops. The popularity of PyTorch has surged due to its performance and agility, aligning well with the demands of modern development. PyTorch's scalability is closely tied to its ease of use, efficiency, parallelism, and hardware acceleration. It has gained significant traction in commercial applications, with notable companies like Tesla, Facebook [11], Uber, and many others adopting it. In academia, PyTorch is already extensively used in research fields such as natural language processing, image processing, object recognition, and more [12, 8, 9].\n",
      "\n",
      "### _Ros_\n",
      "\n",
      "ROS is considered a meta operating system that offers several standard services commonly associated with operating systems [13]. ROS adopts a graph architecture to represent the running processes, referred to as nodes, within the system. Communication between two or more processes is achieved through messages, which are exchanged over topics. In ROS, message exchange between nodes and topics follows the publishing and subscribing paradigm. Publishing involves sending data to a topic, while subscribing entails reading and receiving the data from that topic. ROS is particularly advantageous in applications that necessitate real-time sensor readings for decision-making by machines [14, 15]. To adhere to good development practices, it is recommended to create a new node for each new feature within the system.\n",
      "\n",
      "### _Gazebo_\n",
      "\n",
      "Gazebo, an open-source 3D simulator [16], serves as a valuable tool for conducting simulated experiments and greatly aids in the development process when used alongside ROS. It boasts a vast and thriving community encompassing academia, scientific research, and industry, which is currently experiencing rapid growth. The utilization of Gazebo as a support tool is crucial during the initial stages of experimentation. It enables researchers to rapidly prototype and test ideas without the need for costly real-world implementations, which are often economically impractical in early development phases. The integration of Gazebo with ROS adds another layer of interest, as both tools are open source and benefit from highly active communities. One of the most significant advantages of this integration lies in the ability to simulate various environments. Gazebo incorporates real-world rules and concepts, thereby facilitating the simulation of practical applications and scenarios.\n",
      "\n",
      "### _Turtlebot_\n",
      "\n",
      "Within its product portfolio, Robotis offers the TurtleBot development kit, a line of educational robotics. This kit is renowned for its affordability in terms of hardware costs and utilization of open-source software, making it highly conducive for project implementation. The TurtleBot3 was used in the article has wheel encoders and a laser distance sensor. The adoption of Raspberry Pi3 further benefits from its widespread usage within the extensive community of TurtleBot users. The turtlebot is shown in Figure 2.\n",
      "\n",
      "### _Experimental Environments_\n",
      "\n",
      "After training the networks in simulation in Gazebo, real-world experiments were conducted using the TurtleBot in a physical environment. Overhead cameras captured images of the surroundings, which were subsequently processed using digital image processing algorithms. OpenCV, the most widely utilized open-source library for computer vision, was employed for image processing.\n",
      "\n",
      "The first real environment, as illustrated in Figure 2(a) does not have obstacles. Figure 2(b) showcases the second environment, which introduced a slightly higher level of difficulty compared to the initial scenario in which there are four obstacles. Lastly, Figure 2(c), exhibits the third and final scenario, characterized by obstacles that have more complex geometry.\n",
      "\n",
      "## V Methodology\n",
      "\n",
      "In this study, our objective is to train, test, and comprehensively compare the efficacy of the DQN and Double DQN\n",
      "\n",
      "Fig. 3: Real experiment scenarios.\n",
      "\n",
      "Fig. 2: Turtlebot3 Burger.\n",
      "algorithms when employed within the context of a Turtlebot3 platform. The algorithms will let the robot navigate and avoid obstacles. The linear velocity is constant and the angular velocity has five discrete values.\n",
      "\n",
      "### _Network Structure_\n",
      "\n",
      "Once the system's states and actions have been defined, a Q-Network was developed to construct both the DQN and Double DQN architectures. The network has 26 inputs that corresponds to readings of the laser sensor, previous angular and linear velocity, and position and orientation of the target. The network's output corresponds to a discrete value within the range of [0,4], representing the angular velocity. Specifically, the values 0, 1, 2, 3, and 4 correspond to -1.5 rad/s, -0.75 rad/s, 0 rad/s, 0.75 rad/s, and 1.5 rad/s, respectively. Figure 4 shows the network architecture.\n",
      "\n",
      "The actor-network has three fully-connected layers with 256 nodes in each layer, the input of the network is the state of the robot. The output of the network is the angular velocities. The linear velocity remains constant and predetermined at 0.15m/s, so there's no backward move on this configuration.\n",
      "\n",
      "### _Reward Function_\n",
      "\n",
      "The reward and penalty functions can be formulated based on empirical knowledge and developed iteratively during the problem-solving process.\n",
      "\n",
      "Regarding the reward system, the following three different conditions presented better results for the resolution of the problem:\n",
      "\n",
      "\\[r(s_{t},a_{t})=\\begin{cases}r_{arrive}\\text{ if }d_{t}<c_{d}\\\\ r_{collide}\\text{ if }min_{x}<c_{o}\\\\ r_{idle}\\text{ if }min_{x}>=c_{o}\\text{ and }d_{t}>=c_{d}\\end{cases} \\tag{4}\\]\n",
      "\n",
      "Only these three rewards were given, \\(r_{arrive}\\) for making the task correctly, \\(r_{collide}\\) in case of failure, and \\(r_{idle}\\) in case of idle. A task is considered successful when the distance to goal (\\(d_{t}\\)) is less than the margin \\(c_{d}\\), and the agent receives \\(200\\) of reward denoted as \\(r_{arrive}\\). This margin \\(c_{d}\\), in this experiment, is set as \\(0.25\\) meters. In the event of a collision against an obstacle or reaching the scenario boundaries, a negative reward \\(r_{collide}\\) of \\(-20\\) is given. A collision is determined by comparing the distance sensor readings to a threshold value \\(c_{o}\\) of \\(0.12\\). Additionally, if the agent maintains a distance \\(d_{t}<c_{d}\\) from the target and its laser findings - expressed by \\(min_{x}\\) - detect that the robot is keeping a distance upper or equal to \\(c_{d}\\) from the obstacles and walls in a time period of 500 steps, the episode ends. In this last case, a reward \\(r_{idle}\\) of \\(0\\) is given, and the episode is denominated idle since it didn't succeed nor collide. Simplifying the reward system into three conditions also helps focus on a more detailed examination of the Deep-RL approaches, their similarities, and differences rather than the intricacies of the scenario itself.\n",
      "\n",
      "## VI Results\n",
      "\n",
      "This section presents the results obtained from this research. An extensive amount of statistical data was collected for each scenario and model. The evaluation was done in a real workplace. A total of 24 test tasks were conducted, consisting of 4 iterations for each pre-trained model in each environment. Also, the trials were divided into four different fixed goals. The number of successful trials was recorded, along with the average navigation time with their standard deviations. The Figure 5, illustrates the learning in the training phase, showing metrics over 3000, 5000, and 5000 episodes respectively in each stage. Figure 6 provides the behavior of the robot during the evaluation. Furthermore, Table I presents the overall results gathered. Within Table I, we present the Episode Time (ET) and Success Rate (SR) corresponding to each test scenario.\n",
      "\n",
      "An essential observation can be derived from Figure 5, where the stable convergence of learning across the three evaluated scenarios is evident. However, the agent exhibited consistent stability throughout the learning process.\n",
      "\n",
      "The robustness of these contributions is substantiated by the statistical analysis showcased in Table I, affirming the commendable performance exhibited by both algorithms.\n",
      "\n",
      "The Double Q algorithm showcased an impressive performance, achieving nearly 100% precision across diverse scenarios, thus underscoring its exceptional suitability for terrestrial mobile robotics.\n",
      "\n",
      "Despite not being as sophisticated as contrastive learning algorithms or other recent Deep-RL approaches for continuous actions, the simplicity of the presented methodology exhibited the capability of reaching good performance levels.\n",
      "\n",
      "## VII Conclusions\n",
      "\n",
      "This study introduces two straightforward Deep-RL techniques tailored to enhance the navigation capabilities of terrestrial mobile robots using low-dimensional data. Our results highlight the remarkable capabilities of the Double Q algorithms, showcasing their robust performance, which\n",
      "\n",
      "\\begin{table}\n",
      "\\begin{tabular}{c c c c} \\hline \\hline\n",
      "**Env** & **Algorithm** & \\(\\boldsymbol{ET_{real}}\\) (s) & \\(\\boldsymbol{SR_{real}}\\) \\\\ \\hline\n",
      "1 & DQN & **12.59\\(\\pm\\)2.61** & **100\\%** \\\\\n",
      "1 & DDON & 14.20 \\(\\pm\\) 6.00 & 100\\% \\\\\n",
      "2 & DQN & 11.39 \\(\\pm\\) 4.24 & 50\\% \\\\\n",
      "2 & DDON & **20.11\\(\\pm\\)5.88** & **100\\%** \\\\\n",
      "3 & DQN & 20.12 \\(\\pm\\) 9.38 & 50\\% \\\\\n",
      "3 & DDON & **22.28\\(\\pm\\)6.69** & **100\\%** \\\\ \\hline \\hline \\end{tabular}\n",
      "\\end{table} TABLE I: Episode Time, Success Rate, Standard and mean deviation metrics over \\(24\\) navigation trials in three different real scenarios for DQN and DDQN approaches.\n",
      "\n",
      "Fig. 4: The Q-Network architecture applied in DQN and DDQN.\n",
      "holds its ground even when juxtaposed with intricate Deep-RL methods such as actor-critic or contrastive architectures. The validation of these algorithms was carried out through real-world testing, underscoring their practical viability. Additionally, future testing of Deep-RL techniques can be scaled up using the framework that is provided in this work.\n",
      "\n",
      "One noteworthy result unveiled by this paper is the coherent and steady learning trend that transcends various scenarios and temporal spans, with minimal signs of typical Deep-RL challenges like gradient convergence or memory loss. To provide additional weight to these conclusions, ongoing research endeavors are underway to validate this pattern across diverse categories of mobile robots and to delve into a spectrum of Deep-RL methodologies.\n",
      "\n",
      "## Acknowledgment\n",
      "\n",
      "We would like to thank the GARRA Research group \\(([https://www.ufsm.br/grupos/garra](https://www.ufsm.br/grupos/garra))\\) and the VersusAI team. This work was partly founded by the Technological University of Uruguay (UTEC) and Federal University of Santa Maria (UFSM).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Sample\n",
    "'''\n",
    "Abstract is usually defined as\n",
    "'###### Abstract'\n",
    "'''\n",
    "idx = 0\n",
    "# idx = 15\n",
    "sample = df.iloc[idx]['markdown']\n",
    "# print(len(sample), sample[:200])\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Enhanced Low-Dimensional Sensing Mapless Navigation'}, page_content=\"###### Abstract  \\nIn this study, we present two distinct approaches of Deep Reinforcement Learning (Deep-RL) algorithms for a mobile robot. The research methodology primarily involves a comparative analysis between a Deep-RL strategy grounded in the foundational Deep Q-Network (DQN) algorithm, and the Double Deep Q-Network (DDQN) algorithm. The agents in these approaches leverage 24 measurements from laser range sampling, coupled with the agent's positional differentials and orientation relative to the target. This amalgamation of data influences the agents' determinations regarding navigation, ultimately dictating the robot's velocities. By embracing this parsimonious sensory framework as proposed, we successfully showcase the training of an agent for proficiently executing navigation tasks and adeptly circumventing obstacles. Notably, this accomplishment is attained without a dependency on intricate sensory inputs like those inherent to image-centric methodologies. The proposed methodology is evaluated in three different real environments, revealing that Double Deep structures significantly enhance the navigation capabilities of mobile robots compared to simple Q structures.\"),\n",
       " Document(metadata={'Header 1': 'Enhanced Low-Dimensional Sensing Mapless Navigation', 'Header 2': 'Supplementary Material'}, page_content='The experimental demonstrations are available at [https://youtu.be/A29er5CGygw](https://youtu.be/A29er5CGygw). Released code, Docker image, and pre-trained models at [https://github.com/LindaMoraes/turtlebot-project](https://github.com/LindaMoraes/turtlebot-project).'),\n",
       " Document(metadata={'Header 1': 'Enhanced Low-Dimensional Sensing Mapless Navigation', 'Header 2': 'I Introduction'}, page_content=\"Reinforcement Learning (RL) provides a promising approach for a multitude of challenges in robotics.These approaches have showcased leading-edge effectiveness in addressing diverse challenges in robot learning scenarios, attributed to the progress made in the field of deep learning neural networks (Deep ANN). The portrayal of the agent through a Deep ANN has notably bolstered its competence in maneuvering intricate settings and accomplishing a spectrum of tasks. Nevertheless, this advancement has also ushered in novel complexities, particularly in the realm of learning from data sets with elevated dimensions. These intricacies arise due to the limitations set by ANN, including those linked to the learning gradient. To overcome this constraint, dedicated Deep-RL approaches such as Contrastive Learning have been utilized to mitigate the issue and streamline agent learning. Particularly noteworthy is the discovery that impressive results can be attained in navigation tasks by relying on basic sensory data. This fact has been validated in the context of ground-based mobile robots, aerial robots, underwater robots, and even hybrid robots. The principle of mapless navigation underpins various challenges in mobile robotics, leading to the successful application of numerous Deep-RL algorithms.  \\nConsidering this perspective, this study aims to exhibit and assess the efficacy of two Deep-RL methodologies in tasks concerning the purpose-driven navigation of a ground-based mobile robot. A real-world evaluation of the DQN and DDQN algorithms is undertaken for comparative analysis. These strategies are rooted in the concept of straightforward sensing, wherein the design encompasses 26 state samples. This compilation comprises 24 readings from laser sensors, coupled with measurements of the mobile robot's distance and orientation relative to the target. Moreover, our emphasis extends to showcasing the performance distinction between Double Q architectures and conventional Q architectures. The architecture we propose for the learning process is illustrated in Figure 1.  \\nOverall, this paper brings forth the subsequent contributions:  \\n* The effectiveness of Double Q approaches in achieving mapless navigation for terrestrial mobile robots is demonstrated through real-world evaluations.\\n* The integration of Double Q methodologies and a simplified sensing approach is demonstrated to be highly effective in tackling crucial obstacles within the domain of Deep-RL. This achievement encompasses pivotal challenges like the convergence intricacies of gradient descent and the persistent concern of catastrophic forgetting, providing a consistent and reliable means of mitigation.\\n* A comprehensive framework is presented to facilitate future testing and exploration of Deep-RL approaches for mobile robots.  \\nThis manuscript comprises seven distinct sections. Commencing with a concise introduction, the subsequent segment (Section II) delves into the studies conducted by fellow researchers in the domain, which have notably influenced and\\nguided this present work. The subsequent portion (Section III) establishes the theoretical foundation for the algorithms that were implemented in the experimental phase. Subsequently, in Section IV, the suite of tools, software components, and environments harnessed for the study are elaborated upon. The instructional approach adopted to train the agent in achieving target objectives is elucidated in Section V, accompanied by a comprehensive explication of the network architecture and the intricacies of the reward function employed. Ultimately, the outcomes and findings attained through this study are expounded upon in Section VI. Lastly, the concluding section delves into the major achievements garnered and the potential applications of Deep-RL.\"),\n",
       " Document(metadata={'Header 1': 'Enhanced Low-Dimensional Sensing Mapless Navigation', 'Header 2': 'II Related Work'}, page_content='Deep Reinforcement Learning has been applied previously to mapless navigation with mobile robots, [1, 2]. Mnih _et al._[3] computed a value function for future reward using an algorithm called deep Q-network (DQN) for the Atari games, [3, 4]. It is important to emphasize that the DQN restricts itself to discrete actions when applied to a problem such as robot control. In order to extend the DQN to continuous control, Lillicrap _et al._[5] proposes the deep deterministic policy gradients (DDPG) algorithm. This innovation cleared the path for using Deep-RL in mobile robot navigation.  \\nTai _et al._[6] pioneered a mapless motion planning solution for a mobile robot, utilizing sensor-derived range data and target position as system input to generate continuous steering commands. Their approach initially employed discrete steering commands [7]. Their study showcased the potential of training an agent, through asynchronous Deep-RL techniques, to achieve a predetermined target using this mapless motion planner.  \\nSimilarly inspired by Tai _et al._ in [6] and related works, the present paper centers on the creation of a mapless motion planning system based on low-dimensional range readings. Diverging from previous approaches, our study uses a deterministic approach based on Double Deep Reinforcement Learning for solving navigation-related problems for a mobile robot and including a dynamic target for the terrestrial mobile robot in environments with no asynchronous training. Overall, we demonstrate that low dimensional sensing data and simple Deep-RL approaches, such as the DDQN, can be used to excel at navigation-related tasks for terrestrial mobile robots.  \\nThrough this, we show that typical Deep-RL issues, such as the convergence of the gradient descent and the forgetting problems, can be effectively mitigated.'),\n",
       " Document(metadata={'Header 1': 'Enhanced Low-Dimensional Sensing Mapless Navigation', 'Header 2': 'III Theoretical Background'}, page_content='### _Deep Reinforcement Learning_  \\nAccording to [8, 9], and [7], with the advancements in deep learning, these techniques started being applied to methods that were previously inefficient. One of these methods was reinforcement learning, which could only be used for problems with limited sample sizes and relied on a linear function approximator. The capability of deep learning techniques to handle large volumes of data and inputs with high dimensionality aligns seamlessly with reinforcement learning methods, resulting in what is now known as deep reinforcement learning (Deep-RL) methods.  \\n### _Deep-Q Network - DQN_  \\nLeading the recent breakthroughs in Deep-RL, we find the method known as Deep-Q Network (DQN) [3], which was developed by Mnih _et al._ The DQN method incorporates key principles of reinforcement learning, including the utilization of the _Bellman equation_, as described by:  \\n\\\\[Q^{*}(s,a)=\\\\mathbb{E}_{s^{\\\\prime}\\\\sim_{e}}[r+\\\\gamma\\\\underset{a^{\\\\prime}}{max} Q^{*}(s^{\\\\prime},a^{\\\\prime})|s,a] \\\\tag{1}\\\\]  \\nAn optimal state-action pair is given by the Bellman equation. By utilizing a neural network with weights \\\\(\\\\theta\\\\) that generates a function that calculates the action-value function, it is found \\\\(Q(s,a,\\\\theta)\\\\approx Q^{*}(s,a)\\\\), ensuring convergence towards the optimal value. The training of this neural network implies minimizing the following equation:  \\n\\\\[\\\\mathcal{L}(\\\\theta)=[(y-Q(s,a,\\\\theta))^{2}]. \\\\tag{2}\\\\]  \\nWith \\\\(L(\\\\theta)\\\\) named as loss function and \\\\(y=\\\\mathbb{E}_{s^{\\\\prime}\\\\sim_{e}}[r+\\\\gamma\\\\underset{a^{\\\\prime}}{max}Q(s^{ \\\\prime},a^{\\\\prime};\\\\theta_{t})|s,a]\\\\) being a target function derived from a network weights.  \\nThe neural network inputs are sampled state-action pairs derived from an experience replay buffer. The experience replay buffer stores each transition obtained by the agent. The agent has a \\\\(\\\\epsilon\\\\)-greedy policy.  \\n### _Double Deep-Q Network - DDQN_  \\nHasselt [10] proposed a solution known as Double Q-Learning. This algorithm calculates the next state value using a double estimator. Building upon the Double Q-Learning strategy in conjunction with DQN, Van Hasselt _et al._[4]. DDQN calculates the target function with the equation  \\n\\\\[y_{t}^{DoubleDQN}=r_{t+1}+\\\\gamma Q(S_{t+1},\\\\underset{a}{argmax}Q(s_{t+1},a; \\\\theta_{t}),\\\\theta_{t}^{-}), \\\\tag{3}\\\\]  \\nFig. 1: The Turtlebot3 Burger operates amidst obstacles in a specific scenario (on the left), while the model architecture, encompassing inputs and outputs of our techniques, namely DQN and Double DQN, is depicted on the right.'),\n",
       " Document(metadata={'Header 1': 'Enhanced Low-Dimensional Sensing Mapless Navigation', 'Header 2': 'IV Experimental Setup'}, page_content=\"This research involved conducting laboratory experiments using real robots, the main tools and experimental setup will be discussed.  \\n### _PyTorch_  \\nThe algorithms used in this work were written in Python and the library PyTorch.It is highly regarded for its user-friendly nature, simplicity, and integration of familiar Python concepts such as classes, structures, and conditional loops. The popularity of PyTorch has surged due to its performance and agility, aligning well with the demands of modern development. PyTorch's scalability is closely tied to its ease of use, efficiency, parallelism, and hardware acceleration. It has gained significant traction in commercial applications, with notable companies like Tesla, Facebook [11], Uber, and many others adopting it. In academia, PyTorch is already extensively used in research fields such as natural language processing, image processing, object recognition, and more [12, 8, 9].  \\n### _Ros_  \\nROS is considered a meta operating system that offers several standard services commonly associated with operating systems [13]. ROS adopts a graph architecture to represent the running processes, referred to as nodes, within the system. Communication between two or more processes is achieved through messages, which are exchanged over topics. In ROS, message exchange between nodes and topics follows the publishing and subscribing paradigm. Publishing involves sending data to a topic, while subscribing entails reading and receiving the data from that topic. ROS is particularly advantageous in applications that necessitate real-time sensor readings for decision-making by machines [14, 15]. To adhere to good development practices, it is recommended to create a new node for each new feature within the system.  \\n### _Gazebo_  \\nGazebo, an open-source 3D simulator [16], serves as a valuable tool for conducting simulated experiments and greatly aids in the development process when used alongside ROS. It boasts a vast and thriving community encompassing academia, scientific research, and industry, which is currently experiencing rapid growth. The utilization of Gazebo as a support tool is crucial during the initial stages of experimentation. It enables researchers to rapidly prototype and test ideas without the need for costly real-world implementations, which are often economically impractical in early development phases. The integration of Gazebo with ROS adds another layer of interest, as both tools are open source and benefit from highly active communities. One of the most significant advantages of this integration lies in the ability to simulate various environments. Gazebo incorporates real-world rules and concepts, thereby facilitating the simulation of practical applications and scenarios.  \\n### _Turtlebot_  \\nWithin its product portfolio, Robotis offers the TurtleBot development kit, a line of educational robotics. This kit is renowned for its affordability in terms of hardware costs and utilization of open-source software, making it highly conducive for project implementation. The TurtleBot3 was used in the article has wheel encoders and a laser distance sensor. The adoption of Raspberry Pi3 further benefits from its widespread usage within the extensive community of TurtleBot users. The turtlebot is shown in Figure 2.  \\n### _Experimental Environments_  \\nAfter training the networks in simulation in Gazebo, real-world experiments were conducted using the TurtleBot in a physical environment. Overhead cameras captured images of the surroundings, which were subsequently processed using digital image processing algorithms. OpenCV, the most widely utilized open-source library for computer vision, was employed for image processing.  \\nThe first real environment, as illustrated in Figure 2(a) does not have obstacles. Figure 2(b) showcases the second environment, which introduced a slightly higher level of difficulty compared to the initial scenario in which there are four obstacles. Lastly, Figure 2(c), exhibits the third and final scenario, characterized by obstacles that have more complex geometry.\"),\n",
       " Document(metadata={'Header 1': 'Enhanced Low-Dimensional Sensing Mapless Navigation', 'Header 2': 'V Methodology'}, page_content=\"In this study, our objective is to train, test, and comprehensively compare the efficacy of the DQN and Double DQN  \\nFig. 3: Real experiment scenarios.  \\nFig. 2: Turtlebot3 Burger.\\nalgorithms when employed within the context of a Turtlebot3 platform. The algorithms will let the robot navigate and avoid obstacles. The linear velocity is constant and the angular velocity has five discrete values.  \\n### _Network Structure_  \\nOnce the system's states and actions have been defined, a Q-Network was developed to construct both the DQN and Double DQN architectures. The network has 26 inputs that corresponds to readings of the laser sensor, previous angular and linear velocity, and position and orientation of the target. The network's output corresponds to a discrete value within the range of [0,4], representing the angular velocity. Specifically, the values 0, 1, 2, 3, and 4 correspond to -1.5 rad/s, -0.75 rad/s, 0 rad/s, 0.75 rad/s, and 1.5 rad/s, respectively. Figure 4 shows the network architecture.  \\nThe actor-network has three fully-connected layers with 256 nodes in each layer, the input of the network is the state of the robot. The output of the network is the angular velocities. The linear velocity remains constant and predetermined at 0.15m/s, so there's no backward move on this configuration.  \\n### _Reward Function_  \\nThe reward and penalty functions can be formulated based on empirical knowledge and developed iteratively during the problem-solving process.  \\nRegarding the reward system, the following three different conditions presented better results for the resolution of the problem:  \\n\\\\[r(s_{t},a_{t})=\\\\begin{cases}r_{arrive}\\\\text{ if }d_{t}<c_{d}\\\\\\\\ r_{collide}\\\\text{ if }min_{x}<c_{o}\\\\\\\\ r_{idle}\\\\text{ if }min_{x}>=c_{o}\\\\text{ and }d_{t}>=c_{d}\\\\end{cases} \\\\tag{4}\\\\]  \\nOnly these three rewards were given, \\\\(r_{arrive}\\\\) for making the task correctly, \\\\(r_{collide}\\\\) in case of failure, and \\\\(r_{idle}\\\\) in case of idle. A task is considered successful when the distance to goal (\\\\(d_{t}\\\\)) is less than the margin \\\\(c_{d}\\\\), and the agent receives \\\\(200\\\\) of reward denoted as \\\\(r_{arrive}\\\\). This margin \\\\(c_{d}\\\\), in this experiment, is set as \\\\(0.25\\\\) meters. In the event of a collision against an obstacle or reaching the scenario boundaries, a negative reward \\\\(r_{collide}\\\\) of \\\\(-20\\\\) is given. A collision is determined by comparing the distance sensor readings to a threshold value \\\\(c_{o}\\\\) of \\\\(0.12\\\\). Additionally, if the agent maintains a distance \\\\(d_{t}<c_{d}\\\\) from the target and its laser findings - expressed by \\\\(min_{x}\\\\) - detect that the robot is keeping a distance upper or equal to \\\\(c_{d}\\\\) from the obstacles and walls in a time period of 500 steps, the episode ends. In this last case, a reward \\\\(r_{idle}\\\\) of \\\\(0\\\\) is given, and the episode is denominated idle since it didn't succeed nor collide. Simplifying the reward system into three conditions also helps focus on a more detailed examination of the Deep-RL approaches, their similarities, and differences rather than the intricacies of the scenario itself.\"),\n",
       " Document(metadata={'Header 1': 'Enhanced Low-Dimensional Sensing Mapless Navigation', 'Header 2': 'VI Results'}, page_content='This section presents the results obtained from this research. An extensive amount of statistical data was collected for each scenario and model. The evaluation was done in a real workplace. A total of 24 test tasks were conducted, consisting of 4 iterations for each pre-trained model in each environment. Also, the trials were divided into four different fixed goals. The number of successful trials was recorded, along with the average navigation time with their standard deviations. The Figure 5, illustrates the learning in the training phase, showing metrics over 3000, 5000, and 5000 episodes respectively in each stage. Figure 6 provides the behavior of the robot during the evaluation. Furthermore, Table I presents the overall results gathered. Within Table I, we present the Episode Time (ET) and Success Rate (SR) corresponding to each test scenario.  \\nAn essential observation can be derived from Figure 5, where the stable convergence of learning across the three evaluated scenarios is evident. However, the agent exhibited consistent stability throughout the learning process.  \\nThe robustness of these contributions is substantiated by the statistical analysis showcased in Table I, affirming the commendable performance exhibited by both algorithms.  \\nThe Double Q algorithm showcased an impressive performance, achieving nearly 100% precision across diverse scenarios, thus underscoring its exceptional suitability for terrestrial mobile robotics.  \\nDespite not being as sophisticated as contrastive learning algorithms or other recent Deep-RL approaches for continuous actions, the simplicity of the presented methodology exhibited the capability of reaching good performance levels.'),\n",
       " Document(metadata={'Header 1': 'Enhanced Low-Dimensional Sensing Mapless Navigation', 'Header 2': 'VII Conclusions'}, page_content='This study introduces two straightforward Deep-RL techniques tailored to enhance the navigation capabilities of terrestrial mobile robots using low-dimensional data. Our results highlight the remarkable capabilities of the Double Q algorithms, showcasing their robust performance, which  \\n\\\\begin{table}\\n\\\\begin{tabular}{c c c c} \\\\hline \\\\hline\\n**Env** & **Algorithm** & \\\\(\\\\boldsymbol{ET_{real}}\\\\) (s) & \\\\(\\\\boldsymbol{SR_{real}}\\\\) \\\\\\\\ \\\\hline\\n1 & DQN & **12.59\\\\(\\\\pm\\\\)2.61** & **100\\\\%** \\\\\\\\\\n1 & DDON & 14.20 \\\\(\\\\pm\\\\) 6.00 & 100\\\\% \\\\\\\\\\n2 & DQN & 11.39 \\\\(\\\\pm\\\\) 4.24 & 50\\\\% \\\\\\\\\\n2 & DDON & **20.11\\\\(\\\\pm\\\\)5.88** & **100\\\\%** \\\\\\\\\\n3 & DQN & 20.12 \\\\(\\\\pm\\\\) 9.38 & 50\\\\% \\\\\\\\\\n3 & DDON & **22.28\\\\(\\\\pm\\\\)6.69** & **100\\\\%** \\\\\\\\ \\\\hline \\\\hline \\\\end{tabular}\\n\\\\end{table} TABLE I: Episode Time, Success Rate, Standard and mean deviation metrics over \\\\(24\\\\) navigation trials in three different real scenarios for DQN and DDQN approaches.  \\nFig. 4: The Q-Network architecture applied in DQN and DDQN.\\nholds its ground even when juxtaposed with intricate Deep-RL methods such as actor-critic or contrastive architectures. The validation of these algorithms was carried out through real-world testing, underscoring their practical viability. Additionally, future testing of Deep-RL techniques can be scaled up using the framework that is provided in this work.  \\nOne noteworthy result unveiled by this paper is the coherent and steady learning trend that transcends various scenarios and temporal spans, with minimal signs of typical Deep-RL challenges like gradient convergence or memory loss. To provide additional weight to these conclusions, ongoing research endeavors are underway to validate this pattern across diverse categories of mobile robots and to delve into a spectrum of Deep-RL methodologies.'),\n",
       " Document(metadata={'Header 1': 'Enhanced Low-Dimensional Sensing Mapless Navigation', 'Header 2': 'Acknowledgment'}, page_content='We would like to thank the GARRA Research group \\\\(([https://www.ufsm.br/grupos/garra](https://www.ufsm.br/grupos/garra))\\\\) and the VersusAI team. This work was partly founded by the Technological University of Uruguay (UTEC) and Federal University of Santa Maria (UFSM).')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_splits = splitter.split_text(sample)\n",
    "print(len(sample_splits))\n",
    "sample_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_markdown_hierarchy(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.strip().split('\\n')\n",
    "    # Define a pattern to match headers (from # to ######)\n",
    "    header_pattern = re.compile(r'^(#{1,6})\\s*(.*)$')\n",
    "\n",
    "    # Initialize the root of the hierarchy\n",
    "    root = {'children': []}\n",
    "    # Stack to keep track of the current hierarchy levels\n",
    "    stack = [{'level': 0, 'node': root}]\n",
    "    # Accumulate text for the current node\n",
    "    current_text = []\n",
    "\n",
    "    for line in lines:\n",
    "        header_match = header_pattern.match(line)\n",
    "        if header_match:\n",
    "            # If we have accumulated text, add it to the current node\n",
    "            if current_text:\n",
    "                # Join accumulated text and add to the last node's 'text'\n",
    "                stack[-1]['node'].setdefault('text', '')\n",
    "                if stack[-1]['node']['text']:\n",
    "                    stack[-1]['node']['text'] += '\\n'\n",
    "                stack[-1]['node']['text'] += '\\n'.join(current_text).strip()\n",
    "                current_text = []\n",
    "            # Extract header level and text\n",
    "            header_marks, header_text = header_match.groups()\n",
    "            level = len(header_marks)\n",
    "            # Pop the stack to find the correct parent level\n",
    "            while stack and stack[-1]['level'] >= level:\n",
    "                stack.pop()\n",
    "            # Create a new node for the header\n",
    "            node = {\n",
    "                'header': f'h{level}',\n",
    "                'value': header_text.strip(),\n",
    "                'children': []\n",
    "            }\n",
    "            # Add the new node to its parent's 'children'\n",
    "            stack[-1]['node']['children'].append(node) ## parent\n",
    "            # Push the new node onto the stack\n",
    "            stack.append({'level': level, 'node': node}) ## for accumulating potential children\n",
    "        else:\n",
    "            # Accumulate non-header lines\n",
    "            current_text.append(line)\n",
    "    # After processing all lines, add any remaining text\n",
    "    if current_text:\n",
    "        stack[-1]['node'].setdefault('text', '')\n",
    "        if stack[-1]['node']['text']:\n",
    "            stack[-1]['node']['text'] += '\\n'\n",
    "        stack[-1]['node']['text'] += '\\n'.join(current_text).strip()\n",
    "    # Return the hierarchy starting from the root's children\n",
    "    return root['children']\n",
    "hierarchical_structure = parse_markdown_hierarchy(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'children': [{'children': [],\n",
      "                'header': 'h6',\n",
      "                'text': 'In this study, we present two distinct approaches of Deep Reinforcement '\n",
      "                        'Learning (Deep-RL) algorithms for a mobile robot. The research '\n",
      "                        'methodology primarily involves a comparative analysis between a Deep-RL '\n",
      "                        'strategy grounded in the foundational Deep Q-Network (DQN) algorithm, and '\n",
      "                        'the Double Deep Q-Network (DDQN) algorithm. The agents in these '\n",
      "                        'approaches leverage 24 measurements from laser range sampling, coupled '\n",
      "                        \"with the agent's positional differentials and orientation relative to the \"\n",
      "                        \"target. This amalgamation of data influences the agents' determinations \"\n",
      "                        \"regarding navigation, ultimately dictating the robot's velocities. By \"\n",
      "                        'embracing this parsimonious sensory framework as proposed, we '\n",
      "                        'successfully showcase the training of an agent for proficiently executing '\n",
      "                        'navigation tasks and adeptly circumventing obstacles. Notably, this '\n",
      "                        'accomplishment is attained without a dependency on intricate sensory '\n",
      "                        'inputs like those inherent to image-centric methodologies. The proposed '\n",
      "                        'methodology is evaluated in three different real environments, revealing '\n",
      "                        'that Double Deep structures significantly enhance the navigation '\n",
      "                        'capabilities of mobile robots compared to simple Q structures.',\n",
      "                'value': 'Abstract'},\n",
      "               {'children': [],\n",
      "                'header': 'h2',\n",
      "                'text': 'The experimental demonstrations are available at '\n",
      "                        '[https://youtu.be/A29er5CGygw](https://youtu.be/A29er5CGygw). Released '\n",
      "                        'code, Docker image, and pre-trained models at '\n",
      "                        '[https://github.com/LindaMoraes/turtlebot-project](https://github.com/LindaMoraes/turtlebot-project).',\n",
      "                'value': 'Supplementary Material'},\n",
      "               {'children': [],\n",
      "                'header': 'h2',\n",
      "                'text': 'Reinforcement Learning (RL) provides a promising approach for a multitude '\n",
      "                        'of challenges in robotics.These approaches have showcased leading-edge '\n",
      "                        'effectiveness in addressing diverse challenges in robot learning '\n",
      "                        'scenarios, attributed to the progress made in the field of deep learning '\n",
      "                        'neural networks (Deep ANN). The portrayal of the agent through a Deep ANN '\n",
      "                        'has notably bolstered its competence in maneuvering intricate settings '\n",
      "                        'and accomplishing a spectrum of tasks. Nevertheless, this advancement has '\n",
      "                        'also ushered in novel complexities, particularly in the realm of learning '\n",
      "                        'from data sets with elevated dimensions. These intricacies arise due to '\n",
      "                        'the limitations set by ANN, including those linked to the learning '\n",
      "                        'gradient. To overcome this constraint, dedicated Deep-RL approaches such '\n",
      "                        'as Contrastive Learning have been utilized to mitigate the issue and '\n",
      "                        'streamline agent learning. Particularly noteworthy is the discovery that '\n",
      "                        'impressive results can be attained in navigation tasks by relying on '\n",
      "                        'basic sensory data. This fact has been validated in the context of '\n",
      "                        'ground-based mobile robots, aerial robots, underwater robots, and even '\n",
      "                        'hybrid robots. The principle of mapless navigation underpins various '\n",
      "                        'challenges in mobile robotics, leading to the successful application of '\n",
      "                        'numerous Deep-RL algorithms.\\n'\n",
      "                        '\\n'\n",
      "                        'Considering this perspective, this study aims to exhibit and assess the '\n",
      "                        'efficacy of two Deep-RL methodologies in tasks concerning the '\n",
      "                        'purpose-driven navigation of a ground-based mobile robot. A real-world '\n",
      "                        'evaluation of the DQN and DDQN algorithms is undertaken for comparative '\n",
      "                        'analysis. These strategies are rooted in the concept of straightforward '\n",
      "                        'sensing, wherein the design encompasses 26 state samples. This '\n",
      "                        'compilation comprises 24 readings from laser sensors, coupled with '\n",
      "                        \"measurements of the mobile robot's distance and orientation relative to \"\n",
      "                        'the target. Moreover, our emphasis extends to showcasing the performance '\n",
      "                        'distinction between Double Q architectures and conventional Q '\n",
      "                        'architectures. The architecture we propose for the learning process is '\n",
      "                        'illustrated in Figure 1.\\n'\n",
      "                        '\\n'\n",
      "                        'Overall, this paper brings forth the subsequent contributions:\\n'\n",
      "                        '\\n'\n",
      "                        '* The effectiveness of Double Q approaches in achieving mapless '\n",
      "                        'navigation for terrestrial mobile robots is demonstrated through '\n",
      "                        'real-world evaluations.\\n'\n",
      "                        '* The integration of Double Q methodologies and a simplified sensing '\n",
      "                        'approach is demonstrated to be highly effective in tackling crucial '\n",
      "                        'obstacles within the domain of Deep-RL. This achievement encompasses '\n",
      "                        'pivotal challenges like the convergence intricacies of gradient descent '\n",
      "                        'and the persistent concern of catastrophic forgetting, providing a '\n",
      "                        'consistent and reliable means of mitigation.\\n'\n",
      "                        '* A comprehensive framework is presented to facilitate future testing and '\n",
      "                        'exploration of Deep-RL approaches for mobile robots.\\n'\n",
      "                        '\\n'\n",
      "                        'This manuscript comprises seven distinct sections. Commencing with a '\n",
      "                        'concise introduction, the subsequent segment (Section II) delves into the '\n",
      "                        'studies conducted by fellow researchers in the domain, which have notably '\n",
      "                        'influenced and\\n'\n",
      "                        'guided this present work. The subsequent portion (Section III) '\n",
      "                        'establishes the theoretical foundation for the algorithms that were '\n",
      "                        'implemented in the experimental phase. Subsequently, in Section IV, the '\n",
      "                        'suite of tools, software components, and environments harnessed for the '\n",
      "                        'study are elaborated upon. The instructional approach adopted to train '\n",
      "                        'the agent in achieving target objectives is elucidated in Section V, '\n",
      "                        'accompanied by a comprehensive explication of the network architecture '\n",
      "                        'and the intricacies of the reward function employed. Ultimately, the '\n",
      "                        'outcomes and findings attained through this study are expounded upon in '\n",
      "                        'Section VI. Lastly, the concluding section delves into the major '\n",
      "                        'achievements garnered and the potential applications of Deep-RL.',\n",
      "                'value': 'I Introduction'},\n",
      "               {'children': [],\n",
      "                'header': 'h2',\n",
      "                'text': 'Deep Reinforcement Learning has been applied previously to mapless '\n",
      "                        'navigation with mobile robots, [1, 2]. Mnih _et al._[3] computed a value '\n",
      "                        'function for future reward using an algorithm called deep Q-network (DQN) '\n",
      "                        'for the Atari games, [3, 4]. It is important to emphasize that the DQN '\n",
      "                        'restricts itself to discrete actions when applied to a problem such as '\n",
      "                        'robot control. In order to extend the DQN to continuous control, '\n",
      "                        'Lillicrap _et al._[5] proposes the deep deterministic policy gradients '\n",
      "                        '(DDPG) algorithm. This innovation cleared the path for using Deep-RL in '\n",
      "                        'mobile robot navigation.\\n'\n",
      "                        '\\n'\n",
      "                        'Tai _et al._[6] pioneered a mapless motion planning solution for a mobile '\n",
      "                        'robot, utilizing sensor-derived range data and target position as system '\n",
      "                        'input to generate continuous steering commands. Their approach initially '\n",
      "                        'employed discrete steering commands [7]. Their study showcased the '\n",
      "                        'potential of training an agent, through asynchronous Deep-RL techniques, '\n",
      "                        'to achieve a predetermined target using this mapless motion planner.\\n'\n",
      "                        '\\n'\n",
      "                        'Similarly inspired by Tai _et al._ in [6] and related works, the present '\n",
      "                        'paper centers on the creation of a mapless motion planning system based '\n",
      "                        'on low-dimensional range readings. Diverging from previous approaches, '\n",
      "                        'our study uses a deterministic approach based on Double Deep '\n",
      "                        'Reinforcement Learning for solving navigation-related problems for a '\n",
      "                        'mobile robot and including a dynamic target for the terrestrial mobile '\n",
      "                        'robot in environments with no asynchronous training. Overall, we '\n",
      "                        'demonstrate that low dimensional sensing data and simple Deep-RL '\n",
      "                        'approaches, such as the DDQN, can be used to excel at navigation-related '\n",
      "                        'tasks for terrestrial mobile robots.\\n'\n",
      "                        '\\n'\n",
      "                        'Through this, we show that typical Deep-RL issues, such as the '\n",
      "                        'convergence of the gradient descent and the forgetting problems, can be '\n",
      "                        'effectively mitigated.',\n",
      "                'value': 'II Related Work'},\n",
      "               {'children': [{'children': [],\n",
      "                              'header': 'h3',\n",
      "                              'text': 'According to [8, 9], and [7], with the advancements in deep '\n",
      "                                      'learning, these techniques started being applied to methods '\n",
      "                                      'that were previously inefficient. One of these methods was '\n",
      "                                      'reinforcement learning, which could only be used for '\n",
      "                                      'problems with limited sample sizes and relied on a linear '\n",
      "                                      'function approximator. The capability of deep learning '\n",
      "                                      'techniques to handle large volumes of data and inputs with '\n",
      "                                      'high dimensionality aligns seamlessly with reinforcement '\n",
      "                                      'learning methods, resulting in what is now known as deep '\n",
      "                                      'reinforcement learning (Deep-RL) methods.',\n",
      "                              'value': '_Deep Reinforcement Learning_'},\n",
      "                             {'children': [],\n",
      "                              'header': 'h3',\n",
      "                              'text': 'Leading the recent breakthroughs in Deep-RL, we find the '\n",
      "                                      'method known as Deep-Q Network (DQN) [3], which was '\n",
      "                                      'developed by Mnih _et al._ The DQN method incorporates key '\n",
      "                                      'principles of reinforcement learning, including the '\n",
      "                                      'utilization of the _Bellman equation_, as described by:\\n'\n",
      "                                      '\\n'\n",
      "                                      '\\\\[Q^{*}(s,a)=\\\\mathbb{E}_{s^{\\\\prime}\\\\sim_{e}}[r+\\\\gamma\\\\underset{a^{\\\\prime}}{max} '\n",
      "                                      'Q^{*}(s^{\\\\prime},a^{\\\\prime})|s,a] \\\\tag{1}\\\\]\\n'\n",
      "                                      '\\n'\n",
      "                                      'An optimal state-action pair is given by the Bellman '\n",
      "                                      'equation. By utilizing a neural network with weights '\n",
      "                                      '\\\\(\\\\theta\\\\) that generates a function that calculates the '\n",
      "                                      'action-value function, it is found '\n",
      "                                      '\\\\(Q(s,a,\\\\theta)\\\\approx Q^{*}(s,a)\\\\), ensuring '\n",
      "                                      'convergence towards the optimal value. The training of this '\n",
      "                                      'neural network implies minimizing the following equation:\\n'\n",
      "                                      '\\n'\n",
      "                                      '\\\\[\\\\mathcal{L}(\\\\theta)=[(y-Q(s,a,\\\\theta))^{2}]. '\n",
      "                                      '\\\\tag{2}\\\\]\\n'\n",
      "                                      '\\n'\n",
      "                                      'With \\\\(L(\\\\theta)\\\\) named as loss function and '\n",
      "                                      '\\\\(y=\\\\mathbb{E}_{s^{\\\\prime}\\\\sim_{e}}[r+\\\\gamma\\\\underset{a^{\\\\prime}}{max}Q(s^{ '\n",
      "                                      '\\\\prime},a^{\\\\prime};\\\\theta_{t})|s,a]\\\\) being a target '\n",
      "                                      'function derived from a network weights.\\n'\n",
      "                                      '\\n'\n",
      "                                      'The neural network inputs are sampled state-action pairs '\n",
      "                                      'derived from an experience replay buffer. The experience '\n",
      "                                      'replay buffer stores each transition obtained by the agent. '\n",
      "                                      'The agent has a \\\\(\\\\epsilon\\\\)-greedy policy.',\n",
      "                              'value': '_Deep-Q Network - DQN_'},\n",
      "                             {'children': [],\n",
      "                              'header': 'h3',\n",
      "                              'text': 'Hasselt [10] proposed a solution known as Double '\n",
      "                                      'Q-Learning. This algorithm calculates the next state value '\n",
      "                                      'using a double estimator. Building upon the Double '\n",
      "                                      'Q-Learning strategy in conjunction with DQN, Van Hasselt '\n",
      "                                      '_et al._[4]. DDQN calculates the target function with the '\n",
      "                                      'equation\\n'\n",
      "                                      '\\n'\n",
      "                                      '\\\\[y_{t}^{DoubleDQN}=r_{t+1}+\\\\gamma '\n",
      "                                      'Q(S_{t+1},\\\\underset{a}{argmax}Q(s_{t+1},a; '\n",
      "                                      '\\\\theta_{t}),\\\\theta_{t}^{-}), \\\\tag{3}\\\\]\\n'\n",
      "                                      '\\n'\n",
      "                                      'Fig. 1: The Turtlebot3 Burger operates amidst obstacles in '\n",
      "                                      'a specific scenario (on the left), while the model '\n",
      "                                      'architecture, encompassing inputs and outputs of our '\n",
      "                                      'techniques, namely DQN and Double DQN, is depicted on the '\n",
      "                                      'right.',\n",
      "                              'value': '_Double Deep-Q Network - DDQN_'}],\n",
      "                'header': 'h2',\n",
      "                'text': '',\n",
      "                'value': 'III Theoretical Background'},\n",
      "               {'children': [{'children': [],\n",
      "                              'header': 'h3',\n",
      "                              'text': 'The algorithms used in this work were written in Python and '\n",
      "                                      'the library PyTorch.It is highly regarded for its '\n",
      "                                      'user-friendly nature, simplicity, and integration of '\n",
      "                                      'familiar Python concepts such as classes, structures, and '\n",
      "                                      'conditional loops. The popularity of PyTorch has surged due '\n",
      "                                      'to its performance and agility, aligning well with the '\n",
      "                                      \"demands of modern development. PyTorch's scalability is \"\n",
      "                                      'closely tied to its ease of use, efficiency, parallelism, '\n",
      "                                      'and hardware acceleration. It has gained significant '\n",
      "                                      'traction in commercial applications, with notable companies '\n",
      "                                      'like Tesla, Facebook [11], Uber, and many others adopting '\n",
      "                                      'it. In academia, PyTorch is already extensively used in '\n",
      "                                      'research fields such as natural language processing, image '\n",
      "                                      'processing, object recognition, and more [12, 8, 9].',\n",
      "                              'value': '_PyTorch_'},\n",
      "                             {'children': [],\n",
      "                              'header': 'h3',\n",
      "                              'text': 'ROS is considered a meta operating system that offers '\n",
      "                                      'several standard services commonly associated with '\n",
      "                                      'operating systems [13]. ROS adopts a graph architecture to '\n",
      "                                      'represent the running processes, referred to as nodes, '\n",
      "                                      'within the system. Communication between two or more '\n",
      "                                      'processes is achieved through messages, which are exchanged '\n",
      "                                      'over topics. In ROS, message exchange between nodes and '\n",
      "                                      'topics follows the publishing and subscribing paradigm. '\n",
      "                                      'Publishing involves sending data to a topic, while '\n",
      "                                      'subscribing entails reading and receiving the data from '\n",
      "                                      'that topic. ROS is particularly advantageous in '\n",
      "                                      'applications that necessitate real-time sensor readings for '\n",
      "                                      'decision-making by machines [14, 15]. To adhere to good '\n",
      "                                      'development practices, it is recommended to create a new '\n",
      "                                      'node for each new feature within the system.',\n",
      "                              'value': '_Ros_'},\n",
      "                             {'children': [],\n",
      "                              'header': 'h3',\n",
      "                              'text': 'Gazebo, an open-source 3D simulator [16], serves as a '\n",
      "                                      'valuable tool for conducting simulated experiments and '\n",
      "                                      'greatly aids in the development process when used alongside '\n",
      "                                      'ROS. It boasts a vast and thriving community encompassing '\n",
      "                                      'academia, scientific research, and industry, which is '\n",
      "                                      'currently experiencing rapid growth. The utilization of '\n",
      "                                      'Gazebo as a support tool is crucial during the initial '\n",
      "                                      'stages of experimentation. It enables researchers to '\n",
      "                                      'rapidly prototype and test ideas without the need for '\n",
      "                                      'costly real-world implementations, which are often '\n",
      "                                      'economically impractical in early development phases. The '\n",
      "                                      'integration of Gazebo with ROS adds another layer of '\n",
      "                                      'interest, as both tools are open source and benefit from '\n",
      "                                      'highly active communities. One of the most significant '\n",
      "                                      'advantages of this integration lies in the ability to '\n",
      "                                      'simulate various environments. Gazebo incorporates '\n",
      "                                      'real-world rules and concepts, thereby facilitating the '\n",
      "                                      'simulation of practical applications and scenarios.',\n",
      "                              'value': '_Gazebo_'},\n",
      "                             {'children': [],\n",
      "                              'header': 'h3',\n",
      "                              'text': 'Within its product portfolio, Robotis offers the TurtleBot '\n",
      "                                      'development kit, a line of educational robotics. This kit '\n",
      "                                      'is renowned for its affordability in terms of hardware '\n",
      "                                      'costs and utilization of open-source software, making it '\n",
      "                                      'highly conducive for project implementation. The TurtleBot3 '\n",
      "                                      'was used in the article has wheel encoders and a laser '\n",
      "                                      'distance sensor. The adoption of Raspberry Pi3 further '\n",
      "                                      'benefits from its widespread usage within the extensive '\n",
      "                                      'community of TurtleBot users. The turtlebot is shown in '\n",
      "                                      'Figure 2.',\n",
      "                              'value': '_Turtlebot_'},\n",
      "                             {'children': [],\n",
      "                              'header': 'h3',\n",
      "                              'text': 'After training the networks in simulation in Gazebo, '\n",
      "                                      'real-world experiments were conducted using the TurtleBot '\n",
      "                                      'in a physical environment. Overhead cameras captured images '\n",
      "                                      'of the surroundings, which were subsequently processed '\n",
      "                                      'using digital image processing algorithms. OpenCV, the most '\n",
      "                                      'widely utilized open-source library for computer vision, '\n",
      "                                      'was employed for image processing.\\n'\n",
      "                                      '\\n'\n",
      "                                      'The first real environment, as illustrated in Figure 2(a) '\n",
      "                                      'does not have obstacles. Figure 2(b) showcases the second '\n",
      "                                      'environment, which introduced a slightly higher level of '\n",
      "                                      'difficulty compared to the initial scenario in which there '\n",
      "                                      'are four obstacles. Lastly, Figure 2(c), exhibits the third '\n",
      "                                      'and final scenario, characterized by obstacles that have '\n",
      "                                      'more complex geometry.',\n",
      "                              'value': '_Experimental Environments_'}],\n",
      "                'header': 'h2',\n",
      "                'text': 'This research involved conducting laboratory experiments using real '\n",
      "                        'robots, the main tools and experimental setup will be discussed.',\n",
      "                'value': 'IV Experimental Setup'},\n",
      "               {'children': [{'children': [],\n",
      "                              'header': 'h3',\n",
      "                              'text': \"Once the system's states and actions have been defined, a \"\n",
      "                                      'Q-Network was developed to construct both the DQN and '\n",
      "                                      'Double DQN architectures. The network has 26 inputs that '\n",
      "                                      'corresponds to readings of the laser sensor, previous '\n",
      "                                      'angular and linear velocity, and position and orientation '\n",
      "                                      \"of the target. The network's output corresponds to a \"\n",
      "                                      'discrete value within the range of [0,4], representing the '\n",
      "                                      'angular velocity. Specifically, the values 0, 1, 2, 3, and '\n",
      "                                      '4 correspond to -1.5 rad/s, -0.75 rad/s, 0 rad/s, 0.75 '\n",
      "                                      'rad/s, and 1.5 rad/s, respectively. Figure 4 shows the '\n",
      "                                      'network architecture.\\n'\n",
      "                                      '\\n'\n",
      "                                      'The actor-network has three fully-connected layers with 256 '\n",
      "                                      'nodes in each layer, the input of the network is the state '\n",
      "                                      'of the robot. The output of the network is the angular '\n",
      "                                      'velocities. The linear velocity remains constant and '\n",
      "                                      \"predetermined at 0.15m/s, so there's no backward move on \"\n",
      "                                      'this configuration.',\n",
      "                              'value': '_Network Structure_'},\n",
      "                             {'children': [],\n",
      "                              'header': 'h3',\n",
      "                              'text': 'The reward and penalty functions can be formulated based on '\n",
      "                                      'empirical knowledge and developed iteratively during the '\n",
      "                                      'problem-solving process.\\n'\n",
      "                                      '\\n'\n",
      "                                      'Regarding the reward system, the following three different '\n",
      "                                      'conditions presented better results for the resolution of '\n",
      "                                      'the problem:\\n'\n",
      "                                      '\\n'\n",
      "                                      '\\\\[r(s_{t},a_{t})=\\\\begin{cases}r_{arrive}\\\\text{ if '\n",
      "                                      '}d_{t}<c_{d}\\\\\\\\ r_{collide}\\\\text{ if }min_{x}<c_{o}\\\\\\\\ '\n",
      "                                      'r_{idle}\\\\text{ if }min_{x}>=c_{o}\\\\text{ and '\n",
      "                                      '}d_{t}>=c_{d}\\\\end{cases} \\\\tag{4}\\\\]\\n'\n",
      "                                      '\\n'\n",
      "                                      'Only these three rewards were given, \\\\(r_{arrive}\\\\) for '\n",
      "                                      'making the task correctly, \\\\(r_{collide}\\\\) in case of '\n",
      "                                      'failure, and \\\\(r_{idle}\\\\) in case of idle. A task is '\n",
      "                                      'considered successful when the distance to goal '\n",
      "                                      '(\\\\(d_{t}\\\\)) is less than the margin \\\\(c_{d}\\\\), and the '\n",
      "                                      'agent receives \\\\(200\\\\) of reward denoted as '\n",
      "                                      '\\\\(r_{arrive}\\\\). This margin \\\\(c_{d}\\\\), in this '\n",
      "                                      'experiment, is set as \\\\(0.25\\\\) meters. In the event of a '\n",
      "                                      'collision against an obstacle or reaching the scenario '\n",
      "                                      'boundaries, a negative reward \\\\(r_{collide}\\\\) of '\n",
      "                                      '\\\\(-20\\\\) is given. A collision is determined by comparing '\n",
      "                                      'the distance sensor readings to a threshold value '\n",
      "                                      '\\\\(c_{o}\\\\) of \\\\(0.12\\\\). Additionally, if the agent '\n",
      "                                      'maintains a distance \\\\(d_{t}<c_{d}\\\\) from the target and '\n",
      "                                      'its laser findings - expressed by \\\\(min_{x}\\\\) - detect '\n",
      "                                      'that the robot is keeping a distance upper or equal to '\n",
      "                                      '\\\\(c_{d}\\\\) from the obstacles and walls in a time period '\n",
      "                                      'of 500 steps, the episode ends. In this last case, a reward '\n",
      "                                      '\\\\(r_{idle}\\\\) of \\\\(0\\\\) is given, and the episode is '\n",
      "                                      \"denominated idle since it didn't succeed nor collide. \"\n",
      "                                      'Simplifying the reward system into three conditions also '\n",
      "                                      'helps focus on a more detailed examination of the Deep-RL '\n",
      "                                      'approaches, their similarities, and differences rather than '\n",
      "                                      'the intricacies of the scenario itself.',\n",
      "                              'value': '_Reward Function_'}],\n",
      "                'header': 'h2',\n",
      "                'text': 'In this study, our objective is to train, test, and comprehensively '\n",
      "                        'compare the efficacy of the DQN and Double DQN\\n'\n",
      "                        '\\n'\n",
      "                        'Fig. 3: Real experiment scenarios.\\n'\n",
      "                        '\\n'\n",
      "                        'Fig. 2: Turtlebot3 Burger.\\n'\n",
      "                        'algorithms when employed within the context of a Turtlebot3 platform. The '\n",
      "                        'algorithms will let the robot navigate and avoid obstacles. The linear '\n",
      "                        'velocity is constant and the angular velocity has five discrete values.',\n",
      "                'value': 'V Methodology'},\n",
      "               {'children': [],\n",
      "                'header': 'h2',\n",
      "                'text': 'This section presents the results obtained from this research. An '\n",
      "                        'extensive amount of statistical data was collected for each scenario and '\n",
      "                        'model. The evaluation was done in a real workplace. A total of 24 test '\n",
      "                        'tasks were conducted, consisting of 4 iterations for each pre-trained '\n",
      "                        'model in each environment. Also, the trials were divided into four '\n",
      "                        'different fixed goals. The number of successful trials was recorded, '\n",
      "                        'along with the average navigation time with their standard deviations. '\n",
      "                        'The Figure 5, illustrates the learning in the training phase, showing '\n",
      "                        'metrics over 3000, 5000, and 5000 episodes respectively in each stage. '\n",
      "                        'Figure 6 provides the behavior of the robot during the evaluation. '\n",
      "                        'Furthermore, Table I presents the overall results gathered. Within Table '\n",
      "                        'I, we present the Episode Time (ET) and Success Rate (SR) corresponding '\n",
      "                        'to each test scenario.\\n'\n",
      "                        '\\n'\n",
      "                        'An essential observation can be derived from Figure 5, where the stable '\n",
      "                        'convergence of learning across the three evaluated scenarios is evident. '\n",
      "                        'However, the agent exhibited consistent stability throughout the learning '\n",
      "                        'process.\\n'\n",
      "                        '\\n'\n",
      "                        'The robustness of these contributions is substantiated by the statistical '\n",
      "                        'analysis showcased in Table I, affirming the commendable performance '\n",
      "                        'exhibited by both algorithms.\\n'\n",
      "                        '\\n'\n",
      "                        'The Double Q algorithm showcased an impressive performance, achieving '\n",
      "                        'nearly 100% precision across diverse scenarios, thus underscoring its '\n",
      "                        'exceptional suitability for terrestrial mobile robotics.\\n'\n",
      "                        '\\n'\n",
      "                        'Despite not being as sophisticated as contrastive learning algorithms or '\n",
      "                        'other recent Deep-RL approaches for continuous actions, the simplicity of '\n",
      "                        'the presented methodology exhibited the capability of reaching good '\n",
      "                        'performance levels.',\n",
      "                'value': 'VI Results'},\n",
      "               {'children': [],\n",
      "                'header': 'h2',\n",
      "                'text': 'This study introduces two straightforward Deep-RL techniques tailored to '\n",
      "                        'enhance the navigation capabilities of terrestrial mobile robots using '\n",
      "                        'low-dimensional data. Our results highlight the remarkable capabilities '\n",
      "                        'of the Double Q algorithms, showcasing their robust performance, which\\n'\n",
      "                        '\\n'\n",
      "                        '\\\\begin{table}\\n'\n",
      "                        '\\\\begin{tabular}{c c c c} \\\\hline \\\\hline\\n'\n",
      "                        '**Env** & **Algorithm** & \\\\(\\\\boldsymbol{ET_{real}}\\\\) (s) & '\n",
      "                        '\\\\(\\\\boldsymbol{SR_{real}}\\\\) \\\\\\\\ \\\\hline\\n'\n",
      "                        '1 & DQN & **12.59\\\\(\\\\pm\\\\)2.61** & **100\\\\%** \\\\\\\\\\n'\n",
      "                        '1 & DDON & 14.20 \\\\(\\\\pm\\\\) 6.00 & 100\\\\% \\\\\\\\\\n'\n",
      "                        '2 & DQN & 11.39 \\\\(\\\\pm\\\\) 4.24 & 50\\\\% \\\\\\\\\\n'\n",
      "                        '2 & DDON & **20.11\\\\(\\\\pm\\\\)5.88** & **100\\\\%** \\\\\\\\\\n'\n",
      "                        '3 & DQN & 20.12 \\\\(\\\\pm\\\\) 9.38 & 50\\\\% \\\\\\\\\\n'\n",
      "                        '3 & DDON & **22.28\\\\(\\\\pm\\\\)6.69** & **100\\\\%** \\\\\\\\ \\\\hline \\\\hline '\n",
      "                        '\\\\end{tabular}\\n'\n",
      "                        '\\\\end{table} TABLE I: Episode Time, Success Rate, Standard and mean '\n",
      "                        'deviation metrics over \\\\(24\\\\) navigation trials in three different real '\n",
      "                        'scenarios for DQN and DDQN approaches.\\n'\n",
      "                        '\\n'\n",
      "                        'Fig. 4: The Q-Network architecture applied in DQN and DDQN.\\n'\n",
      "                        'holds its ground even when juxtaposed with intricate Deep-RL methods such '\n",
      "                        'as actor-critic or contrastive architectures. The validation of these '\n",
      "                        'algorithms was carried out through real-world testing, underscoring their '\n",
      "                        'practical viability. Additionally, future testing of Deep-RL techniques '\n",
      "                        'can be scaled up using the framework that is provided in this work.\\n'\n",
      "                        '\\n'\n",
      "                        'One noteworthy result unveiled by this paper is the coherent and steady '\n",
      "                        'learning trend that transcends various scenarios and temporal spans, with '\n",
      "                        'minimal signs of typical Deep-RL challenges like gradient convergence or '\n",
      "                        'memory loss. To provide additional weight to these conclusions, ongoing '\n",
      "                        'research endeavors are underway to validate this pattern across diverse '\n",
      "                        'categories of mobile robots and to delve into a spectrum of Deep-RL '\n",
      "                        'methodologies.',\n",
      "                'value': 'VII Conclusions'},\n",
      "               {'children': [],\n",
      "                'header': 'h2',\n",
      "                'text': 'We would like to thank the GARRA Research group '\n",
      "                        '\\\\(([https://www.ufsm.br/grupos/garra](https://www.ufsm.br/grupos/garra))\\\\) '\n",
      "                        'and the VersusAI team. This work was partly founded by the Technological '\n",
      "                        'University of Uruguay (UTEC) and Federal University of Santa Maria '\n",
      "                        '(UFSM).',\n",
      "                'value': 'Acknowledgment'}],\n",
      "  'header': 'h1',\n",
      "  'text': '',\n",
      "  'value': 'Enhanced Low-Dimensional Sensing Mapless Navigation'}]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(hierarchical_structure, width=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
