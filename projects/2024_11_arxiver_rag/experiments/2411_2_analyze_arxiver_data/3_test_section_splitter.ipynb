{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ArxivPaperSection(BaseModel):\n",
    "    header: Literal[\"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]\n",
    "    title:str = Field(..., description=\"Section title\")\n",
    "    text: str = Field(\"\", description = \"Section contents\")\n",
    "    children: List[\"ArxivPaperSection\"] = Field(list(), description=\"child sections\")\n",
    "\n",
    "class ArxivPaperMetadata(BaseModel):\n",
    "    authors: str\n",
    "    published_date: datetime\n",
    "    link: str\n",
    "\n",
    "class ArxivPaper(BaseModel):\n",
    "    id: str\n",
    "    title: str\n",
    "    abstract: str\n",
    "    sections: List[ArxivPaperSection]\n",
    "    metadata: ArxivPaperMetadata = Field(None, description=\"paper metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 7) Index(['id', 'title', 'abstract', 'authors', 'published_date', 'link',\n",
      "       'markdown'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## Load Sample\n",
    "df = pd.read_parquet(\"sample.parquet\")\n",
    "print(df.shape, df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                       2310.03187\n",
       "title             Synthesis of Data-Driven Nonlinear State Obser...\n",
       "abstract          This paper focuses on the model-free synthesis...\n",
       "authors                                                 Wentao Tang\n",
       "published_date                                 2023-10-04T22:19:53Z\n",
       "link                              http://arxiv.org/abs/2310.03187v1\n",
       "markdown          # Synthesis of Data-Driven Nonlinear State Obs...\n",
       "Name: 62774, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## md2py based parser\n",
    "from markdownify import markdownify as md\n",
    "from src.custom_md2py import md2py, TreeOfContents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toc_text(toc: TreeOfContents) -> str:\n",
    "    html_texts = []\n",
    "    for desc in toc.descendants:\n",
    "        html_texts.append(repr(desc))\n",
    "    md_text = md(\"\".join(html_texts))\n",
    "    return md_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "class BaseArxivPaperSectionSplitter:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def split(self, toc: TreeOfContents) -> List[ArxivPaperSection]:\n",
    "        raise NotImplementedError(\"split must be implemented\")\n",
    "    \n",
    "    @classmethod\n",
    "    def is_type(cls):\n",
    "        return False\n",
    "    \n",
    "class Case1Filter(BaseArxivPaperSectionSplitter):\n",
    "    \"\"\"Case1: everything under h1 title\"\"\"\n",
    "    @classmethod\n",
    "    def is_type(cls, toc: TreeOfContents):\n",
    "        if len(toc.branches)==1 and toc.branches[0].name==\"h1\":\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def split(self, toc: TreeOfContents) -> List[ArxivPaperSection]:\n",
    "        h1_child = list(toc.branches)[0]\n",
    "        sections = []\n",
    "        for child in h1_child.branches:\n",
    "            # p - title, h6 - abstract\n",
    "            if not child.name==\"h2\":\n",
    "                continue\n",
    "            # print(\"CHILD:\", child.name, child.string, str(child))\n",
    "            child_text = get_toc_text(child)\n",
    "            # print(\"title:\",child.name, repr(child), child.string)\n",
    "            section = ArxivPaperSection(\n",
    "                header = child.name,\n",
    "                title = child.getText(),\n",
    "                text = child.getDescendantsMarkdown()\n",
    "            )\n",
    "            sections.append(section)\n",
    "        return sections\n",
    "    \n",
    "class Case2Filter(BaseArxivPaperSectionSplitter):\n",
    "    \"\"\"Case2: title & contents in same level\"\"\"\n",
    "    @classmethod\n",
    "    def is_type(cls, toc: TreeOfContents):\n",
    "        if len(toc.branches)>1:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def split(self, toc: TreeOfContents) -> List[ArxivPaperSection]:\n",
    "        sections = []\n",
    "        for child in toc.branches:\n",
    "            # p - title, h6 - abstract\n",
    "            if not child.name==\"h2\":\n",
    "                continue\n",
    "            # print(\"CHILD:\", child.name, child.string, str(child))\n",
    "            child_text = get_toc_text(child)\n",
    "            \n",
    "            section = ArxivPaperSection(\n",
    "                header = child.name,\n",
    "                title = child.getText(),\n",
    "                text = child.getDescendantsMarkdown()\n",
    "            )\n",
    "            sections.append(section)\n",
    "        return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND <class '__main__.Case1Filter'>\n",
      "SECTIONS: 5\n",
      "header='h2' title='I Introduction' text='For nonlinear systems that arise from realistic engineering applications such as transport\\\\-reaction processes, modern control theory relies on *state\\\\-space representations* for their modeling, analysis, and control \\\\[1, 2, 3]. Recent advances in nonlinear control have highlighted the role of data\\\\-driven (machine learning) techniques in identifying governing equations or underlying dynamical structures \\\\[4, 5, 6], analyzing system and control\\\\-theoretic properties \\\\[7, 8], and synthesizing model\\\\-free controllers \\\\[9, 10, 11]. In these efforts, it is often assumed that the *state* information is available for analysis or control; for example, in reinforcement learning (RL) literature, it is common to apply stochastic first\\\\-order optimization to learn a value (cost) function or (Q) function based on temporal actions and state measurements. In many (if not most) control engineering applications, such as in chemical processes, however, it is more likely that the states are not measurable.\\n\\n*state\\\\-space representations**state*Hence, for nonlinear control in a state\\\\-space framework, a *state observer* is necessary, whereby the states are estimated based on input and output history \\\\[12]. A recent review on model\\\\-based approaches to synthesize state observers is found in Bernard, Andrieu, and Astolfi \\\\[13]. A classical form of state observer for linear systems is known as Luenberger observer \\\\[14], which an auxiliary linear time\\\\-invariant (LTI) system that uses the plant outputs as inputs and returns state estimates. The observer states are in fact a linear transform of the plant states \\\\[15]. The idea was extended to nonlinear systems in the seminal work of Kazantzis and Kravaris \\\\[16]. In their Kazantzis\\\\-Kravaris/Luenberger (KKL) observer (as named in Andrieu and Praly \\\\[17]) still uses an LTI system to convert plant outputs to observer states, which turn out to be the plant states transformed via a nonlinear immersion. Thus, the observer synthesis problem reduces to the determination of this nonlinear immersion and its inverse, via solving (model\\\\-based) partial differential equations (PDEs). Such a KKL observer was extended from autonomous to actuated systems in \\\\[18], where the LTI part is replaced by an input\\\\-affine one with an additional nonlinear drift term associated with the actuated inputs.\\n\\n*state observer*This paper focuses on the *synthesis of KKL observer* in a *model\\\\-free* manner, without assuming prior knowledge on the plant dynamics. This is motivated by two reasons: (i) many nonlinear systems that involve complex kinetic or kinematic mechanisms are often hard to model accurately, and (ii) it can be challenging to solve the associated PDEs, especially in high\\\\-dimensional state space (in fact, there may not be well\\\\-posed boundary conditions). In the recent years, there have been several works that pioneered the use of neural networks in the observer problem. For example, Ramos et al. \\\\[19] first trained neural networks to approximate the inverse immersion map to reconstruct the actual states from observer states. Then, the optimization of pole placement was considered along with the training of inverse immersion in \\\\[20]. Niazi et al. \\\\[21] used physics\\\\-informed neural networks (PINNs) to approach a surrogate solution to solve the PDEs. Miao and Gatsis \\\\[22] formulated a dynamic optimization problem to minimize the accumulated squared state observation error, whereby the optimality condition, through calculus of variations results in neural ODEs.\\n\\n*synthesis of KKL observer**model\\\\-free*It is commonly known that neural networks, when over\\\\-parameterized with large widths and depths, may cause a deteriorated capability of generalization. It has also been argued that neural networks can be fragile to adversarial attacks to the training data and thus must be equipped with a self\\\\-defense mechanisms that warranty robustness \\\\[23, 24]. In particular, controlling the Lipschitz constant of the mapping specified by the neural network has been studied as a promising approach \\\\[25, 26, 27]. However, in these works, estimating and minimizing the Lipschitz constant requires the use of semidefinite programming routines, which has a high complexity when the number of neurons is large. An alternative way, called *direct paramterizaton*, as recently proposed in Wang and Manchester \\\\[28], is to translate the Lipschitz bound constraint into a special architecture of\\nthe neural layers, thus allowing the use of typical back\\\\-propagation (BP) to train the network in an unconstrained way.\\n\\n*direct paramterizaton*Hence, in this work, the Wang\\\\-Manchester direct parameterization is adopted to train Lipschitz\\\\-bounded neural networks in a KKL state observer for any unknown nonlinear autonomous system. The paper establishes a relation between the generalized observation error and the Lipschitz bound of the neural network as well as the (H\\\\_{2})\\\\-norm of the LTI observer dynamics, under a typical white noise assumption on the plant outputs. Hence, by varying the Lipschitz bound, the optimal observer can be synthesized.\\n\\n' children=[]\n",
      "header='h2' title='II Preliminaries' text='We consider a nonlinear autonomous system:\\n\\n\\\\[\\\\\\\\dot{x}(t)\\\\=f(x(t)),\\\\\\\\quad y(t)\\\\=h(x(t)) \\\\\\\\tag{1}]\\n\\nwhere (x(t)\\\\\\\\in\\\\\\\\mathcal{X}\\\\\\\\subseteq\\\\\\\\mathbb{R}^{n}) is the vector of states and (y(t)\\\\\\\\in\\\\\\\\mathbb{R}^{m}) represents the outputs. For simplicity, we will consider (m\\\\=1\\\\). It is assumed that (f) and (h) are smooth on (\\\\\\\\mathcal{X}) to guarantee existence and uniqueness of solution but unknown for model\\\\-based synthesis.\\n\\n### *KKL Observer*\\n\\n*KKL Observer*For nonlinear systems, KKL observer generalizes the notion of Luenberger observers that were restricted to linear systems \\\\[14], providing a generic method for state observation with mild assumptions to guarantee existence. Specifically, the KKL observer for (1\\\\) is expressed as\\n\\n\\\\[\\\\\\\\dot{z}(t)\\\\=Az(t)\\\\+By(t),\\\\\\\\quad\\\\\\\\hat{x}(t)\\\\=T^{\\\\\\\\dagger}(z(t)). \\\\\\\\tag{2}]\\n\\nHere the observer states (z\\\\\\\\in\\\\\\\\mathbb{R}^{n\\\\_{z}}) has an LTI dynamics. The matrices (A) and (B) are chosen under the requirements of (i) controllability of ((A,B)) should be controllable, (ii) Hurwitz property of (A), and (iii) sufficiently high dimension of (z) ((n\\\\_{z})), which should be at least (n\\\\+1\\\\) if ((A,B)) is complex \\\\[17] and at least (2n\\\\+1\\\\) if ((A,B)) is real \\\\[29]. The mapping from the observer states (z) to the state estimates (\\\\\\\\hat{x}) is a static one, (T^{\\\\\\\\dagger}), which is the left\\\\-pseudoinverse of a nonlinear immersion (T) (i.e., a differentiable injection satisfying (T^{\\\\\\\\dagger}\\\\\\\\circ T\\\\=\\\\\\\\mathsf{id})). This immersion (T) should satisfy the following PDE:\\n\\n\\\\[\\\\\\\\frac{\\\\\\\\partial T}{\\\\\\\\partial x}(x)f(x)\\\\=AT(x)\\\\+Bh(x),\\\\\\\\quad\\\\\\\\forall x\\\\\\\\in\\\\\\\\mathcal{X}, \\\\\\\\tag{3}]\\n\\nwhere (\\\\\\\\partial T/\\\\\\\\partial x) denotes the Jacobian matrix of (T). It can be easily verified that under the above PDE, (dT(x)/dt\\\\=AT(x)\\\\+By), and thus (z\\\\-T(x)) has an exponentially decaying dynamics, as (A) is Hurwitz.\\n\\nThe conditions for the existence of a KKL observer, namely the solution to its defining PDE (3\\\\), have been established based on the condition of backward distinguishability. In below, we denote the solution to the ODEs (\\\\\\\\dot{x}\\\\=f(x)) at time (t) with initial condition (x(0\\\\)\\\\=\\\\\\\\xi) as (\\\\\\\\Phi\\\\_{t}(\\\\\\\\xi)). For any open set (\\\\\\\\mathcal{O}) in (\\\\\\\\mathcal{X}), denote the backward time instant after which the solution does not escape this region by (\\\\\\\\varsigma\\\\_{\\\\\\\\mathcal{O}}(\\\\\\\\xi)\\\\=\\\\\\\\inf{t\\\\|\\\\\\\\Phi\\\\_{t}(\\\\\\\\xi)\\\\\\\\in\\\\\\\\mathcal{O}}). Also denote (\\\\\\\\mathcal{O}\\\\+\\\\\\\\epsilon:\\\\={\\\\\\\\xi\\\\+\\\\\\\\eta\\\\|\\\\\\\\xi\\\\\\\\in\\\\\\\\mathcal{O},\\\\|\\\\\\\\eta\\\\|\\\\<\\\\\\\\epsilon}).\\n\\n**Definition 1** (Backward distinguishability).: *The system (1\\\\) is ((\\\\\\\\mathcal{O},\\\\\\\\epsilon))\\\\-backward distinguishable if for any distinct (\\\\\\\\xi,\\\\\\\\xi^{\\\\\\\\prime}\\\\\\\\in\\\\\\\\mathcal{X}) there exists a negative (t\\\\>\\\\\\\\varsigma*{\\\\\\\\mathcal{O}\\\\+\\\\\\\\epsilon}(\\\\\\\\xi)\\\\\\\\wedge\\\\\\\\varsigma\\\\_{\\\\\\\\mathcal{O}\\\\+\\\\\\\\epsilon} (\\\\\\\\xi^{\\\\\\\\prime})) such that (h(\\\\\\\\Phi\\\\_{t}(\\\\\\\\xi))\\\\\\\\neq h(\\\\\\\\Phi\\\\_{t}(\\\\\\\\xi^{\\\\\\\\prime}))).\\\\_\\n\\n**Definition 1***The system (1\\\\) is ((\\\\\\\\mathcal{O},\\\\\\\\epsilon))\\\\-backward distinguishable if for any distinct (\\\\\\\\xi,\\\\\\\\xi^{\\\\\\\\prime}\\\\\\\\in\\\\\\\\mathcal{X}) there exists a negative (t\\\\>\\\\\\\\varsigma***Fact 1** (Existence of KKL observer, cf. Brivadis et al. \\\\[29]).: *Assume that there is an open (\\\\\\\\mathcal{O}\\\\\\\\subseteq\\\\\\\\bar{\\\\\\\\mathcal{X}}) and a positive constant (\\\\\\\\epsilon) such that the system (1\\\\) is ((\\\\\\\\mathcal{O},\\\\\\\\epsilon))\\\\-backward distinguishable. Then there exists a constant (\\\\\\\\rho\\\\>0\\\\) such that for all but a Lebesgue\\\\-zero\\\\-measure set of ((A,B)\\\\\\\\in\\\\\\\\mathbb{R}^{(2n\\\\+1\\\\)\\\\\\\\times(2n\\\\+1\\\\)}\\\\\\\\times\\\\\\\\mathbb{R}^{(2n\\\\+1\\\\)}), if (A\\\\+\\\\\\\\rho I) Hurwitz, then there exists an immersion (T:\\\\\\\\mathcal{O}\\\\\\\\rightarrow\\\\\\\\mathbb{R}^{(2n\\\\+1\\\\)}) solving the PDEs (3\\\\).*\\n\\n**Fact 1***Assume that there is an open (\\\\\\\\mathcal{O}\\\\\\\\subseteq\\\\\\\\bar{\\\\\\\\mathcal{X}}) and a positive constant (\\\\\\\\epsilon) such that the system (1\\\\) is ((\\\\\\\\mathcal{O},\\\\\\\\epsilon))\\\\-backward distinguishable. Then there exists a constant (\\\\\\\\rho\\\\>0\\\\) such that for all but a Lebesgue\\\\-zero\\\\-measure set of ((A,B)\\\\\\\\in\\\\\\\\mathbb{R}^{(2n\\\\+1\\\\)\\\\\\\\times(2n\\\\+1\\\\)}\\\\\\\\times\\\\\\\\mathbb{R}^{(2n\\\\+1\\\\)}), if (A\\\\+\\\\\\\\rho I) Hurwitz, then there exists an immersion (T:\\\\\\\\mathcal{O}\\\\\\\\rightarrow\\\\\\\\mathbb{R}^{(2n\\\\+1\\\\)}) solving the PDEs (3\\\\).*The above theorem clarifies that as long as the spectrum of (A) is restricted to the left of (\\\\-\\\\\\\\rho\\\\+i\\\\\\\\mathbb{R}), the LTI dynamics in the KKL observer can be almost arbitrarily assigned. Once ((A,B)) are chosen, the remaining question for synthesis a KKL observer (2\\\\) is to numerically determine the solution. In view of the computational challenge in directly solving the PDEs (3\\\\) and the recent trend of handling the problem by neural approaches \\\\[19, 20, 21], this work will seek to approximate (T^{\\\\\\\\dagger}) by a neural network. Yet, instead of using a vanilla multi\\\\-layer perceptron architecture, a Lipschitz\\\\-bounded neural network will be adopted, which safeguards the generalization performance of state observation, which will be discussed in SSIII. This overall idea is illustrated in Fig. 1\\\\.\\n\\n### *Lipschitz\\\\-Bounded Neural Networks*\\n\\n*Lipschitz\\\\-Bounded Neural Networks*Consider a (\\\\\\\\nu)\\\\-layer neural network (\\\\\\\\hat{x}\\\\=S(z,\\\\\\\\theta)) with all parameters denoted as a single vector (\\\\\\\\theta). Without loss of generality, assume that the activation function (element\\\\-wise applied to vectors) is (\\\\\\\\sigma:\\\\\\\\mathbb{R}\\\\\\\\rightarrow\\\\\\\\mathbb{R}), with slope bounded in (\\\\[0,1]) (in this work, rectified linear units (ReLU) are used to prevent gradient decay in BP training). The neural network then can be expressed as\\n\\n\\\\[\\\\\\\\begin{split}\\\\& z^{\\\\\\\\ell\\\\+1}\\\\=\\\\\\\\sigma(W^{\\\\\\\\ell}z^{\\\\\\\\ell}\\\\+b^{\\\\\\\\ell}),\\\\\\\\ \\\\\\\\ \\\\\\\\ell\\\\=0,\\\\\\\\ldots,\\\\\\\\nu\\\\-1\\\\\\\\ \\\\& z^{0}\\\\=z,\\\\\\\\quad\\\\\\\\hat{x}\\\\=W^{\\\\\\\\nu}z^{\\\\\\\\nu}\\\\+b^{\\\\\\\\nu}.\\\\\\\\end{split} \\\\\\\\tag{4}]\\n\\nwhere (W^{0},\\\\\\\\ldots,W^{\\\\\\\\nu}) are the weight matrices and (b^{0},\\\\\\\\ldots,b^{\\\\\\\\nu}) are the biases. In total there are (\\\\\\\\nu) activation layers inserted between (\\\\\\\\nu\\\\+1\\\\) fully connected layers. (z) represents the inputs to the neural network and (\\\\\\\\hat{x}) is the output vector, as we will use such a neural network to approximate the (T^{\\\\\\\\dagger}) mapping in the KKL observer.\\n\\nGiven a neural network with fixed parameters (\\\\\\\\theta\\\\=(W^{0},b^{0},\\\\\\\\ldots,W^{\\\\\\\\nu},b^{\\\\\\\\nu})), a rough estimate of the Lipschitz\\n\\nFig. 1: KKL observer with a Lipschitz\\\\-bounded neural network to be trained.\\nconstant of (S) can be obviously obtained as\\n\\n\\\\[L\\\\_{S}(\\\\\\\\theta)\\\\=\\\\\\\\prod\\\\_{\\\\\\\\ell\\\\=0}^{\\\\\\\\nu}\\\\|W^{\\\\\\\\ell}\\\\|\\\\_{2}, \\\\\\\\tag{5}]\\n\\nwhere (\\\\|\\\\\\\\cdot\\\\|*{2}) for a matrix refers to its operator norm induced by the (\\\\\\\\ell*{2})\\\\-norm of vectors, i.e., its largest singular value. To reduce the conservativeness, Fazlyab et al. \\\\[25] leverages the control\\\\-theoretic tool of integral quadratic constraints to formulate the Lipschitz bound condition as a linear matrix inequality, thus estimating the Lipschitz constants and training Lipschitz\\\\-bounded neural networks through solving semidefinite programming problems \\\\[27]. The pertinent matrix size, however, proportionally scales with the total number of neurons, which results in high computational complexity unless the neural network is very small.\\n\\n*{2}) for a matrix refers to its operator norm induced by the (\\\\\\\\ell*The recent work of Wang and Manchester \\\\[28] proposed a *direct parameterization* approach to accommodate Lipschitz bound by a special design of the neural network architecture instead of imposing extra parameter constraints. By this approach, the training of neural networks is an unconstrained optimization problem and is thus amenable to the typical, computationally lightweight back\\\\-propagation (BP) routine. Wang\\\\-Manchester direct parameterization is conceptually related to, and arguably motivated by, the theory of controller parameterization \\\\[30, 31].\\n\\n*direct parameterization***Definition 2** ((1\\\\)\\\\-Lipschitz sandwich layer, cf. \\\\[28]).: *Given parameters (X\\\\\\\\in\\\\\\\\mathbb{R}^{d\\\\\\\\times d}), (Y\\\\\\\\in\\\\\\\\mathbb{R}^{c\\\\\\\\times d}), (s\\\\\\\\in\\\\\\\\mathbb{R}^{d}), and (b\\\\\\\\in\\\\\\\\mathbb{R}^{d}), a (1\\\\)\\\\-Lipschitz sandwich layer is defined as such a mapping (\\\\\\\\Xi:\\\\\\\\mathbb{R}^{c}\\\\\\\\rightarrow\\\\\\\\mathbb{R}^{d}) that maps any (h\\\\\\\\in\\\\\\\\mathbb{R}^{c}) into a (\\\\\\\\Xi(h;X,Y,s,b)\\\\\\\\in\\\\\\\\mathbb{R}^{d}) according to the following formulas:*\\n\\n**Definition 2***Given parameters (X\\\\\\\\in\\\\\\\\mathbb{R}^{d\\\\\\\\times d}), (Y\\\\\\\\in\\\\\\\\mathbb{R}^{c\\\\\\\\times d}), (s\\\\\\\\in\\\\\\\\mathbb{R}^{d}), and (b\\\\\\\\in\\\\\\\\mathbb{R}^{d}), a (1\\\\)\\\\-Lipschitz sandwich layer is defined as such a mapping (\\\\\\\\Xi:\\\\\\\\mathbb{R}^{c}\\\\\\\\rightarrow\\\\\\\\mathbb{R}^{d}) that maps any (h\\\\\\\\in\\\\\\\\mathbb{R}^{c}) into a (\\\\\\\\Xi(h;X,Y,s,b)\\\\\\\\in\\\\\\\\mathbb{R}^{d}) according to the following formulas:*\\\\[Z \\\\=X\\\\-X^{\\\\\\\\top}\\\\+Y^{\\\\\\\\top}Y,\\\\\\\\ \\\\\\\\Psi\\\\_{s}\\\\=\\\\\\\\mathrm{diag}(e^{s}) \\\\\\\\tag{6}] \\\\[M\\\\_{X,Y} \\\\=\\\\\\\\left\\\\[(I\\\\+Z)^{\\\\-1}(I\\\\-Z)\\\\\\\\right]^{\\\\\\\\top},] \\\\[N\\\\_{X,Y} \\\\=\\\\\\\\left\\\\[\\\\-2Y(I\\\\+Z)^{\\\\-1}\\\\\\\\right]^{\\\\\\\\top},] \\\\[\\\\\\\\Xi(h) \\\\=\\\\\\\\sqrt{2}M\\\\_{X,Y}^{\\\\\\\\top}\\\\\\\\Psi\\\\_{s}\\\\\\\\sigma(\\\\\\\\sqrt{2}\\\\\\\\Psi\\\\_{s}^{\\\\-1}N\\\\_{X, Y}h\\\\+b).]\\n\\nIt turns out that the Lipschitz constant of the above\\\\-defined sandwich layer is guaranteed to be upper bounded by (1\\\\)\\\\[28, Theorem 3\\\\.3]. The mapping from the input (h) to the output (\\\\\\\\Xi(h)) can be regarded as comprising of an activation layer in the midst of two fully connected layers with related parameters. The operation from ((X,Y)) to ((M,N)) is known as the *Cayley transform*. The structure and the parameters of a sandwich layer is shown in Fig. 2\\\\.\\n\\n*Cayley transform*Thus, by stacking a number of such sandwich layers after a scaling by (\\\\\\\\sqrt{\\\\\\\\gamma}) and before a non\\\\-activated half\\\\-sandwich layer (meaning a layer containing only the terms in the parentheses of (\\\\\\\\Xi) as in Equation (6\\\\)), a neural network with Lipschitz bound (\\\\\\\\gamma) can be obtained, for any provided (\\\\\\\\gamma\\\\>0\\\\).\\n\\n**Definition 3** (Wang\\\\-Manchester network).: *In this work, we refer to Wang\\\\-Manchester network, (S(\\\\\\\\cdot\\\\|\\\\\\\\theta)), by a neural network in the following architecture:*\\n\\n**Definition 3***In this work, we refer to Wang\\\\-Manchester network, (S(\\\\\\\\cdot\\\\|\\\\\\\\theta)), by a neural network in the following architecture:*\\\\[h^{0} \\\\=\\\\\\\\sqrt{\\\\\\\\gamma}z; \\\\\\\\tag{7}] \\\\[h^{\\\\\\\\ell\\\\+1} \\\\=\\\\\\\\Xi(h^{\\\\\\\\ell};X^{\\\\\\\\ell},Y^{\\\\\\\\ell},s^{\\\\\\\\ell},b^{\\\\\\\\ell}),\\\\\\\\ \\\\\\\\ell\\\\=0,1,\\\\\\\\ldots,\\\\\\\\nu\\\\-1;] \\\\[\\\\\\\\hat{x} \\\\=\\\\\\\\sqrt{\\\\\\\\gamma}N\\\\_{X^{\\\\\\\\nu},Y^{\\\\\\\\nu}}h^{\\\\\\\\nu}\\\\+b^{\\\\\\\\nu}.]\\n\\n*Here the parameters include*\\n\\n*Here the parameters include*\\\\[\\\\\\\\theta\\\\={X^{\\\\\\\\ell},Y^{\\\\\\\\ell},s^{\\\\\\\\ell},b^{\\\\\\\\ell}}\\\\_{\\\\\\\\ell\\\\=0}^{\\\\\\\\nu\\\\-1}\\\\\\\\cup{X^{\\\\\\\\nu}, Y^{\\\\\\\\nu},b^{\\\\\\\\nu}}]\\n\\n*which can be trained in an unconstrained way using back\\\\-propagation. The inputs and outputs of the network are (z) and (\\\\\\\\hat{x}), respectively.*\\n\\n*which can be trained in an unconstrained way using back\\\\-propagation. The inputs and outputs of the network are (z) and (\\\\\\\\hat{x}), respectively.*The above\\\\-defined Wang\\\\-Manchester network satisfies (\\\\|S(\\\\\\\\cdot\\\\|\\\\\\\\theta)\\\\|\\\\_{\\\\\\\\mathrm{Lip}}\\\\\\\\leq\\\\\\\\gamma). In this work, the network is defined and trained with data using PyTorch (version 2\\\\.0\\\\.1\\\\) on Google Colaboratory, which allows the auto\\\\-differentiation of a user\\\\-defined loss function with respect to the neural network parameters for the parameters to be iteratively updated.\\n\\n' children=[]\n",
      "header='h2' title='III Analysis on the Generalized Loss' text=\"Here we shall provide a justification for requiring a Lipschitz bound on the neural network. We will make the following standing assumptions on the training data collection procedure for subsequent analysis.\\n\\n**Assumption 1** (Ergodicity).: *Assume that a sample trajectory is collected from the system, whose initial state is sampled from a probability distribution (\\\\\\\\mathcal{F}) on (\\\\\\\\mathcal{X}). The distribution (\\\\\\\\mathcal{F}) is time\\\\-invariant (i.e., an eigenmeasure of the Perron\\\\-Frobenius operator), so that any point of the trajectory comes from (\\\\\\\\mathcal{F}).*\\n\\n**Assumption 1***Assume that a sample trajectory is collected from the system, whose initial state is sampled from a probability distribution (\\\\\\\\mathcal{F}) on (\\\\\\\\mathcal{X}). The distribution (\\\\\\\\mathcal{F}) is time\\\\-invariant (i.e., an eigenmeasure of the Perron\\\\-Frobenius operator), so that any point of the trajectory comes from (\\\\\\\\mathcal{F}).*Suppose that The LTI dynamics of the KKL observer, ((A,B)), is fixed. Then the observer states can be simulated from this linear dynamics.\\n\\n**Assumption 2** (Noisy measurements).: *Assume that the input signal for this LTI system is not noise\\\\-free measurements (y\\\\=h(x)), but instead containing a white noise of unknown covariance (\\\\\\\\sigma^{2}). In other words, the simulation from (y) to (z) is*\\n\\n**Assumption 2***Assume that the input signal for this LTI system is not noise\\\\-free measurements (y\\\\=h(x)), but instead containing a white noise of unknown covariance (\\\\\\\\sigma^{2}). In other words, the simulation from (y) to (z) is*\\\\[\\\\\\\\begin{split}\\\\&\\\\\\\\dot{z}\\\\=Ax\\\\+By\\\\+w,\\\\\\\\quad\\\\\\\\mathbb{E}\\\\[w(t)]\\\\=0,\\\\\\\\,\\\\\\\\forall t \\\\\\\\in\\\\\\\\mathbb{R}\\\\\\\\ \\\\&\\\\\\\\mathbb{E}\\\\[w(t)w(s)]\\\\=\\\\\\\\delta(t\\\\-s)\\\\\\\\sigma^{2},\\\\\\\\,\\\\\\\\forall t,s\\\\\\\\in \\\\\\\\mathbb{R}.\\\\\\\\end{split} \\\\\\\\tag{8}]\\n\\nIn this way, the collected sample, denoted as ({(x(t\\\\_{i}),z(t\\\\_{i}))}*{i\\\\=1}^{m}\\\\={(x*{i},z\\\\_{i})}\\\\_{i\\\\=1}^{m}), in fact satisfies the following relation:\\n\\n*{i\\\\=1}^{m}\\\\={(x*\\\\[z\\\\_{i}\\\\=\\\\\\\\bar{z}*{i}\\\\+v*{i},\\\\\\\\quad\\\\\\\\delta\\\\_{i}\\\\=\\\\\\\\int\\\\_{\\\\-\\\\\\\\infty}^{t\\\\_{i}}g(\\\\\\\\tau)w(t\\\\_{i}\\\\- \\\\\\\\tau)d\\\\\\\\tau. \\\\\\\\tag{9}]\\n\\n*{i}\\\\+v*Here (g(\\\\\\\\tau)) is the impulse response of LTI system ((A,B)); (\\\\\\\\bar{z}) is the value of (z(t\\\\_{i})) that would be otherwise obtained if there were no white noises in the output measurements.\\n\\n**Assumption 3** (Sufficient decay).: *After a significantly long time (t*{\\\\\\\\epsilon}), (\\\\|z\\\\-T(x)\\\\|\\\\\\\\leq\\\\\\\\epsilon) for a small enough (z). Here (T) is the nonlinear immersion map specified by (3\\\\).\\\\_\\n\\n**Assumption 3***After a significantly long time (t*Fig. 2: A sandwich layer and its parameters.\\nThen, (\\\\|\\\\\\\\bar{z}*{i}\\\\-T(x*{i})\\\\|\\\\\\\\leq\\\\\\\\epsilon). Thus, we may write\\n\\n*{i}\\\\-T(x*\\\\[z\\\\_{i}\\\\=T(x\\\\_{i})\\\\+v\\\\_{i}\\\\+v\\\\_{i}^{\\\\\\\\prime},\\\\\\\\quad\\\\|v\\\\_{i}^{\\\\\\\\prime}\\\\|\\\\\\\\leq\\\\\\\\epsilon. \\\\\\\\tag{10}]\\n\\nNow we suppose that the sample ({(x\\\\_{i},z\\\\_{i})}*{i\\\\=1}^{m}) is used to train a neural network (S(\\\\\\\\cdot\\\\|\\\\\\\\theta)), which gives the state observations (\\\\\\\\hat{x}*{i}\\\\=S(z\\\\_{i}\\\\|\\\\\\\\theta)), and that the resulting empirical loss, if defined as the average squared observation error, is\\n\\n*{i\\\\=1}^{m}) is used to train a neural network (S(\\\\\\\\cdot\\\\|\\\\\\\\theta)), which gives the state observations (\\\\\\\\hat{x}*\\\\[\\\\\\\\hat{R}*{S}(\\\\\\\\theta):\\\\=\\\\\\\\frac{1}{m}\\\\\\\\sum*{i\\\\=1}^{m}\\\\|\\\\\\\\hat{x}*{i}\\\\-x*{i}\\\\|^{2}. \\\\\\\\tag{11}]\\n\\n*{S}(\\\\\\\\theta):\\\\=\\\\\\\\frac{1}{m}\\\\\\\\sum**{i}\\\\-x*Then we get\\n\\n\\\\[\\\\\\\\hat{R}*{S}(\\\\\\\\theta)\\\\=\\\\\\\\frac{1}{m}\\\\\\\\sum*{i\\\\=1}^{m}\\\\\\\\left\\\\|S\\\\\\\\left(T(x\\\\_{i})\\\\+v\\\\_{i}\\\\+v\\\\_{i }^{\\\\\\\\prime}\\\\|\\\\\\\\theta\\\\\\\\right)\\\\-x\\\\_{i}\\\\\\\\right\\\\|^{2}. \\\\\\\\tag{12}]\\n\\n*{S}(\\\\\\\\theta)\\\\=\\\\\\\\frac{1}{m}\\\\\\\\sum***Assumption 4**.: *Assume that the probability distribution (\\\\\\\\mathcal{F}) is supported by a compact set, i.e., if (x\\\\\\\\sim\\\\\\\\mathcal{F}), then (x) should be almost surely bounded.*\\n\\n**Assumption 4***Assume that the probability distribution (\\\\\\\\mathcal{F}) is supported by a compact set, i.e., if (x\\\\\\\\sim\\\\\\\\mathcal{F}), then (x) should be almost surely bounded.*It follows that both (S(\\\\\\\\cdot\\\\|\\\\\\\\theta)) and (T) should be Lipschitz continuous. Denote their Lipschitz constants as (L\\\\_{S}(\\\\\\\\theta)) and (L\\\\_{T}), respectively. We have\\n\\n\\\\[\\\\|S\\\\\\\\left(T(x\\\\_{i})\\\\+\\\\\\\\delta\\\\_{i}\\\\+\\\\\\\\delta\\\\_{i}^{\\\\\\\\prime}\\\\|\\\\\\\\theta\\\\\\\\right)\\\\-S\\\\\\\\left(T(x\\\\_{i}) \\\\|\\\\\\\\theta\\\\\\\\right)\\\\|\\\\\\\\leq L\\\\_{S}(\\\\\\\\theta)L\\\\_{T}(\\\\|v\\\\_{i}\\\\|\\\\+\\\\\\\\epsilon). \\\\\\\\tag{13}]\\n\\nDenote (D) as the essential upper bound of (\\\\|x\\\\|) on the distribution (\\\\\\\\mathcal{F}). As such, without loss of generality, let (S(T(0\\\\))\\\\=0\\\\). Then (\\\\|x\\\\-S(T(x))\\\\|\\\\\\\\leq(L\\\\_{S}(\\\\\\\\theta)L\\\\_{T}\\\\+1\\\\)D) almost surely. Combining the above two equations, we further get\\n\\n\\\\[\\\\\\\\frac{1}{m}\\\\\\\\sum\\\\_{i\\\\=1}^{m}\\\\|x\\\\_{i}\\\\-S(T(x\\\\_{i})\\\\|\\\\\\\\theta)\\\\|^{2}\\\\\\\\leq \\\\\\\\hat{R}*{S}(\\\\\\\\theta) \\\\\\\\tag{14}] \\\\[\\\\\\\\quad\\\\+\\\\\\\\frac{1}{m}\\\\\\\\sum*{i\\\\=1}^{m}L\\\\_{S}(\\\\\\\\theta)L\\\\_{T}(L\\\\_{S}(\\\\\\\\theta)L *{T}\\\\+1\\\\)D(\\\\|v*{i}\\\\|\\\\+\\\\\\\\epsilon)] \\\\[\\\\\\\\quad\\\\+\\\\\\\\frac{1}{m}\\\\\\\\sum\\\\_{i\\\\=1}^{m}L\\\\_{S}^{2}(\\\\\\\\theta)L\\\\_{T}^{2}(\\\\|v\\\\_{i} \\\\|\\\\+\\\\\\\\epsilon)^{2}.]\\n\\n*{S}(\\\\\\\\theta) \\\\\\\\tag{14}] \\\\[\\\\\\\\quad\\\\+\\\\\\\\frac{1}{m}\\\\\\\\sum**{T}\\\\+1\\\\)D(\\\\|v*That is,\\n\\n\\\\[\\\\\\\\frac{1}{m}\\\\\\\\sum\\\\_{i\\\\=1}^{m}\\\\|x\\\\_{i}\\\\-S(T(x\\\\_{i})\\\\|\\\\\\\\theta)\\\\|^{2}\\\\\\\\leq \\\\\\\\hat{R}*{S}(\\\\\\\\theta) \\\\\\\\tag{15}] \\\\[\\\\\\\\quad\\\\+\\\\\\\\frac{1}{m}\\\\\\\\sum*{i\\\\=1}^{m}(L\\\\_{S}(\\\\\\\\theta)L\\\\_{T}\\\\+1\\\\)^{2}\\\\\\\\left(D\\\\+ \\\\|v\\\\_{i}\\\\|\\\\+\\\\\\\\epsilon\\\\\\\\right)(\\\\|v\\\\_{i}\\\\|\\\\+\\\\\\\\epsilon).]\\n\\n*{S}(\\\\\\\\theta) \\\\\\\\tag{15}] \\\\[\\\\\\\\quad\\\\+\\\\\\\\frac{1}{m}\\\\\\\\sum*The left\\\\-hand side gives an estimation of the empirical loss when observing the states from perfect output measurements (namely when (z\\\\_{i}\\\\=T(x\\\\_{i})), (i\\\\=1,\\\\\\\\ldots,m)). Further expanding the last term and applying Cauchy\\\\-Schwarz inequality, we have\\n\\n\\\\[\\\\\\\\frac{1}{m}\\\\\\\\sum\\\\_{i\\\\=1}^{m}\\\\|x\\\\_{i}\\\\-S(T(x\\\\_{i})\\\\|\\\\\\\\theta)\\\\|^{2}\\\\\\\\leq\\\\\\\\hat {R}*{S}(\\\\\\\\theta)\\\\+(L*{S}(\\\\\\\\theta)L\\\\_{T}\\\\+1\\\\)^{2}\\\\\\\\times \\\\\\\\tag{16}] \\\\[\\\\\\\\quad\\\\\\\\left\\\\[\\\\\\\\frac{1}{m}\\\\\\\\sum\\\\_{i\\\\=1}^{m}\\\\|v\\\\_{i}\\\\|^{2}\\\\+(D\\\\+2\\\\\\\\epsilon) \\\\\\\\left(\\\\\\\\sum\\\\_{i\\\\=1}^{m}\\\\|v\\\\_{i}\\\\|^{2}\\\\\\\\right)^{1/2}\\\\+(D\\\\+\\\\\\\\epsilon)\\\\\\\\epsilon\\\\\\\\right].]\\n\\n*{S}(\\\\\\\\theta)\\\\+(L*Given that (v\\\\_{i}) is the response of LTI system ((A,B)) to a white noise of covariance (\\\\\\\\sigma^{2}), (\\\\\\\\mathbb{E}(\\\\|v\\\\_{i}\\\\|^{2})\\\\=h^{2}\\\\\\\\sigma^{2}) where (h) is the (H\\\\_{2})\\\\-norm of the system ((A,B)) where (A) is Hurwitz. Therefore,\\n\\n\\\\[\\\\\\\\mathbb{E}\\\\\\\\left(\\\\\\\\frac{1}{m}\\\\\\\\sum\\\\_{i\\\\=1}^{m}\\\\|v\\\\_{i}\\\\|^{2}\\\\\\\\right)\\\\=h^{2}\\\\\\\\sigma^{2}. \\\\\\\\tag{17}]\\n\\nLet (\\\\\\\\alpha) be a small positive number. With confidence (1\\\\-\\\\\\\\alpha/2\\\\), a conservative estimation for its upper bound can be found according to Markov inequality:\\n\\n\\\\[\\\\\\\\frac{1}{m}\\\\\\\\sum\\\\_{i\\\\=1}^{m}\\\\|v\\\\_{i}\\\\|^{2}\\\\\\\\leq\\\\\\\\frac{1}{1\\\\-\\\\\\\\alpha/2}h^{2}\\\\\\\\sigma^{2}. \\\\\\\\tag{18}]\\n\\nTherefore,\\n\\n\\\\[\\\\\\\\frac{1}{m}\\\\\\\\sum\\\\_{i\\\\=1}^{m}\\\\|x\\\\_{i}\\\\-S(T(x\\\\_{i})\\\\|\\\\\\\\theta)\\\\|^{2}\\\\\\\\leq\\\\\\\\hat {R}*{S}(\\\\\\\\theta)\\\\+(L*{S}(\\\\\\\\theta)L\\\\_{T}\\\\+1\\\\)^{2}\\\\\\\\times \\\\\\\\tag{19}] \\\\[\\\\\\\\quad\\\\\\\\left\\\\[\\\\\\\\frac{h^{2}\\\\\\\\sigma^{2}}{1\\\\-\\\\\\\\alpha/2}\\\\+(D\\\\+2\\\\\\\\epsilon)\\\\\\\\frac{ h\\\\\\\\sigma}{\\\\\\\\sqrt{1\\\\-\\\\\\\\alpha/2}}\\\\+(D\\\\+\\\\\\\\epsilon)\\\\\\\\epsilon\\\\\\\\right].]\\n\\n*{S}(\\\\\\\\theta)\\\\+(L*Finally, we note that for (x\\\\\\\\sim\\\\\\\\mathcal{F}), now that (\\\\|x\\\\-S(T(x)\\\\|\\\\\\\\theta)\\\\|\\\\\\\\leq(L\\\\_{S}(\\\\\\\\theta)L\\\\_{T}\\\\+1\\\\)D) almost surely, by Hoeffding's inequality, for any (\\\\\\\\varepsilon\\\\>0\\\\),\\n\\n\\\\[\\\\\\\\mathbb{P}\\\\\\\\bigg{(}\\\\\\\\bigg{\\\\|}\\\\\\\\frac{1}{m}\\\\\\\\sum\\\\_{i\\\\=1}^{m}\\\\|x\\\\_{i}\\\\-S(T(x\\\\_{ i})\\\\|\\\\\\\\theta)\\\\|^{2}\\\\-\\\\\\\\mathbb{E}\\\\\\\\left(\\\\|x\\\\-S(T(x))\\\\|^{2}\\\\\\\\right)\\\\\\\\bigg{\\\\|} \\\\\\\\tag{20}] \\\\[\\\\\\\\geq(L\\\\_{S}(\\\\\\\\theta)L\\\\_{T}\\\\+1\\\\)^{2}D^{2}\\\\\\\\varepsilon\\\\\\\\bigg{)}\\\\\\\\leq 2\\\\\\\\exp \\\\\\\\left(\\\\-2m\\\\\\\\varepsilon^{2}\\\\\\\\right).]\\n\\nThus, with confidence (1\\\\-\\\\\\\\alpha/2\\\\), we have\\n\\n\\\\[\\\\\\\\left\\\\|\\\\\\\\frac{1}{m}\\\\\\\\sum\\\\_{i\\\\=1}^{m}\\\\|x\\\\_{i}\\\\-S(T(x\\\\_{i})\\\\|\\\\\\\\theta)\\\\|^{2}\\\\- \\\\\\\\mathbb{E}\\\\\\\\left(\\\\|x\\\\-S(T(x)\\\\|\\\\\\\\theta)\\\\|^{2}\\\\\\\\right)\\\\\\\\bigg{\\\\|}\\\\\\\\right. \\\\\\\\tag{21}] \\\\[\\\\\\\\quad\\\\<(L\\\\_{S}(\\\\\\\\theta)L\\\\_{T}\\\\+1\\\\)^{2}D^{2}\\\\\\\\sqrt{\\\\\\\\frac{\\\\\\\\ln(4/\\\\\\\\alpha)}{2m }}.]\\n\\nCombining (19\\\\) and (21\\\\), we have the following theorem.\\n\\n**Theorem 1**.: *Under the afore\\\\-mentioned assumptions, the generalization loss, defined as*\\n\\n**Theorem 1***Under the afore\\\\-mentioned assumptions, the generalization loss, defined as*\\\\[R\\\\_{S}(\\\\\\\\theta)\\\\=\\\\\\\\mathbb{E}\\\\\\\\left(\\\\|x\\\\-S(T(x)\\\\|\\\\\\\\theta)\\\\|^{2}\\\\\\\\right), \\\\\\\\tag{22}]\\n\\n*is related to the empirical loss as defined in (11\\\\) by*\\n\\n*is related to the empirical loss as defined in (11\\\\) by*\\\\[R\\\\_{S}(\\\\\\\\theta)\\\\<\\\\\\\\hat{R}*{S}(\\\\\\\\theta)\\\\+(L*{S}(\\\\\\\\theta)L\\\\_{T}\\\\+1\\\\)^{2}\\\\\\\\Delta(h,\\\\\\\\sigma, \\\\\\\\alpha,\\\\\\\\epsilon). \\\\\\\\tag{23}]\\n\\n*{S}(\\\\\\\\theta)\\\\+(L**with confidence (1\\\\-\\\\\\\\alpha) ((\\\\\\\\alpha\\\\\\\\in(0,1\\\\))). Here*\\n\\n*with confidence (1\\\\-\\\\\\\\alpha) ((\\\\\\\\alpha\\\\\\\\in(0,1\\\\))). Here*\\\\[\\\\\\\\Delta(h,\\\\\\\\sigma,\\\\\\\\alpha,\\\\\\\\epsilon)\\\\= D^{2}\\\\\\\\sqrt{\\\\\\\\frac{\\\\\\\\ln(4/\\\\\\\\alpha)}{2m}}\\\\+\\\\\\\\frac{h^{2}\\\\\\\\sigma^{2}}{1\\\\- \\\\\\\\alpha/2} \\\\\\\\tag{24}] \\\\[\\\\+(D\\\\+2\\\\\\\\epsilon)\\\\\\\\frac{h\\\\\\\\sigma}{\\\\\\\\sqrt{1\\\\-\\\\\\\\alpha/2}}\\\\+(D\\\\+\\\\\\\\epsilon)\\\\\\\\epsilon.]\\n\\nThe theorem shows that the Lipschitz constant of the neural network trained plays an important role in the generalized performance of the resulting state observer. The effect of (L\\\\_{S}(\\\\\\\\theta)) is mainly that of amplifying the first and third terms defined on the right\\\\-hand side of (24\\\\), supposing that (\\\\\\\\sigma)\\nand (\\\\\\\\epsilon) are small enough. These two terms respectively arise from (i) the overall upper bound of the observation error (\\\\|x\\\\-S(T(x)\\\\|\\\\\\\\theta)\\\\|), which acts as a coefficient before the Hoeffding term (\\\\\\\\sqrt{\\\\\\\\ln(4/\\\\\\\\alpha)/2m}), and (ii) the effect of noisy measurements on the observer states.\\n\\n**Remark 1**.: *It is noted that the performance bound stated in the above theorem can be conservative. The conclusion that (L*{S}(\\\\\\\\theta)) amplifies the generalization error and measurement noise should be considered as qualitative. The theorem also does not suggest a tractable algorithm to optimize the selection of ((A,B)) along with the neural network (S(\\\\\\\\cdot\\\\|\\\\\\\\theta)), as the dependence of (L\\\\_{T}) on ((A,B)) is highly implicit. Hence, this paper does not consider the problem of simultaneously training ((A,B)) and the neural network.\\\\_\\n\\n**Remark 1***It is noted that the performance bound stated in the above theorem can be conservative. The conclusion that (L*\" children=[]\n",
      "header='h2' title='IV Case Study' text='Let us consider a Lorenz system in a 3\\\\-dimensional state space with chaotic behavior. The equation is written as:\\n\\n\\\\[\\\\\\\\dot{x}*{1} \\\\=10(x*{2}\\\\-x\\\\_{1}), \\\\\\\\tag{25}] \\\\[\\\\\\\\dot{x}*{2} \\\\=x*{1}(28\\\\-10x\\\\_{3})\\\\-x\\\\_{2},] \\\\[\\\\\\\\dot{x}*{3} \\\\=10x*{1}x\\\\_{2}\\\\-(8/3\\\\)x\\\\_{3}.]\\n\\n*{1} \\\\=10(x**{2} \\\\=x**{3} \\\\=10x*Suppose that the measurement used for state observation is (y\\\\=x\\\\_{2}), where a white noise exists. We assign different values to the variance of the measurement noise and investigate how the resulting neural network should be chosen differently. To simulate the process we will use a sampling time of (0\\\\.01\\\\). The LTI part of the KKL observer, (A\\\\=\\\\-\\\\\\\\mathrm{diag}(8,4,2,1\\\\)) and (B\\\\=\\\\[1,1,1,1]^{\\\\\\\\top}) are chosen. At the beginning of the observer simulation, (z(0\\\\)\\\\=0\\\\) is set as the initial condition; we simulate the dynamics until (t\\\\=500\\\\) and randomly collect (m\\\\=2000\\\\) time instants between (t\\\\=20\\\\) and (t\\\\=500\\\\) as the training data.\\n\\nConsider first the case with noiseless measurement ((\\\\\\\\sigma\\\\=0\\\\)). The sample ({(x\\\\_{i},z\\\\_{i})}*{i\\\\=1}^{2000}) is plotted in Fig. 3, which shows that the data points are representative on the forward invariant set of the system, and that the observer states (z*{i}) indeed captures the structure of such a Lorenz attractor in a (4\\\\)\\\\-dimensional space. Hence, we train the Wang\\\\-Manchester network using a randomly selected (80\\\\\\\\%) of the sample under the mean\\\\-squares loss metric, and validate using the remaining (20\\\\\\\\%) sample points. Stochastic gradient descent (SGD) algorithm with a learning rate of (10^{\\\\-3}) is used for optimization. The number of epochs is empirically tuned to (300\\\\). The neural network has (2\\\\) hidden layers, each containing (8\\\\) neurons, resulting in (292\\\\) parameters to train in total. After training, the Lipschitz constant is evaluated a posteriori via the semidefinite programming approach of Fazlyab et al. \\\\[25] using cvxpy, which costs approximately (1\\\\.5\\\\) seconds (for a randomly initialized network).\\n\\n*{i\\\\=1}^{2000}) is plotted in Fig. 3, which shows that the data points are representative on the forward invariant set of the system, and that the observer states (z*Varying the prior bound on the Lipschitz constant, the resulting training loss, validation loss, and the posterior Lipschitz bound obtained from the same training conditions are illustrated in Fig. 4\\\\. The following observations can be made from these results.\\n\\n* As anticipated, as the set bound on the Lipschitz bound increases, the Lipschitz constant of the trained neural network becomes higher. The Lipschitz constants estimated a posteriori are lower than the prior bound on the Wang\\\\-Manchester network, validating the direct parameterization approach on constraining the slope. On the other hand, the actually posterior Lipschitz constant has an increasingly large lag behind the prior bound; for example, when the prior bound is (1000\\\\), the (L\\\\_{S}) after training does not exceed (300\\\\). This indicates that even for the training objective alone, there is a \"resistance\" to pursue the maximally possible Lipschitz constant.\\n* When the Lipschitz bound is small, relaxing the restriction on (L\\\\_{S}) is beneficial for decreasing the training loss as well as the validation loss, showing that the Lipschitz bound is a bottleneck causing underfitting. When (L\\\\_{S}) is high enough, such underfitting no longer exists; instead, overfitting will appear, with rising training and\\n\\n- As anticipated, as the set bound on the Lipschitz bound increases, the Lipschitz constant of the trained neural network becomes higher. The Lipschitz constants estimated a posteriori are lower than the prior bound on the Wang\\\\-Manchester network, validating the direct parameterization approach on constraining the slope. On the other hand, the actually posterior Lipschitz constant has an increasingly large lag behind the prior bound; for example, when the prior bound is (1000\\\\), the (L\\\\_{S}) after training does not exceed (300\\\\). This indicates that even for the training objective alone, there is a \"resistance\" to pursue the maximally possible Lipschitz constant.\\n- When the Lipschitz bound is small, relaxing the restriction on (L\\\\_{S}) is beneficial for decreasing the training loss as well as the validation loss, showing that the Lipschitz bound is a bottleneck causing underfitting. When (L\\\\_{S}) is high enough, such underfitting no longer exists; instead, overfitting will appear, with rising training and\\nFig. 3: Sample collected from the Lorenz system.\\nvalidation losses. The overfitting phenomenon is more significant when the noise is large. Thus, there should be optimal values to be set as the Lipschitz bound.\\n\\\\* Depending on the noise magnitude, the deviation of posterior Lipschitz constant from the prior bound and the emergence of overfitting phenomenon occur at different threshold values of the Lipschitz bound. Thus, the Lipschitz bound to be used for neural network training should be tuned differently as the noise intensity varies. For example, at (\\\\\\\\sigma\\\\=1\\\\), a suitable choice can be (\\\\\\\\gamma\\\\=100\\\\), whereas at (\\\\\\\\sigma\\\\=5\\\\) and (\\\\\\\\sigma\\\\=10\\\\), (\\\\\\\\gamma) can be chosen as (30\\\\) and (10\\\\), respectively.\\n\\nNow suppose that at the observer design stage, the Wang\\\\-Manchester network is trained by the simulated data from a perfect digital twin of the true dynamics, i.e., (\\\\\\\\sigma\\\\=0\\\\); yet, when applying the network trained to observe the states of the physical system, the environment is noisy. In Fig. 5, the resulting loss (mean squared state observation error) is plotted against varying prior Lipschitz bounds under multiple values of the environment noise magnitude. It is seen that when the noise is low, roughly speaking, increasing (L\\\\_{S}) leads to monotonic decrease in the observation error within a large range. On the other hand, when the environment is highly noisy (e.g., when (\\\\\\\\sigma\\\\\\\\geq 3\\\\)), the Lipschitz bound has a severe effect on the generalization loss, and since the achievable performance is restrictive, the fine\\\\-tuning of Lipschitz bound as a hyperparameter becomes critical.\\n\\nFinally, the performance of the state observer is examined. Consider using the network trained with noiseless simulation data under the prior Lipschitz bound (L\\\\_{S}\\\\=10\\\\), and applying it to environments with noise (\\\\\\\\sigma\\\\=0\\\\.1\\\\), (0\\\\.3\\\\), (1\\\\.0\\\\), (3\\\\.0\\\\). The trajectories of the three components of estimated states by the observer are plotted against the true states in Fig. 6, within a time horizon of (10\\\\) time units. Naturally, when (\\\\\\\\sigma) is low, the state estimates can well track the true states and capture the trends in the correct directions; as (\\\\\\\\sigma) increases, the accuracy is lowered and the signals constructed by the observer are more noisy, occasionally yielding incorrect directions of evolution (e.g., on (3\\\\<t\\\\<4\\\\) or (8\\\\<t\\\\<9\\\\), where the states swing between the two foils of the Lorenz attractor). Overall, the state estimates mollifies the true state trajectories, which is due to the structure of our KKL observer \\\\- a linear filter (LTI system) as the state dynamics and a Lipschitz\\\\-bounded neural network as the static output map.\\n\\n' children=[]\n",
      "header='h2' title='V Conclusions and Discussions' text=\"This work leverages the recent tools of Lipschitz\\\\-bounded neural networks for the synthesis of nonlinear state observers in a model\\\\-free setting. The observer, which has a Kazantzis\\\\-Kravaris structure, turns out to have a provable generalization performance that is related to the Lipschitz constant of the trained neural network (which represents the mapping from the observer states to the plant states). As such, by varying the Lipschitz bound and re\\\\-training the neural network, the optimal training result can yield the minimum generalized state observation error. The importance of bounding the Lipschitz constant has been demonstrated by a numerical case study on the Lorenz system.\\n\\nFig. 4: Loss and Lipschitz constants under different prior Lipschitz bounds. (Blue wedges: training loss, blue circles: validation loss, green circles: prior Lipschitz bound; green wedges: posterior Lipschitz bound.)\\n\\nFig. 5: Errors of noiselessly trained observers in noisy environments.\\nWe implicitly assumed here that a simulator of the dynamics is available, so that the true states' trajectories can be used to train the neural network. However, such ground truth for supervised learning may not actually exist in real applications, i.e., only inputs and outputs are recorded, yet a state observation mechanism is still needed or desired for feedback control. To this end, the author's recent work \\\\[32] proposed a data\\\\-driven KKL observer by appending a kernel dimensionality reduction scheme to the LTI dynamics, thus obtaining estimates that are diffeomorphic to the states.\\n\\nAlso, the current approach is yet restricted to autonomous systems. For control purposes, it should be further extended to non\\\\-autonomous ones, where the Bernard\\\\-Andrieu observer structure \\\\[18] is anticipated. Also, the application of such data\\\\-driven state observers to learning control\\\\-relevant properties of nonlinear dynamical systems and controller synthesis \\\\[33, 34] is undergoing active research.\\n\\n\" children=[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idx = 0 ## Case 1\n",
    "# idx = 14 ## Case 2\n",
    "sample = df.iloc[idx]['markdown']\n",
    "\n",
    "toc = md2py(sample) \n",
    "## Test Filters\n",
    "sections = []\n",
    "found_filter = False\n",
    "for filter_cls in BaseArxivPaperSectionSplitter.__subclasses__():\n",
    "    try:\n",
    "        if filter_cls.is_type(toc):\n",
    "            print(\"FOUND\",filter_cls)\n",
    "            found_filter = True\n",
    "            sections = filter_cls().split(toc)\n",
    "    except Exception as e:\n",
    "        print(\"ERROR\")\n",
    "        raise e\n",
    "    \n",
    "print(\"SECTIONS:\", len(sections))\n",
    "for section in sections:\n",
    "    print(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [11:50<00:00, 14.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 10000 no filter 1520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Test Filters\n",
    "no_filter_idxs = []\n",
    "for idx in tqdm(range(df.shape[0])):\n",
    "    sample = df.iloc[idx]['markdown']\n",
    "    toc = md2py(sample)\n",
    "    sections = []\n",
    "    found_filter = False\n",
    "    for filter_cls in BaseArxivPaperSectionSplitter.__subclasses__():\n",
    "        try:\n",
    "            if filter_cls.is_type(toc):\n",
    "                # print(\"FOUND\",filter_cls)\n",
    "                found_filter = True\n",
    "                sections = filter_cls().split(toc)\n",
    "        except Exception as e:\n",
    "            print(\"ERROR\")\n",
    "            raise e\n",
    "        if not found_filter:\n",
    "            no_filter_idxs.append(idx)\n",
    "print(\"total {} no filter {}\".format(df.shape[0], len(no_filter_idxs)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 18, 25]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_filter_idxs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Chart Understanding in Vision and Language Task via Cross-modal Pre-training on Plot Table Pairs\n",
      "\n",
      "###### Abstract\n",
      "\n",
      "Building cross-model intelligence that can understand charts and communicate the salient information hidden behind them is an appealing challenge in the vision and language (V+L) community. The capability to uncover the underlined table data of chart figures is a critical key to automatic chart understanding. We introduce ChartT5, a V+L model that learns how to interpret table information from chart images via cross-modal pre-training on plot table pairs. Specifically, we propose two novel pre-training objectives: Masked Header Prediction (MHP) and Masked Value Prediction (MVP) to facilitate the model with different skills to interpret the table information. We have conducted extensive experiments on chart question answering and chart summarization to verify the effectiveness of the proposed pre-training strategies. In particular, on the ChartQA benchmark, our ChartT5 outperforms the state-of-the-art non-pretraining methods by over \\(8\\%\\) performance gains.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Chart figures serve as the visual summary of tabular data, which helps to convey rich context in various documents, such as scientific papers, textbooks, and technical news. An intelligent agent that can understand and communicate chart plots can lead to many useful applications. For example, a virtual doctor who knows how to answer the patient's question on a complex medical report or a reading assistant who can summarize the key findings from scientific papers in brief language. In the past few years, there has been a growing interest in our community to explore chart understanding in vision and language (V+L) tasks and many related benchmarks like Chart Question Answering **(CQA)**Masry et al. (2022); Kafle et al. (2018); Methani et al. (2020) and Chart Summarization **(CS)**Kantharaj et al. (2022) are introduced.\n",
      "\n",
      "While prevalent in the research community, automatic chart understanding remains a challenging problem due to its complex compositions of various shapes, lines, colors, and scene text. Although tremendous success is achieved in the V+L research, applying these existing methods to handle chart-related tasks is hard. Recent research ChartQA Masry et al. (2022) and Chart-to-Text Kantharaj et al. (2022) attempt to first convert chart images to their underlined tables and use the extracted tables to perform chart-related V+L task. As the extracted tables always have clean and organized structures, it makes extracting relevant information to solve downstream reasoning tasks much more accessible. Empirically, using tables yields promising results on both CQA and CS.\n",
      "\n",
      "Despite valuing table as a significant ingredient for chart understanding, we have two main concerns about this approach: (1) Automatic table extraction is unreliable. Existing methods Luo et al. (2021); Kato et al. (2022) are often limited to work on a few particular types of chart images and do not generalize well. Moreover, the extracted table is likely to contain incorrect noisy predictions that potentially harm the performance of the following task. (2) In most cases, the whole table is optional for resolving the chart-related V+L task. As illus\n",
      "\n",
      "Figure 1: A data sample from the ChartQA dataset. The corresponding chart table is displayed in the top right corner.\n",
      "trated in Fig 1, to answer the question _\"What is the value of India Bar\"_, the model just needs access to the second row to give the correct answer. In contrast, having redundant table information makes finding the relevant information challenging. To better leverage the table data, we argue that it is important to equip the V+L model with the capability to dynamically interpret the table value from the chart information.\n",
      "\n",
      "Therefore, in this paper, we propose **ChartT5**, an OCR-based image-to-text generation model pre-trained on a self-collected chart table pairs corpus. More specifically, ChartT5 learns how to uncover a masked table with two proposed pre-training objectives: Masked Header Prediction (MHP), and Masked Value Prediction (MVP). MHP helps improve the model's capability of linking scene text to the corresponding table headers. MVP requires the model to perform mathematical reasoning over chart structure units and the scene text to predict the correct data value.\n",
      "\n",
      "We evaluate our ChartT5 on two tasks and benchmarks: ChartQA and Chart-to-Text. In ChartQA, ChartT5 outperforms all the non-pretraining methods that use extracted tables by at least \\(8\\%\\) performance gains. ChartT5 also beats the pre-training table-based methods, which demonstrates the effectiveness of the proposed pre-training strategies. On Chart-to-Text, ChartT5 consistly outperforms the existing SOTA on the content selection metrics Barzilay and Lapata (2005) which values the model's capability to extract the critical information from the chart.\n",
      "\n",
      "In summary, our contributions are summarized below:\n",
      "\n",
      "* We propose chart-to-table pre-training for V+L model to learn the capability of interpreting table data from the chart.\n",
      "* We demonstrate that the pre-trained model consistently outperforms table-based methods on two chart understanding tasks.\n",
      "* We conduct comprehensive ablation studies to validate the effectiveness of chart-to-table pre-training and the proposed pre-training objectives.\n",
      "\n",
      "## 2 Related Work\n",
      "\n",
      "### Vision and Language Research on Charts\n",
      "\n",
      "Researching chart understanding in V+L tasks is a popular field nowadays. The most prevalent problem is chart question answering (CQA) Kafle et al. (2018); Kahou et al. (2018); Methani et al. (2020); Masry et al. (2022); Chaudhry et al. (2020), where researchers build models to answer complex questions on chart images. Another popular one is chart summarization (CS) Kantharaj et al. (2022); Obeid and Hoque (2020), which requires machine learning models to create a summary of key insights conveyed by a chart. Hsu et al. (2021) collected a large-scale scientific figures captioning dataset from research papers where many images are chart plots.\n",
      "\n",
      "There are two main approaches for chart vision and language tasks. The first approach adapts existing visual question answering (VQA) and image captioning models to CQA and CS tasks with some specialized designs for chart images Kafle et al. (2020); Singh and Shekhar (2020); Chaudhry et al. (2020); Kafle et al. (2018); Hsu et al. (2021); Spreafico and Carenini (2020). The other approach assumes the table data of charts is accessible from the dataset Kim et al. (2020); Masry (2021) or can be extracted from the chart images using vision to table techniques Methani et al. (2020); Masry et al. (2022); Kantharaj et al. (2022). Then, the researchers will either use a table-to-text generation model Kim et al. (2020); Masry (2021); Methani et al. (2020) or combine the embedding of tables and charts via a multi-modal fusion method to generate the text output Masry et al. (2022); Kantharaj et al. (2022). It is clear from these efforts that adding tables as the additional representation of charts will dramatically improve the model's capability to understand and interpret chart information.\n",
      "\n",
      "Following the table-based approach, we also value the information provided by the underlined table data of chart images. However, instead of directly concatenating the extracted table into the chart understanding model, we facilitate our model with the capability to interpret the table data from chart images via pre-training on chart-table pairs.\n",
      "\n",
      "### Vision and Language Pre-training\n",
      "\n",
      "Vision and language pre-training has received growing interest over the past few years. Researchers build transformer-based multi-modal fusion models and perform self-supervised learning on a large-scale corpus of image-text pairs to learn robust cross-modal representations that can benefit the performance of various downstream tasks Chen et al. (2020); Lu et al. (2019); Tan and Bansal (2019);\n",
      "Su et al., 2019; Li et al., 2020; Zhang et al., 2021).\n",
      "\n",
      "While the pre-trained models achieve great success on tasks like VQA (Antol et al., 2015) and Image Captioning (Chen et al., 2015), they have only focused on the domain of natural images. However, chart understanding is still challenging for the existing vision and language methods due to their lack of knowledge of scene text and structured visual units such as \"bars\" and \"lines\".\n",
      "\n",
      "To address the limitation of conventional vision and language pre-training, TAP (Yang et al., 2021) and PreSTU (Kil et al., 2022) propose OCR-based vision and language pre-training frameworks that focus on scene text understanding in natural images where they design various pre-training objectives around the extracted OCR texts. Most recently, Donut (Kim et al., 2022) and Pix2Struct (Lee et al., 2022) propose OCR-free pre-training frameworks, where the pre-trained model directly generates a text output from a raw image input. Donut focuses on document image (_e.g._, receipt) understanding, and Pix2Struct aims to handle broader types of synthetic images that contain visually-situated texts such as infographics and user interfaces via parsing web-page screenshots into their HTML Code. Different from these works, we take the first step to explore vision and language pre-training that focuses on chart image understanding. Specifically, we propose novel pre-training objectives to parse charts to their underlined tables.\n",
      "\n",
      "## 3 Method\n",
      "\n",
      "In this section, we first introduce the dataset for pre-training. We then go over our ChartT5 model architecture and pre-training objectives to predict masked tables from the chart and OCR information.\n",
      "\n",
      "### Pre-training Dataset Collection\n",
      "\n",
      "To collect large-scale pairs of chart-table data, we collect synthetic data from existing chart question-answering corpora, including PlotQA (Methani et al., 2020), DVQA (Kafle et al., 2018), and FigureQA (Kahou et al., 2018). Specifically, DVQA and FigureQA render chart images from synthetic tables that are randomly generated from limited vocabularies. PlotQA first scrapes tables from online resources like World Bank Open Data and then synthesizes the charts from the scraped data, where the tables and charts contain more diverse language information. Our pre-training corpus consists of 495K chart-table pairs, which cover a diverse range of chart types. Our pre-training corpus contains three chart types: bar, line, and pie. The distribution of different chart types from the three chart question-answering benchmarks is summarized in table 1.\n",
      "\n",
      "### Model Overview\n",
      "\n",
      "ChartT5 is an extension of the existing V+L Pre-training framework, VLT5 (Cho et al., 2021), an encoder-decoder architecture that unifies the vision-language tasks as text generation conditioned on multi-modal inputs. Given a chart image, we first extract the scene texts. For the synthetic chart images that are collected from DVQA (Kafle\n",
      "\n",
      "\\begin{table}\n",
      "\\begin{tabular}{l|c c c|c} \\hline \\hline Type & PlotQA & DVQA & FigureQA & Total \\\\ \\hline Bar & 142,587 & 204,514 & 40,000 & 387,101 \\\\ Line & 48,133 & 0 & 40,000 & 88,133 \\\\ Pie & 0 & 0 & 20,001 & 20,001 \\\\ \\hline \\hline \\end{tabular}\n",
      "\\end{table}\n",
      "Table 1: Distribution of the three chart types: bar, line, and pie from different resources in the pre-training corpus.\n",
      "\n",
      "Figure 2: An Overview of ChartT5. Given the input chart image and the extracted OCR tokens, ChartT5 predicts the masked values of the table in the output.\n",
      "et al., 2018), FigureQA (Kahou et al., 2018), and PlotQA (Methani et al., 2020), the ground-truth scene texts are available. The visual context is then represented as combining visual features extracted from the chart image and the language features obtained on the detected scene text. We then flat the paired table of the chart image into a string and extract the text features via the language encoder. The multi-modal features are then concatenated and fused via the multi-layer encoder, and the output hidden vectors can then be used for various pre-training tasks.\n",
      "\n",
      "#### 3.2.1 Chart Image Encoder\n",
      "\n",
      "Given an input chart image, to recognize the critical marks (_e.g._, bars and lines) of chart images, we first utilize a pre-trained Mask R-CNN object detector from (Masry et al., 2022) to extract the visual region features \\(\\mathbf{v}=\\{v_{1},v_{2},\\cdots,v_{l^{v}}\\}\\). Next, the chart object detector is trained on the synthetic chart images from the previous CQA datasets (Kahou et al., 2018; Kafle et al., 2018; Masry et al., 2022; Methani et al., 2020) which is defined to identify 15 chart-related objects1. For each detected object region, we also extract location features as a 5-d vector: \\([\\frac{x_{1}}{W},\\frac{y_{1}}{H},\\frac{x_{2}}{W},\\frac{y_{2}}{H},\\frac{(y_{2} -y_{1})(x_{2}-x_{1})}{W.H}]\\), which denotes the normalized top left coordinates, bottom right coordinates, and the normalized area of the detected region box. The position feature is then fed through fully-connected layers to be projected to the visual region feature embedding space. The final representation of the visual feature is obtained by summing up the projected region feature and corresponding location feature.\n",
      "\n",
      "Footnote 1: These 15 categories are: Legends, yAxisTitle, ChartTitle, xAxisTitle, LegendPreview, PlotArea, yAxisLabel, xAxisLabel, LegendLabel, PieLabel, bar, pie, pieSlice, line, and dotLine.\n",
      "\n",
      "#### 3.2.2 OCR Encoder\n",
      "\n",
      "After extracting the list of OCR words from the chart image, we obtain a set of OCR text embeddings \\(\\mathbf{o}=\\{o_{1},o_{2},\\cdots,o_{l^{o}}\\}\\) via a learned word embedding layer. We also get each OCR token's 5-d position vector similar to the visual position vector from the OCR token's detected bounding box. We then obtain the position embedding vector using the shared projecting layer from the Chart Image Encoder. The shared position encoding mechanism between OCR tokens and chart object regions would help the model to capture their relative positional relations, which is a critical clue to predict the table data from the chart image. For example, the bar associated with an x-axis label should share a similar x-coordinate position in a vertical bar chart. The final OCR embedding vector is gained by summing up the OCR text token embeddings and the OCR position embedding.\n",
      "\n",
      "#### 3.2.3 Language Encoder\n",
      "\n",
      "Following the setting of the original VLT5 (Cho et al., 2021), we add a prefix to the flattened underlying table to indicate different pre-training tasks. We then get the table token embeddings \\(\\mathbf{t}=\\{t_{1},t_{2},\\cdots,t_{l^{t}}\\}\\) with a shared word embedding layer. We apply the original T5's (Raffel et al., 2020) relative position bias to obtain the position information of each token in the caption and the flattened table. We know that the tables have very different structures compared to natural language captions, and several efforts are exploring specialized position embeddings for tables (Yin et al., 2020; 1). We leave the exploration of the specialized table position embedding for chart table pre-training in the future.\n",
      "\n",
      "**Scene Text Copy Mechanism.** A critical ingredient to the success of chart-to-table translation is the ability to predict the table headers from the corresponding OCR texts. For example, in the horizontal bar chart, the table column header is usually obtained from the x-axis labels, and the row header is often copied from the legend labels. Although presenting OCR text and the table to the model helps link the shared OCR tokens and table values, generating the correct table prediction from the corresponding OCR source is still challenging due to the large candidate token vocabulary. To encourage direct copy from the OCR text to the associated table cell value, we introduce OCR sentinel tokens \\(\\{<\\text{ocr\\_1}>,<\\text{ocr\\_2}>,\\cdots,<\\text{ocr\\_1}^{o}>\\}\\), which corresponds to the detected OCR texts. As illustrated in Figure 2, we replace each OCR token with a unique corresponding OCR sentinel token. Then, for every OCR token, we find if there is a matched existing table cell value. If a matched pair is found, we replace the table cell value with its paired OCR sentinel token. During pre-training, as all the plot images are synthesized from a paired table, the one-to-one scene text to table value mapping is already provided. With this prepossessing procedure, we successfully distinguish the table values that are copied from OCR tokens and those that need to be generated from the general token vocabularies, encouraging more accurate table pre\n",
      "diction from the relevant resources.\n",
      "\n",
      "### Pre-training Objectives\n",
      "\n",
      "Given the chart-table pairs, we propose Masked Header Prediction (MHP) and Masked Value Prediction (MHP) to teach the model to recover incomplete tables with the chart information. Specifically, this objective aims to predict a masked table token \\(t_{m}\\) with the remaining table info \\(t_{\\backslash m}\\) as well as the chart image region \\(\\mathbf{v}\\) and the scene text \\(\\mathbf{o}\\). Compared to the traditional masked language modeling applied to the natural language text, we adjust the table masking strategy based on two hypotheses: (1) We alternatively mask just the table headers or numerical table values, as we think interpreting these two types of information requires different skills. Predicting table headers requires retrieving the correct scene text, while predicting numerical table values depends more on the capability to conduct mathematic reasoning over both the visual elements and the scene text. Therefore, it is better to format them as two separate pre-training objectives. (2) We increase the masking rate from 15\\(\\%\\) to 45\\(\\%\\), as the masked table token has less dependence on the surrounding table values.\n",
      "\n",
      "## 4 Experiment\n",
      "\n",
      "In this section, We detailed our experiment setups to evaluate the proposed ChartT5 on two tasks: chart question answering and chart summarization. We then introduce the main results of the two evaluation tasks. Finally, we present the ablation study on chart-table pre-training and the two pre-training objectives.\n",
      "\n",
      "**Chart Question Answering.** Given a chart image and a query question, the goal for the model is to provide an accurate answer string by interpreting the provided chart image. For this task, we consider the ChartQA dataset Masry et al. (2022), which collects question-answer pairs on realistic chart images scraped from the internet. Their annotations are collected in two fashions: (1) Human-written question-answer pairs; and (2) machine-generated question-answer pairs derived from the human-written chart summaries. In total 32.7K question-answer pairs are collected on 21.9K scraped chart images, where about 9.6K question-and-answer pairs are human-written. Compared to the previously collected CQA datasets, ChartQA is more challenging to handle due to the diverse visual style from the realistic chart images and the complex language from human annotations. Following previous work Masry et al. (2022); Methani et al. (2020), we also apply the relaxed accuracy to measure the performance on the CQA task, which allows a minor inaccuracy on numerical value prediction (within 5\\(\\%\\) of the gold answer). For non-numerical answers, the prediction needs to be exactly matched to the gold-standard answer.\n",
      "\n",
      "**Chart Summarization.** Given a chart image, the target is to summarize the key insights of the chart in natural language. For this task, we evaluate our model on the most recently proposed Chart-to-Text benchmark Kantharaj et al. (2022), which collects roughly 36.5K chart images with one summary for each image. They split the collected charts into two sets: Statista and Pew, representing the two separate websites from which the chart plots come. The summaries in Statista are human-written which is well grounded on the chart image. Meanwhile, the summaries from Pew are automatically extracted from the news paragraphs surrounding the chart images. Pew is noisier and more challenging to handle. We follow Kantharaj et al. (2022) to split the two sets for training and testing. We adopt BLEU-4, Content Selection, and CIDER as the evaluation metrics to measure the quality of the generated summary following Kantharaj et al. (2022).\n",
      "\n",
      "**Implementation details.** We initialized our ChartT5 from T5\\({}_{\\text{base}}\\) and pre-trained on our self-collected corpus for 30 epochs with a batch size of 60. We used Adam optimizer Kingma and Ba (2015) with a linear warm-up for the first 5\\(\\%\\) training steps, and the peak learning rate is set as 1e-4. After warming up, a linear decayed learning-rate scheduler gradually drops the learning rate for the rest of the training steps. The pre-training experiments are conducted on 2 Nvidia TITAN RTX GPUs, and it roughly takes two days to accomplish the experiment. We kept the last checkpoint of each pre-training run as our final checkpoint for fine-tuning.\n",
      "\n",
      "We also applied warming-up for downstream fine-tuning to gradually increase the learning rate to the pick value during the first 5\\(\\%\\) of training epochs. After that, a linear decayed learning-rate scheduler gradually drops the learning rate for the remaining training. For CQA task, we set batch size as 24 and fine-tune ChartT5 for 60 epochs with a peak learning rate 2e-4 on 2 Nvidia TITAN RTX GPUs. The best checkpoint was saved as the one that achieves the highest accuracy on the validation\n",
      "split. On the CS task, we use batch size 20 and a peak learning rate 5e-5. On the Pew split, we fine-tune ChartT5for 20 epochs, and on Statista, we fine-tune ChartT5for 25 epochs. The best checkpoint is also saved as achieving the best BLEU score on the validation split. All the reported numbers are one-time runs.\n",
      "\n",
      "### Main Results\n",
      "\n",
      "We first compare ChartT5 to various state-of-the-art methods with or without pre-training on the two downstream tasks.\n",
      "\n",
      "#### 4.1.1 Evaluation on CQA\n",
      "\n",
      "We compare ChartT5 with SOTA non-pretraining and pre-training methods on CQA tasks. The best-performed non-pretraining baselines are introduced in (Masry et al., 2022). The authors first predict the table data from the chart image via an automatic data extraction tool (Luo et al., 2021). Then they extend various language-only models (T5, Tapas) and multi-modal models (VLT5, VisionTapas) to predict the answer conditioned on the extracted table. On the line of pre-training baselines, we compare to VLT5\\({}_{pre}\\) and VisionTapas\\({}_{pre}\\) which pre-trains VLT5 and Vision Tapas on PlotQA with the visual question answering tasks. We also compare chartT5 to the current SOTA method Pix2Struct which is pre-trained on 80 million webpage screenshots to HTML code parsing objectives. The result is summarized in Table 2.\n",
      "\n",
      "**Comparison to Non-Pretraining Method** Even without access to the predicted tables, ChartT5 has outperformed all non-pretraining methods by a large margin (a minimum 7.3\\(\\%\\) gain on the overall performance). ChartT5 also outperforms all non-pretraining baselines on the human-written questions and machine-generated questions. Although the predicted table covers 54\\(\\%\\) of the answers in the test data of ChartQA, simply feeding it as an input does not make the existing models fully leverage the valuable information. The significant improvement achieved by ChartT5 indicates the effectiveness of the proposed pre-training to help the model to obtain the relevant table information for chart understanding.\n",
      "\n",
      "**Comparison to Pre-training Method** Although the performance of VLT5 and VisionTapas is improved significantly by pre-training on additional CQA data, ChartT5 still outperform them by at least 1.3\\(\\%\\). Specifically, on machine-augmented questions, ChartT5 outperforms VLT5\\({}_{pre}\\) by 8\\(\\%\\). However, both visionTapas\\({}_{pre}\\) and VLT5\\({}_{pre}\\) achieve better accuracy on the human split, which means that the in-domain question answering objectives helps the model to improve the numerical reasoning capability. ChartT5 underperforms Pix2Struct by 2.3\\(\\%\\) on the overall test split. However, pix2struct is pre-trained on a more than 100 times larger pre-training corpus than the rest of the pre-training methods. Given the same scale of the pre-training dataset, we expect to gain additional performance improvement, and we leave this for future exploration.\n",
      "\n",
      "#### 4.1.2 Evaluation on Chart Summarization\n",
      "\n",
      "For the chart summarization task, we compare ChartT5 to the best non-pretraining approaches introduced in (Kantharaj et al., 2022). Given a chart image, The authors build the chart summarization models by extending the pre-trained language generation model T5 (Raffel et al., 2020) and BART(Lewis et al., 2019) whose generation processes are conditioned on: (1) a set of scene texts extracted by a trained OCR detector. (2) the ground truth table that is paired with the chart. The evaluation result is summarized in Table 3.\n",
      "\n",
      "From Table 3, we can see that on Statista, ChartT5 outperforms all baseline methods on BLUE score, but only a slight improvement is achieved over the best baseline. On Pew, ChartT5 underperforms T5-OCR by almost 1.5 percent. The proposed ChartT5 also slightly underperforms against the baseline methods in CIDER on both datasets. However, ChartT5 consistently outperforms all baselines on content selection scores\n",
      "\n",
      "\\begin{table}\n",
      "\\begin{tabular}{l|c c c} \\hline \\hline \\multirow{2}{*}{Model} & \\multicolumn{3}{c}{ChartQA} \\\\  & Human & Augment & Overall \\\\ \\hline T5 & 25.12 & 56.96 & 41.56 \\\\ Tapas & 28.72 & 53.84 & 41.28 \\\\ VLT5 & 26.24 & 56.88 & 41.56 \\\\ VisionTapas & 29.60 & 61.44 & 45.52 \\\\ \\hline VLT5\\({}_{pre}\\) & **40.08** & 63.60 & 51.84 \\\\ VisionTapas\\({}_{pre}\\) & 32.56 & 61.60 & 47.08 \\\\ Pix2Struct & - & - & **56.00** \\\\ ChartT5 & 31.8 & **74.4** & 53.16 \\\\ \\hline \\hline \\end{tabular}\n",
      "\\end{table}\n",
      "Table 2: Evaluation results on ChartQA. We report relaxed accuracy on the test split annotated by humans and that generated by the machine. In the last column, we report the overall accuracy by computing the mean values with human split and augment split.\n",
      "across both Statista and Pew sets. The under-performance on BLEU and CIDER indicates that Chart-table pre-training is limited to benefit high-quality natural language generation. However, the strong performance on content selection, which values the key information appearance in the generation, suggests the advantage of chart-table pre-training on extracting relevant chart information. Therefore, a potential direction to explore is combining different types of pre-training objectives, such as chart-to-text pre-training and chart-table pre-training goals, to facilitate the model with diverse strengths.\n",
      "\n",
      "### Ablation Study\n",
      "\n",
      "We conduct ablation experiments to validate the effectiveness of chart-table pre-training and the pre-training objectives. We also evaluate the effectiveness of the proposed scene text copy mechanism.\n",
      "\n",
      "#### 4.2.1 Chart-Table Pre-training\n",
      "\n",
      "We conduct detailed analyses on the effectiveness of chart-table pre-training. First, we measure the performance gain from the chart-table pre-training on the full test set of ChartQA data. We then study what type of questions benefit most from the chart-table pre-training by picking three subsets of questions that measure different capabilities of the model: (1) Human-written questions, (2) Machine-generated questions, and (3) Table covered questions, where the answers can be directly found in the ground truth tables. The results are summarized in Table 4. From Table 4, we find that after chart-table pre-training the model's performance on these three sets of questions is all improved. The most significant gain is obtained on machine-generated questions, which mainly focus on extractive-type questions. This indicates that chart-table pre-training benefits the model to localize and retrieve the requested information presented on Chart Image. The second biggest gain is achieved on table-cover questions, where the model demonstrates significant improvement in the capability of chart-to-table interpretation.\n",
      "\n",
      "#### 4.2.2 Pre-training Objectives\n",
      "\n",
      "We validate the effectiveness of the two pre-training objectives, Masked Header Prediction and Masked Value Prediction. We remove one pre-training objective at a time and pre-train the ChartTSwith only one table prediction task. The pre-trained model\n",
      "\n",
      "\\begin{table}\n",
      "\\begin{tabular}{l|c c c} \\hline \\hline \\multirow{2}{*}{Pretraining?} & \\multicolumn{3}{c}{Question Types} \\\\  & Table & Human & Augment \\\\ \\hline No & 60.7 & 30.8 & 66.7 \\\\ Yes & **64.7** & **31.8** & **74.4** \\\\ \\hline \\hline \\end{tabular}\n",
      "\\end{table}\n",
      "Table 4: Ablation Study on Chart Table Pre-training with ChartQA Dataset. We report results on three subsets of questions: Table cover questions, human-written questions, and machine-generated questions.\n",
      "\n",
      "\\begin{table}\n",
      "\\begin{tabular}{l|c c c|c c c} \\hline \\hline \\multirow{2}{*}{Model} & \\multicolumn{3}{c|}{Statista} & \\multicolumn{3}{c}{Pew} \\\\  & BLEU & CS & CIDER & BLEU & CS & CIDER \\\\ \\hline T5-OCR & 35.29 & 73.77 & 4.43 & **10.49** & 40.87 & **2.20** \\\\ BART-OCR & - & - & - & 9.09 & 39.99 & 1.97 \\\\ T5-TAB & 37.01 & 75.72 & **4.68** & - & - & - \\\\ BART-TAB & 36.36 & 77.14 & 4.40 & - & - & - \\\\ ChartT5 & **37.51** & **82.16** & 3.45 & 9.05 & **55.1** & 1.23 \\\\ \\hline \\hline \\end{tabular}\n",
      "\\end{table}\n",
      "Table 3: Evaluation results on Chart Summarization. We display BLEU, CS and CIDER scores for the Pew and Statista Split. The ground truth table is not available to Pew thus the table-based method does not have results on Pew split.\n",
      "\n",
      "\\begin{table}\n",
      "\\begin{tabular}{l|c c c} \\hline \\hline  & \\multicolumn{3}{c}{Question Types} \\\\  & Human & Augment & Overall \\\\ \\hline Full & 31.8 & 74.4 & 53.1 \\\\ - MVP & 30.9 & 73.7 & 52.3 \\\\ - MHP & 31.2 & 68.3 & 49.7 \\\\ - STC & 30.8 & 72.4 & 51.6 \\\\ \\hline \\hline \\end{tabular}\n",
      "\\end{table}\n",
      "Table 5: Ablation Study on the two proposed pre-training objectives and the Scene Text Copy Mechanism (STC). The first row is the result of the full ChartT5 model. Then we remove one of the pre-training objectives and the scene-text-copy mechanism. We report the results of different ablation experiments on both human and machine-generated splits as well as the overall performance.\n",
      "is then fine-tuned and evaluated on the human and augmented split for comparison. The result is displayed in table 5. As can be seen from the table, removing Masked Value Prediction Loss has a negligible impact on the performance of ChartT5 on ChartQA dataset. There is a slightly more drop in human written questions which suggests that predicting table numerical values still has a miner positive impact on helping the model's mathematical reasoning. Remove Masked Header Prediction have a significant impact on the machine-generated question-answering accuracy. As expected, Masked header modeling mainly helps the model learn how to link the scene text to the table headers, which is a critical ability to extract relevant information given a specific query.\n",
      "\n",
      "#### 4.2.3 Scene Text Copy\n",
      "\n",
      "We also validate the effectiveness of the scene-text-copy mechanism, where we train a ChartT5model by simply representing OCR tokens in their original text format. The model is fine-tuned and evaluated on the human and augmented split of the chartQA dataset to compare against the full ChartT5. The result is displayed in Table 5. Disabling the scene-text-copy mechanism leads to a 1.5\\(\\%\\) overall performance drop on ChartQA tasks. Specifically, it leads to more degradation on the augmented split than the human split, as scene-text-copy helps enhance the alignment between OCR and table values to benefit accurate information extraction from the chart.\n",
      "\n",
      "### Qualitative Error Analysis\n",
      "\n",
      "We have manually analyzed model predictions to understand its limitation. We found that our model suffers most from noisy OCR detection and complex question that requires multi-hop reasoning.\n",
      "\n",
      "**Noisy OCR Prediction.** As an OCR-based model, ChartT5 often suffers from a wrong OCR detection. An example is shown in Figure 3; the model localizes the right scene text \"1.18\" to answer the question, but the OCR text is mistakenly detected as \"1:18\". To further understand the limitation of OCR detection, we randomly sample 20K PlotQA test split and compare the performance of our model using detected OCRs against Ground Truth OCRs. We observe a 5\\(\\%\\) performance drop when using detected OCRs. We can improve the OCR detector for future work by training on a large Plot scene-text detection benchmark. Another promising direction is to attempt OCR-free end-to-end plot recognition method like Pix2Struct (Lee et al., 2022).\n",
      "\n",
      "**Multi-Hop Reasoning.** Our model is also quite vulnerable to handling complex questions requiring multi-hop reasoning. An example is shown in Figure 4; the model cannot perform the complex logic reasoning to add the stats of the two smallest bars and compare that to the large bar. We will consider exploring pre-training on the mathematic reasoning datasets to address this limitation.\n",
      "\n",
      "## 5 Conclusion\n",
      "\n",
      "We propose ChartT5 to enhance the vision language model's ability to understand chart images via chart-table pre-training. The model learns to interpret the masked tables via our proposed masked header prediction and masked value prediction objectives. ChartT5 achieves significant improvement over table-based non-pretraining SOTA methods on the ChartQA dataset, especially on the extractive question sets. We also achieve a new SOTA Content Selection Score on the Chart-to-text summarization dataset. We conduct comprehensive ablation studies to identify the impact of chart-table pre-training, and we find that the proposed pre-training is extremely helpful to extract accurate\n",
      "\n",
      "Figure 4: An error prediction from our model due to complex multi-hop reasoning\n",
      "\n",
      "Figure 3: An error prediction from our model due to noisy OCR prediction\n",
      "information from the Chart. For future research directions, we believe it may also be meaningful to explore chart understanding under data-efficient settings Hsu et al. (2022); Zeng et al. (2023) and for evidence retrieval tasks Lu et al. (2022); Ji et al. (2023).\n",
      "\n",
      "## 6 Limitations\n",
      "\n",
      "Although introducing chart value prediction objective, it only provides minor improvement to the model's performance on doing complex reasoning. There is still a large room to improve the model's capability in math calculation. Our model also suffers from the noisy OCR prediction of off-the-shelf object detector, whose performance will depend highly on the extracted OCR text qualities. Another possible limitation of our approach is the quality of the pre-training data, which only contains synthetic images. Although the proposed model works fairly well on the ChartQA dataset, it is unclear if the improved performance can be generalized to other realistic chart images.\n",
      "\n",
      "## 7 Ethics Statement\n",
      "\n",
      "When we collect the pre-training dataset, we ensure we respect the intellectual property of dataset sources. All the ChartQA dataset we used for the collection of chart-table pairs allows public access for research. To ensure the reproducibility of our experiment results, we provide details of the hyperparameter setting in our paper, and we will also publish our code later. Our models can mislead the public's understanding of chart content due to the potential bias from our training corpus. Therefore, we don't recommend using our model for any real-world decision on chart images.\n",
      "\n",
      "## Acknowledgement\n",
      "\n",
      "This research work is supported by U.S DARPA SemaFor Program No. HR001120C0123. The views and conclusions contained in this work only belong to the authors and should not represent the official policies implied by DARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. We also thank Ahmed Masry and Shankar Kantharaj for providing us with ChartQA and Chart Summary-related data and baseline model outputs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 14\n",
    "sample = df.iloc[idx]['markdown']\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>V Analytical characterization of (\\mathbb{L}<em>{x,M}) and (\\mathbb{H}</em>{x,M})</h2>\n",
      "None\n",
      "<h2>V Analytical characterization of (\\mathbb{L}<em>{x,M}) and (\\mathbb{H}</em>{x,M})</h2>\n",
      "'V Analytical characterization of (\\\\mathbb{L}{x,M}) and (\\\\mathbb{H}{x,M})'\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "sample_html = '''<h2>V Analytical characterization of (\\mathbb{L}<em>{x,M}) and (\\mathbb{H}</em>{x,M})</h2>'''\n",
    "soup = BeautifulSoup(sample_html, 'html.parser')\n",
    "children = list(soup.children)\n",
    "print(repr(soup))\n",
    "print(soup.string)\n",
    "print(str(soup))\n",
    "print(repr(soup.get_text()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 104\n",
    "sample = df.iloc[idx]['markdown']\n",
    "toc = md2py(sample)\n",
    "sections = []\n",
    "found_filter = False\n",
    "for filter_cls in BaseArxivPaperSectionSplitter.__subclasses__():\n",
    "    try:\n",
    "        if filter_cls.is_type(toc):\n",
    "            # print(\"FOUND\",filter_cls)\n",
    "            found_filter = True\n",
    "            sections = filter_cls().split(toc)\n",
    "    except Exception as e:\n",
    "        print(\"ERROR\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Low-ground/High ground capacity regions analysis for Bosonic Gaussian Channels\n",
      "\n",
      "###### Abstract\n",
      "\n",
      "We present a comprehensive characterization of the interconnections between single-mode, phase-insensitive Gaussian Bosonic Channels resulting from channel concatenation. This characterization enables us to identify, in the parameter space of these maps, two distinct regions: low-ground and high-ground. In the low-ground region, the information capacities are smaller than a designated reference value, while in the high-ground region, they are provably greater. As a direct consequence, we systematically outline an explicit set of upper bounds for the quantum and private capacity of these maps, which combine known upper bounds and composition rules, improving upon existing results.\n",
      "\n",
      "pacs: 03.67.-a, 03.67.Ac, 03.65.Ta.\n",
      "\n",
      "## I Introduction\n",
      "\n",
      "The efficiency of classical communication lines can be expressed using a single, simple formula [1; 2]. However, when it comes to quantum communication lines (quantum channels) that utilize quantum systems as information carriers instead of classical signals [3; 4; 5; 6], this simplification no longer holds. Instead, a multitude of different and computationally challenging capacity functionals are required to fully assess the quality of these transmission lines. For instance, the classical capacity of a quantum channel, characterizes the optimal rate of classical bits that can be reliably transferred per channel uses; the quantum capacity instead provides the optimal rate of transmitted qubits, and the private capacity, the optimal rate of bits that can be transmitted privately along the channel. In our study, we specifically focus on a special class of quantum communication lines known as Gaussian Bosonic Channels (GBCs), which are commonly employed to model communication procedures utilizing the electromagnetic field as the carrier of transmitted messages [7; 8; 9; 10]. Despite significant progress made in recent years, the analysis of GBCs still presents complex challenges. Specifically, computing the exact values of certain information capacities for these maps requires optimization techniques that remain difficult to tackle. In principle, calculating these quantities necessitates taking the limit of properly regularized entropic functionals, considering the potential utilization of entanglement across multiple channel uses [11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29]. Given these complexities, the derivation of upper and lower bounds for the capacities of significant channels represents crucial progress in the field.\n",
      "\n",
      "An established strategy for upper bounding information capacities involves utilizing data processing inequalities. For instance, it is possible to obtain an upper bound for the capacity of a specific channel by expressing it as a concatenation of channels whose capacities are already known or upper bounded [30; 31; 32; 33; 34; 35; 36; 37; 38; 39; 40; 41]. In this article, we employ this method to enhance the previously established bounds in the literature [39; 40; 41; 42] for the quantum and private capacity of single-mode, phase-insensitive Gaussian Bosonic Channels (PI-GBCs). To achieve this result, we present a detailed decomposition of the parameter space of PI-GBC maps into regions encompassing all channels that can simulate a given channel through concatenation with other PI-GBC elements.\n",
      "\n",
      "The structure of the article is as follows: In Sec. II we introduce the fundamental concepts and notation used in this article. We start by giving an overview of continuous variable quantum channels and establish crucial notation. Then, we briefly discuss various quantum capacities of a quantum channel, namely quantum capacity, private capacity, two-way quantum capacity, and secret-key capacity. Following that, we introduce phase-insensitive one-mode GBCs and establish specific notation to aid us throughout the manuscript. In Sec. III we provide a concise review of the current state-of-the-art bounds for the quantum capacity of GBCs. We discuss the main techniques used to derive these bounds, including the use of data processing inequalities and channel concatenation. We also highlight the key challenges that remain in computing the exact quantum capacity for these channels. In Sec. IV we study the parameter space of PI-GBCs in terms of channel concatenation. We present a detailed decomposition of the parameter space of PI-GBC maps into regions encompassing all channels that can simulate a given channel through concatenation with other PI-GBC elements. This analysis allows us to identify the channels that can be used to upper bound the quantum capacity of a given PI-GBC. In Sec. VI we derive new upper bounds for the quantum and private capacity of single-mode, phase-insensitive Gaussian Bosonic Channels (PI-GBCs) by employing the channel concatenation method discussed in Sec. IV. We demonstrate that our new bounds improve upon the previously established bounds in the literature, providing a more accurate estimation of the capacities for these channels. Sec. VII concludes the manuscript.\n",
      "Preliminaries\n",
      "\n",
      "A quantum communication line connecting two distant parties can be seen as a physical transformation that associates the states of a system \\(A\\), representing the input messages of the model, with the states of a second system \\(B\\), representing the associated output messages. At mathematical level such an object is described as a completely positive trace preserving (LCPTP) linear map \\(\\Lambda:\\mathcal{B}_{1}(\\mathcal{H}_{A})\\mapsto\\mathcal{B}_{1}(\\mathcal{H}_{B})\\) that links the set of the trace-class operators of the (possibly infinite dimensional) Hilbert spaces \\(\\mathcal{H}_{A}\\), \\(\\mathcal{H}_{B}\\) associated with \\(A\\) and \\(B\\) respectively [5; 6]. By Stinespring representation we can always express \\(\\Lambda\\) as a reduction of an isometry \\(\\hat{V}\\) that connects \\(\\mathcal{H}_{A}\\) to an extension \\(\\mathcal{H}_{BE}\\) of \\(\\mathcal{H}_{B}\\), i.e. \\(\\Lambda(\\cdots)=\\mathrm{Tr}_{E}(\\hat{V}\\cdots\\hat{V}^{\\dagger})\\,,\\) with \\(\\mathrm{Tr}_{E}\\) being the partial trace with respect to \\(E\\). Such a construction allows us to introduce the notion of complementary channel \\(\\tilde{\\Lambda}:\\mathcal{B}_{1}(\\mathcal{H}_{A})\\mapsto\\mathcal{B}_{1}( \\mathcal{H}_{E})\\) defined as \\(\\tilde{\\Lambda}(\\cdots):=\\mathrm{Tr}_{A}(\\hat{V}\\cdots\\hat{V}^{\\dagger})\\), which can be interpreted as the transformation induced on the environment of the communication line by the signaling process [43].\n",
      "\n",
      "Similarly to what happens in classical information theory, the efficiency of a quantum channel \\(\\Lambda\\) can be gauged in terms of a series figures of merit (the quantum capacities of the channel) that evaluate the optimal ratio between the amount of data which can be sent reliably through the channel and the total amount of redundancy needed to achieve such a goal [3; 4; 5; 6]. In this paper we focus on special instances of such quantities which in the context of continuous variable quantum information processing (see next section), admit optimal finite values even when allowing unbounded energy resources, i.e. the quantum capacity \\(Q(\\Lambda)\\), the private capacity \\(P(\\Lambda)\\), the two-way quantum capacity \\(Q_{2}(\\Lambda)\\), and the secret-key capacity \\(K(\\Lambda)\\)[6]. They are hierarchically ordered via the inequalities\n",
      "\n",
      "\\[K(\\Lambda)\\geq Q_{2}(\\Lambda),P(\\Lambda)\\geq Q(\\Lambda)\\;. \\tag{1}\\]\n",
      "\n",
      "The smallest among such terms, i.e. \\(Q(\\Lambda)\\), measures the maximum rate at which the communication line can transmit quantum information reliably over asymptotically many uses of the channel [11; 12; 13]; \\(P(\\Lambda)\\) is instead the maximum rate at which we can transmit classical messages through the channel \\(\\Lambda\\) in such a way that an external party that is monitoring the line, will not be able to read such messages [13]; \\(Q_{2}(\\Lambda)\\) represents the maximum quantum information transmission rate attainable by allowing the communicating party to use (arbitrary) distillation protocols through the use of a classical side-channel [44]; and finally the largest of these quantities, i.e. \\(K(\\Lambda)\\), describes the maximum rate at which two parties can use the channel to distill secret random string of bits.\n",
      "\n",
      "Despite being operationally well defined, no universal formula is known that allows one to explicitly compute the values of \\(Q_{2}(\\Lambda)\\) and \\(K(\\Lambda)\\) as entropic functionals. On the contrary, such characterizations exist for \\(Q(\\Lambda)\\) and for \\(P(\\Lambda)\\), based on regularized optimizations of, respectively, the output coherent information for \\(Q\\), and the Holevo information gap between \\(\\Lambda\\) and its complementary map \\(\\tilde{\\Lambda}\\), for \\(P\\). Even in these cases, however, the explicit computation of \\(Q(\\Lambda)\\) and \\(P(\\Lambda)\\) is typically rather challenging and has been carried out only a very limited set of models (in particular for the special classes of degradable and anti-degradable maps). A possible way to circumvent this problem is to make use of data-processing inequalities. Specifically a simple resource counting argument can be invoked to observe that, if a quantum channel \\(\\Lambda\\) can be expressed in terms of a LCPTP map \\(\\Lambda^{\\prime}\\) via the concatenated action of other two LCPTP linear maps \\(\\Lambda_{1}\\), \\(\\Lambda_{2}\\), then the following relations hold\n",
      "\n",
      "\\[\\Lambda=\\Lambda_{2}\\circ\\Lambda^{\\prime}\\circ\\Lambda_{1}\\quad \\Longrightarrow\\quad\\mathcal{K}(\\Lambda)\\leq\\mathcal{K}(\\Lambda^{\\prime})\\;, \\tag{2}\\]\n",
      "\n",
      "where hereafter we shall use the symbol \\(\\mathcal{K}\\) to represent an arbitrary capacity (e.g. \\(Q\\), \\(P\\), \\(Q_{2}\\), or \\(K\\)) see e.g. [4; 5; 6]. Accordingly if the capacity values of \\(\\Lambda_{1}\\) or \\(\\Lambda_{2}\\) are known, or if upper bounds for those quantities are available, we can then use (2) to constraint the performances of \\(\\Lambda\\). Alternatively, if instead the capacity of \\(\\Lambda\\) is known or if a lower bound for it is available, we can use (2) to provide lower bounds for those of \\(\\Lambda_{1}\\) and \\(\\Lambda_{2}\\). In what follows, we shall make use of this simple idea to improve the capacity analysis of a special class of quantum maps which plays an important role in quantum information theory, that is the Bosonic Gaussian Channels set, whose properties are briefly reviewed in the next subsection.\n",
      "\n",
      "### Bosonic Gaussian Channels\n",
      "\n",
      "Bosonic Gaussian Channels (BGCs) model a vast collection of noise models that tamper communication schemes which rely on the uses of e.m. signals [4; 9]. Formally they can be introduced as a special set of LCPT transformations which act on the Hilbert space \\(L^{2}(\\mathbb{R}^{n})\\) of the square integrable functions, representing the states of \\(n\\) independent harmonic oscillators each corresponding to an individual mode of the field. Indicating with \\(\\mathbf{\\hat{r}}:=(\\hat{x}_{1},\\hat{p}_{1},...,\\hat{x}_{n},\\hat{p}_{n})^{ \\mathrm{T}}\\) the set of canonical position and momentum operators of the modes, the action of a BGC map can be assigned in terms of linear mappings they induce on the first and second canonical momenta of the quantum states \\(\\hat{\\rho}\\in\\mathfrak{S}(L^{2}(\\mathbb{R}^{n}))\\) of the model, i.e. the \\(2n\\)-dimensional real vector \\(\\mathbf{m}:=\\mathrm{Tr}(\\mathbf{\\hat{r}}\\hat{\\rho})\\) and the \\(2n\\times 2n\\) real matrix \\(V:=\\mathrm{Tr}\\Big{(}\\{(\\mathbf{\\hat{r}}-\\mathbf{m}),(\\mathbf{\\hat{r}}- \\mathbf{m})^{\\mathrm{T}}\\}\\hat{\\rho}\\Big{)}\\).\n",
      "\n",
      "For what it concerns the present work we shall limit the analysis to a special subset of single-mode (\\(n=1\\)) Phase insensitive GBCs (or PI-GBCs in brief) formed by the maps \\(\\Phi_{x,M}\\) characterized by two positive noise parameters \\(x,M\\geq 0\\), whose action on the system is fully\n",
      "determined by the transformations\n",
      "\n",
      "\\[\\begin{cases}&\\mathbf{m}\\xrightarrow{\\Phi_{x,M}}\\mathbf{m}^{\\prime}=\\sqrt{x}\\; \\mathbf{m}\\;,\\\\ \\\\ &V\\xrightarrow{\\Phi_{x,M}}V^{\\prime}=xV+(2M+|1-x|)I_{2}\\;,\\end{cases} \\tag{3}\\]\n",
      "\n",
      "with \\(\\mathbf{m}^{\\prime}\\) and \\(V^{\\prime}\\) being respectively the first and second momenta of the output state \\(\\Phi_{x,M}(\\hat{\\rho})\\), and with \\(I_{2}\\) the \\(2\\times 2\\) identity matrix. For \\(x=\\eta\\in[0,1]\\), and \\(M=(1-\\eta)N\\) with \\(N\\geq 0\\), the mapping (3) corresponds to the thermal attenuator channel \\(\\mathcal{E}_{\\eta,N}\\) which describes the interaction of the a single-mode of the e.m. field with an external thermal reservoir with \\(N\\) mean photon number, mediated by a beam-splitter coupling of transmissivity \\(\\eta\\); for \\(x=g\\geq 1\\) and \\(M=(g-1)N\\) with \\(N\\geq 0\\) instead, \\(\\Phi_{x,M}\\) reduces to a thermal amplifier \\(\\mathcal{A}_{g,N}\\) which describes the interaction with the input mode with a thermal bath of mean photon number \\(N\\), through a two mode squeezing operator with gain parameter \\(g\\); finally for \\(x=1\\) and \\(M=N\\geq 0\\), \\(\\Phi_{x,M}\\) reduces to the the additive classical noise GBC \\(\\mathcal{N}_{N}\\), i.e.\n",
      "\n",
      "\\[\\begin{cases}&\\mathcal{E}_{\\eta,N}:=\\Phi_{x=\\eta,M=(1-\\eta)N},\\quad\\eta\\in[0,1],N\\geq 0\\;,\\\\ &\\mathcal{N}_{N}:=\\Phi_{x=1,M=N},\\qquad\\qquad\\qquad N\\geq 0\\;.\\\\ &\\mathcal{A}_{g,N}:=\\Phi_{x=g,M=(g-1)N},\\qquad g\\geq 1,N\\geq 0\\;.\\end{cases} \\tag{4}\\]\n",
      "\n",
      "It is easy to check that the maps \\(\\Phi_{x,M}\\) are closed under concatenation, specifically given \\((x_{1},M_{1}),(x_{2},M_{2})\\in\\mathbb{R}_{+}^{2}\\) we have that\n",
      "\n",
      "\\[\\Phi_{x_{3},M_{3}}=\\Phi_{x_{2},M_{2}}\\circ\\Phi_{x_{1},M_{1}}\\;, \\tag{5}\\]\n",
      "\n",
      "is also a channel of the model with noise parameters \\((x_{3},M_{3})\\in\\left(\\mathbb{R}^{+}\\right)^{2}\\) fulfilling the identities\n",
      "\n",
      "\\[\\begin{cases}&x_{3}=x_{2}x_{1}\\;,\\\\ &\\\\ &M_{3}=M_{2}+x_{2}M_{1}+\\frac{|x_{2}-1|+x_{2}|x_{1}-1|-|x_{2}x_{1}-1|}{2}\\;, \\end{cases} \\tag{6}\\]\n",
      "\n",
      "which we express in terms of attenuators and amplifiers in Table 1.\n",
      "\n",
      "## III A brief review on PI-GbC capacities\n",
      "\n",
      "We start by recalling that for \\(N\\) sufficiently large the channels \\(\\mathcal{E}_{\\eta,N}\\), \\(\\mathcal{N}_{N}\\), and \\(\\mathcal{A}_{g,N}\\) are Entanglement-Breaking (EB) [5; 9], specifically\n",
      "\n",
      "\\[\\left\\{\\begin{array}{l}\\mathcal{E}_{\\eta,N}\\equiv\\text{EB}\\iff\\eta\\in[0,1] \\;,N\\geq\\frac{\\eta}{1-\\eta}\\;,\\\\ \\mathcal{N}_{N}\\equiv\\text{EB}\\iff N\\geq 1\\;,\\\\ \\mathcal{A}_{g,N}\\equiv\\text{EB}\\iff g\\geq 1\\;,N\\geq\\frac{1}{g-1}\\;.\\end{array}\\right. \\tag{7}\\]\n",
      "\n",
      "In the notation (3) this translates into the condition\n",
      "\n",
      "\\[\\Phi_{x,M}\\equiv\\text{EB}\\qquad\\Longleftrightarrow\\qquad(x,M)\\in\\mathbb{EB }\\;, \\tag{8}\\]\n",
      "\n",
      "with the set\n",
      "\n",
      "\\[\\mathbb{EB}:=\\{(x,M)\\in\\left(\\mathbb{R}^{+}\\right)^{2}:M\\geq M_{\\text{EB}}(x) \\}\\;, \\tag{9}\\]\n",
      "\n",
      "defined by the threshold function\n",
      "\n",
      "\\[M_{\\text{EB}}(x):=\\min\\{1,x\\}\\;, \\tag{10}\\]\n",
      "\n",
      "(see Fig. 1). By construction EB maps have all zero capacity values, i.e.\n",
      "\n",
      "\\[(x,M)\\in\\mathbb{EB}\\quad\\Longrightarrow\\quad\\mathcal{K}(\\Phi_{x,M})=0\\;, \\tag{11}\\]\n",
      "\n",
      "with the implication that can be reversed for the two-way capacity and for the secret-key capacity [45], meaning that \\(\\mathbb{EB}\\) corresponds to the largest parameter region for which \\(Q_{2}\\) and \\(K\\) are null. The case of \\(Q\\) and \\(P\\) is different as it is known that these capacities nullify also for channels which do not belong to \\(\\mathbb{EB}\\). In particular one has that\n",
      "\n",
      "\\[\\begin{cases}Q(\\mathcal{E}_{\\eta,N})=P(\\mathcal{E}_{\\eta,N})=0\\;,\\quad\\eta\\in [0,1]\\;,N\\geq\\frac{2\\eta-1}{2(1-\\eta)}\\;,\\\\ \\\\ Q(\\mathcal{A}_{g,N})=P(\\mathcal{A}_{g,N})=0\\;,\\quad g\\geq 1\\;,N\\geq\\frac{1}{2(g-1)}\\;, \\end{cases} \\tag{12}\\]\n",
      "\n",
      "or\n",
      "\n",
      "\\[(x,M)\\in\\mathbb{AD}\\quad\\Longrightarrow\\quad Q(\\Phi_{x,M})=P(\\Phi_{x,M})=0\\;, \\tag{13}\\]\n",
      "\n",
      "with\n",
      "\n",
      "\\[\\mathbb{AD}\\;:=\\;\\{(x,M)\\in\\left(\\mathbb{R}^{+}\\right)^{2}:M\\geq M_{\\text{AD }}(x)\\}\\;, \\tag{14}\\]\n",
      "\n",
      "\\[M_{\\text{AD}}(x)\\;:=\\;\\min\\{x-1/2,1/2\\}\\;, \\tag{15}\\]\n",
      "\n",
      "corresponding to the Anti-Degradability (AD) region for the single-model PI-GBCs [46; 47; 48; 49] (see Fig. 1). Notice that at present it is still not clear whether or not \\(\\mathbb{AD}\\) is the largest set where \\(Q\\) and/or \\(P\\) nullify. What it is known are the exact values of these quantities for the special cases where \\(M=0\\). Specifically in the case of the quantum and private capacities we have [50]\n",
      "\n",
      "\\[P(\\Phi_{x,0})=Q(\\Phi_{x,0})=\\max\\{0,\\log_{2}\\frac{x}{|1-x|}\\}\\;, \\tag{16}\\]\n",
      "\n",
      "while for the secret-key and two-way capacities it holds [42]\n",
      "\n",
      "\\[K(\\Phi_{x,0})=Q_{2}(\\Phi_{x,0})=\\log_{2}(\\tfrac{\\max\\{1,x\\}}{|1-x|})\\;, \\tag{17}\\]\n",
      "\n",
      "(notice that for amplifiers, i.e. \\(x\\geq 1\\), Eq. (16) and (17) coincide).\n",
      "### Upper bounds\n",
      "\n",
      "State-of-the-art upper bounds for the capacities of thermal attenuators and amplifiers are given in Refs. [39; 40; 41; 42]. Specifically in [42] it has been showed that, outside the EB region (8), all the quantum capacities \\(\\mathcal{K}\\) of thermal attenuators and amplifiers can be bounded as follows\n",
      "\n",
      "\\[\\begin{cases}\\mathcal{K}(\\mathcal{E}_{\\eta,N})\\leq\\mathcal{K}^{\\rm att}_{\\rm PLOB }(\\eta,N):=-h(N)-\\log_{2}((1-\\eta)\\eta^{N})\\;,\\\\ \\\\ \\mathcal{K}(\\mathcal{A}_{g,N})\\leq\\mathcal{K}^{\\rm amp}_{\\rm PLOB}(g,N):=-h(N) +\\log_{2}(\\frac{g^{N+1}}{g-1})\\;,\\end{cases} \\tag{18}\\]\n",
      "\n",
      "with\n",
      "\n",
      "\\[h(x):=(x+1)\\log_{2}(x+1)-x\\log_{2}(x)\\;, \\tag{19}\\]\n",
      "\n",
      "(see also Ref. [51] for a strong-converse extension of these inequalities). Partial improvements w.r.t. to the above inequalities for the case of quantum and private capacities, have been reported in Refs. [39; 40] where special instances of the decomposition rules (\\(\\mathbf{C_{3.1}}\\)) and (\\(\\mathbf{C_{3.2}}\\)) were employed to observe that outside the AD region (12) (i.e. for \\(N\\leq\\frac{2\\eta-1}{2(1-\\eta)}\\) for \\(\\mathcal{E}_{\\eta,N}\\) and \\(N\\leq\\frac{1}{2(g-1)}\\) for \\(\\mathcal{A}_{g,N}\\)) one can write \\(\\mathcal{E}_{\\eta,N}=\\mathcal{E}_{\\eta-N(1-\\eta),0}\\circ\\mathcal{A}_{\\frac{ \\eta}{N-N(1-\\eta)},0}\\) and \\(\\mathcal{A}_{g,N}=\\mathcal{E}_{1-(g-1)N,0}\\circ\\mathcal{A}_{\\frac{\\eta}{1-(g- 1)N},0}\\), which ultimately leads to\n",
      "\n",
      "\\[\\begin{cases}Q(\\mathcal{E}_{\\eta,N}),P(\\mathcal{E}_{\\eta,N})\\leq Q(\\mathcal{E }_{\\eta-N(1-\\eta),0})=\\log_{2}(\\frac{\\eta-N(1-\\eta)}{1-\\eta+N(1-\\eta)})\\\\ \\\\ Q(\\mathcal{A}_{g,N}),P(\\mathcal{A}_{g,N})\\leq Q(\\mathcal{E}_{1-(g-1)N,0})= \\log_{2}(\\frac{1-(g-1)N}{(g-1)N})\\end{cases} \\tag{20}\\]\n",
      "\n",
      "(notice that the bounds nullifies at the border with the AD region). In Ref. [41] instead, using degradable extensions of thermal attenuators, it was proven that\n",
      "\n",
      "\\[\\begin{cases}Q(\\mathcal{E}_{\\eta,N}),P(\\mathcal{E}_{\\eta,N})\\leq Q^{\\rm att}_ {\\rm FKG}(\\eta,N)\\;,\\\\ \\\\ Q(\\mathcal{A}_{g,N}),P(\\mathcal{A}_{g,N})\\leq Q^{\\rm amp}_{\\rm FKG}((g-1)N)\\;, \\end{cases} \\tag{21}\\]\n",
      "\n",
      "with\n",
      "\n",
      "\\[Q^{\\rm att}_{\\rm FKG}(\\eta,N) := \\log_{2}(\\frac{\\eta}{1-\\eta})+h((1-\\eta)N)-h(\\eta N)\\;,\\] \\[Q^{\\rm amp}_{\\rm FKG}(M) := -\\log_{2}(eM)+2h\\left(\\frac{\\sqrt{M^{2}+1}-1}{2}\\right)\\;.\\]\n",
      "\n",
      "Notice that the first is a function which, for fixed \\(\\eta\\) is monotonically decreasing w.r.t. \\(N\\) and, for fixed \\(N\\), monotonically increasing in \\(\\eta\\) (being null for \\(\\eta\\leq 0.5\\)).\n",
      "\n",
      "Figure 1: Left panel: zero capacity regions for the PI-GBCs \\(\\Phi_{x,M}\\); Right panel: same plot expressed in terms of the parametrization (4). In both plots, the greenish areas represent the regions where the all the capacities (\\(Q_{2}\\), \\(K\\), \\(Q\\), and \\(P\\)) are zero. The bluish areas represent the region where \\(Q\\) and \\(P\\) are zero but \\(Q_{2}\\) and \\(K\\) are not. Whether \\(Q\\) and \\(P\\) can be zero also for points in the white region is still an open problem.\n",
      "\n",
      "\\begin{table}\n",
      "\\begin{tabular}{|c|l|l|} \\hline \\(\\mathbf{(C_{1})}\\) & \\(\\mathcal{E}_{\\eta_{2},N_{2}}\\circ\\mathcal{E}_{\\eta_{1},N_{1}}=\\mathcal{E}_{ \\eta_{3},N_{3}}\\) & \\(\\left\\{\\begin{array}{l}\\eta_{3}=\\eta_{2}\\eta_{1}\\\\ (1-\\eta_{3})N_{3}=(1-\\eta_{2})N_{2}+(1-\\eta_{1})\\eta_{2}N_{1}\\end{array}\\right.\\) \\\\ \\hline \\(\\mathbf{(C_{2})}\\) & \\(\\mathcal{A}_{g_{2},N_{2}}\\circ\\mathcal{A}_{g_{1},N_{1}}=\\mathcal{A}_{g_{3},N_{3}}\\) & \\(\\left\\{\\begin{array}{l}g_{3}=g_{2}g_{1}\\\\ (g_{3}-1)N_{3}=(g_{2}-1)N_{2}+(g_{1}-1)g_{2}N_{1}\\end{array}\\right.\\) \\\\ \\hline \\(\\mathbf{(C_{3.1})}\\) & \\(\\mathcal{E}_{\\eta_{2},N_{2}}\\circ\\mathcal{A}_{g_{1},N_{1}}=\\left\\{\\begin{array}{l} \\mathcal{E}_{\\eta_{3},N_{3}}\\\\ (1-\\eta_{3})(2N_{3}+1)=(1-\\eta_{2})(2N_{2}+1)+(\\eta_{3}-\\eta_{2})(2N_{1}+1) \\end{array}\\right.\\) \\\\ \\(\\mathbf{(C_{3.2})}\\) & \\(\\mathcal{A}_{g_{3},N_{3}}\\) & \\(\\left\\{\\begin{array}{l}g_{3}=\\eta_{2}g_{1}\\\\ (g_{3}-1)(2N_{3}+1)=(1-\\eta_{2})(2N_{2}+1)+(g_{3}-\\eta_{2})(2N_{1}+1)\\end{array}\\right.\\) \\\\ \\hline \\(\\mathbf{(C_{4.1})}\\) & \\(\\mathcal{A}_{g_{2},N_{2}}\\circ\\mathcal{E}_{\\eta_{1},N_{1}}=\\left\\{\\begin{array}{l} \\mathcal{E}_{\\eta_{3},N_{3}}\\\\ (1-\\eta_{3})(2N_{3}+1)=(g_{2}-1)(2N_{2}+1)+(g_{2}-\\eta_{3})(2N_{1}+1)\\end{array} \\right.\\) \\\\ \\(\\mathbf{(C_{4.2})}\\) & \\(\\mathcal{A}_{g_{3},N_{3}}\\) & \\(\\left\\{\\begin{array}{l}g_{3}=g_{2}g_{1}\\\\ (g_{3}-1)(2N_{3}+1)=(g_{2}-1)(2N_{2}+1)+(g_{2}-g_{3})(2N_{1}+1)\\end{array} \\right.\\) \\\\ \\hline \\end{tabular}\n",
      "\\end{table}\n",
      "Table 1: Composition rules (5) and (6) expressed in terms of thermal attenuators and amplifiers via the identities (4).\n",
      "On the contrary, for \\(M=(g-1)N\\in[0,1/2]\\) (i.e. the only region where, in view of Eq. (12) it makes sense to consider (21)) \\(Q^{\\rm amp}_{\\rm FKG}(M)\\) is a positive, monotonically increasing function. As explicitly shown in the left part of Fig. 2, one may notice that while for low values of \\(\\eta\\) the upper bound (20) outperform the others, as \\(\\eta\\) approaches 1, (18) and (21) win (in particular \\(Q^{\\rm int}_{\\rm FKG}\\) provides the best bound in the low noise regime \\(N\\ll 1\\), while \\(Q^{\\rm att}_{\\rm PLOB}\\) does it for higher \\(N\\)).\n",
      "\n",
      "### Lower bounds\n",
      "\n",
      "Lower bounds for the two-way and secret capacities are provided by the inequalities [8; 52]\n",
      "\n",
      "\\[\\begin{cases}K(\\mathcal{E}_{\\eta,N})\\geq Q_{2}(\\mathcal{E}_{\\eta,N})\\geq\\max \\{0,-h(N)-\\log_{2}(1-\\eta)\\}\\;,\\\\ \\\\ K(\\mathcal{A}_{g,N})\\geq Q_{2}(\\mathcal{A}_{g,N})\\geq\\max\\{0,-h(N)+\\log_{2}( \\frac{g}{g-1})\\}\\,,\\end{cases} \\tag{22}\\]\n",
      "\n",
      "which have been improved in [45; 53]. [53] improved the lower bound for secret capacity adopting Gaussian protocols based on suitable trusted-noise detectors, and [45] showed that the region where \\(Q_{2}\\) and \\(K\\) are non-zero extend beyond what predicted by the above equations, including all the non-EB region. A lower bound on \\(Q\\) and \\(P\\) is given instead by the coherent information for one use of the channel, evaluated on an infinite temperature state\n",
      "\n",
      "\\[Q(\\mathcal{E}_{\\eta,N})\\geq Q^{\\rm att}_{\\rm low}(\\eta,N)=\\max\\{0,\\log_{2}( \\frac{\\eta}{1-\\eta})-h(N)\\}\\,. \\tag{23}\\]\n",
      "\n",
      "## IV Low ground/high ground capacity regions analysis\n",
      "\n",
      "Using the composition rules (5), (6) together with the data-processing inequality (2), in the parameter space of the maps (3) we can identify regions where the capacities are provably smaller or larger than an assigned reference value. For this purpose given \\((x,M)\\in\\left(\\mathbb{R}^{+}\\right)^{2}\\) we define \\(\\mathbb{L}_{x,M}:=\\mathbb{L}(\\Phi_{x,M})\\) the collection of points \\((x^{\\prime},M^{\\prime})\\in\\left(\\mathbb{R}^{+}\\right)^{2}\\) such that such that the channel \\(\\Phi_{x^{\\prime},M^{\\prime}}\\) can be decomposed as a three-elements concatenation \\(\\Phi_{x^{\\prime},M^{\\prime}}=\\Phi_{\\bar{x}_{1},\\bar{M}_{1}}\\circ\\Phi_{x,M} \\circ\\Phi_{\\bar{x}_{2},\\bar{M}_{2}}\\) that involves \\(\\Phi_{x,M}\\) together with two other maps \\(\\Phi_{\\bar{x}_{1},\\bar{M}_{1}}\\) and \\(\\Phi_{\\bar{x}_{2},\\bar{M}_{2}}\\), i.e. [54]\n",
      "\n",
      "\\[\\mathbb{L}_{x,M}:=\\left\\{(x^{\\prime},M^{\\prime})\\in\\left(\\mathbb{R}^{+} \\right)^{2}:\\exists(\\bar{x}_{1},\\bar{M}_{1}),(\\bar{x}_{2},\\bar{M}_{2})\\in \\left(\\mathbb{R}^{+}\\right)^{2}\\right.\\]\n",
      "\n",
      "\\[\\left.\\Phi_{x^{\\prime},M^{\\prime}}=\\Phi_{\\bar{x}_{1},\\bar{M}_{1}}\\circ\\Phi_{x,M}\\circ\\Phi_{\\bar{x}_{2},\\bar{M}_{2}}\\right\\}\\,. \\tag{24}\\]\n",
      "\n",
      "From (2) it turns out that\n",
      "\n",
      "\\[\\mathcal{K}(\\Phi_{x^{\\prime},M^{\\prime}})\\leq\\mathcal{K}(\\Phi_{x,M})\\;,\\qquad \\forall(x^{\\prime},M^{\\prime})\\in\\mathbb{L}_{x,M}\\;. \\tag{25}\\]\n",
      "\n",
      "Figure 2: **Left:** The top panel shows a numerical comparison between the upper bounds (18), (20), and (21) for the quantum capacity \\(Q\\) of the channels \\(\\mathcal{E}_{\\eta,N}\\) and \\(\\mathcal{A}_{g,N}\\). Each region with different colour indicates which result is the best upper bound. Purple region is where the quantum capacity is zero according to Eq. (15). The bottom panel presents the same comparison expressed in terms of the \\(x,M\\) parametrization (3). **Right:** updated version of previous figure which includes the improved bounds \\(Q^{(1)}_{\\rm FKG}(x,M)\\) and \\(\\underline{Q}^{(2)}_{\\rm FKG}(x,M)\\) of Eq. (103): the orange and yellow regions (marked with the script _NEW_) are where they provide better constraints than the upper bounds of Sec. III.1 (notice that no improvement is obtained for amplifiers). The top panel reports the result in terms of the \\(\\eta,N\\) and \\(g,N\\) parametrization, while the bottom panel reports the same result in the \\(x,M\\) parametrization.\n",
      "so we dub \\(\\mathbb{L}_{x,M}\\) the _low-ground capacity region_ of the channel \\(\\Phi_{x,M}\\). Notice in particular that, since \\(\\Phi_{0,M^{\\prime}}\\circ\\Phi_{x,M}=\\Phi_{0,M^{\\prime}}\\) for all \\((x,M)\\) and \\(M^{\\prime}\\geq 0\\), one has that all the points \\((0,M^{\\prime})\\) are included into \\(\\mathbb{L}_{x,M}\\), i.e.\n",
      "\n",
      "\\[(0,M^{\\prime})\\in\\mathbb{L}_{x,M}\\;,\\quad\\forall M^{\\prime}\\geq 0\\;,\\forall(x,M) \\in\\left(\\mathbb{R}^{+}\\right)^{2}\\;. \\tag{26}\\]\n",
      "\n",
      "By reversing the ordering of the concatenations in Eq. (24) we also introduce the _high-ground capacity region_\\(\\mathbb{H}_{x,M}:=\\mathbb{H}(\\Phi_{x,M})\\) of the channel \\(\\Phi_{x,M}\\), i.e.\n",
      "\n",
      "\\[\\mathbb{H}_{x,M}:=\\left\\{(x^{\\prime},M^{\\prime})\\in\\left(\\mathbb{ R}^{+}\\right)^{2}:\\exists(\\bar{x}_{1},\\bar{M}_{1}),(\\bar{x}_{2},\\bar{M}_{2}) \\in\\left(\\mathbb{R}^{+}\\right)^{2}\\right.\\] \\[\\qquad\\qquad\\left.\\Phi_{x,M}=\\Phi_{\\bar{x}_{1},\\bar{M}_{1}}\\circ \\Phi_{x^{\\prime},M^{\\prime}}\\circ\\Phi_{\\bar{x}_{2},\\bar{M}_{2}}\\right\\}, \\tag{27}\\]\n",
      "\n",
      "which fulfils the condition\n",
      "\n",
      "\\[\\mathcal{K}(\\Phi_{x^{\\prime},M^{\\prime}})\\geq\\mathcal{K}(\\Phi_{x,M})\\;,\\qquad \\forall(x^{\\prime},M^{\\prime})\\in\\mathbb{H}_{x,M}\\;. \\tag{28}\\]\n",
      "\n",
      "Notice that by construction \\(\\mathbb{L}_{x,M}\\) and \\(\\mathbb{H}_{x,M}\\) obey a natural ordering\n",
      "\n",
      "\\[\\mathbb{L}_{x^{\\prime},M^{\\prime}} \\subseteq \\mathbb{L}_{x,M} \\forall(x^{\\prime},M^{\\prime})\\in\\mathbb{L}_{x,M}\\;, \\tag{29}\\] \\[\\mathbb{H}_{x^{\\prime},M^{\\prime}} \\subseteq \\mathbb{H}_{x,M} \\forall(x^{\\prime},M^{\\prime})\\in\\mathbb{H}_{x,M}\\;, \\tag{30}\\]\n",
      "\n",
      "and fulfil the complementary relation\n",
      "\n",
      "\\[(x^{\\prime},M^{\\prime})\\in\\mathbb{H}_{x,M}\\quad\\Longleftrightarrow\\quad(x,M) \\in\\mathbb{L}_{x^{\\prime},M^{\\prime}}\\;. \\tag{31}\\]\n",
      "\n",
      "In the next subsections we shall provide an analytic characterization of \\(\\mathbb{L}_{x,M}\\) and \\(\\mathbb{H}_{x,M}\\). As we shall see one can identify two different regimes ruled by the function \\(M_{\\mathrm{EB}}(x)\\) of Eq. (10) which identifies the EB sector. Indeed for points \\((x,M)\\) with\n",
      "\n",
      "\\[M\\leq M_{\\mathrm{EB}}(x)=\\min\\{1,x\\}\\;, \\tag{32}\\]\n",
      "\n",
      "that is for channels which are non-EB and for the EB ones which are on the border line of the region \\(\\mathtt{EB}\\), the sets \\(\\mathbb{L}_{x,M}\\) and \\(\\mathbb{H}_{x,M}\\) are defined by the functions\n",
      "\n",
      "\\[f^{(1)}_{x,M}(x^{\\prime}) := M+(1-x)\\Theta(1-x)+(x^{\\prime}-1)\\Theta(1-x^{\\prime})\\;,\\] \\[f^{(2)}_{x,M}(x^{\\prime}) := (x^{\\prime}/x)\\big{[}M+(x-1)\\Theta(x-1)\\big{]} \\tag{33}\\] \\[\\qquad\\qquad-(x^{\\prime}-1)\\Theta(x^{\\prime}-1)\\;,\\]\n",
      "\n",
      "with \\(\\Theta(x)\\) being the Heaviside step-function. Specifically we shall prove that under the condition (32), \\(\\mathbb{L}_{x,M}\\) is formed by all points which are above \\(f^{(1)}_{x,M}(x^{\\prime})\\) and \\(f^{(2)}_{x,M}(x^{\\prime})\\), i.e.\n",
      "\n",
      "\\[\\mathbb{L}_{x,M} = \\left\\{(x^{\\prime},M^{\\prime})\\in\\left(\\mathbb{R}^{+}\\right)^{2}:\\right.\\] \\[\\qquad\\left.M^{\\prime}\\geq\\max\\{f^{(1)}_{x,M}(x^{\\prime}),f^{(2)}_ {x,M}(x^{\\prime})\\}\\right\\},\\]\n",
      "\n",
      "while \\(\\mathbb{H}_{x,M}\\) is given by the polytope formed by the points below such curves, i.e.\n",
      "\n",
      "\\[\\mathbb{H}_{x,M} = \\left\\{(x^{\\prime},M^{\\prime})\\in\\left(\\mathbb{R}^{+}\\right)^{2}:\\right.\\] \\[\\qquad\\left.M^{\\prime}\\leq\\min\\{f^{(1)}_{x,M}(x^{\\prime}),f^{(2)}_ {x,M}(x^{\\prime})\\}\\right\\}.\\]\n",
      "\n",
      "As evident from Fig. 3, the domains identified by Eqs. (34) and (35) admit \\((x,M)\\) as unique contact point, implying that for a relative large portion of the phase space \\(\\left(\\mathbb{R}^{+}\\right)^{2}\\) we cannot assign a definite ordering w.r.t. \\(\\mathcal{K}(\\Phi_{x,M})\\) (white regions of plots). The situation change however when the map \\(\\Phi_{x,M}\\) is deep inside the EB region, i.e. for\n",
      "\n",
      "\\[M>M_{\\mathrm{EB}}(x)=\\min\\{1,x\\}\\;. \\tag{36}\\]\n",
      "\n",
      "Under the constraint (36) the sets \\(\\mathbb{H}_{x,M}\\) and \\(\\mathbb{L}_{x,M}\\) provide a complete covering of the phase space and have a non trivial overlap \\(\\mathbb{0}_{x,M}:=\\mathbb{H}_{x,M}\\bigcap\\mathbb{L}_{x,M}\\). Indeed, one can show that irrespectively from the specific choice of \\((x,M)\\), \\(\\mathbb{H}_{x,M}\\) coincides with the entire space \\(\\left(\\mathbb{R}^{+}\\right)^{2}\\), i.e.\n",
      "\n",
      "\\[\\mathbb{H}_{x,M}=\\left(\\mathbb{R}^{+}\\right)^{2}\\;, \\tag{37}\\]\n",
      "\n",
      "while \\(\\mathbb{L}_{x,M}\\) (and hence \\(\\mathbb{0}_{x,M}\\)) corresponds to the \\(x\\to 0,M\\to 0\\), limit of Eq. (34), i.e. [55]\n",
      "\n",
      "\\[\\mathbb{L}_{x,M} = \\mathbb{0}_{x,M}\\] \\[= \\mathbb{L}_{0,0}:=\\left\\{(x^{\\prime},M^{\\prime})\\in\\left( \\mathbb{R}^{+}\\right)^{2}:M^{\\prime}>M_{\\mathrm{EB}}(x^{\\prime})\\right\\}\\,,\\]\n",
      "\n",
      "which coincides with the subset identified by Eq. (36). This implies that given any two points \\((x_{1},M_{1})\\) and \\((x_{2},M_{2})\\) in \\(\\mathbb{L}_{0,0}\\), their associated channel are equivalent up to concatenation with extra GBCs (3), i.e. there exist proper choices of the maps \\(\\Phi_{\\bar{x}_{1},\\bar{M}_{1}}\\), \\(\\Phi_{\\bar{x}_{2},\\bar{M}_{2}}\\), \\(\\Phi_{\\bar{x}_{3},\\bar{M}_{3}}\\), and \\(\\Phi_{\\bar{x}_{4},\\bar{M}_{4}}\\), such that we can write\n",
      "\n",
      "\\[\\begin{cases}\\Phi_{x_{1},M_{1}}=\\Phi_{\\bar{x}_{1},\\bar{M}_{1}}\\circ\\Phi_{x_{2},M_{2}}\\circ\\Phi_{\\bar{x}_{2},\\bar{M}_{2}}\\;,\\\\ \\Phi_{x_{2},M_{2}}=\\Phi_{\\bar{x}_{3},\\bar{M}_{3}}\\circ\\Phi_{x_{1},M_{1}}\\circ\\Phi _{\\bar{x}_{4},\\bar{M}_{4}}\\;,\\end{cases} \\tag{39}\\]\n",
      "\n",
      "which in turn imposes \\(\\mathcal{K}(\\Phi_{x_{1},M_{1}})=\\mathcal{K}(\\Phi_{x_{2},M_{2}})\\) in agreement with the property (11).\n",
      "\n",
      "## V Analytical characterization of \\(\\mathbb{L}_{x,M}\\) and \\(\\mathbb{H}_{x,M}\\)\n",
      "\n",
      "In this section we give an analytical characterization of the low-ground and high-ground regions for an arbitrary channel \\(\\Phi_{x,M}\\). We start in Sec. V.1 by focusing on a simplified version of the concatenation rules entering in the definitions (24) and (27) where \\(\\Phi_{x,M}\\) is connected with the elements \\(\\Phi_{x^{\\prime},M^{\\prime}}\\) via only a single extra PI-GBC element \\(\\Phi_{\\bar{x},\\bar{M}}\\). This will allows us to identify two regions\n",
      "\n",
      "\\[\\mathbb{L}^{(0)}_{x,M}:=\\left\\{(x^{\\prime},M^{\\prime})\\in\\left( \\mathbb{R}^{+}\\right)^{2}:\\exists(\\bar{x},\\bar{M})\\in\\left(\\mathbb{R}^{+} \\right)^{2}\\right. \\tag{40}\\] \\[\\left.\\Phi_{x^{\\prime},M^{\\prime}}=\\Phi_{\\bar{x},\\bar{M}}\\circ\\Phi_{ x,M}\\;\\text{or}\\;\\;\\Phi_{x^{\\prime},M^{\\prime}}=\\Phi_{x,M}\\circ\\Phi_{\\bar{x},\\bar{M}} \\right\\}\\,,\\]\n",
      "\n",
      "and\n",
      "\n",
      "\\[\\mathbb{H}^{(0)}_{x,M}:=\\left\\{(x^{\\prime},M^{\\prime})\\in\\left( \\mathbb{R}^{+}\\right)^{2}:\\exists(\\bar{x},\\bar{M})\\in\\left(\\mathbb{R}^{+} \\right)^{2}\\right. \\tag{41}\\] \\[\\left.\\Phi_{x,M}=\\Phi_{\\bar{x},\\bar{M}}\\circ\\Phi_{x^{\\prime},M^{ \\prime}}\\;\\text{or}\\;\\;\\Phi_{x,M}=\\Phi_{x^{\\prime},M^{\\prime}}\\circ\\Phi_{\\bar{x}, \\bar{M}}\\right\\}.\\]\n",
      "which by construction are subsets of \\(\\mathbb{L}_{x,M}\\) and \\(\\mathbb{H}_{x,M}\\), i.e.\n",
      "\n",
      "\\[\\mathbb{L}_{x,M}^{(0)}\\subseteq\\mathbb{L}_{x,M}\\;,\\qquad\\mathbb{H}_{x,M}^{(0)} \\subseteq\\mathbb{H}_{x,M}\\;. \\tag{42}\\]\n",
      "\n",
      "In Sec. V.2 we shall prove that for \\((x,M)\\) fulfilling the constraint (32), \\(\\mathbb{L}_{x,M}^{(0)}\\) and \\(\\mathbb{H}_{x,M}^{(0)}\\) indeed coincide with \\(\\mathbb{L}_{x,M}\\) and \\(\\mathbb{H}_{x,M}\\) leading to Eqs. (34) and (35). The derivation of Eqs. (37) and (38) for maps not fulfilling (32) will instead be given in Sec. V.3.\n",
      "\n",
      "### Two-elements concatenations\n",
      "\n",
      "To determine \\(\\mathbb{L}_{x,M}^{(0)}\\) and \\(\\mathbb{H}_{x,M}^{(0)}\\) we adopt the parametrization (4) to better underline the role played by amplifiers and attenuators. Given hence \\(\\eta\\in[0,1]\\) and \\(N\\geq 0\\), let us introduce the following functions\n",
      "\n",
      "\\[N_{\\eta,N}^{(1)}(\\eta^{\\prime}) := N\\left(\\frac{1-\\eta}{\\eta}\\right)\\left(\\frac{\\eta^{\\prime}}{1- \\eta^{\\prime}}\\right)\\;, \\tag{43}\\] \\[N_{\\eta,N}^{(2)}(\\eta^{\\prime}) := (N+1)\\left(\\frac{1-\\eta}{1-\\eta^{\\prime}}\\right)-1\\;. \\tag{44}\\]\n",
      "\n",
      "It then turns out that\n",
      "\n",
      "**Property 1**.: _The attenuator map \\(\\mathcal{E}_{\\eta,N}\\) admits_\n",
      "\n",
      "\\[\\begin{cases}\\mathbb{L}_{\\eta,N}^{(\\text{att},1)}&:=\\left\\{(\\eta^{\\prime},N^{ \\prime}):0\\leq\\eta^{\\prime}\\leq\\eta\\,,N^{\\prime}\\geq\\max\\{N_{\\eta,N}^{(1)}( \\eta^{\\prime}),0\\}\\right\\},\\\\ \\mathbb{L}_{\\eta,N}^{(\\text{att},2)}&:=\\left\\{(\\eta^{\\prime},N^{\\prime}):1\\geq \\eta^{\\prime}\\geq\\eta\\,,N^{\\prime}\\geq\\max\\{N_{\\eta,N}^{(2)}(\\eta^{\\prime}),0 \\}\\right\\},\\\\ \\mathbb{L}_{\\eta,N}^{(\\text{att})}&:=\\mathbb{L}_{\\eta,N}^{(\\text{att},1)}\\cup \\mathbb{L}_{\\eta,N}^{(\\text{att},2)}\\;,\\end{cases} \\tag{45}\\]\n",
      "\n",
      "_(light blue area on the left-hand-side of the top panel of Fig. 4), as subset of the corresponding two-element concatenation, low-ground capacity region \\(\\mathbb{L}^{(0)}(\\mathcal{E}_{\\eta,N})\\), and_\n",
      "\n",
      "\\[\\begin{cases}\\mathbb{H}_{\\eta,N}^{(\\text{att},1)}&:=\\left\\{(\\eta^{\\prime},N^{ \\prime}):1\\geq\\eta^{\\prime}\\geq\\eta\\,,0\\leq N^{\\prime}\\leq N_{\\eta,N}^{(1)}( \\eta^{\\prime})\\right\\}\\;,\\\\ \\mathbb{H}_{\\eta,N}^{(\\text{att},2)}&:=\\left\\{(\\eta^{\\prime},N^{\\prime}):1\\leq \\eta^{\\prime}\\leq\\eta\\,,0\\leq N^{\\prime}\\leq N_{\\eta,N}^{(2)}(\\eta^{\\prime}) \\right\\}\\;,\\\\ \\mathbb{H}_{\\eta,N}^{(\\text{att})}&:=\\mathbb{H}_{\\eta,N}^{(\\text{att},1)}\\cup \\mathbb{H}_{\\eta,N}^{(\\text{att},2)}\\;,\\end{cases} \\tag{46}\\]\n",
      "\n",
      "_(yellow area on the left-hand-side of top panel of Fig. 4) as subset of \\(\\mathbb{H}^{(0)}(\\mathcal{E}_{\\eta,N})\\), i.e._\n",
      "\n",
      "\\[\\mathbb{L}_{\\eta,N}^{(\\text{att})}\\subseteq\\mathbb{L}^{(0)}(\\mathcal{E}_{\\eta, N})\\;,\\qquad\\mathbb{H}_{\\eta,N}^{(\\text{att})}\\subseteq\\mathbb{H}^{(0)}( \\mathcal{E}_{\\eta,N})\\;. \\tag{47}\\]\n",
      "\n",
      "Proof.: To derive the first inclusion of Eq. (47) we set \\((\\eta_{3},N_{3})=(\\eta^{\\prime},N^{\\prime})\\) and \\((\\eta_{1},N_{1})=(\\eta,N)\\) in Eq. (\\(\\mathbf{C_{1}}\\)) of Tab. 1 to observe that for all \\(\\eta_{2}\\in[0,1]\\) and \\(N_{2}\\geq 0\\),\n",
      "\n",
      "\\[\\mathcal{E}_{\\eta^{\\prime},N^{\\prime}}=\\mathcal{E}_{\\eta_{2},N_{2}}\\circ \\mathcal{E}_{\\eta,N}\\;, \\tag{48}\\]\n",
      "\n",
      "is also a thermal channel with parameters\n",
      "\n",
      "\\[\\begin{cases}\\eta^{\\prime}=\\eta\\eta_{2}\\leq\\eta\\;,\\\\ N^{\\prime}=\\frac{(1-\\eta)\\eta_{2}N+(1-\\eta_{2})N_{2}}{1-\\eta\\eta_{2}}\\geq\\frac{(1 -\\eta)\\eta_{2}}{1-\\eta\\eta_{2}}N=N_{\\eta,N}^{(1)}(\\eta^{\\prime})\\;,\\end{cases} \\tag{49}\\]\n",
      "\n",
      "that span the entire set \\(\\mathbb{L}_{\\eta,N}^{(\\text{att},1)}\\), as the inequality is saturated whenever \\(N_{2}=0\\). Therefore in view of the definition (24) we can conclude that\n",
      "\n",
      "\\[\\mathbb{L}_{\\eta,N}^{(\\text{att},1)}\\subseteq\\mathbb{L}^{(0)}(\\mathcal{E}_{ \\eta,N})\\;. \\tag{50}\\]\n",
      "\n",
      "Figure 3: Low-ground capacity region \\(\\mathbb{L}_{x,M}\\) (24) (light blue area) and high-ground capacity region \\(\\mathbb{H}_{x,M}\\) (27) (yellow) for the channel \\(\\Phi_{x,M}\\) (red dot element). The first three top panels refer to the regime (32) where \\(\\mathbb{L}_{x,M}\\) and \\(\\mathbb{H}_{x,M}\\) are expressed respectively by Eqs. (34) and (35): specifically in panel a) the reference map is an attenuator (\\(x=0.6\\), \\(M=0.1\\)), in panel b) it is an additive classical noise map (\\(x=1\\), \\(M=0.5\\)), and in panel c) it is an amplifier (\\(x=1.15\\), \\(M=0.1\\)). Panel d) instead presents a case where \\(\\Phi_{x,M}\\) fulfils the EB condition (36); here \\(\\mathbb{H}_{x,M}\\) coincides with the full plane \\(\\left(\\mathbb{R}^{+}\\right)^{2}\\), while \\(\\mathbb{L}_{x,M}\\) (and hence the overlap \\(\\mathbb{O}_{x,M}=\\mathbb{H}_{x,M}\\bigcap\\mathbb{L}_{x,M}\\)), are given by the set \\(\\mathbb{L}_{0,0}\\) of Eq. (38): by construction any two points in this area are equivalent under GBC concatenation, see Eq. (39). The border lines which identify the various regions are the functions \\(f_{x,M}^{(1)}(x^{\\prime})\\) (blue curve) and \\(f_{x,M}^{(2)}(x^{\\prime})\\) (orange curve) defined in Eq. (33) – for panel d) the border is given by \\(f_{0,0}^{(1)}(x^{\\prime})=\\min\\{1,x^{\\prime}\\}\\)[55]. The arrows in the plots show the direction where the capacities have to decrease (or at most remain constant) and the white regions describe portions of the parameter space where the composition rules cannot be used to determine a specific capacity ordering with respect to the reference channel. In panel a) we have \\(\\mathbf{A}=(x-M,0)\\), \\(\\mathbf{B}=(1,M-x+1)\\), \\(\\mathbf{C}=(1,M/x)\\), \\(\\mathbf{D}=(x/(x-M),0)\\); in panel b) and c) instead \\(\\mathbf{A}=(1-M,0)\\), \\(\\mathbf{B}=(1,M)\\), \\(\\mathbf{C}=(1,(M+x-1)/x)\\), \\(\\mathbf{D}=(x/(1-M),0)\\) (notice that for b), \\(\\mathbf{B}=\\mathbf{C}=(x,M)\\)).\n",
      "We next invoke Eq. (\\(\\mathbf{C_{3.1}}\\)) of Tab. 1 to observe that\n",
      "\n",
      "\\[\\mathcal{E}_{\\eta^{\\prime},N^{\\prime}}=\\mathcal{E}_{\\eta,N}\\circ\\mathcal{A}_{g_{1},N_{1}}\\;, \\tag{51}\\]\n",
      "\n",
      "with\n",
      "\n",
      "\\[\\begin{cases}\\eta^{\\prime}=g_{1}\\eta\\geq\\eta\\;,\\\\ \\\\ N^{\\prime}=\\frac{(1-\\eta)(2N+1)+\\eta(g_{1}-1)(2N_{1}+1)-(1-\\eta^{\\prime})}{2(1 -\\eta^{\\prime})}\\\\ \\qquad\\geq\\frac{\\eta^{\\prime}-\\eta+(1-\\eta)N}{1-\\eta^{\\prime}}=N^{(2)}_{\\eta,N }(\\eta^{\\prime})\\;,\\end{cases} \\tag{52}\\]\n",
      "\n",
      "is a thermal map too which spans the entire subset \\(\\mathbb{L}^{(\\text{att},2)}_{\\eta,N}\\), as the inequality is saturated whenever \\(N_{1}=0\\). Accordingly we can claim that\n",
      "\n",
      "\\[\\mathbb{L}^{(\\text{att},2)}_{\\eta,N}\\subseteq\\mathbb{L}^{(0)}(\\mathcal{E}_{ \\eta,N})\\;, \\tag{53}\\]\n",
      "\n",
      "which together with (50) yields the first identity of Eq. (47). The derivation of the second identity of Eq. (47) follows along the same lines by simply inverting the roles of \\((\\eta^{\\prime},N^{\\prime})\\) and \\((\\eta,N)\\) in the previous passages. \n",
      "\n",
      "Given next \\(g\\geq 1\\) and \\(N\\geq 0\\) and the functions\n",
      "\n",
      "\\[N^{(1)}_{g,N}(g^{\\prime}) := N\\left(\\tfrac{g-1}{g^{\\prime}-1}\\right)\\;, \\tag{54}\\] \\[N^{(2)}_{g,N}(g^{\\prime}) := (N+1)\\left(\\tfrac{g-1}{g}\\right)\\left(\\tfrac{g^{\\prime}}{g^{ \\prime}-1}\\right)-1\\;. \\tag{55}\\]\n",
      "\n",
      "We can show that\n",
      "\n",
      "**Property 2**.: _The amplifier channel \\(\\mathcal{A}_{g,N}\\) admits_\n",
      "\n",
      "\\[\\begin{cases}\\mathbb{L}^{(\\text{amp},1)}_{g,N}&:=\\left\\{(g^{\\prime},N^{\\prime }):g^{\\prime}\\geq g\\,,N^{\\prime}\\geq\\max\\{N^{(1)}_{g,N}(g^{\\prime}),0\\}\\right\\},\\\\ \\mathbb{L}^{(\\text{amp},2)}_{g,N}&:=\\left\\{(g^{\\prime},N^{\\prime}):g^{\\prime} \\leq g\\,,N^{\\prime}\\geq\\max\\{N^{(2)}_{g,N}(g^{\\prime}),0\\}\\right\\},\\\\ \\mathbb{L}^{(\\text{amp})}_{g,N}&:=\\mathbb{L}^{(\\text{amp},1)}_{g,N}\\cup \\mathbb{L}^{(\\text{amp},2)}_{g,N}\\;,\\end{cases} \\tag{56}\\]\n",
      "\n",
      "_(light blue area on the right-hand-side of the bottom panel of Fig. 4), as subset of the corresponding two-element concatenation, low-ground capacity region \\(\\mathbb{L}^{(0)}(\\mathcal{A}_{\\eta,N})\\), and_\n",
      "\n",
      "\\[\\begin{cases}\\mathbb{H}^{(\\text{amp},1)}_{g,N}&:=\\left\\{(g^{\\prime},N^{\\prime }):1\\leq g^{\\prime}\\leq g\\,,0\\leq N^{\\prime}\\leq N^{(1)}_{g,N}(g^{\\prime}) \\right\\},\\\\ \\mathbb{H}^{(\\text{amp},2)}_{g,N}&:=\\left\\{(g^{\\prime},N^{\\prime}):g^{\\prime} \\geq g\\,,0\\leq N^{\\prime}\\leq N^{(2)}_{g,N}(g^{\\prime})\\right\\},\\end{cases} \\tag{57}\\]\n",
      "\n",
      "_(yellow area on the right-hand-side of bottom panel of Fig. 4), as subspace of \\(\\mathbb{H}^{(0)}(\\mathcal{A}_{\\eta,N})\\), i.e._\n",
      "\n",
      "\\[\\mathbb{L}^{(\\text{amp})}_{g,N}\\subseteq\\mathbb{L}^{(0)}(\\mathcal{A}_{g,N}) \\;,\\qquad\\mathbb{H}^{(\\text{amp})}_{g,N}\\subseteq\\mathbb{H}^{(0)}(\\mathcal{A} _{g,N})\\;. \\tag{58}\\]\n",
      "\n",
      "Proof.: To derive the first inclusion we set \\((g_{3},N_{3})=(g^{\\prime},N^{\\prime})\\) and \\((g_{2},N_{2})=(g,N)\\) in Eq. (\\(\\mathbf{C_{2}}\\)) of Tab. 1 to observe for all \\(g_{1}\\geq 1\\) and \\(N_{1}\\geq 0\\),\n",
      "\n",
      "\\[\\mathcal{A}_{g^{\\prime},N^{\\prime}}=\\mathcal{A}_{g,N}\\circ\\mathcal{A}_{g_{1},N _{1}} \\tag{59}\\]\n",
      "\n",
      "is also a thermal channel with parameters\n",
      "\n",
      "\\[\\begin{cases}g^{\\prime}=g_{1}g\\geq g\\;,\\\\ \\\\ N^{\\prime}=\\frac{(g_{1}-1)gN_{1}+(g-1)N}{g^{\\prime}-1}\\geq N^{(1)}_{g,N}(g^{ \\prime})\\;,\\end{cases} \\tag{60}\\]\n",
      "\n",
      "which span the entire area \\(\\mathbb{L}^{(\\text{amp},1)}_{g,N}\\), as the inequality is saturated whenever \\(N_{1}=0\\), leading to\n",
      "\n",
      "\\[\\mathbb{L}^{(\\text{amp},1)}_{g,N}\\subseteq\\mathbb{L}^{(0)}(\\mathcal{A}_{g,N})\\;. \\tag{61}\\]\n",
      "\n",
      "We next invoke (\\(\\mathbf{C_{3.2}}\\)) of Tab. 1 to observe that\n",
      "\n",
      "\\[\\mathcal{A}_{g^{\\prime},N^{\\prime}}=\\mathcal{E}_{\\eta_{2},N_{2}}\\circ\\mathcal{A }_{g,N}\\;, \\tag{62}\\]\n",
      "\n",
      "is an amplifier too with parameters\n",
      "\n",
      "\\[\\begin{cases}&g^{\\prime}=\\eta_{2}g\\leq g\\;,\\\\ \\\\ &N^{\\prime}=\\frac{(1-\\eta_{2})(2N_{2}+1)+\\eta_{2}(g-1)(2N+1)-(g^{\\prime}-1)}{2(g ^{\\prime}-1)}\\geq N^{(2)}_{g,N}(g^{\\prime})\\;,\\end{cases} \\tag{63}\\]\n",
      "\n",
      "that cover the entire subset \\(\\mathbb{L}^{(\\text{amp},2)}_{g,N}\\), as the inequality is\n",
      "\n",
      "Figure 4: Two-element concatenation analysis of the low-ground and high-ground capacity regions for thermal attenuators and amplifiers (for all the examples reported in the plots the channels are non-EB, i.e. fulfil the condition (32)). Top panel: given a thermal attenuator \\(\\mathcal{E}_{\\eta,N}\\) (red dot element), the light blue areas correspond respectively to \\(\\mathbb{L}^{(\\text{att})}_{\\eta,N}\\) and \\(\\mathbb{X}^{(\\text{att},-)}_{\\eta,N}\\); the yellow areas describe instead \\(\\mathbb{H}^{(\\text{att})}_{\\eta,N}\\) and \\(\\mathbb{X}^{(\\text{att},+)}_{\\eta,N}\\). Plot realized using \\(N=0.1\\) and \\(\\eta=0.6\\). Bottom panel: given a thermal amplifier \\(\\mathcal{A}_{g,N}\\) (red dot element), the light blue area describe the sets \\(\\mathbb{L}^{(\\text{amp})}_{g,N}\\) and \\(\\mathbb{X}^{(\\text{amp},-)}_{g,N}\\) while the yellow area \\(\\mathbb{H}^{(\\text{amp})}_{g,N}\\) and \\(\\mathbb{X}^{(\\text{amp},+)}_{g,N}\\). Plot realized using \\(N=0.2\\) and \\(g=1.5\\). The orange and blue curves represent the border lines of the low ground/high ground regions defined by the functions \\(N^{(1,2)}_{\\eta,N}(\\eta^{\\prime})\\) of Eqs. (43), (44), \\(N^{(3,4)}_{\\eta,N}(g^{\\prime})\\) of Eqs. (65), (66), \\(N^{(1,2)}_{g,N}(g^{\\prime})\\) of Eqs. (54), (55), and \\(N^{(3,4)}_{g,N}(\\eta^{\\prime})\\) of Eqs. (71), (72): along these curves the arrows show the direction where the capacities have to decrease (or at most remain constant) – see Corollaries 4.2 and 4.3 of App. A. For the points in the white regions the composition rules cannot be used to determine a specific capacity ordering with respect to the capacity of the red dot element.\n",
      "saturated whenever \\(N_{2}=0\\). Hence we have\n",
      "\n",
      "\\[\\mathbb{L}_{g,N}^{(\\mathrm{amp},1)}\\subseteq\\mathbb{L}^{(0)}(\\mathcal{A}_{g,N})\\;. \\tag{64}\\]\n",
      "\n",
      "which together with (64) gives us the thesis. The derivation of the second inclusion of Eq. (68) follows along the same lines by simply inverting the roles of \\((g^{\\prime},N^{\\prime})\\) and \\((g,N)\\) in the previous passages. \n",
      "\n",
      "It is finally possible to establish a partial ordering between the capacities of attenuators and those of the amplifiers. Specifically given \\(\\eta\\in[0,1]\\) and \\(N\\geq 0\\) and the functions\n",
      "\n",
      "\\[N_{\\eta,N}^{(3)}(g) := N\\left(\\tfrac{1-\\eta}{\\eta}\\right)\\left(\\tfrac{g}{g-1}\\right)-1\\;, \\tag{65}\\] \\[N_{\\eta,N}^{(4)}(g) := (N+1)\\left(\\tfrac{1-\\eta}{g-1}\\right)\\;, \\tag{66}\\]\n",
      "\n",
      "it follows that\n",
      "\n",
      "**Property 3**.: _The attenuator map \\(\\mathcal{E}_{\\eta,N}\\) admits_\n",
      "\n",
      "\\[\\chi_{\\eta,N}^{(\\mathrm{att},+)} := \\left\\{(g,N^{\\prime})\\cdot g\\geq 1,0\\leq N^{\\prime}\\leq N_{g,N}^{( 3)}(g)\\right\\}, \\tag{67}\\] \\[\\chi_{\\eta,N}^{(\\mathrm{att},-)} := \\left\\{(g,N^{\\prime})\\cdot g\\geq 1,N^{\\prime}\\geq\\max\\{N_{ \\eta,N}^{(4)}(g),0\\}\\right\\},\\]\n",
      "\n",
      "_(yellow and light blue areas on the right-hand-side of the top panel of Fig. 4) as subsets of \\(\\mathbb{H}^{(0)}(\\mathcal{E}_{\\eta,N})\\) and \\(\\mathbb{L}^{(0)}(\\mathcal{E}_{\\eta,N})\\) respectively, i.e._\n",
      "\n",
      "\\[\\chi_{\\eta,N}^{(\\mathrm{att},+)}\\subseteq\\mathbb{H}^{(0)}(\\mathcal{E}_{\\eta,N })\\;,\\qquad\\chi_{\\eta,N}^{(\\mathrm{att},-)}\\subseteq\\mathbb{L}^{(0)}( \\mathcal{E}_{\\eta,N})\\;. \\tag{68}\\]\n",
      "\n",
      "Proof.: To prove the first inclusion we use the decomposition rule \\((\\mathbf{C_{3.1}})\\) of Tab. 1, which setting \\((\\eta_{3},N_{3})=(\\eta,N)\\), \\((g_{1},N_{1})=(g,N^{\\prime})\\), and arbitrary \\(\\eta_{2}\\in[0,1]\\), \\(N_{2}\\geq 0\\), allows us to write\n",
      "\n",
      "\\[\\mathcal{E}_{\\eta,N}=\\mathcal{E}_{\\eta_{2},N_{2}}\\circ\\mathcal{A}_{g,N^{ \\prime}}\\;, \\tag{69}\\]\n",
      "\n",
      "for all \\(g\\geq 1\\) and \\(N^{\\prime}\\leq N_{\\eta,N}^{(3)}(g)\\), i.e. for the entire set \\(\\chi_{\\eta,N}^{(\\mathrm{att},+)}\\). The second inclusion follows instead from Eq. \\((\\mathbf{C_{3.2}})\\) of Tab. 1, which setting setting \\((\\eta_{2},N_{2})=(\\eta,N)\\), \\((g_{3},N_{3})=(g,N^{\\prime})\\), and arbitrary \\(g_{1}\\geq 1\\), \\(N_{1}\\geq 0\\) allows us to write\n",
      "\n",
      "\\[\\mathcal{A}_{g,N^{\\prime}}=\\mathcal{E}_{\\eta,N}\\circ\\mathcal{A}_{g_{1},N_{1}}\\;, \\tag{70}\\]\n",
      "\n",
      "for all \\(g\\geq 1\\) and \\(N^{\\prime}\\geq N_{\\eta,N}^{(4)}(g)\\), i.e. for the entire set \\(\\chi_{\\eta,N}^{(\\mathrm{att},-)}\\). \n",
      "\n",
      "In a similar way, given\n",
      "\n",
      "\\[N_{g,N}^{(3)}(\\eta) := N\\left(\\tfrac{g-1}{1-\\eta}\\right)-1\\;, \\tag{71}\\] \\[N_{g,N}^{(4)}(\\eta) := (N+1)\\left(\\tfrac{g-1}{g}\\right)\\left(\\tfrac{\\eta}{1-\\eta} \\right)\\;, \\tag{72}\\]\n",
      "\n",
      "we have that\n",
      "\n",
      "**Property 4**.: _The amplifier map \\(\\mathcal{A}_{g,N}\\) admits_\n",
      "\n",
      "\\[\\chi_{g,N}^{(\\mathrm{amp},+)} := \\tag{73}\\] \\[\\chi_{g,N}^{(\\mathrm{amp},-)} := \\left\\{(\\eta,N^{\\prime})\\cdot g\\geq 1,N^{\\prime}\\geq\\max\\{N_{g, N}^{(4)}(\\eta),0\\}\\right\\},\\]\n",
      "\n",
      "_(yellow and light blue areas on the left-hand-side of the bottom panel of Fig. 4) as subsets of \\(\\mathbb{H}^{(0)}(\\mathcal{A}_{g,N})\\) and \\(\\mathbb{L}^{(0)}(\\mathcal{A}_{g,N})\\) respectively, i.e._\n",
      "\n",
      "\\[\\chi_{g,N}^{(\\mathrm{amp},+)}\\subseteq\\mathbb{H}^{(0)}(\\mathcal{A}_{g,N})\\;, \\qquad\\chi_{g,N}^{(\\mathrm{amp},-)}\\subseteq\\mathbb{L}^{(0)}(\\mathcal{A}_{g,N })\\;. \\tag{74}\\]\n",
      "\n",
      "Proof.: The above inclusions are just an alternative way to cast the results of Property 3. For the sake of symmetry we provide however an independent derivation. The first can be proven by using the decomposition rule \\((\\mathbf{C_{3.2}})\\) of Tab. 1, which setting \\((g_{3},N_{3})=(g,N)\\), \\((\\eta_{2},N_{2})=(\\eta,N^{\\prime})\\), and \\(g_{1}\\geq 1\\), \\(N_{1}\\geq 0\\), allows us to write\n",
      "\n",
      "\\[\\mathcal{A}_{g,N}=\\mathcal{E}_{\\eta,N^{\\prime}}\\circ\\mathcal{A}_{g_{1},N_{1}}\\;, \\tag{75}\\]\n",
      "\n",
      "for all \\(\\eta\\in[0,1]\\) and \\(0\\leq N^{\\prime}\\leq N_{g,N}^{(3)}(\\eta)\\), i.e. for all the points of \\(\\chi_{g,N}^{(\\mathrm{amp},+)}\\). The second inclusion of Eq. (74) follows instead from Eq. \\((\\mathbf{C_{3.1}})\\) of Tab. 1, which setting setting \\((\\eta_{3},N_{3})=(\\eta,N^{\\prime})\\), \\((g_{1},N_{1})=(g,N)\\), and \\(\\eta_{2}\\in[0,1]\\), \\(N_{2}\\geq 0\\), gives us\n",
      "\n",
      "\\[\\mathcal{E}_{\\eta,N^{\\prime}}=\\mathcal{E}_{\\eta_{2},N_{2}}\\circ\\mathcal{A}_{g,N }\\;, \\tag{76}\\]\n",
      "\n",
      "for all \\(\\eta\\in[0,1]\\) and \\(N^{\\prime}\\geq N_{g,N}^{(4)}(\\eta)\\), i.e. for all the points of \\(\\chi_{g,N}^{(\\mathrm{amp},-)}\\). \n",
      "\n",
      "Inclusions which are different with respect to the one reported in the Properties can be obtained by reversing the order of the decompositions employed in the proofs. Such inequalities however are provably less performant than those presented. For instance setting \\((\\eta_{3},N_{3})=(\\eta^{\\prime},N^{\\prime})\\) and \\((\\eta_{2},N_{2})=(\\eta,N)\\) in Eq. \\((\\mathbf{C_{1}})\\) allows us to determine that \\((\\eta^{\\prime},N^{\\prime})\\in\\mathbb{L}(\\mathcal{E}_{\\eta,N})\\) for all\n",
      "\n",
      "\\[\\eta^{\\prime}\\leq\\eta\\;,\\qquad N^{\\prime}\\geq\\left(\\frac{1-\\eta}{1-\\eta^{ \\prime}}\\right)N\\;, \\tag{77}\\]\n",
      "\n",
      "a result which is implied by (50) since the set \\(\\mathbb{L}_{\\eta,N}^{(\\mathrm{att},2)}\\) includes all points falling the condition (77). Similarly using Eq. \\((\\mathbf{C_{4.1}})\\) instead of \\((\\mathbf{C_{3.1}})\\) allows one to show that i) \\((\\eta^{\\prime},N^{\\prime})\\in\\mathbb{L}(\\mathcal{E}_{\\eta,N})\\) for all \\(\\eta^{\\prime}\\geq\\eta\\) and \\(N^{\\prime}\\geq\\frac{\\eta^{\\prime}-\\eta+\\eta^{\\prime}(1-\\eta)N}{\\eta(1-\\eta^{ \\prime})}\\) (a condition that is already implied by (47)); ii) \\((\\eta^{\\prime},N^{\\prime})\\in\\mathbb{H}(\\mathcal{E}_{\\eta,N})\\) for all \\(\\eta^{\\prime}\\leq\\eta\\) and \\(N^{\\prime}\\leq\\frac{\\eta^{\\prime}-\\eta+\\eta^{\\prime}(1-\\eta)N}{\\eta(1-\\eta^{ \\prime})}\\) (which again is implied by (47)); iii) \\((g,N^{\\prime})\\in\\mathbb{H}(\\mathcal{E}_{\\eta,N})\\) for all \\(g\\geq 1\\) and \\(N^{\\prime}\\leq N\\left(\\frac{1-\\eta}{g-1}\\right)-1\\) (implied by the first inclusion of Eq. (68)); _iv)_\\(g,N^{\\prime})\\in\\mathbb{L}(\\mathcal{E}_{\\eta,N})\\) for all \\(g\\geq 1\\) and \\(N^{\\prime}\\geq(N+1)\\left(\\frac{g}{g-1}\\right)\\left(\\frac{1-\\eta}{\\eta}\\right)\\) (implied by the second inclusion of Eq. (68)). Putting together these results we can hence conclude that the two-elements concatenation\n",
      "regions \\(\\mathbb{L}^{(0)}(\\mathcal{E}_{\\eta,N})\\) and \\(\\mathbb{L}^{(0)}(\\mathcal{A}_{g,N})\\) coincide respectively with \\(\\mathbb{L}^{(\\text{att})}_{\\eta,N}\\bigcup\\mathbb{X}^{(\\text{att},-)}_{\\eta,N}\\) and \\(\\mathbb{L}^{(\\text{amp})}_{g,N}\\bigcup\\mathbb{X}^{(\\text{amp},-)}_{g,N}\\), a condition which translated into the parametrization (3) can be expressed as\n",
      "\n",
      "\\[\\mathbb{L}^{(0)}_{x,M} = \\left\\{\\begin{array}{ll}\\mathbb{L}^{(\\text{att})}_{x,M/(1-x)} \\bigcup\\mathbb{X}^{(\\text{att},-)}_{x,M/(1-x)}&\\text{for $x\\in[0,1]$}\\;,\\\\ \\\\ \\mathbb{L}^{(\\text{amp})}_{x,M/(x-1)}\\bigcup\\mathbb{X}^{(\\text{amp},-)}_{x,M/( x-1)}&\\text{for $x\\geq 1$}\\;.\\end{array}\\right. \\tag{78}\\]\n",
      "\n",
      "Analogously we have that \\(\\mathbb{H}^{(0)}(\\mathcal{E}_{\\eta,N})\\) and \\(\\mathbb{H}^{(0)}(\\mathcal{A}_{g,N})\\) correspond respectively to \\(\\mathbb{H}^{(\\text{att})}_{\\eta,N}\\bigcup\\mathbb{X}^{(\\text{att},+)}_{\\eta,N}\\) and \\(\\mathbb{H}^{(\\text{amp})}_{g,N}\\bigcup\\mathbb{X}^{(\\text{amp},+)}_{g,N}\\), so that\n",
      "\n",
      "\\[\\mathbb{H}^{(0)}_{x,M} = \\left\\{\\begin{array}{ll}\\mathbb{H}^{(\\text{att})}_{x,M/(1-x)} \\bigcup\\mathbb{X}^{(\\text{att},+)}_{x,M/(1-x)}&\\text{for $x\\in[0,1]$}\\;,\\\\ \\\\ \\mathbb{H}^{(\\text{amp})}_{x,M/(x-1)}\\bigcup\\mathbb{X}^{(\\text{amp},+)}_{x,M/( x-1)}&\\text{for $x\\geq 1$}\\;.\\end{array}\\right. \\tag{79}\\]\n",
      "\n",
      "The border lines of these regions are provided by the curves \\(M^{(j)}_{x,M}(x^{\\prime})\\) of Tab. 2 obtained from \\(N^{(1,2)}_{\\eta,N}(\\eta^{\\prime})\\), \\(N^{(1,2)}_{g,N}(g^{\\prime})\\), \\(N^{(3,4)}_{\\eta,N}(g^{\\prime})\\), and \\(N^{(1,2)}_{g,N}(\\eta^{\\prime})\\) via the substitutions\n",
      "\n",
      "\\[M^{(j)}_{x,M}(x^{\\prime}):=|1-x^{\\prime}|N^{(j)}_{x,M/|1-x|}(x^{\\prime})\\;. \\tag{80}\\]\n",
      "\n",
      "For instance one has that \\(\\mathbb{L}^{(0)}_{x,M}\\) is given by the points \\((x^{\\prime},M^{\\prime})\\) such that\n",
      "\n",
      "\\[M^{\\prime}\\geq\\left\\{\\begin{array}{ll}M^{(1)}_{x,M}(x^{\\prime})=M\\frac{x^{ \\prime}}{x},&(0\\leq x^{\\prime}\\leq x),\\\\ M^{(2)}_{x,M}(x^{\\prime})=M-x+x^{\\prime},&(x\\leq x^{\\prime}\\leq 1),\\\\ M^{(4)}_{x,M}(x^{\\prime})=M+1-x,&(1\\leq x^{\\prime}),\\end{array}\\right. \\tag{81}\\]\n",
      "\n",
      "for \\(x\\leq 1\\), and\n",
      "\n",
      "\\[M^{\\prime}\\geq\\left\\{\\begin{array}{ll}M^{(4)}_{x,M}(x^{\\prime})=(M+x-1)\\frac {x^{\\prime}}{x},&(0\\leq x^{\\prime}\\leq 1),\\\\ M^{(2)}_{x,M}(x^{\\prime})=(M-1)\\frac{x^{\\prime}}{x}+1,&(1\\leq x^{\\prime}\\leq x ),\\\\ M^{(1)}_{x,M}(x^{\\prime})=M,&(x\\leq x^{\\prime}),\\end{array}\\right. \\tag{82}\\]\n",
      "\n",
      "for \\(x\\geq 1\\). Viceversa we have that \\(\\mathbb{H}^{(0)}_{x,M}\\) includes all points \\((x^{\\prime},M^{\\prime})\\) such that\n",
      "\n",
      "\\[0\\leq M^{\\prime}\\leq\\left\\{\\begin{array}{ll}M^{(2)}_{x,M}(x^{\\prime})=M-x+x^ {\\prime},&(0\\leq x^{\\prime}\\leq x),\\\\ M^{(1)}_{x,M}(x^{\\prime})=M\\frac{x^{\\prime}}{x},&(x\\leq x^{\\prime}\\leq 1),\\\\ M^{(3)}_{x,M}(x^{\\prime})=(M-x)\\frac{x^{\\prime}}{x}+1,&(1\\leq x^{\\prime}), \\end{array}\\right. \\tag{83}\\]\n",
      "\n",
      "for \\(x\\leq 1\\), and\n",
      "\n",
      "\\[0\\leq M^{\\prime}\\leq\\left\\{\\begin{array}{ll}M^{(3)}_{x,M}(x^{\\prime})=M-1+x ^{\\prime},&(0\\leq x^{\\prime}\\leq 1),\\\\ M^{(1)}_{x,M}(x^{\\prime})=M,&(1\\leq x^{\\prime}\\leq x),\\\\ M^{(2)}_{x,M}(x^{\\prime})=(M-1)\\frac{x^{\\prime}}{x}+1,&(x\\leq x^{\\prime}),\\end{array}\\right. \\tag{84}\\]\n",
      "\n",
      "for \\(x\\geq 1\\).\n",
      "\n",
      "### Three-elements concatenation regions for channels \\(\\Phi_{x,M}\\) fulfilling Eq. (32)\n",
      "\n",
      "Comparing the l.h.s. of Eqs. (81)-(84) with the functions \\(f^{(1)}_{x,M}(x^{\\prime})\\) and \\(f^{(1)}_{x,M}(x^{\\prime})\\) of Eq. (33), one can easily check that for channels \\(\\Phi_{x,M}\\) which fulfil Eq. (32), the two-element concatenation regions \\(\\mathbb{L}^{(0)}_{x,M}\\) and \\(\\mathbb{H}^{(0)}_{x,M}\\) can be expressed as\n",
      "\n",
      "\\[\\mathbb{L}^{(0)}_{x,M} = \\left\\{(x^{\\prime},M^{\\prime})\\in\\left(\\mathbb{R}^{+}\\right)^{2}:\\right.\\] \\[\\left.M^{\\prime}\\geq\\max\\{f^{(1)}_{x,M}(x^{\\prime}),f^{(2)}_{x,M} (x^{\\prime})\\}\\right\\}\\,,\\] \\[\\mathbb{H}^{(0)}_{x,M} = \\left\\{(x^{\\prime},M^{\\prime})\\in\\left(\\mathbb{R}^{+}\\right)^{2}:\\right.\\] \\[\\left.M^{\\prime}\\leq\\min\\{f^{(1)}_{x,M}(x^{\\prime}),f^{(2)}_{x,M} (x^{\\prime})\\}\\right\\}\\,.\\]\n",
      "\n",
      "Accordingly under the condition (32), the proof of the identities (34) and (35) reduces hence to show that the two-element concatenation regions \\(\\mathbb{L}^{(0)}_{x,M}\\) and \\(\\mathbb{H}^{(0)}_{x,M}\\) correspond to the three-elements concatenations sets \\(\\mathbb{L}_{x,M}\\) and \\(\\mathbb{H}_{x,M}\\), i.e.\n",
      "\n",
      "**Corollary 4.1**.: _Given \\((x,M)\\) such that \\(M\\leq M_{EB}(x)\\) we have that_\n",
      "\n",
      "\\[\\mathbb{L}^{(0)}_{x,M}=\\mathbb{L}_{x,M}\\;,\\qquad\\mathbb{H}^{(0)}_{x,M}=\\mathbb{H }_{x,M}\\;. \\tag{87}\\]\n",
      "\n",
      "Proof.: This result can be derived by noticing that for channels \\(\\Phi_{x,M}\\) which are non-EB, \\(\\mathbb{L}^{(0)}_{x,M}\\) and \\(\\mathbb{H}^{(0)}_{x,M}\\) fulfil the same ordering rules of \\(\\mathbb{L}_{x,M}\\) and \\(\\mathbb{H}_{x,M}\\) given in Eqs. (29) and (30), i.e.\n",
      "\n",
      "\\[M<M_{\\text{EB}}(x)\\Longrightarrow\\left\\{\\begin{array}{ll}\\mathbb{L}^{(0)}_{x^{ \\prime},M^{\\prime}}\\subseteq\\mathbb{L}^{(0)}_{x,M}&,\\forall(x^{\\prime},M^{ \\prime})\\in\\mathbb{L}^{(0)}_{x,M}\\\\ \\\\ \\mathbb{H}^{(0)}_{x^{\\prime},M^{\\prime}}\\subseteq\\mathbb{H}^{(0)}_{x,M}&,\\forall(x^{ \\prime},M^{\\prime})\\in\\mathbb{H}^{(0)}_{x,M}\\\\ \\end{array}\\right. \\tag{88}\\]\n",
      "\n",
      "The derivation of these identities relies on a series geometric relations in which one has to compare the relative size and position of the two-dimensional polytopes defined in Eqs. (93) and (94). It suffices to do show this for the high ground region. In fact, since \\((x^{\\prime},M^{\\prime})\\in\\mathbb{L}^{(0)}_{x,M}\\) if and only if \\((x,M)\\in\\mathbb{H}^{(0)}_{x^{\\prime},M^{\\prime}}\\), we have that \\((x^{\\prime\\prime},M^{\\prime\\prime})\\in\\mathbb{L}^{(0)}_{x,M}\\), \\((x^{\\prime\\prime},M^{\\prime\\prime})\\in\\mathbb{L}^{(0)}_{x^{\\prime},M^{\\prime}}\\), \\((x^{\\prime},M^{\\prime})\\in\\mathbb{L}^{(0)}_{x,M}\\) if and only if \\((x,M)\\in\\mathbb{H}^{(0)}_{x^{\\prime\\prime},M^{\\prime\\prime}}\\), \\((x^{\\prime},M^{\\prime})\\in\\mathbb{H}^{(0)}_{x^{\\prime\\prime},M^{\\prime\\prime}}\\), \\((x,M)\\in\\mathbb{H}^{(0)}_{x^{\\prime\\prime},M^{\\prime\\prime}}\\). Therefore it suffices to prove only that the latter is true whenever \\((x^{\\prime},M^{\\prime})\\in\\mathbb{H}^{(0)}_{x^{\\prime\\prime},M^{\\prime\\prime\n",
      "of \\(\\mathbb{L}_{x_{2},M_{2}}^{(0)}\\) (indeed \\(\\Phi_{x^{\\prime},M^{\\prime}}=\\Phi_{\\bar{x}_{1},\\bar{M}_{1}}\\circ\\Phi_{x_{2},M_{2}}\\)). However, because of Eq. (88) the latter is a subset of \\(\\mathbb{L}_{x,M}^{(0)}\\), so we can conclude that\n",
      "\n",
      "\\[(x^{\\prime},M^{\\prime})\\in\\mathbb{L}_{x,M}\\Longrightarrow(x^{\\prime},M^{\\prime })\\in\\mathbb{L}_{x,M}^{(0)}\\;, \\tag{90}\\]\n",
      "\n",
      "which together with Eq. (42) gives the first of the identities Eq. (87).\n",
      "\n",
      "By the same token, let \\((x^{\\prime},M^{\\prime})\\in\\mathbb{H}_{x,M}\\): from (27) it follows that we can write\n",
      "\n",
      "\\[\\Phi_{x,M}=\\Phi_{\\bar{x}_{1},\\bar{M}_{1}}\\circ\\Phi_{x^{\\prime},M^{\\prime}}\\circ \\Phi_{\\bar{x}_{2},\\bar{M}_{2}}\\;, \\tag{91}\\]\n",
      "\n",
      "for some proper choice of \\((\\bar{x}_{1},\\bar{M}_{1}),(\\bar{x}_{2},\\bar{M}_{2})\\in\\left(\\mathbb{R}^{+} \\right)^{2}\\). Setting then \\(\\Phi_{x_{2},M_{2}}:=\\Phi_{x^{\\prime},M^{\\prime}}\\circ\\Phi_{\\bar{x}_{2},\\bar{M }_{2}}\\), we can claim that \\((x^{\\prime},M^{\\prime})\\) is an element of \\(\\mathbb{H}_{x_{2},M_{2}}^{(0)}\\), and \\((x_{2},M_{2})\\) an element of \\(\\mathbb{H}_{x,M}^{(0)}\\) (indeed \\(\\Phi_{x,M}=\\Phi_{\\bar{x}_{1},\\bar{M}_{1}}\\circ\\Phi_{x_{2},M_{2}}\\)). However, because of Eq. (88) it follows that \\(\\mathbb{H}_{x_{2},M_{2}}^{(0)}\\) is included into \\(\\mathbb{H}_{x,M}^{(0)}\\), so that\n",
      "\n",
      "\\[(x^{\\prime},M^{\\prime})\\in\\mathbb{H}_{x,M}\\Longrightarrow(x^{\\prime},M^{\\prime })\\in\\mathbb{H}_{x,M}^{(0)}\\;, \\tag{92}\\]\n",
      "\n",
      "which gives the second of the identities Eq. (87). \n",
      "\n",
      "### Three-elements concatenation regions for channels \\(\\Phi_{x,M}\\) fulfilling (36)\n",
      "\n",
      "Here we show that for channels \\(\\Phi_{x,M}\\) which are deep in the EB region (i.e. such that (36) holds true), the low-ground/high-ground regions are determined by Eqs. (38) and (37). To begin with let us observe that, for \\(M\\geq M_{\\rm EB}(x)\\), Eqs. (81)-(84) lead to express the two-elements concatenation sets \\(\\mathbb{L}_{x,M}^{(0)}\\) and \\(\\mathbb{H}_{x,M}^{(0)}\\) as\n",
      "\n",
      "\\[\\mathbb{L}_{x,M}^{(0)} = \\Big{\\{}(x^{\\prime},M^{\\prime})\\in\\left(\\mathbb{R}^{+}\\right)^{2}:\\] \\[\\quad M^{\\prime}\\geq\\min\\{f_{x,M}^{(1)}(x^{\\prime}),f_{x,M}^{(2)} (x^{\\prime})\\}\\Big{\\}}\\;,\\] \\[\\mathbb{H}_{x,M}^{(0)} = \\Big{\\{}(x^{\\prime},M^{\\prime})\\in\\left(\\mathbb{R}^{+}\\right)^{2}:\\] \\[\\quad M^{\\prime}\\leq\\max\\{f_{x,M}^{(1)}(x^{\\prime}),f_{x,M}^{(2)} (x^{\\prime})\\}\\Big{\\}}\\;,\\]\n",
      "\n",
      "with \\(f_{x,M}^{(1,2)}(x^{\\prime})\\) the functions defined in Eq. (33). It turns out that such regions always admit a non trivial overlap \\(\\mathbb{O}_{x,M}^{(0)}:=\\mathbb{L}_{x,M}^{(0)}\\bigcap\\mathbb{H}_{x,M}^{(0)}\\) that includes a finite portion of the plane \\(\\left(\\mathbb{R}^{+}\\right)^{2}\\) in the neighbourhood of the origin - see Fig. 6. As a matter of fact \\((0,0)\\) itself can be considered as element of \\(\\mathbb{O}_{x,M}^{(0)}\\) (to be precise, it is an element of the closure of \\(\\mathbb{O}_{x,M}^{(0)}\\) - see App. B for a refinement of the following argument, taking this into account), i.e.\n",
      "\n",
      "\\[M>M_{\\rm EB}(x)\\Longrightarrow(0,0)\\in\\mathbb{O}_{x,M}^{(0)}\\subseteq\\mathbb{ H}_{x,M}^{(0)}\\subseteq\\mathbb{H}_{x,M}\\;. \\tag{95}\\]\n",
      "\n",
      "Now from (26) and (31) we know that\n",
      "\n",
      "\\[(x^{\\prime},M^{\\prime})\\in\\mathbb{H}_{0,0}\\;,\\qquad\\forall(x^{\\prime},M^{ \\prime})\\in\\left(\\mathbb{R}^{+}\\right)^{2}\\;, \\tag{96}\\]\n",
      "\n",
      "which using (29) gives\n",
      "\n",
      "\\[(x^{\\prime},M^{\\prime})\\in\\mathbb{H}_{x,M}\\;,\\qquad\\forall(x^{\\prime},M^{ \\prime})\\in\\left(\\mathbb{R}^{+}\\right)^{2}\\;, \\tag{97}\\]\n",
      "\n",
      "hence proving Eq. (37) Observe next that if \\((x^{\\prime},M^{\\prime})\\) is also EB, then from (97) we also have \\((x,M)\\in\\mathbb{H}_{x^{\\prime},M^{\\prime}}\\) which lead to Eq. (38) via the complementarity rule (31).\n",
      "## VI Stabilizing bounds\n",
      "\n",
      "Building up from the low-ground/high-ground analysis presented in the previous sections we can now detail a general technique that allows one to potentially improve existing upper and lower bounds for the capacities of single-mode PI-GBCs. Indeed suppose that \\(\\mathcal{K}^{(+)}(x,M)\\), \\(\\mathcal{K}^{(-)}(x,M)\\) are two functions which bound for the capacity \\(\\mathcal{K}(x,M):=\\mathcal{K}(\\Phi_{x,M})\\) of the channel \\(\\Phi_{x,M}\\), i.e.\n",
      "\n",
      "\\[\\mathcal{K}^{(+)}(x,M)\\geq\\mathcal{K}(x,M)\\geq\\mathcal{K}^{(+)}(x,M)\\;, \\tag{98}\\]\n",
      "\n",
      "for all \\((x,M)\\in\\left(\\mathbb{R}^{+}\\right)^{2}\\). From (25) and (28) it follows that the quantities\n",
      "\n",
      "\\[\\begin{cases}\\underline{\\mathcal{K}}^{(+)}(x,M)&:=\\min_{(x^{\\prime},M^{\\prime })\\in\\mathbb{H}_{x,M}}\\mathcal{K}^{(+)}(x^{\\prime},M^{\\prime})\\;,\\\\ \\overline{\\mathcal{K}}^{(-)}(x,M)&:=\\max_{(x^{\\prime},M^{\\prime})\\in \\mathbb{L}_{x,M}}\\mathcal{K}^{(-)}(x^{\\prime},M^{\\prime})\\;,\\end{cases} \\tag{99}\\]\n",
      "\n",
      "can potentially improve the inequalities (98), i.e.\n",
      "\n",
      "\\[\\begin{cases}&\\mathcal{K}^{(+)}(x,M)\\geq\\underline{\\mathcal{K}}^{(+)}(x,M)\\;, \\\\ &\\\\ &\\underline{\\mathcal{K}}^{(+)}(x,M)\\geq\\mathcal{K}(x,M)\\geq\\overline{\\mathcal{ K}}^{(-)}(x,M)\\;,\\\\ &\\\\ &\\overline{\\mathcal{K}}^{(-)}(x,M)\\geq\\mathcal{K}^{(-)}(x,M)\\;.\\end{cases} \\tag{100}\\]\n",
      "\n",
      "Of course it is very possible that the functions \\(\\underline{\\mathcal{K}}^{(+)}(x,M)\\) and \\(\\overline{\\mathcal{K}}^{(-)}(x,M)\\) will coincides with \\(\\mathcal{K}^{(+)}(x,M)\\) and \\(\\mathcal{K}^{(-)}(x,M)\\), respectively: this happens for instance for all bounding functions (98) which arise from operational procedures that automatically incorporate data processing, e.g. the bounds (18), (22), and (23). There are however examples where the construction (99) leads to non trivial overall improvements. In what follow we shall detail one of such cases.\n",
      "\n",
      "### Improving the upper bounds for the quantum capacity of thermal attenuators\n",
      "\n",
      "Expressing the functions (22) in terms of the parametrization (3) we can claim that the quantum and\n",
      "\n",
      "Figure 5: Graphical explanation of inclusions rules of Eq. (88). The border of the high ground region can have two different shapes, depending on \\(x_{i}\\) being smaller or larger than \\(1\\) (the degenerate case \\(x_{i}=1\\) is not plotted but it is analogous). In the case \\(x_{i}<1\\), the region is obtained as the intersection of \\(\\left(\\mathbb{R}^{+}\\right)^{2}\\) with the half-planes delimited by a \\(45\\) degrees line passing through \\((x_{i},M_{i})\\), a line passing through the origin and \\((x_{i},M_{i})\\) and intersecting the \\(x=1\\) line at \\(\\mathbf{C_{i}}\\), and a line passing through \\(\\mathbf{C_{i}}\\) and \\((0,1)\\). In the case \\(x_{i}>1\\), the region is obtained as the intersection of \\(\\left(\\mathbb{R}^{+}\\right)^{2}\\) with the half-planes delimited by an horizontal line passing through \\((x_{i},M_{i})\\) and intersecting the \\(x=1\\) line at \\(\\mathbf{B_{i}}\\), a \\(45\\) degrees line intersecting \\(\\mathbf{B_{i}}\\), and a line passing through \\((x_{i},M_{i})\\) and \\((0,1)\\). If \\(x_{0}<1\\) and \\(x_{1}<1\\), all the the intersection of \\(\\left(\\mathbb{R}^{+}\\right)^{2}\\) and the half-planes individuated by the segments \\(\\mathbf{A_{1}}-(x,M)\\), \\((x,M)-\\mathbf{C_{1}}\\) and \\(\\mathbf{C_{1}}-\\mathbf{D_{1}}\\) are contained in the corresponding region individuated by \\(x_{0}\\), and therefore their intersection also satisfy the same inclusions. If \\(x_{0}>1\\) and \\(x_{1}>1\\), all the the intersection of \\(\\left(\\mathbb{R}^{+}\\right)^{2}\\) and the half-planes individuated by the segments \\(\\mathbf{A_{1}}-\\mathbf{B_{1}}\\), \\(\\mathbf{B_{1}}-(x,M)\\),and \\((x,M)-\\mathbf{D_{1}}\\) are contained in the corresponding region individuated by \\(x_{0}\\), and therefore their intersection also satisfy the same inclusions. If \\(x_{0}<1\\) and \\(x_{1}>1\\), a calculation shows that in the non EB region the region on the left is convex, therefore it contains the triangle \\(\\mathbf{A_{1}}-\\mathbf{B_{1}}-(1,0)\\), while the inclusion of the triangle \\(\\mathbf{C_{1}}-\\mathbf{D_{1}}-(1,0)\\) follows from the same half-plane argument as before. If \\(x_{0}>1\\) and \\(x_{1}<1\\), a calculation shows that in the non EB region the region on the right is convex, therefore it contains the triangle \\(\\mathbf{C_{1}}-\\mathbf{D_{1}}-(1,0)\\), while the inclusion of the triangle \\(\\mathbf{A_{1}}-\\mathbf{C_{1}}-(1,0)\\) follows from the same half-plane argument as before.\n",
      "\n",
      "Figure 6: Examples of the two-elements compositions low-ground/high-ground regions \\(\\mathbb{L}_{x,M}^{(0)}\\) and \\(\\mathbb{H}_{x,M}^{(0)}\\) for EB channels \\(\\Phi_{x,M}\\). Notice that such sets admits a non trivial overlap \\(\\mathbb{O}_{x,M}^{(0)}:=\\mathbb{L}_{x,M}^{(0)}\\bigcap\\mathbb{H}_{x,M}^{(0)}\\) (pale yellow region) which always includes the origin point \\((0,0)\\).\n",
      "private capacities of the channel \\(\\Phi_{x,M}\\) is always smaller than or equal to\n",
      "\n",
      "\\[Q_{\\rm FKG}(x,M):=\\left\\{\\begin{array}{ll}Q_{\\rm FKG}^{\\rm att}(x,\\frac{M}{1-x}) \\;,&\\forall x\\in[0,1]\\;,\\\\ \\\\ Q_{\\rm FKG}^{\\rm amp}(M)\\;,&\\forall x\\geq 1\\;.\\end{array}\\right. \\tag{101}\\]\n",
      "\n",
      "Following Eq. (99) we can produce an upper bound \\(\\underline{Q}_{\\rm FKG}(x,M)\\) by taking the minimum of \\(\\underline{Q}_{\\rm FKG}(x^{\\prime},M^{\\prime})\\) over the set \\(\\mathbb{H}_{x,M}\\). This strategy had been suggested and shown to give improvements in [41], and it will be fully explored here. Without loss of generality we assume \\((x,M)\\) not to belong to the AD domain \\(\\mathbb{A}\\), i.e.\n",
      "\n",
      "\\[\\begin{cases}x\\geq 1/2,\\\\ M\\leq M_{\\rm AD}(x)=\\min\\{x-1/2,1/2\\}\\;,\\end{cases} \\tag{102}\\]\n",
      "\n",
      "a condition which via Eq. (35), allows us to identify the high-ground set of the model with the yellow regions of Fig. 3. We next compute the value \\(\\underline{Q}_{\\rm FKG}^{(1)}(x,M)\\) which represents the minimum of \\(Q_{\\rm FKG}(x^{\\prime},M^{\\prime})\\) for points of \\(\\mathbb{H}_{x,M}\\) which have \\(x^{\\prime}\\in[0,1]\\), and the value \\(\\underline{Q}_{\\rm FKG}^{(2)}(x,M)\\) which instead involves points of \\(\\mathbb{H}_{x,M}\\) with \\(x\\geq 1\\), i.e.\n",
      "\n",
      "\\[\\underline{Q}_{\\rm FKG}^{(1)}(x,M) := \\min_{(x^{\\prime},M^{\\prime})\\in\\mathbb{H}_{x,M};x^{\\prime}\\in[0,1]}Q_{\\rm FKG}^{\\rm att}(x^{\\prime},\\frac{M^{\\prime}}{1-x^{\\prime}})\\;,\\] \\[\\underline{Q}_{\\rm FKG}^{(2)}(x,M) := \\min_{(x^{\\prime},M^{\\prime})\\in\\mathbb{H}_{x,M};x^{\\prime}\\geq 1 }Q_{\\rm FKG}^{\\rm amp}(M^{\\prime})\\;. \\tag{103}\\]\n",
      "\n",
      "Once we have these terms we can then write the global minimum of \\(Q_{\\rm FKG}(x,M)\\) over \\(\\mathbb{H}_{x,M}\\) as\n",
      "\n",
      "\\[\\underline{Q}_{\\rm FKG}(x,M)=\\min\\{\\underline{Q}_{\\rm FKG}^{(1)}(x,M), \\underline{Q}_{\\rm FKG}^{(2)}(x,M)\\}\\;. \\tag{104}\\]\n",
      "\n",
      "Consider first the evaluation of \\(\\underline{Q}_{\\rm FKG}^{(2)}(x,M)\\). Let us start observing that from Eqs. (83) and (84), the maximum value of \\(M^{\\prime}\\) we can get for points \\((x^{\\prime},M^{\\prime})\\) of \\(\\mathbb{H}_{x,M}\\) with \\(x^{\\prime}\\geq 1\\) is\n",
      "\n",
      "\\[M_{\\rm max}^{(>)}(x):=\\left\\{\\begin{array}{ll}M/x\\;,&\\forall x\\in[\\frac{1}{ 2},1]\\\\ M\\;,&\\forall x\\geq 1\\end{array}\\right.=\\frac{M}{M_{\\rm EB}(x)}\\;, \\tag{105}\\]\n",
      "\n",
      "which, in virtue of Eq. (VII) is always smaller than or equal to \\(1/2\\). Recalling hence that on the interval \\(\\kappa\\in[0,1/2]\\) the function \\(Q_{\\rm FKG}^{\\rm amp}(\\kappa)\\) is monotonically decreasing, we can write\n",
      "\n",
      "\\[\\underline{Q}_{\\rm FKG}^{(2)}(x,M)=Q_{\\rm FKG}^{\\rm amp}(M_{\\rm max }^{(>)}(x))\\] \\[=-\\log_{2}(\\frac{\\epsilon M}{M_{\\rm EB}(x)})+2h\\left(\\frac{\\sqrt{M ^{2}+M_{\\rm EB}^{2}(x)}-M_{\\rm EB}^{2}(x)}{2M_{\\rm EB}(x)}\\right).\\]\n",
      "\n",
      "In the case of amplifiers (i.e. for \\(x\\geq 1\\)) this implies that \\(\\underline{Q}_{\\rm FKG}^{(2)}(x,M)\\) always coincides with the old bound (101), so no improvement can be obtained.\n",
      "\n",
      "Consider next \\(\\underline{Q}_{\\rm FKG}^{(1)}(x,M)\\). Here the key observation is that for fixed value of \\(x^{\\prime}\\), \\(Q_{\\rm FKG}^{\\rm att}(x^{\\prime},\\frac{M^{\\prime}}{1-x^{\\prime}})\\) is a decreasing function of \\(M^{\\prime}\\). Observe also that for \\(x\\in[\\frac{1}{2},1]\\), Eqs. (VII.2) and (VII.2) implies that the maximum value of \\(M^{\\prime}\\) we can get for points \\((x^{\\prime},M^{\\prime})\\) of \\(\\mathbb{H}_{x,M}\\) which have \\(x^{\\prime}\\leq 1\\) is\n",
      "\n",
      "\\[M_{\\rm max}^{(<)}(x^{\\prime},x):=\\left\\{\\begin{array}{ll}M+x^{\\prime}-x\\;,& \\forall x^{\\prime}\\in[x-M,x],\\\\ \\frac{x^{\\prime}}{x}M\\;,&\\forall x^{\\prime}\\in[x,1].\\end{array}\\right. \\tag{107}\\]\n",
      "\n",
      "Therefore we can write\n",
      "\n",
      "\\[\\underline{Q}_{\\rm FKG}^{(1)}(x,M) = \\min_{x^{\\prime}\\in[x-M,1]}Q_{\\rm FKG}^{\\rm att}(x^{\\prime},\\frac {M_{\\rm F}^{(<)}(x^{\\prime},x)}{1-x^{\\prime}})\\] \\[= \\min\\{\\underline{Q}_{\\rm FKG}^{(1,1)}(x,M),\\underline{Q}_{\\rm FKG }^{(1,2)}(x,M)\\}\\;,\\]\n",
      "\n",
      "with\n",
      "\n",
      "\\[\\underline{Q}_{\\rm FKG}^{(1.1)}(x,M) := \\min_{x^{\\prime}\\in[x-M,x]}Q_{\\rm FKG}^{\\rm att}(x^{\\prime},\\frac {M+x^{\\prime}-x}{1-x^{\\prime}})\\] \\[= \\min_{\\epsilon\\in[0,1]}Q_{\\rm FKG}^{\\rm att}(x-\\epsilon M,\\frac{(1 -\\epsilon)M}{1-x+\\epsilon M})\\;,\\]\n",
      "\n",
      "where the second identify simply follows from a proper parametrization of \\(x^{\\prime}\\), and\n",
      "\n",
      "\\[\\underline{Q}_{\\rm FKG}^{(1.2)}(x,M) := \\min_{x^{\\prime}\\in[x,1]}Q_{\\rm FKG}^{\\rm att}(x^{\\prime},\\frac{x^ {\\prime}M}{(1-x^{\\prime})x})\\] \\[= \\min_{\\epsilon\\in[0,1]}Q_{\\rm FKG}^{\\rm att}\\left(1+\\epsilon(1-x), \\frac{1+\\epsilon(1-x)M}{(1-x)x\\epsilon}\\right)\\;.\\]\n",
      "\n",
      "Notice that for \\(\\epsilon=0\\) the function \\(Q_{\\rm FKG}^{\\rm att}(x-\\epsilon M,\\frac{(1-\\epsilon)M}{1-x+\\epsilon M})\\) corresponds to \\(Q_{\\rm FKG}^{\\rm att}(x,M)\\) in Eq. (22) while for \\(\\epsilon=1\\) we recover the bound \\(Q(\\mathcal{E}_{x-M,0})\\) of Eq. (20). Therefore for the attenuators we have that \\(\\underline{Q}_{\\rm FKG}^{(1.1)}(x,M)\\) (and hence \\(\\underline{Q}_{\\rm FKG}^{(1)}(x,M)\\)) is always guaranteed to provide bounds which are at least equal than those reported in Eqs. (22) and (20), i.e.\n",
      "\n",
      "\\[\\underline{Q}_{\\rm FKG}^{(1)}(x,M)\\leq\\min\\{Q(\\mathcal{E}_{x-M,0}),Q_{\\rm FKG}^{ \\rm att}(x,M)\\}\\;. \\tag{111}\\]\n",
      "\n",
      "On the contrary for \\(x\\geq 1\\), Eq. (107) gets replaced\n",
      "\n",
      "\\[M_{\\rm max}^{(<)}(x^{\\prime},x):=M+x^{\\prime}-1\\;,\\forall x^{\\prime}\\in[1-M,1], \\tag{112}\\]\n",
      "\n",
      "leading to\n",
      "\n",
      "\\[\\underline{Q}_{\\rm FKG}^{(1)}(x,M) := \\min_{x^{\\prime}\\in[1-M,1]}Q_{\\rm FKG}^{\\rm att}(x^{\\prime},\\frac {M+x^{\\prime}-1}{1-x^{\\prime}}) \\tag{113}\\] \\[= \\min_{\\epsilon\\in[0,1]}Q_{\\rm FKG}^{\\rm att}\\left(1-\\epsilon M, \\frac{1-\\epsilon}{\\epsilon}\\right)\\;.\\]\n",
      "\n",
      "#### vi.2.1 Numerical analysis\n",
      "\n",
      "Numerical analysis shows that \\(\\underline{Q}_{\\rm FKG}^{(1.2)}(x,M)\\) is always less performant than \\(\\underline{Q}_{\\rm FKG}^{(1.1)}(x,M)\\) so we can drop it form Eq. (104). Accordingly we can claim that for an attenuator channel (\\(x\\leq 1\\)) the quantum capacity \\(Q(\\Phi_{x,M})\\) must fulfil two new sets of inequalities, i.e.\n",
      "\n",
      "\\[Q(\\Phi_{x,M})\\leq Q_{\\rm FKG}^{(2)}(x,M)=Q_{\\rm FKG}^{\\rm amp}(M/x)\\;, \\tag{114}\\]\n",
      "which follows from (106) and (105), and\n",
      "\n",
      "\\[Q(\\Phi_{x,M})\\leq\\underline{Q}^{(1)}_{\\rm FKG}(x,M)=\\min_{\\epsilon\\in[0,1]}Q^{ \\rm att}_{\\rm FKG}(x-\\epsilon M,\\tfrac{(1-\\epsilon)M}{1-x+\\epsilon M})\\;, \\tag{115}\\]\n",
      "\n",
      "which instead follows from (109). As shown in Fig. 7 for some value of the channel parameter these two functions provide better upper bounds that those reported in Sec. III.1.\n",
      "\n",
      "For amplifiers (i.e. \\(x\\geq 1\\)) we have already observed that \\(\\underline{Q}^{(2)}_{\\rm FKG}(x,M)\\) always coincides with the old bound (101). Accordingly new results can only derive from \\(\\underline{Q}^{(1)}_{\\rm FKG}(x,M)\\) (i.e. from \\(\\underline{Q}^{(1.1)}_{\\rm FKG}(x,M)\\)): unfortunately numerical study reveals that such function is always less performant than the bounds of Sec. III.1. A comprehensive comparison between the old bounds and the improved versions derived in this section is presented in the right part of Fig. 2.\n",
      "\n",
      "## VII Conclusion\n",
      "\n",
      "In the present manuscript, we sought to better understand the behavior of the capacities of single-mode phase-insensitive Gaussian bosonic channels (PI-GBCs) and their concatenation rules. Through extensive analysis, we were able to establish, for each point in the parameter space of PI-GBCs, an analytical characterization of two regions in the parameter space, of higher and lower capacity. That is, the capacity of points within each region was found to be respectively higher or lower than that of the original channel. Using these regions, we were able to improve upon the previous upper bounds. This structure of parameter phase space of PI-GBCs can be used to potentially improve any new upper or lower bound.\n",
      "\n",
      "We thank F. A. Mele and L. Lami for comments and suggestions. We also acknowledge financial support by MUR (Ministero dell' Universita e della Ricerca) through the PNRR MUR project PE0000023-NQSTI. MF is supported by Juan de la Cierva - Formacion (Spanish MICIN project FJC2021-047404-I), with funding from MCIN/AEI/10.13039/501100011033 and European Union \"NextGenerationEU\"/PRTRR.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['source', 'depth', 'branches', 'descendants'])\n",
      "TOC [document]\n",
      "10\n",
      "CHILD: h2 1 Introduction 1 Introduction\n",
      "Chart figures serve as the visual summary of tabular data, which helps to convey rich context in various documents, such as scientific papers, textbooks, and technical news. An intelligent agent that can understand and communicate chart plots can lead to many useful applications. For example, a virtual doctor who knows how to answer the patient's question on a complex medical report or a reading assistant who can summarize the key findings from scientific papers in brief language. In the past few years, there has been a growing interest in our community to explore chart understanding in vision and language (V\\+L) tasks and many related benchmarks like Chart Question Answering **(CQA)**Masry et al. (2022\\); Kafle et al. (2018\\); Methani et al. (2020\\) and Chart Summarization **(CS)**Kantharaj et al. (2022\\) are introduced.\n",
      "\n",
      "While prevalent in the research community, automatic chart understanding remains a challenging problem due to its complex compositions of various shapes, lines, colors, and scene text. Although tremendous success is achieved in the V\\+L research, applying these existing methods to handle chart\\-related tasks is hard. Recent research ChartQA Masry et al. (2022\\) and Chart\\-to\\-Text Kantharaj et al. (2022\\) attempt to first convert chart images to their underlined tables and use the extracted tables to perform chart\\-related V\\+L task. As the extracted tables always have clean and organized structures, it makes extracting relevant information to solve downstream reasoning tasks much more accessible. Empirically, using tables yields promising results on both CQA and CS.\n",
      "\n",
      "Despite valuing table as a significant ingredient for chart understanding, we have two main concerns about this approach: (1\\) Automatic table extraction is unreliable. Existing methods Luo et al. (2021\\); Kato et al. (2022\\) are often limited to work on a few particular types of chart images and do not generalize well. Moreover, the extracted table is likely to contain incorrect noisy predictions that potentially harm the performance of the following task. (2\\) In most cases, the whole table is optional for resolving the chart\\-related V\\+L task. As illus\n",
      "\n",
      "Figure 1: A data sample from the ChartQA dataset. The corresponding chart table is displayed in the top right corner.\n",
      "trated in Fig 1, to answer the question *\"What is the value of India Bar\"*, the model just needs access to the second row to give the correct answer. In contrast, having redundant table information makes finding the relevant information challenging. To better leverage the table data, we argue that it is important to equip the V\\+L model with the capability to dynamically interpret the table value from the chart information.\n",
      "\n",
      "Therefore, in this paper, we propose **ChartT5**, an OCR\\-based image\\-to\\-text generation model pre\\-trained on a self\\-collected chart table pairs corpus. More specifically, ChartT5 learns how to uncover a masked table with two proposed pre\\-training objectives: Masked Header Prediction (MHP), and Masked Value Prediction (MVP). MHP helps improve the model's capability of linking scene text to the corresponding table headers. MVP requires the model to perform mathematical reasoning over chart structure units and the scene text to predict the correct data value.\n",
      "\n",
      "We evaluate our ChartT5 on two tasks and benchmarks: ChartQA and Chart\\-to\\-Text. In ChartQA, ChartT5 outperforms all the non\\-pretraining methods that use extracted tables by at least (8\\\\%) performance gains. ChartT5 also beats the pre\\-training table\\-based methods, which demonstrates the effectiveness of the proposed pre\\-training strategies. On Chart\\-to\\-Text, ChartT5 consistly outperforms the existing SOTA on the content selection metrics Barzilay and Lapata (2005\\) which values the model's capability to extract the critical information from the chart.\n",
      "\n",
      "In summary, our contributions are summarized below:\n",
      "\n",
      "* We propose chart\\-to\\-table pre\\-training for V\\+L model to learn the capability of interpreting table data from the chart.\n",
      "* We demonstrate that the pre\\-trained model consistently outperforms table\\-based methods on two chart understanding tasks.\n",
      "* We conduct comprehensive ablation studies to validate the effectiveness of chart\\-to\\-table pre\\-training and the proposed pre\\-training objectives.\n",
      "\n",
      "------------------------------\n",
      "CHILD: h2 2 Related Work 2 Related Work\n",
      "### Vision and Language Research on Charts\n",
      "\n",
      "Researching chart understanding in V\\+L tasks is a popular field nowadays. The most prevalent problem is chart question answering (CQA) Kafle et al. (2018\\); Kahou et al. (2018\\); Methani et al. (2020\\); Masry et al. (2022\\); Chaudhry et al. (2020\\), where researchers build models to answer complex questions on chart images. Another popular one is chart summarization (CS) Kantharaj et al. (2022\\); Obeid and Hoque (2020\\), which requires machine learning models to create a summary of key insights conveyed by a chart. Hsu et al. (2021\\) collected a large\\-scale scientific figures captioning dataset from research papers where many images are chart plots.\n",
      "\n",
      "There are two main approaches for chart vision and language tasks. The first approach adapts existing visual question answering (VQA) and image captioning models to CQA and CS tasks with some specialized designs for chart images Kafle et al. (2020\\); Singh and Shekhar (2020\\); Chaudhry et al. (2020\\); Kafle et al. (2018\\); Hsu et al. (2021\\); Spreafico and Carenini (2020\\). The other approach assumes the table data of charts is accessible from the dataset Kim et al. (2020\\); Masry (2021\\) or can be extracted from the chart images using vision to table techniques Methani et al. (2020\\); Masry et al. (2022\\); Kantharaj et al. (2022\\). Then, the researchers will either use a table\\-to\\-text generation model Kim et al. (2020\\); Masry (2021\\); Methani et al. (2020\\) or combine the embedding of tables and charts via a multi\\-modal fusion method to generate the text output Masry et al. (2022\\); Kantharaj et al. (2022\\). It is clear from these efforts that adding tables as the additional representation of charts will dramatically improve the model's capability to understand and interpret chart information.\n",
      "\n",
      "Following the table\\-based approach, we also value the information provided by the underlined table data of chart images. However, instead of directly concatenating the extracted table into the chart understanding model, we facilitate our model with the capability to interpret the table data from chart images via pre\\-training on chart\\-table pairs.\n",
      "\n",
      "### Vision and Language Pre\\-training\n",
      "\n",
      "Vision and language pre\\-training has received growing interest over the past few years. Researchers build transformer\\-based multi\\-modal fusion models and perform self\\-supervised learning on a large\\-scale corpus of image\\-text pairs to learn robust cross\\-modal representations that can benefit the performance of various downstream tasks Chen et al. (2020\\); Lu et al. (2019\\); Tan and Bansal (2019\\);\n",
      "Su et al., 2019; Li et al., 2020; Zhang et al., 2021\\).\n",
      "\n",
      "While the pre\\-trained models achieve great success on tasks like VQA (Antol et al., 2015\\) and Image Captioning (Chen et al., 2015\\), they have only focused on the domain of natural images. However, chart understanding is still challenging for the existing vision and language methods due to their lack of knowledge of scene text and structured visual units such as \"bars\" and \"lines\".\n",
      "\n",
      "To address the limitation of conventional vision and language pre\\-training, TAP (Yang et al., 2021\\) and PreSTU (Kil et al., 2022\\) propose OCR\\-based vision and language pre\\-training frameworks that focus on scene text understanding in natural images where they design various pre\\-training objectives around the extracted OCR texts. Most recently, Donut (Kim et al., 2022\\) and Pix2Struct (Lee et al., 2022\\) propose OCR\\-free pre\\-training frameworks, where the pre\\-trained model directly generates a text output from a raw image input. Donut focuses on document image (*e.g.*, receipt) understanding, and Pix2Struct aims to handle broader types of synthetic images that contain visually\\-situated texts such as infographics and user interfaces via parsing web\\-page screenshots into their HTML Code. Different from these works, we take the first step to explore vision and language pre\\-training that focuses on chart image understanding. Specifically, we propose novel pre\\-training objectives to parse charts to their underlined tables.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "CHILD: h2 3 Method 3 Method\n",
      "In this section, we first introduce the dataset for pre\\-training. We then go over our ChartT5 model architecture and pre\\-training objectives to predict masked tables from the chart and OCR information.\n",
      "\n",
      "### Pre\\-training Dataset Collection\n",
      "\n",
      "To collect large\\-scale pairs of chart\\-table data, we collect synthetic data from existing chart question\\-answering corpora, including PlotQA (Methani et al., 2020\\), DVQA (Kafle et al., 2018\\), and FigureQA (Kahou et al., 2018\\). Specifically, DVQA and FigureQA render chart images from synthetic tables that are randomly generated from limited vocabularies. PlotQA first scrapes tables from online resources like World Bank Open Data and then synthesizes the charts from the scraped data, where the tables and charts contain more diverse language information. Our pre\\-training corpus consists of 495K chart\\-table pairs, which cover a diverse range of chart types. Our pre\\-training corpus contains three chart types: bar, line, and pie. The distribution of different chart types from the three chart question\\-answering benchmarks is summarized in table 1\\.\n",
      "\n",
      "### Model Overview\n",
      "\n",
      "ChartT5 is an extension of the existing V\\+L Pre\\-training framework, VLT5 (Cho et al., 2021\\), an encoder\\-decoder architecture that unifies the vision\\-language tasks as text generation conditioned on multi\\-modal inputs. Given a chart image, we first extract the scene texts. For the synthetic chart images that are collected from DVQA (Kafle\n",
      "\n",
      "\\\\begin{table}\n",
      "\\\\begin{tabular}{l\\|c c c\\|c} \\\\hline \\\\hline Type \\& PlotQA \\& DVQA \\& FigureQA \\& Total \\\\ \\\\hline Bar \\& 142,587 \\& 204,514 \\& 40,000 \\& 387,101 \\\\ Line \\& 48,133 \\& 0 \\& 40,000 \\& 88,133 \\\\ Pie \\& 0 \\& 0 \\& 20,001 \\& 20,001 \\\\ \\\\hline \\\\hline \\\\end{tabular}\n",
      "\\\\end{table}\n",
      "Table 1: Distribution of the three chart types: bar, line, and pie from different resources in the pre\\-training corpus.\n",
      "\n",
      "Figure 2: An Overview of ChartT5\\. Given the input chart image and the extracted OCR tokens, ChartT5 predicts the masked values of the table in the output.\n",
      "et al., 2018\\), FigureQA (Kahou et al., 2018\\), and PlotQA (Methani et al., 2020\\), the ground\\-truth scene texts are available. The visual context is then represented as combining visual features extracted from the chart image and the language features obtained on the detected scene text. We then flat the paired table of the chart image into a string and extract the text features via the language encoder. The multi\\-modal features are then concatenated and fused via the multi\\-layer encoder, and the output hidden vectors can then be used for various pre\\-training tasks.\n",
      "\n",
      "#### 3\\.2\\.1 Chart Image Encoder\n",
      "\n",
      "Given an input chart image, to recognize the critical marks (*e.g.*, bars and lines) of chart images, we first utilize a pre\\-trained Mask R\\-CNN object detector from (Masry et al., 2022\\) to extract the visual region features (\\\\mathbf{v}\\={v\\_{1},v\\_{2},\\\\cdots,v\\_{l^{v}}}). Next, the chart object detector is trained on the synthetic chart images from the previous CQA datasets (Kahou et al., 2018; Kafle et al., 2018; Masry et al., 2022; Methani et al., 2020\\) which is defined to identify 15 chart\\-related objects1\\. For each detected object region, we also extract location features as a 5\\-d vector: (\\[\\\\frac{x\\_{1}}{W},\\\\frac{y\\_{1}}{H},\\\\frac{x\\_{2}}{W},\\\\frac{y\\_{2}}{H},\\\\frac{(y\\_{2} \\-y\\_{1})(x\\_{2}\\-x\\_{1})}{W.H}]), which denotes the normalized top left coordinates, bottom right coordinates, and the normalized area of the detected region box. The position feature is then fed through fully\\-connected layers to be projected to the visual region feature embedding space. The final representation of the visual feature is obtained by summing up the projected region feature and corresponding location feature.\n",
      "\n",
      "Footnote 1: These 15 categories are: Legends, yAxisTitle, ChartTitle, xAxisTitle, LegendPreview, PlotArea, yAxisLabel, xAxisLabel, LegendLabel, PieLabel, bar, pie, pieSlice, line, and dotLine.\n",
      "\n",
      "#### 3\\.2\\.2 OCR Encoder\n",
      "\n",
      "After extracting the list of OCR words from the chart image, we obtain a set of OCR text embeddings (\\\\mathbf{o}\\={o\\_{1},o\\_{2},\\\\cdots,o\\_{l^{o}}}) via a learned word embedding layer. We also get each OCR token's 5\\-d position vector similar to the visual position vector from the OCR token's detected bounding box. We then obtain the position embedding vector using the shared projecting layer from the Chart Image Encoder. The shared position encoding mechanism between OCR tokens and chart object regions would help the model to capture their relative positional relations, which is a critical clue to predict the table data from the chart image. For example, the bar associated with an x\\-axis label should share a similar x\\-coordinate position in a vertical bar chart. The final OCR embedding vector is gained by summing up the OCR text token embeddings and the OCR position embedding.\n",
      "\n",
      "#### 3\\.2\\.3 Language Encoder\n",
      "\n",
      "Following the setting of the original VLT5 (Cho et al., 2021\\), we add a prefix to the flattened underlying table to indicate different pre\\-training tasks. We then get the table token embeddings (\\\\mathbf{t}\\={t\\_{1},t\\_{2},\\\\cdots,t\\_{l^{t}}}) with a shared word embedding layer. We apply the original T5's (Raffel et al., 2020\\) relative position bias to obtain the position information of each token in the caption and the flattened table. We know that the tables have very different structures compared to natural language captions, and several efforts are exploring specialized position embeddings for tables (Yin et al., 2020; 1\\). We leave the exploration of the specialized table position embedding for chart table pre\\-training in the future.\n",
      "\n",
      "**Scene Text Copy Mechanism.** A critical ingredient to the success of chart\\-to\\-table translation is the ability to predict the table headers from the corresponding OCR texts. For example, in the horizontal bar chart, the table column header is usually obtained from the x\\-axis labels, and the row header is often copied from the legend labels. Although presenting OCR text and the table to the model helps link the shared OCR tokens and table values, generating the correct table prediction from the corresponding OCR source is still challenging due to the large candidate token vocabulary. To encourage direct copy from the OCR text to the associated table cell value, we introduce OCR sentinel tokens ({\\<\\\\text{ocr\\_1}\\>,\\<\\\\text{ocr\\_2}\\>,\\\\cdots,\\<\\\\text{ocr\\_1}^{o}\\>}), which corresponds to the detected OCR texts. As illustrated in Figure 2, we replace each OCR token with a unique corresponding OCR sentinel token. Then, for every OCR token, we find if there is a matched existing table cell value. If a matched pair is found, we replace the table cell value with its paired OCR sentinel token. During pre\\-training, as all the plot images are synthesized from a paired table, the one\\-to\\-one scene text to table value mapping is already provided. With this prepossessing procedure, we successfully distinguish the table values that are copied from OCR tokens and those that need to be generated from the general token vocabularies, encouraging more accurate table pre\n",
      "diction from the relevant resources.\n",
      "\n",
      "### Pre\\-training Objectives\n",
      "\n",
      "Given the chart\\-table pairs, we propose Masked Header Prediction (MHP) and Masked Value Prediction (MHP) to teach the model to recover incomplete tables with the chart information. Specifically, this objective aims to predict a masked table token (t\\_{m}) with the remaining table info (t\\_{\\\\backslash m}) as well as the chart image region (\\\\mathbf{v}) and the scene text (\\\\mathbf{o}). Compared to the traditional masked language modeling applied to the natural language text, we adjust the table masking strategy based on two hypotheses: (1\\) We alternatively mask just the table headers or numerical table values, as we think interpreting these two types of information requires different skills. Predicting table headers requires retrieving the correct scene text, while predicting numerical table values depends more on the capability to conduct mathematic reasoning over both the visual elements and the scene text. Therefore, it is better to format them as two separate pre\\-training objectives. (2\\) We increase the masking rate from 15(\\\\%) to 45(\\\\%), as the masked table token has less dependence on the surrounding table values.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "CHILD: h2 4 Experiment 4 Experiment\n",
      "In this section, We detailed our experiment setups to evaluate the proposed ChartT5 on two tasks: chart question answering and chart summarization. We then introduce the main results of the two evaluation tasks. Finally, we present the ablation study on chart\\-table pre\\-training and the two pre\\-training objectives.\n",
      "\n",
      "**Chart Question Answering.** Given a chart image and a query question, the goal for the model is to provide an accurate answer string by interpreting the provided chart image. For this task, we consider the ChartQA dataset Masry et al. (2022\\), which collects question\\-answer pairs on realistic chart images scraped from the internet. Their annotations are collected in two fashions: (1\\) Human\\-written question\\-answer pairs; and (2\\) machine\\-generated question\\-answer pairs derived from the human\\-written chart summaries. In total 32\\.7K question\\-answer pairs are collected on 21\\.9K scraped chart images, where about 9\\.6K question\\-and\\-answer pairs are human\\-written. Compared to the previously collected CQA datasets, ChartQA is more challenging to handle due to the diverse visual style from the realistic chart images and the complex language from human annotations. Following previous work Masry et al. (2022\\); Methani et al. (2020\\), we also apply the relaxed accuracy to measure the performance on the CQA task, which allows a minor inaccuracy on numerical value prediction (within 5(\\\\%) of the gold answer). For non\\-numerical answers, the prediction needs to be exactly matched to the gold\\-standard answer.\n",
      "\n",
      "**Chart Summarization.** Given a chart image, the target is to summarize the key insights of the chart in natural language. For this task, we evaluate our model on the most recently proposed Chart\\-to\\-Text benchmark Kantharaj et al. (2022\\), which collects roughly 36\\.5K chart images with one summary for each image. They split the collected charts into two sets: Statista and Pew, representing the two separate websites from which the chart plots come. The summaries in Statista are human\\-written which is well grounded on the chart image. Meanwhile, the summaries from Pew are automatically extracted from the news paragraphs surrounding the chart images. Pew is noisier and more challenging to handle. We follow Kantharaj et al. (2022\\) to split the two sets for training and testing. We adopt BLEU\\-4, Content Selection, and CIDER as the evaluation metrics to measure the quality of the generated summary following Kantharaj et al. (2022\\).\n",
      "\n",
      "**Implementation details.** We initialized our ChartT5 from T5({}\\_{\\\\text{base}}) and pre\\-trained on our self\\-collected corpus for 30 epochs with a batch size of 60\\. We used Adam optimizer Kingma and Ba (2015\\) with a linear warm\\-up for the first 5(\\\\%) training steps, and the peak learning rate is set as 1e\\-4\\. After warming up, a linear decayed learning\\-rate scheduler gradually drops the learning rate for the rest of the training steps. The pre\\-training experiments are conducted on 2 Nvidia TITAN RTX GPUs, and it roughly takes two days to accomplish the experiment. We kept the last checkpoint of each pre\\-training run as our final checkpoint for fine\\-tuning.\n",
      "\n",
      "We also applied warming\\-up for downstream fine\\-tuning to gradually increase the learning rate to the pick value during the first 5(\\\\%) of training epochs. After that, a linear decayed learning\\-rate scheduler gradually drops the learning rate for the remaining training. For CQA task, we set batch size as 24 and fine\\-tune ChartT5 for 60 epochs with a peak learning rate 2e\\-4 on 2 Nvidia TITAN RTX GPUs. The best checkpoint was saved as the one that achieves the highest accuracy on the validation\n",
      "split. On the CS task, we use batch size 20 and a peak learning rate 5e\\-5\\. On the Pew split, we fine\\-tune ChartT5for 20 epochs, and on Statista, we fine\\-tune ChartT5for 25 epochs. The best checkpoint is also saved as achieving the best BLEU score on the validation split. All the reported numbers are one\\-time runs.\n",
      "\n",
      "### Main Results\n",
      "\n",
      "We first compare ChartT5 to various state\\-of\\-the\\-art methods with or without pre\\-training on the two downstream tasks.\n",
      "\n",
      "#### 4\\.1\\.1 Evaluation on CQA\n",
      "\n",
      "We compare ChartT5 with SOTA non\\-pretraining and pre\\-training methods on CQA tasks. The best\\-performed non\\-pretraining baselines are introduced in (Masry et al., 2022\\). The authors first predict the table data from the chart image via an automatic data extraction tool (Luo et al., 2021\\). Then they extend various language\\-only models (T5, Tapas) and multi\\-modal models (VLT5, VisionTapas) to predict the answer conditioned on the extracted table. On the line of pre\\-training baselines, we compare to VLT5({}*{pre}) and VisionTapas({}*{pre}) which pre\\-trains VLT5 and Vision Tapas on PlotQA with the visual question answering tasks. We also compare chartT5 to the current SOTA method Pix2Struct which is pre\\-trained on 80 million webpage screenshots to HTML code parsing objectives. The result is summarized in Table 2\\.\n",
      "\n",
      "**Comparison to Non\\-Pretraining Method** Even without access to the predicted tables, ChartT5 has outperformed all non\\-pretraining methods by a large margin (a minimum 7\\.3(\\\\%) gain on the overall performance). ChartT5 also outperforms all non\\-pretraining baselines on the human\\-written questions and machine\\-generated questions. Although the predicted table covers 54(\\\\%) of the answers in the test data of ChartQA, simply feeding it as an input does not make the existing models fully leverage the valuable information. The significant improvement achieved by ChartT5 indicates the effectiveness of the proposed pre\\-training to help the model to obtain the relevant table information for chart understanding.\n",
      "\n",
      "**Comparison to Pre\\-training Method** Although the performance of VLT5 and VisionTapas is improved significantly by pre\\-training on additional CQA data, ChartT5 still outperform them by at least 1\\.3(\\\\%). Specifically, on machine\\-augmented questions, ChartT5 outperforms VLT5({}*{pre}) by 8(\\\\%). However, both visionTapas({}*{pre}) and VLT5({}\\_{pre}) achieve better accuracy on the human split, which means that the in\\-domain question answering objectives helps the model to improve the numerical reasoning capability. ChartT5 underperforms Pix2Struct by 2\\.3(\\\\%) on the overall test split. However, pix2struct is pre\\-trained on a more than 100 times larger pre\\-training corpus than the rest of the pre\\-training methods. Given the same scale of the pre\\-training dataset, we expect to gain additional performance improvement, and we leave this for future exploration.\n",
      "\n",
      "#### 4\\.1\\.2 Evaluation on Chart Summarization\n",
      "\n",
      "For the chart summarization task, we compare ChartT5 to the best non\\-pretraining approaches introduced in (Kantharaj et al., 2022\\). Given a chart image, The authors build the chart summarization models by extending the pre\\-trained language generation model T5 (Raffel et al., 2020\\) and BART(Lewis et al., 2019\\) whose generation processes are conditioned on: (1\\) a set of scene texts extracted by a trained OCR detector. (2\\) the ground truth table that is paired with the chart. The evaluation result is summarized in Table 3\\.\n",
      "\n",
      "From Table 3, we can see that on Statista, ChartT5 outperforms all baseline methods on BLUE score, but only a slight improvement is achieved over the best baseline. On Pew, ChartT5 underperforms T5\\-OCR by almost 1\\.5 percent. The proposed ChartT5 also slightly underperforms against the baseline methods in CIDER on both datasets. However, ChartT5 consistently outperforms all baselines on content selection scores\n",
      "\n",
      "\\\\begin{table}\n",
      "\\\\begin{tabular}{l\\|c c c} \\\\hline \\\\hline \\\\multirow{2}{*}{Model} \\& \\\\multicolumn{3}{c}{ChartQA} \\\\ \\& Human \\& Augment \\& Overall \\\\ \\\\hline T5 \\& 25\\.12 \\& 56\\.96 \\& 41\\.56 \\\\ Tapas \\& 28\\.72 \\& 53\\.84 \\& 41\\.28 \\\\ VLT5 \\& 26\\.24 \\& 56\\.88 \\& 41\\.56 \\\\ VisionTapas \\& 29\\.60 \\& 61\\.44 \\& 45\\.52 \\\\ \\\\hline VLT5({}\\_{pre}) \\&* *40\\.08* *\\& 63\\.60 \\& 51\\.84 \\\\ VisionTapas({}\\_{pre}) \\& 32\\.56 \\& 61\\.60 \\& 47\\.08 \\\\ Pix2Struct \\& \\- \\& \\- \\&* *56\\.00* *\\\\ ChartT5 \\& 31\\.8 \\&* *74\\.4*\\* \\& 53\\.16 \\\\ \\\\hline \\\\hline \\\\end{tabular}\n",
      "\\\\end{table}\n",
      "Table 2: Evaluation results on ChartQA. We report relaxed accuracy on the test split annotated by humans and that generated by the machine. In the last column, we report the overall accuracy by computing the mean values with human split and augment split.\n",
      "across both Statista and Pew sets. The under\\-performance on BLEU and CIDER indicates that Chart\\-table pre\\-training is limited to benefit high\\-quality natural language generation. However, the strong performance on content selection, which values the key information appearance in the generation, suggests the advantage of chart\\-table pre\\-training on extracting relevant chart information. Therefore, a potential direction to explore is combining different types of pre\\-training objectives, such as chart\\-to\\-text pre\\-training and chart\\-table pre\\-training goals, to facilitate the model with diverse strengths.\n",
      "\n",
      "### Ablation Study\n",
      "\n",
      "We conduct ablation experiments to validate the effectiveness of chart\\-table pre\\-training and the pre\\-training objectives. We also evaluate the effectiveness of the proposed scene text copy mechanism.\n",
      "\n",
      "#### 4\\.2\\.1 Chart\\-Table Pre\\-training\n",
      "\n",
      "We conduct detailed analyses on the effectiveness of chart\\-table pre\\-training. First, we measure the performance gain from the chart\\-table pre\\-training on the full test set of ChartQA data. We then study what type of questions benefit most from the chart\\-table pre\\-training by picking three subsets of questions that measure different capabilities of the model: (1\\) Human\\-written questions, (2\\) Machine\\-generated questions, and (3\\) Table covered questions, where the answers can be directly found in the ground truth tables. The results are summarized in Table 4\\. From Table 4, we find that after chart\\-table pre\\-training the model's performance on these three sets of questions is all improved. The most significant gain is obtained on machine\\-generated questions, which mainly focus on extractive\\-type questions. This indicates that chart\\-table pre\\-training benefits the model to localize and retrieve the requested information presented on Chart Image. The second biggest gain is achieved on table\\-cover questions, where the model demonstrates significant improvement in the capability of chart\\-to\\-table interpretation.\n",
      "\n",
      "#### 4\\.2\\.2 Pre\\-training Objectives\n",
      "\n",
      "We validate the effectiveness of the two pre\\-training objectives, Masked Header Prediction and Masked Value Prediction. We remove one pre\\-training objective at a time and pre\\-train the ChartTSwith only one table prediction task. The pre\\-trained model\n",
      "\n",
      "\\\\begin{table}\n",
      "\\\\begin{tabular}{l\\|c c c} \\\\hline \\\\hline \\\\multirow{2}{*}{Pretraining?} \\& \\\\multicolumn{3}{c}{Question Types} \\\\ \\& Table \\& Human \\& Augment \\\\ \\\\hline No \\& 60\\.7 \\& 30\\.8 \\& 66\\.7 \\\\ Yes \\&* *64\\.7* *\\&* *31\\.8* *\\&* *74\\.4*\\* \\\\ \\\\hline \\\\hline \\\\end{tabular}\n",
      "\\\\end{table}\n",
      "Table 4: Ablation Study on Chart Table Pre\\-training with ChartQA Dataset. We report results on three subsets of questions: Table cover questions, human\\-written questions, and machine\\-generated questions.\n",
      "\n",
      "\\\\begin{table}\n",
      "\\\\begin{tabular}{l\\|c c c\\|c c c} \\\\hline \\\\hline \\\\multirow{2}{*}{Model} \\& \\\\multicolumn{3}{c\\|}{Statista} \\& \\\\multicolumn{3}{c}{Pew} \\\\ \\& BLEU \\& CS \\& CIDER \\& BLEU \\& CS \\& CIDER \\\\ \\\\hline T5\\-OCR \\& 35\\.29 \\& 73\\.77 \\& 4\\.43 \\&* *10\\.49* *\\& 40\\.87 \\&* *2\\.20* *\\\\ BART\\-OCR \\& \\- \\& \\- \\& \\- \\& 9\\.09 \\& 39\\.99 \\& 1\\.97 \\\\ T5\\-TAB \\& 37\\.01 \\& 75\\.72 \\&* *4\\.68* *\\& \\- \\& \\- \\& \\- \\\\ BART\\-TAB \\& 36\\.36 \\& 77\\.14 \\& 4\\.40 \\& \\- \\& \\- \\& \\- \\\\ ChartT5 \\&* *37\\.51* *\\&* *82\\.16* *\\& 3\\.45 \\& 9\\.05 \\&* *55\\.1*\\* \\& 1\\.23 \\\\ \\\\hline \\\\hline \\\\end{tabular}\n",
      "\\\\end{table}\n",
      "Table 3: Evaluation results on Chart Summarization. We display BLEU, CS and CIDER scores for the Pew and Statista Split. The ground truth table is not available to Pew thus the table\\-based method does not have results on Pew split.\n",
      "\n",
      "\\\\begin{table}\n",
      "\\\\begin{tabular}{l\\|c c c} \\\\hline \\\\hline \\& \\\\multicolumn{3}{c}{Question Types} \\\\ \\& Human \\& Augment \\& Overall \\\\ \\\\hline Full \\& 31\\.8 \\& 74\\.4 \\& 53\\.1 \\\\ \\- MVP \\& 30\\.9 \\& 73\\.7 \\& 52\\.3 \\\\ \\- MHP \\& 31\\.2 \\& 68\\.3 \\& 49\\.7 \\\\ \\- STC \\& 30\\.8 \\& 72\\.4 \\& 51\\.6 \\\\ \\\\hline \\\\hline \\\\end{tabular}\n",
      "\\\\end{table}\n",
      "Table 5: Ablation Study on the two proposed pre\\-training objectives and the Scene Text Copy Mechanism (STC). The first row is the result of the full ChartT5 model. Then we remove one of the pre\\-training objectives and the scene\\-text\\-copy mechanism. We report the results of different ablation experiments on both human and machine\\-generated splits as well as the overall performance.\n",
      "is then fine\\-tuned and evaluated on the human and augmented split for comparison. The result is displayed in table 5\\. As can be seen from the table, removing Masked Value Prediction Loss has a negligible impact on the performance of ChartT5 on ChartQA dataset. There is a slightly more drop in human written questions which suggests that predicting table numerical values still has a miner positive impact on helping the model's mathematical reasoning. Remove Masked Header Prediction have a significant impact on the machine\\-generated question\\-answering accuracy. As expected, Masked header modeling mainly helps the model learn how to link the scene text to the table headers, which is a critical ability to extract relevant information given a specific query.\n",
      "\n",
      "#### 4\\.2\\.3 Scene Text Copy\n",
      "\n",
      "We also validate the effectiveness of the scene\\-text\\-copy mechanism, where we train a ChartT5model by simply representing OCR tokens in their original text format. The model is fine\\-tuned and evaluated on the human and augmented split of the chartQA dataset to compare against the full ChartT5\\. The result is displayed in Table 5\\. Disabling the scene\\-text\\-copy mechanism leads to a 1\\.5(\\\\%) overall performance drop on ChartQA tasks. Specifically, it leads to more degradation on the augmented split than the human split, as scene\\-text\\-copy helps enhance the alignment between OCR and table values to benefit accurate information extraction from the chart.\n",
      "\n",
      "### Qualitative Error Analysis\n",
      "\n",
      "We have manually analyzed model predictions to understand its limitation. We found that our model suffers most from noisy OCR detection and complex question that requires multi\\-hop reasoning.\n",
      "\n",
      "**Noisy OCR Prediction.** As an OCR\\-based model, ChartT5 often suffers from a wrong OCR detection. An example is shown in Figure 3; the model localizes the right scene text \"1\\.18\" to answer the question, but the OCR text is mistakenly detected as \"1:18\". To further understand the limitation of OCR detection, we randomly sample 20K PlotQA test split and compare the performance of our model using detected OCRs against Ground Truth OCRs. We observe a 5(\\\\%) performance drop when using detected OCRs. We can improve the OCR detector for future work by training on a large Plot scene\\-text detection benchmark. Another promising direction is to attempt OCR\\-free end\\-to\\-end plot recognition method like Pix2Struct (Lee et al., 2022\\).\n",
      "\n",
      "**Multi\\-Hop Reasoning.** Our model is also quite vulnerable to handling complex questions requiring multi\\-hop reasoning. An example is shown in Figure 4; the model cannot perform the complex logic reasoning to add the stats of the two smallest bars and compare that to the large bar. We will consider exploring pre\\-training on the mathematic reasoning datasets to address this limitation.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "CHILD: h2 5 Conclusion 5 Conclusion\n",
      "We propose ChartT5 to enhance the vision language model's ability to understand chart images via chart\\-table pre\\-training. The model learns to interpret the masked tables via our proposed masked header prediction and masked value prediction objectives. ChartT5 achieves significant improvement over table\\-based non\\-pretraining SOTA methods on the ChartQA dataset, especially on the extractive question sets. We also achieve a new SOTA Content Selection Score on the Chart\\-to\\-text summarization dataset. We conduct comprehensive ablation studies to identify the impact of chart\\-table pre\\-training, and we find that the proposed pre\\-training is extremely helpful to extract accurate\n",
      "\n",
      "Figure 4: An error prediction from our model due to complex multi\\-hop reasoning\n",
      "\n",
      "Figure 3: An error prediction from our model due to noisy OCR prediction\n",
      "information from the Chart. For future research directions, we believe it may also be meaningful to explore chart understanding under data\\-efficient settings Hsu et al. (2022\\); Zeng et al. (2023\\) and for evidence retrieval tasks Lu et al. (2022\\); Ji et al. (2023\\).\n",
      "\n",
      "\n",
      "------------------------------\n",
      "CHILD: h2 6 Limitations 6 Limitations\n",
      "Although introducing chart value prediction objective, it only provides minor improvement to the model's performance on doing complex reasoning. There is still a large room to improve the model's capability in math calculation. Our model also suffers from the noisy OCR prediction of off\\-the\\-shelf object detector, whose performance will depend highly on the extracted OCR text qualities. Another possible limitation of our approach is the quality of the pre\\-training data, which only contains synthetic images. Although the proposed model works fairly well on the ChartQA dataset, it is unclear if the improved performance can be generalized to other realistic chart images.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "CHILD: h2 7 Ethics Statement 7 Ethics Statement\n",
      "When we collect the pre\\-training dataset, we ensure we respect the intellectual property of dataset sources. All the ChartQA dataset we used for the collection of chart\\-table pairs allows public access for research. To ensure the reproducibility of our experiment results, we provide details of the hyperparameter setting in our paper, and we will also publish our code later. Our models can mislead the public's understanding of chart content due to the potential bias from our training corpus. Therefore, we don't recommend using our model for any real\\-world decision on chart images.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "CHILD: h2 Acknowledgement Acknowledgement\n",
      "This research work is supported by U.S DARPA SemaFor Program No. HR001120C0123\\. The views and conclusions contained in this work only belong to the authors and should not represent the official policies implied by DARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. We also thank Ahmed Masry and Shankar Kantharaj for providing us with ChartQA and Chart Summary\\-related data and baseline model outputs.\n",
      "\n",
      "\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ArxivPaperSection(header='h2', title='1 Introduction', text='Chart figures serve as the visual summary of tabular data, which helps to convey rich context in various documents, such as scientific papers, textbooks, and technical news. An intelligent agent that can understand and communicate chart plots can lead to many useful applications. For example, a virtual doctor who knows how to answer the patient\\'s question on a complex medical report or a reading assistant who can summarize the key findings from scientific papers in brief language. In the past few years, there has been a growing interest in our community to explore chart understanding in vision and language (V\\\\+L) tasks and many related benchmarks like Chart Question Answering **(CQA)**Masry et al. (2022\\\\); Kafle et al. (2018\\\\); Methani et al. (2020\\\\) and Chart Summarization **(CS)**Kantharaj et al. (2022\\\\) are introduced.\\n\\nWhile prevalent in the research community, automatic chart understanding remains a challenging problem due to its complex compositions of various shapes, lines, colors, and scene text. Although tremendous success is achieved in the V\\\\+L research, applying these existing methods to handle chart\\\\-related tasks is hard. Recent research ChartQA Masry et al. (2022\\\\) and Chart\\\\-to\\\\-Text Kantharaj et al. (2022\\\\) attempt to first convert chart images to their underlined tables and use the extracted tables to perform chart\\\\-related V\\\\+L task. As the extracted tables always have clean and organized structures, it makes extracting relevant information to solve downstream reasoning tasks much more accessible. Empirically, using tables yields promising results on both CQA and CS.\\n\\nDespite valuing table as a significant ingredient for chart understanding, we have two main concerns about this approach: (1\\\\) Automatic table extraction is unreliable. Existing methods Luo et al. (2021\\\\); Kato et al. (2022\\\\) are often limited to work on a few particular types of chart images and do not generalize well. Moreover, the extracted table is likely to contain incorrect noisy predictions that potentially harm the performance of the following task. (2\\\\) In most cases, the whole table is optional for resolving the chart\\\\-related V\\\\+L task. As illus\\n\\nFigure 1: A data sample from the ChartQA dataset. The corresponding chart table is displayed in the top right corner.\\ntrated in Fig 1, to answer the question *\"What is the value of India Bar\"*, the model just needs access to the second row to give the correct answer. In contrast, having redundant table information makes finding the relevant information challenging. To better leverage the table data, we argue that it is important to equip the V\\\\+L model with the capability to dynamically interpret the table value from the chart information.\\n\\nTherefore, in this paper, we propose **ChartT5**, an OCR\\\\-based image\\\\-to\\\\-text generation model pre\\\\-trained on a self\\\\-collected chart table pairs corpus. More specifically, ChartT5 learns how to uncover a masked table with two proposed pre\\\\-training objectives: Masked Header Prediction (MHP), and Masked Value Prediction (MVP). MHP helps improve the model\\'s capability of linking scene text to the corresponding table headers. MVP requires the model to perform mathematical reasoning over chart structure units and the scene text to predict the correct data value.\\n\\nWe evaluate our ChartT5 on two tasks and benchmarks: ChartQA and Chart\\\\-to\\\\-Text. In ChartQA, ChartT5 outperforms all the non\\\\-pretraining methods that use extracted tables by at least (8\\\\\\\\%) performance gains. ChartT5 also beats the pre\\\\-training table\\\\-based methods, which demonstrates the effectiveness of the proposed pre\\\\-training strategies. On Chart\\\\-to\\\\-Text, ChartT5 consistly outperforms the existing SOTA on the content selection metrics Barzilay and Lapata (2005\\\\) which values the model\\'s capability to extract the critical information from the chart.\\n\\nIn summary, our contributions are summarized below:\\n\\n* We propose chart\\\\-to\\\\-table pre\\\\-training for V\\\\+L model to learn the capability of interpreting table data from the chart.\\n* We demonstrate that the pre\\\\-trained model consistently outperforms table\\\\-based methods on two chart understanding tasks.\\n* We conduct comprehensive ablation studies to validate the effectiveness of chart\\\\-to\\\\-table pre\\\\-training and the proposed pre\\\\-training objectives.\\n', children=[]),\n",
       " ArxivPaperSection(header='h2', title='2 Related Work', text='### Vision and Language Research on Charts\\n\\nResearching chart understanding in V\\\\+L tasks is a popular field nowadays. The most prevalent problem is chart question answering (CQA) Kafle et al. (2018\\\\); Kahou et al. (2018\\\\); Methani et al. (2020\\\\); Masry et al. (2022\\\\); Chaudhry et al. (2020\\\\), where researchers build models to answer complex questions on chart images. Another popular one is chart summarization (CS) Kantharaj et al. (2022\\\\); Obeid and Hoque (2020\\\\), which requires machine learning models to create a summary of key insights conveyed by a chart. Hsu et al. (2021\\\\) collected a large\\\\-scale scientific figures captioning dataset from research papers where many images are chart plots.\\n\\nThere are two main approaches for chart vision and language tasks. The first approach adapts existing visual question answering (VQA) and image captioning models to CQA and CS tasks with some specialized designs for chart images Kafle et al. (2020\\\\); Singh and Shekhar (2020\\\\); Chaudhry et al. (2020\\\\); Kafle et al. (2018\\\\); Hsu et al. (2021\\\\); Spreafico and Carenini (2020\\\\). The other approach assumes the table data of charts is accessible from the dataset Kim et al. (2020\\\\); Masry (2021\\\\) or can be extracted from the chart images using vision to table techniques Methani et al. (2020\\\\); Masry et al. (2022\\\\); Kantharaj et al. (2022\\\\). Then, the researchers will either use a table\\\\-to\\\\-text generation model Kim et al. (2020\\\\); Masry (2021\\\\); Methani et al. (2020\\\\) or combine the embedding of tables and charts via a multi\\\\-modal fusion method to generate the text output Masry et al. (2022\\\\); Kantharaj et al. (2022\\\\). It is clear from these efforts that adding tables as the additional representation of charts will dramatically improve the model\\'s capability to understand and interpret chart information.\\n\\nFollowing the table\\\\-based approach, we also value the information provided by the underlined table data of chart images. However, instead of directly concatenating the extracted table into the chart understanding model, we facilitate our model with the capability to interpret the table data from chart images via pre\\\\-training on chart\\\\-table pairs.\\n\\n### Vision and Language Pre\\\\-training\\n\\nVision and language pre\\\\-training has received growing interest over the past few years. Researchers build transformer\\\\-based multi\\\\-modal fusion models and perform self\\\\-supervised learning on a large\\\\-scale corpus of image\\\\-text pairs to learn robust cross\\\\-modal representations that can benefit the performance of various downstream tasks Chen et al. (2020\\\\); Lu et al. (2019\\\\); Tan and Bansal (2019\\\\);\\nSu et al., 2019; Li et al., 2020; Zhang et al., 2021\\\\).\\n\\nWhile the pre\\\\-trained models achieve great success on tasks like VQA (Antol et al., 2015\\\\) and Image Captioning (Chen et al., 2015\\\\), they have only focused on the domain of natural images. However, chart understanding is still challenging for the existing vision and language methods due to their lack of knowledge of scene text and structured visual units such as \"bars\" and \"lines\".\\n\\nTo address the limitation of conventional vision and language pre\\\\-training, TAP (Yang et al., 2021\\\\) and PreSTU (Kil et al., 2022\\\\) propose OCR\\\\-based vision and language pre\\\\-training frameworks that focus on scene text understanding in natural images where they design various pre\\\\-training objectives around the extracted OCR texts. Most recently, Donut (Kim et al., 2022\\\\) and Pix2Struct (Lee et al., 2022\\\\) propose OCR\\\\-free pre\\\\-training frameworks, where the pre\\\\-trained model directly generates a text output from a raw image input. Donut focuses on document image (*e.g.*, receipt) understanding, and Pix2Struct aims to handle broader types of synthetic images that contain visually\\\\-situated texts such as infographics and user interfaces via parsing web\\\\-page screenshots into their HTML Code. Different from these works, we take the first step to explore vision and language pre\\\\-training that focuses on chart image understanding. Specifically, we propose novel pre\\\\-training objectives to parse charts to their underlined tables.\\n\\n', children=[]),\n",
       " ArxivPaperSection(header='h2', title='3 Method', text=\"In this section, we first introduce the dataset for pre\\\\-training. We then go over our ChartT5 model architecture and pre\\\\-training objectives to predict masked tables from the chart and OCR information.\\n\\n### Pre\\\\-training Dataset Collection\\n\\nTo collect large\\\\-scale pairs of chart\\\\-table data, we collect synthetic data from existing chart question\\\\-answering corpora, including PlotQA (Methani et al., 2020\\\\), DVQA (Kafle et al., 2018\\\\), and FigureQA (Kahou et al., 2018\\\\). Specifically, DVQA and FigureQA render chart images from synthetic tables that are randomly generated from limited vocabularies. PlotQA first scrapes tables from online resources like World Bank Open Data and then synthesizes the charts from the scraped data, where the tables and charts contain more diverse language information. Our pre\\\\-training corpus consists of 495K chart\\\\-table pairs, which cover a diverse range of chart types. Our pre\\\\-training corpus contains three chart types: bar, line, and pie. The distribution of different chart types from the three chart question\\\\-answering benchmarks is summarized in table 1\\\\.\\n\\n### Model Overview\\n\\nChartT5 is an extension of the existing V\\\\+L Pre\\\\-training framework, VLT5 (Cho et al., 2021\\\\), an encoder\\\\-decoder architecture that unifies the vision\\\\-language tasks as text generation conditioned on multi\\\\-modal inputs. Given a chart image, we first extract the scene texts. For the synthetic chart images that are collected from DVQA (Kafle\\n\\n\\\\\\\\begin{table}\\n\\\\\\\\begin{tabular}{l\\\\|c c c\\\\|c} \\\\\\\\hline \\\\\\\\hline Type \\\\& PlotQA \\\\& DVQA \\\\& FigureQA \\\\& Total \\\\\\\\ \\\\\\\\hline Bar \\\\& 142,587 \\\\& 204,514 \\\\& 40,000 \\\\& 387,101 \\\\\\\\ Line \\\\& 48,133 \\\\& 0 \\\\& 40,000 \\\\& 88,133 \\\\\\\\ Pie \\\\& 0 \\\\& 0 \\\\& 20,001 \\\\& 20,001 \\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\n\\\\\\\\end{table}\\nTable 1: Distribution of the three chart types: bar, line, and pie from different resources in the pre\\\\-training corpus.\\n\\nFigure 2: An Overview of ChartT5\\\\. Given the input chart image and the extracted OCR tokens, ChartT5 predicts the masked values of the table in the output.\\net al., 2018\\\\), FigureQA (Kahou et al., 2018\\\\), and PlotQA (Methani et al., 2020\\\\), the ground\\\\-truth scene texts are available. The visual context is then represented as combining visual features extracted from the chart image and the language features obtained on the detected scene text. We then flat the paired table of the chart image into a string and extract the text features via the language encoder. The multi\\\\-modal features are then concatenated and fused via the multi\\\\-layer encoder, and the output hidden vectors can then be used for various pre\\\\-training tasks.\\n\\n#### 3\\\\.2\\\\.1 Chart Image Encoder\\n\\nGiven an input chart image, to recognize the critical marks (*e.g.*, bars and lines) of chart images, we first utilize a pre\\\\-trained Mask R\\\\-CNN object detector from (Masry et al., 2022\\\\) to extract the visual region features (\\\\\\\\mathbf{v}\\\\={v\\\\_{1},v\\\\_{2},\\\\\\\\cdots,v\\\\_{l^{v}}}). Next, the chart object detector is trained on the synthetic chart images from the previous CQA datasets (Kahou et al., 2018; Kafle et al., 2018; Masry et al., 2022; Methani et al., 2020\\\\) which is defined to identify 15 chart\\\\-related objects1\\\\. For each detected object region, we also extract location features as a 5\\\\-d vector: (\\\\[\\\\\\\\frac{x\\\\_{1}}{W},\\\\\\\\frac{y\\\\_{1}}{H},\\\\\\\\frac{x\\\\_{2}}{W},\\\\\\\\frac{y\\\\_{2}}{H},\\\\\\\\frac{(y\\\\_{2} \\\\-y\\\\_{1})(x\\\\_{2}\\\\-x\\\\_{1})}{W.H}]), which denotes the normalized top left coordinates, bottom right coordinates, and the normalized area of the detected region box. The position feature is then fed through fully\\\\-connected layers to be projected to the visual region feature embedding space. The final representation of the visual feature is obtained by summing up the projected region feature and corresponding location feature.\\n\\nFootnote 1: These 15 categories are: Legends, yAxisTitle, ChartTitle, xAxisTitle, LegendPreview, PlotArea, yAxisLabel, xAxisLabel, LegendLabel, PieLabel, bar, pie, pieSlice, line, and dotLine.\\n\\n#### 3\\\\.2\\\\.2 OCR Encoder\\n\\nAfter extracting the list of OCR words from the chart image, we obtain a set of OCR text embeddings (\\\\\\\\mathbf{o}\\\\={o\\\\_{1},o\\\\_{2},\\\\\\\\cdots,o\\\\_{l^{o}}}) via a learned word embedding layer. We also get each OCR token's 5\\\\-d position vector similar to the visual position vector from the OCR token's detected bounding box. We then obtain the position embedding vector using the shared projecting layer from the Chart Image Encoder. The shared position encoding mechanism between OCR tokens and chart object regions would help the model to capture their relative positional relations, which is a critical clue to predict the table data from the chart image. For example, the bar associated with an x\\\\-axis label should share a similar x\\\\-coordinate position in a vertical bar chart. The final OCR embedding vector is gained by summing up the OCR text token embeddings and the OCR position embedding.\\n\\n#### 3\\\\.2\\\\.3 Language Encoder\\n\\nFollowing the setting of the original VLT5 (Cho et al., 2021\\\\), we add a prefix to the flattened underlying table to indicate different pre\\\\-training tasks. We then get the table token embeddings (\\\\\\\\mathbf{t}\\\\={t\\\\_{1},t\\\\_{2},\\\\\\\\cdots,t\\\\_{l^{t}}}) with a shared word embedding layer. We apply the original T5's (Raffel et al., 2020\\\\) relative position bias to obtain the position information of each token in the caption and the flattened table. We know that the tables have very different structures compared to natural language captions, and several efforts are exploring specialized position embeddings for tables (Yin et al., 2020; 1\\\\). We leave the exploration of the specialized table position embedding for chart table pre\\\\-training in the future.\\n\\n**Scene Text Copy Mechanism.** A critical ingredient to the success of chart\\\\-to\\\\-table translation is the ability to predict the table headers from the corresponding OCR texts. For example, in the horizontal bar chart, the table column header is usually obtained from the x\\\\-axis labels, and the row header is often copied from the legend labels. Although presenting OCR text and the table to the model helps link the shared OCR tokens and table values, generating the correct table prediction from the corresponding OCR source is still challenging due to the large candidate token vocabulary. To encourage direct copy from the OCR text to the associated table cell value, we introduce OCR sentinel tokens ({\\\\<\\\\\\\\text{ocr\\\\_1}\\\\>,\\\\<\\\\\\\\text{ocr\\\\_2}\\\\>,\\\\\\\\cdots,\\\\<\\\\\\\\text{ocr\\\\_1}^{o}\\\\>}), which corresponds to the detected OCR texts. As illustrated in Figure 2, we replace each OCR token with a unique corresponding OCR sentinel token. Then, for every OCR token, we find if there is a matched existing table cell value. If a matched pair is found, we replace the table cell value with its paired OCR sentinel token. During pre\\\\-training, as all the plot images are synthesized from a paired table, the one\\\\-to\\\\-one scene text to table value mapping is already provided. With this prepossessing procedure, we successfully distinguish the table values that are copied from OCR tokens and those that need to be generated from the general token vocabularies, encouraging more accurate table pre\\ndiction from the relevant resources.\\n\\n### Pre\\\\-training Objectives\\n\\nGiven the chart\\\\-table pairs, we propose Masked Header Prediction (MHP) and Masked Value Prediction (MHP) to teach the model to recover incomplete tables with the chart information. Specifically, this objective aims to predict a masked table token (t\\\\_{m}) with the remaining table info (t\\\\_{\\\\\\\\backslash m}) as well as the chart image region (\\\\\\\\mathbf{v}) and the scene text (\\\\\\\\mathbf{o}). Compared to the traditional masked language modeling applied to the natural language text, we adjust the table masking strategy based on two hypotheses: (1\\\\) We alternatively mask just the table headers or numerical table values, as we think interpreting these two types of information requires different skills. Predicting table headers requires retrieving the correct scene text, while predicting numerical table values depends more on the capability to conduct mathematic reasoning over both the visual elements and the scene text. Therefore, it is better to format them as two separate pre\\\\-training objectives. (2\\\\) We increase the masking rate from 15(\\\\\\\\%) to 45(\\\\\\\\%), as the masked table token has less dependence on the surrounding table values.\\n\\n\", children=[]),\n",
       " ArxivPaperSection(header='h2', title='4 Experiment', text='In this section, We detailed our experiment setups to evaluate the proposed ChartT5 on two tasks: chart question answering and chart summarization. We then introduce the main results of the two evaluation tasks. Finally, we present the ablation study on chart\\\\-table pre\\\\-training and the two pre\\\\-training objectives.\\n\\n**Chart Question Answering.** Given a chart image and a query question, the goal for the model is to provide an accurate answer string by interpreting the provided chart image. For this task, we consider the ChartQA dataset Masry et al. (2022\\\\), which collects question\\\\-answer pairs on realistic chart images scraped from the internet. Their annotations are collected in two fashions: (1\\\\) Human\\\\-written question\\\\-answer pairs; and (2\\\\) machine\\\\-generated question\\\\-answer pairs derived from the human\\\\-written chart summaries. In total 32\\\\.7K question\\\\-answer pairs are collected on 21\\\\.9K scraped chart images, where about 9\\\\.6K question\\\\-and\\\\-answer pairs are human\\\\-written. Compared to the previously collected CQA datasets, ChartQA is more challenging to handle due to the diverse visual style from the realistic chart images and the complex language from human annotations. Following previous work Masry et al. (2022\\\\); Methani et al. (2020\\\\), we also apply the relaxed accuracy to measure the performance on the CQA task, which allows a minor inaccuracy on numerical value prediction (within 5(\\\\\\\\%) of the gold answer). For non\\\\-numerical answers, the prediction needs to be exactly matched to the gold\\\\-standard answer.\\n\\n**Chart Summarization.** Given a chart image, the target is to summarize the key insights of the chart in natural language. For this task, we evaluate our model on the most recently proposed Chart\\\\-to\\\\-Text benchmark Kantharaj et al. (2022\\\\), which collects roughly 36\\\\.5K chart images with one summary for each image. They split the collected charts into two sets: Statista and Pew, representing the two separate websites from which the chart plots come. The summaries in Statista are human\\\\-written which is well grounded on the chart image. Meanwhile, the summaries from Pew are automatically extracted from the news paragraphs surrounding the chart images. Pew is noisier and more challenging to handle. We follow Kantharaj et al. (2022\\\\) to split the two sets for training and testing. We adopt BLEU\\\\-4, Content Selection, and CIDER as the evaluation metrics to measure the quality of the generated summary following Kantharaj et al. (2022\\\\).\\n\\n**Implementation details.** We initialized our ChartT5 from T5({}\\\\_{\\\\\\\\text{base}}) and pre\\\\-trained on our self\\\\-collected corpus for 30 epochs with a batch size of 60\\\\. We used Adam optimizer Kingma and Ba (2015\\\\) with a linear warm\\\\-up for the first 5(\\\\\\\\%) training steps, and the peak learning rate is set as 1e\\\\-4\\\\. After warming up, a linear decayed learning\\\\-rate scheduler gradually drops the learning rate for the rest of the training steps. The pre\\\\-training experiments are conducted on 2 Nvidia TITAN RTX GPUs, and it roughly takes two days to accomplish the experiment. We kept the last checkpoint of each pre\\\\-training run as our final checkpoint for fine\\\\-tuning.\\n\\nWe also applied warming\\\\-up for downstream fine\\\\-tuning to gradually increase the learning rate to the pick value during the first 5(\\\\\\\\%) of training epochs. After that, a linear decayed learning\\\\-rate scheduler gradually drops the learning rate for the remaining training. For CQA task, we set batch size as 24 and fine\\\\-tune ChartT5 for 60 epochs with a peak learning rate 2e\\\\-4 on 2 Nvidia TITAN RTX GPUs. The best checkpoint was saved as the one that achieves the highest accuracy on the validation\\nsplit. On the CS task, we use batch size 20 and a peak learning rate 5e\\\\-5\\\\. On the Pew split, we fine\\\\-tune ChartT5for 20 epochs, and on Statista, we fine\\\\-tune ChartT5for 25 epochs. The best checkpoint is also saved as achieving the best BLEU score on the validation split. All the reported numbers are one\\\\-time runs.\\n\\n### Main Results\\n\\nWe first compare ChartT5 to various state\\\\-of\\\\-the\\\\-art methods with or without pre\\\\-training on the two downstream tasks.\\n\\n#### 4\\\\.1\\\\.1 Evaluation on CQA\\n\\nWe compare ChartT5 with SOTA non\\\\-pretraining and pre\\\\-training methods on CQA tasks. The best\\\\-performed non\\\\-pretraining baselines are introduced in (Masry et al., 2022\\\\). The authors first predict the table data from the chart image via an automatic data extraction tool (Luo et al., 2021\\\\). Then they extend various language\\\\-only models (T5, Tapas) and multi\\\\-modal models (VLT5, VisionTapas) to predict the answer conditioned on the extracted table. On the line of pre\\\\-training baselines, we compare to VLT5({}*{pre}) and VisionTapas({}*{pre}) which pre\\\\-trains VLT5 and Vision Tapas on PlotQA with the visual question answering tasks. We also compare chartT5 to the current SOTA method Pix2Struct which is pre\\\\-trained on 80 million webpage screenshots to HTML code parsing objectives. The result is summarized in Table 2\\\\.\\n\\n**Comparison to Non\\\\-Pretraining Method** Even without access to the predicted tables, ChartT5 has outperformed all non\\\\-pretraining methods by a large margin (a minimum 7\\\\.3(\\\\\\\\%) gain on the overall performance). ChartT5 also outperforms all non\\\\-pretraining baselines on the human\\\\-written questions and machine\\\\-generated questions. Although the predicted table covers 54(\\\\\\\\%) of the answers in the test data of ChartQA, simply feeding it as an input does not make the existing models fully leverage the valuable information. The significant improvement achieved by ChartT5 indicates the effectiveness of the proposed pre\\\\-training to help the model to obtain the relevant table information for chart understanding.\\n\\n**Comparison to Pre\\\\-training Method** Although the performance of VLT5 and VisionTapas is improved significantly by pre\\\\-training on additional CQA data, ChartT5 still outperform them by at least 1\\\\.3(\\\\\\\\%). Specifically, on machine\\\\-augmented questions, ChartT5 outperforms VLT5({}*{pre}) by 8(\\\\\\\\%). However, both visionTapas({}*{pre}) and VLT5({}\\\\_{pre}) achieve better accuracy on the human split, which means that the in\\\\-domain question answering objectives helps the model to improve the numerical reasoning capability. ChartT5 underperforms Pix2Struct by 2\\\\.3(\\\\\\\\%) on the overall test split. However, pix2struct is pre\\\\-trained on a more than 100 times larger pre\\\\-training corpus than the rest of the pre\\\\-training methods. Given the same scale of the pre\\\\-training dataset, we expect to gain additional performance improvement, and we leave this for future exploration.\\n\\n#### 4\\\\.1\\\\.2 Evaluation on Chart Summarization\\n\\nFor the chart summarization task, we compare ChartT5 to the best non\\\\-pretraining approaches introduced in (Kantharaj et al., 2022\\\\). Given a chart image, The authors build the chart summarization models by extending the pre\\\\-trained language generation model T5 (Raffel et al., 2020\\\\) and BART(Lewis et al., 2019\\\\) whose generation processes are conditioned on: (1\\\\) a set of scene texts extracted by a trained OCR detector. (2\\\\) the ground truth table that is paired with the chart. The evaluation result is summarized in Table 3\\\\.\\n\\nFrom Table 3, we can see that on Statista, ChartT5 outperforms all baseline methods on BLUE score, but only a slight improvement is achieved over the best baseline. On Pew, ChartT5 underperforms T5\\\\-OCR by almost 1\\\\.5 percent. The proposed ChartT5 also slightly underperforms against the baseline methods in CIDER on both datasets. However, ChartT5 consistently outperforms all baselines on content selection scores\\n\\n\\\\\\\\begin{table}\\n\\\\\\\\begin{tabular}{l\\\\|c c c} \\\\\\\\hline \\\\\\\\hline \\\\\\\\multirow{2}{*}{Model} \\\\& \\\\\\\\multicolumn{3}{c}{ChartQA} \\\\\\\\ \\\\& Human \\\\& Augment \\\\& Overall \\\\\\\\ \\\\\\\\hline T5 \\\\& 25\\\\.12 \\\\& 56\\\\.96 \\\\& 41\\\\.56 \\\\\\\\ Tapas \\\\& 28\\\\.72 \\\\& 53\\\\.84 \\\\& 41\\\\.28 \\\\\\\\ VLT5 \\\\& 26\\\\.24 \\\\& 56\\\\.88 \\\\& 41\\\\.56 \\\\\\\\ VisionTapas \\\\& 29\\\\.60 \\\\& 61\\\\.44 \\\\& 45\\\\.52 \\\\\\\\ \\\\\\\\hline VLT5({}\\\\_{pre}) \\\\&* *40\\\\.08* *\\\\& 63\\\\.60 \\\\& 51\\\\.84 \\\\\\\\ VisionTapas({}\\\\_{pre}) \\\\& 32\\\\.56 \\\\& 61\\\\.60 \\\\& 47\\\\.08 \\\\\\\\ Pix2Struct \\\\& \\\\- \\\\& \\\\- \\\\&* *56\\\\.00* *\\\\\\\\ ChartT5 \\\\& 31\\\\.8 \\\\&* *74\\\\.4*\\\\* \\\\& 53\\\\.16 \\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\n\\\\\\\\end{table}\\nTable 2: Evaluation results on ChartQA. We report relaxed accuracy on the test split annotated by humans and that generated by the machine. In the last column, we report the overall accuracy by computing the mean values with human split and augment split.\\nacross both Statista and Pew sets. The under\\\\-performance on BLEU and CIDER indicates that Chart\\\\-table pre\\\\-training is limited to benefit high\\\\-quality natural language generation. However, the strong performance on content selection, which values the key information appearance in the generation, suggests the advantage of chart\\\\-table pre\\\\-training on extracting relevant chart information. Therefore, a potential direction to explore is combining different types of pre\\\\-training objectives, such as chart\\\\-to\\\\-text pre\\\\-training and chart\\\\-table pre\\\\-training goals, to facilitate the model with diverse strengths.\\n\\n### Ablation Study\\n\\nWe conduct ablation experiments to validate the effectiveness of chart\\\\-table pre\\\\-training and the pre\\\\-training objectives. We also evaluate the effectiveness of the proposed scene text copy mechanism.\\n\\n#### 4\\\\.2\\\\.1 Chart\\\\-Table Pre\\\\-training\\n\\nWe conduct detailed analyses on the effectiveness of chart\\\\-table pre\\\\-training. First, we measure the performance gain from the chart\\\\-table pre\\\\-training on the full test set of ChartQA data. We then study what type of questions benefit most from the chart\\\\-table pre\\\\-training by picking three subsets of questions that measure different capabilities of the model: (1\\\\) Human\\\\-written questions, (2\\\\) Machine\\\\-generated questions, and (3\\\\) Table covered questions, where the answers can be directly found in the ground truth tables. The results are summarized in Table 4\\\\. From Table 4, we find that after chart\\\\-table pre\\\\-training the model\\'s performance on these three sets of questions is all improved. The most significant gain is obtained on machine\\\\-generated questions, which mainly focus on extractive\\\\-type questions. This indicates that chart\\\\-table pre\\\\-training benefits the model to localize and retrieve the requested information presented on Chart Image. The second biggest gain is achieved on table\\\\-cover questions, where the model demonstrates significant improvement in the capability of chart\\\\-to\\\\-table interpretation.\\n\\n#### 4\\\\.2\\\\.2 Pre\\\\-training Objectives\\n\\nWe validate the effectiveness of the two pre\\\\-training objectives, Masked Header Prediction and Masked Value Prediction. We remove one pre\\\\-training objective at a time and pre\\\\-train the ChartTSwith only one table prediction task. The pre\\\\-trained model\\n\\n\\\\\\\\begin{table}\\n\\\\\\\\begin{tabular}{l\\\\|c c c} \\\\\\\\hline \\\\\\\\hline \\\\\\\\multirow{2}{*}{Pretraining?} \\\\& \\\\\\\\multicolumn{3}{c}{Question Types} \\\\\\\\ \\\\& Table \\\\& Human \\\\& Augment \\\\\\\\ \\\\\\\\hline No \\\\& 60\\\\.7 \\\\& 30\\\\.8 \\\\& 66\\\\.7 \\\\\\\\ Yes \\\\&* *64\\\\.7* *\\\\&* *31\\\\.8* *\\\\&* *74\\\\.4*\\\\* \\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\n\\\\\\\\end{table}\\nTable 4: Ablation Study on Chart Table Pre\\\\-training with ChartQA Dataset. We report results on three subsets of questions: Table cover questions, human\\\\-written questions, and machine\\\\-generated questions.\\n\\n\\\\\\\\begin{table}\\n\\\\\\\\begin{tabular}{l\\\\|c c c\\\\|c c c} \\\\\\\\hline \\\\\\\\hline \\\\\\\\multirow{2}{*}{Model} \\\\& \\\\\\\\multicolumn{3}{c\\\\|}{Statista} \\\\& \\\\\\\\multicolumn{3}{c}{Pew} \\\\\\\\ \\\\& BLEU \\\\& CS \\\\& CIDER \\\\& BLEU \\\\& CS \\\\& CIDER \\\\\\\\ \\\\\\\\hline T5\\\\-OCR \\\\& 35\\\\.29 \\\\& 73\\\\.77 \\\\& 4\\\\.43 \\\\&* *10\\\\.49* *\\\\& 40\\\\.87 \\\\&* *2\\\\.20* *\\\\\\\\ BART\\\\-OCR \\\\& \\\\- \\\\& \\\\- \\\\& \\\\- \\\\& 9\\\\.09 \\\\& 39\\\\.99 \\\\& 1\\\\.97 \\\\\\\\ T5\\\\-TAB \\\\& 37\\\\.01 \\\\& 75\\\\.72 \\\\&* *4\\\\.68* *\\\\& \\\\- \\\\& \\\\- \\\\& \\\\- \\\\\\\\ BART\\\\-TAB \\\\& 36\\\\.36 \\\\& 77\\\\.14 \\\\& 4\\\\.40 \\\\& \\\\- \\\\& \\\\- \\\\& \\\\- \\\\\\\\ ChartT5 \\\\&* *37\\\\.51* *\\\\&* *82\\\\.16* *\\\\& 3\\\\.45 \\\\& 9\\\\.05 \\\\&* *55\\\\.1*\\\\* \\\\& 1\\\\.23 \\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\n\\\\\\\\end{table}\\nTable 3: Evaluation results on Chart Summarization. We display BLEU, CS and CIDER scores for the Pew and Statista Split. The ground truth table is not available to Pew thus the table\\\\-based method does not have results on Pew split.\\n\\n\\\\\\\\begin{table}\\n\\\\\\\\begin{tabular}{l\\\\|c c c} \\\\\\\\hline \\\\\\\\hline \\\\& \\\\\\\\multicolumn{3}{c}{Question Types} \\\\\\\\ \\\\& Human \\\\& Augment \\\\& Overall \\\\\\\\ \\\\\\\\hline Full \\\\& 31\\\\.8 \\\\& 74\\\\.4 \\\\& 53\\\\.1 \\\\\\\\ \\\\- MVP \\\\& 30\\\\.9 \\\\& 73\\\\.7 \\\\& 52\\\\.3 \\\\\\\\ \\\\- MHP \\\\& 31\\\\.2 \\\\& 68\\\\.3 \\\\& 49\\\\.7 \\\\\\\\ \\\\- STC \\\\& 30\\\\.8 \\\\& 72\\\\.4 \\\\& 51\\\\.6 \\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\n\\\\\\\\end{table}\\nTable 5: Ablation Study on the two proposed pre\\\\-training objectives and the Scene Text Copy Mechanism (STC). The first row is the result of the full ChartT5 model. Then we remove one of the pre\\\\-training objectives and the scene\\\\-text\\\\-copy mechanism. We report the results of different ablation experiments on both human and machine\\\\-generated splits as well as the overall performance.\\nis then fine\\\\-tuned and evaluated on the human and augmented split for comparison. The result is displayed in table 5\\\\. As can be seen from the table, removing Masked Value Prediction Loss has a negligible impact on the performance of ChartT5 on ChartQA dataset. There is a slightly more drop in human written questions which suggests that predicting table numerical values still has a miner positive impact on helping the model\\'s mathematical reasoning. Remove Masked Header Prediction have a significant impact on the machine\\\\-generated question\\\\-answering accuracy. As expected, Masked header modeling mainly helps the model learn how to link the scene text to the table headers, which is a critical ability to extract relevant information given a specific query.\\n\\n#### 4\\\\.2\\\\.3 Scene Text Copy\\n\\nWe also validate the effectiveness of the scene\\\\-text\\\\-copy mechanism, where we train a ChartT5model by simply representing OCR tokens in their original text format. The model is fine\\\\-tuned and evaluated on the human and augmented split of the chartQA dataset to compare against the full ChartT5\\\\. The result is displayed in Table 5\\\\. Disabling the scene\\\\-text\\\\-copy mechanism leads to a 1\\\\.5(\\\\\\\\%) overall performance drop on ChartQA tasks. Specifically, it leads to more degradation on the augmented split than the human split, as scene\\\\-text\\\\-copy helps enhance the alignment between OCR and table values to benefit accurate information extraction from the chart.\\n\\n### Qualitative Error Analysis\\n\\nWe have manually analyzed model predictions to understand its limitation. We found that our model suffers most from noisy OCR detection and complex question that requires multi\\\\-hop reasoning.\\n\\n**Noisy OCR Prediction.** As an OCR\\\\-based model, ChartT5 often suffers from a wrong OCR detection. An example is shown in Figure 3; the model localizes the right scene text \"1\\\\.18\" to answer the question, but the OCR text is mistakenly detected as \"1:18\". To further understand the limitation of OCR detection, we randomly sample 20K PlotQA test split and compare the performance of our model using detected OCRs against Ground Truth OCRs. We observe a 5(\\\\\\\\%) performance drop when using detected OCRs. We can improve the OCR detector for future work by training on a large Plot scene\\\\-text detection benchmark. Another promising direction is to attempt OCR\\\\-free end\\\\-to\\\\-end plot recognition method like Pix2Struct (Lee et al., 2022\\\\).\\n\\n**Multi\\\\-Hop Reasoning.** Our model is also quite vulnerable to handling complex questions requiring multi\\\\-hop reasoning. An example is shown in Figure 4; the model cannot perform the complex logic reasoning to add the stats of the two smallest bars and compare that to the large bar. We will consider exploring pre\\\\-training on the mathematic reasoning datasets to address this limitation.\\n\\n', children=[]),\n",
       " ArxivPaperSection(header='h2', title='5 Conclusion', text=\"We propose ChartT5 to enhance the vision language model's ability to understand chart images via chart\\\\-table pre\\\\-training. The model learns to interpret the masked tables via our proposed masked header prediction and masked value prediction objectives. ChartT5 achieves significant improvement over table\\\\-based non\\\\-pretraining SOTA methods on the ChartQA dataset, especially on the extractive question sets. We also achieve a new SOTA Content Selection Score on the Chart\\\\-to\\\\-text summarization dataset. We conduct comprehensive ablation studies to identify the impact of chart\\\\-table pre\\\\-training, and we find that the proposed pre\\\\-training is extremely helpful to extract accurate\\n\\nFigure 4: An error prediction from our model due to complex multi\\\\-hop reasoning\\n\\nFigure 3: An error prediction from our model due to noisy OCR prediction\\ninformation from the Chart. For future research directions, we believe it may also be meaningful to explore chart understanding under data\\\\-efficient settings Hsu et al. (2022\\\\); Zeng et al. (2023\\\\) and for evidence retrieval tasks Lu et al. (2022\\\\); Ji et al. (2023\\\\).\\n\\n\", children=[]),\n",
       " ArxivPaperSection(header='h2', title='6 Limitations', text=\"Although introducing chart value prediction objective, it only provides minor improvement to the model's performance on doing complex reasoning. There is still a large room to improve the model's capability in math calculation. Our model also suffers from the noisy OCR prediction of off\\\\-the\\\\-shelf object detector, whose performance will depend highly on the extracted OCR text qualities. Another possible limitation of our approach is the quality of the pre\\\\-training data, which only contains synthetic images. Although the proposed model works fairly well on the ChartQA dataset, it is unclear if the improved performance can be generalized to other realistic chart images.\\n\\n\", children=[]),\n",
       " ArxivPaperSection(header='h2', title='7 Ethics Statement', text=\"When we collect the pre\\\\-training dataset, we ensure we respect the intellectual property of dataset sources. All the ChartQA dataset we used for the collection of chart\\\\-table pairs allows public access for research. To ensure the reproducibility of our experiment results, we provide details of the hyperparameter setting in our paper, and we will also publish our code later. Our models can mislead the public's understanding of chart content due to the potential bias from our training corpus. Therefore, we don't recommend using our model for any real\\\\-world decision on chart images.\\n\\n\", children=[]),\n",
       " ArxivPaperSection(header='h2', title='Acknowledgement', text='This research work is supported by U.S DARPA SemaFor Program No. HR001120C0123\\\\. The views and conclusions contained in this work only belong to the authors and should not represent the official policies implied by DARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. We also thank Ahmed Masry and Shankar Kantharaj for providing us with ChartQA and Chart Summary\\\\-related data and baseline model outputs.\\n\\n', children=[])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_case1(toc: TreeOfContents):\n",
    "    \"\"\"Case 1: contents under h1 title\"\"\"\n",
    "\n",
    "def split_case2(document: TreeOfContents):\n",
    "    \"\"\"Case 2: title & contents in same level\"\"\"\n",
    "    sections = []\n",
    "    for child in document.branches:\n",
    "        # p - title, h6 - abstract\n",
    "        if not child.name==\"h2\":\n",
    "            continue\n",
    "        print(\"CHILD:\", child.name, child.string, str(child))\n",
    "        child_text = get_toc_text(child)\n",
    "        print(child_text)\n",
    "        \n",
    "        print('-'*30)\n",
    "        # for subchild in child.expandDescendants(child):\n",
    "        #     print(subchild.name, subchild.string)\n",
    "        #     print(md(str(subchild)))\n",
    "        #     print(vars(subchild).keys())\n",
    "        #     print(len(subchild.contents))\n",
    "        #     print(repr(subchild.contents))\n",
    "        \n",
    "        # section_texts = [\n",
    "            \n",
    "        #     for subchild in child.expandDescendants(child)\n",
    "        # ]\n",
    "        section = ArxivPaperSection(\n",
    "            header = child.name,\n",
    "            title = child.string,\n",
    "            text = child_text\n",
    "        )\n",
    "        sections.append(section)\n",
    "    return sections\n",
    "\n",
    "def split_markdown_text(text: str) -> List[ArxivPaperSection]:\n",
    "    toc = md2py(text) # md -> html -> bs4\n",
    "    print(vars(toc).keys())\n",
    "    print(\"TOC\", toc.name)\n",
    "    l1_branches = toc.branches\n",
    "    print(len(l1_branches))\n",
    "    \n",
    "    sections = []\n",
    "    ## Case 1 (h1 title)\n",
    "    if len(l1_branches)==1 and l1_branches[0].name==\"h1\":\n",
    "        print(l1_branches[0])\n",
    "        pass\n",
    "    ## Case2\n",
    "    elif len(l1_branches)>1:\n",
    "        sections = split_case2(toc)\n",
    "        \n",
    "    return sections\n",
    "    \n",
    "idx = 0 ## Case 1\n",
    "idx = 14 ## Case 2\n",
    "sample = df.iloc[idx]['markdown']\n",
    "split_markdown_text(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arxiv_paper(row):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
