{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ArxivPaperSection(BaseModel):\n",
    "    header: Literal[\"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]\n",
    "    title:str = Field(..., description=\"Section title\")\n",
    "    text: str = Field(\"\", description = \"Section contents\")\n",
    "    children: List[\"ArxivPaperSection\"] = Field(list(), description=\"child sections\")\n",
    "\n",
    "class ArxivPaperMetadata(BaseModel):\n",
    "    authors: str\n",
    "    published_date: datetime\n",
    "    link: str\n",
    "\n",
    "class ArxivPaper(BaseModel):\n",
    "    id: str\n",
    "    title: str\n",
    "    abstract: str\n",
    "    sections: List[ArxivPaperSection]\n",
    "    metadata: ArxivPaperMetadata = Field(None, description=\"paper metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 7) Index(['id', 'title', 'abstract', 'authors', 'published_date', 'link',\n",
      "       'markdown'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## Load Sample\n",
    "df = pd.read_parquet(\"sample.parquet\")\n",
    "print(df.shape, df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                       2310.03187\n",
       "title             Synthesis of Data-Driven Nonlinear State Obser...\n",
       "abstract          This paper focuses on the model-free synthesis...\n",
       "authors                                                 Wentao Tang\n",
       "published_date                                 2023-10-04T22:19:53Z\n",
       "link                              http://arxiv.org/abs/2310.03187v1\n",
       "markdown          # Synthesis of Data-Driven Nonlinear State Obs...\n",
       "Name: 62774, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## md2py based parser\n",
    "from markdownify import markdownify as md\n",
    "from src.custom_md2py import md2py, TreeOfContents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted html: '<h1>some list</h1>\\n<ul>\\n<li>head<ul>\\n<li>tail</li>\\n<li>tail2</li>\\n</ul>\\n</li>\\n<li>head2</li>\\n</ul>'\n",
      "source: [document]\n",
      "CHILD 0: h1\n",
      "'some list' '<h1>some list</h1>'\n",
      "------------------------------\n",
      "CHILD 1: None\n",
      "'\\n' '\\n'\n",
      "------------------------------\n",
      "CHILD 2: ul\n",
      "None '<ul>\\n<li>head<ul>\\n<li>tail</li>\\n<li>tail2</li>\\n</ul>\\n</li>\\n<li>head2</li>\\n</ul>'\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from markdown import markdownFromFile, markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parseBranches(descendants):\n",
    "    \"\"\"\n",
    "    Parse top level of markdown\n",
    "\n",
    "    :param list elements: list of source objects\n",
    "    :return: list of filtered TreeOfContents objects\n",
    "    \"\"\"\n",
    "    parsed, parent, cond = [], False, lambda b: (b.string or '').strip()\n",
    "    for branch in filter(cond, descendants):\n",
    "        print(branch)\n",
    "        # if self.getHeadingLevel(branch) == self.depth:\n",
    "        #     parsed.append({'root':branch.string, 'source':branch})\n",
    "        #     parent = True\n",
    "        # elif not parent:\n",
    "        #     parsed.append({'root':branch.string, 'source':branch})\n",
    "        # else:\n",
    "        #     parsed[-1].setdefault('descendants', []).append(branch)\n",
    "    return parsed\n",
    "list_sample = '''# some list\n",
    "- head\n",
    "    - tail\n",
    "    - tail2\n",
    "- head2'''\n",
    "list_html = markdown(list_sample)\n",
    "print(\"converted html:\", repr(list_html))\n",
    "source = BeautifulSoup(list_html, 'html.parser')\n",
    "print(\"source:\", source.name)\n",
    "for child_i, child in enumerate(source.children):\n",
    "    print(\"CHILD {}: {}\".format(child_i, child.name))\n",
    "    print(repr(child.string), repr(str(child)))\n",
    "    print('-'*30)\n",
    "# print(\"children\",list(source.children))\n",
    "# descendants = list(source.children)\n",
    "# print(type(descendants[0]))\n",
    "# print(\"items:\", descendants[0].li, descendants[0].attrs)\n",
    "\n",
    "# print(\"STRING\", descendants[0].string, repr(str(descendants[0])))\n",
    "# # print(len(descendants))\n",
    "# # parseBranches(descendants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<li>head<ul>\n",
      "<li>tail</li>\n",
      "<li>tail2</li>\n",
      "</ul>\n",
      "</li>\n",
      "attrs: {}\n",
      "ul <li>head<ul>\n",
      "<li>tail</li>\n",
      "<li>tail2</li>\n",
      "</ul>\n",
      "</li>\n",
      "<class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "html_list_sample = '''<ul>\n",
    "<li>head<ul>\n",
    "<li>tail</li>\n",
    "<li>tail2</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>head2</li>\n",
    "</ul>'''\n",
    "\n",
    "soup = BeautifulSoup(html_list_sample, 'html.parser')\n",
    "# soup.find(\"ul\")\n",
    "print(soup.li)\n",
    "print(\"attrs:\",soup.attrs)\n",
    "found_list = soup.find(\"ul\")\n",
    "print(found_list.name, found_list.find('li'))\n",
    "print(type(soup.find(\"ul\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "## markdown text -> List[Section]\n",
    "header_name_pattern = r\"^h([1-6])\"\n",
    "\n",
    "def get_toc_text(toc: TreeOfContents) -> str:\n",
    "    texts = []\n",
    "    for child in toc.expandDescendants(toc):\n",
    "        header_match = re.search(header_name_pattern, child.name)\n",
    "        if header_match:\n",
    "            header_num = header_match.group(1)\n",
    "            ## header\n",
    "            text = \"\".join([\"#\"*int(header_num), child.string])\n",
    "        elif child.name==\"ul\":\n",
    "            # List\n",
    "            text = md(str(child))\n",
    "            pass\n",
    "        else:\n",
    "            print(child.name, repr(child.string), str(child))\n",
    "            text = md(str(child))\n",
    "            # text = child.string\n",
    "        texts.append(text)\n",
    "    print(texts)\n",
    "    return \"\\n\\n\".join(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "child 0 h2 Title2 3\n",
      "p 'something something\\ns2omething2 something2'\n",
      "p 'something something\\ns2omething2 something2'\n",
      "p 'ddd'\n",
      "p '| a | b |\\n| --- | --- |\\n| hi | hey |'\n",
      "p 'writing something about title2'\n",
      "['something something\\ns2omething2 something2', 'something something\\ns2omething2 something2', '* something\\n* something\\n', 'ddd', '| a | b |\\n| --- | --- |\\n| hi | hey |', 'writing something about title2', '###Title2-1', '###Title2-2']\n",
      "text: 'something something\\ns2omething2 something2\\n\\nsomething something\\ns2omething2 something2\\n\\n* something\\n* something\\n\\n\\nddd\\n\\n| a | b |\\n| --- | --- |\\n| hi | hey |\\n\\nwriting something about title2\\n\\n###Title2-1\\n\\n###Title2-2'\n",
      "------------------------------\n",
      "child 1 h2 Title 3 3\n",
      "p 'something3 something3\\ns2omething3-2 something3-2'\n",
      "p 'sth3'\n",
      "['something3 something3\\ns2omething3-2 something3-2', 'sth3', '###Title3-2']\n",
      "text: 'something3 something3\\ns2omething3-2 something3-2\\n\\nsth3\\n\\n###Title3-2'\n",
      "------------------------------\n",
      "child 2 h2 Title 4 3\n",
      "[]\n",
      "text: ''\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_sample = '''# Title1\n",
    "## Title2\n",
    "writing something about title2\n",
    "### Title2-1\n",
    "something something\n",
    "s2omething2 something2\n",
    "### Title2-2\n",
    "something something\n",
    "s2omething2 something2\n",
    "\n",
    "- something\n",
    "- something\n",
    "\n",
    "ddd\n",
    "\n",
    "| a | b |\n",
    "| --- | --- |\n",
    "| hi | hey |\n",
    "\n",
    "## Title 3\n",
    "sth3\n",
    "### Title3-2\n",
    "something3 something3\n",
    "s2omething3-2 something3-2\n",
    "## Title 4'''\n",
    "\n",
    "### Title2-1\n",
    "toc = md2py(test_sample)\n",
    "print(toc.depth)\n",
    "document = toc.branches[0]\n",
    "print(document.depth)\n",
    "for child_i, child in enumerate(document.branches):\n",
    "    print(\"child {}\".format(child_i), child.name, child.string, child.depth)\n",
    "    child_text = get_toc_text(child)\n",
    "    print(\"text:\",repr(child_text))\n",
    "    print('-'*30)\n",
    "    \n",
    "    # for subchild_i, subchild in enumerate(child.branches):\n",
    "    #     print(subchild_i, subchild.name, subchild.string)\n",
    "    #     print(vars(subchild))\n",
    "    \n",
    "    #     subchild_children = [x.name for x in subchild.branches]\n",
    "    #     print(subchild_children)\n",
    "    #     subchild_children_text = [x.string for x in subchild.branches]\n",
    "    #     print(subchild_children_text)\n",
    "    #     print('-'*30)\n",
    "    \n",
    "    # break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['source', 'depth', 'descendants', 'branches'])\n",
      "TOC [document]\n",
      "11\n",
      "CHILD: h2 1 Introduction 1 Introduction\n",
      "p None <p>Chart figures serve as the visual summary of tabular data, which helps to convey rich context in various documents, such as scientific papers, textbooks, and technical news. An intelligent agent that can understand and communicate chart plots can lead to many useful applications. For example, a virtual doctor who knows how to answer the patient's question on a complex medical report or a reading assistant who can summarize the key findings from scientific papers in brief language. In the past few years, there has been a growing interest in our community to explore chart understanding in vision and language (V+L) tasks and many related benchmarks like Chart Question Answering <strong>(CQA)</strong>Masry et al. (2022); Kafle et al. (2018); Methani et al. (2020) and Chart Summarization <strong>(CS)</strong>Kantharaj et al. (2022) are introduced.</p>\n",
      "p 'While prevalent in the research community, automatic chart understanding remains a challenging problem due to its complex compositions of various shapes, lines, colors, and scene text. Although tremendous success is achieved in the V+L research, applying these existing methods to handle chart-related tasks is hard. Recent research ChartQA Masry et al. (2022) and Chart-to-Text Kantharaj et al. (2022) attempt to first convert chart images to their underlined tables and use the extracted tables to perform chart-related V+L task. As the extracted tables always have clean and organized structures, it makes extracting relevant information to solve downstream reasoning tasks much more accessible. Empirically, using tables yields promising results on both CQA and CS.' <p>While prevalent in the research community, automatic chart understanding remains a challenging problem due to its complex compositions of various shapes, lines, colors, and scene text. Although tremendous success is achieved in the V+L research, applying these existing methods to handle chart-related tasks is hard. Recent research ChartQA Masry et al. (2022) and Chart-to-Text Kantharaj et al. (2022) attempt to first convert chart images to their underlined tables and use the extracted tables to perform chart-related V+L task. As the extracted tables always have clean and organized structures, it makes extracting relevant information to solve downstream reasoning tasks much more accessible. Empirically, using tables yields promising results on both CQA and CS.</p>\n",
      "p 'Despite valuing table as a significant ingredient for chart understanding, we have two main concerns about this approach: (1) Automatic table extraction is unreliable. Existing methods Luo et al. (2021); Kato et al. (2022) are often limited to work on a few particular types of chart images and do not generalize well. Moreover, the extracted table is likely to contain incorrect noisy predictions that potentially harm the performance of the following task. (2) In most cases, the whole table is optional for resolving the chart-related V+L task. As illus' <p>Despite valuing table as a significant ingredient for chart understanding, we have two main concerns about this approach: (1) Automatic table extraction is unreliable. Existing methods Luo et al. (2021); Kato et al. (2022) are often limited to work on a few particular types of chart images and do not generalize well. Moreover, the extracted table is likely to contain incorrect noisy predictions that potentially harm the performance of the following task. (2) In most cases, the whole table is optional for resolving the chart-related V+L task. As illus</p>\n",
      "p None <p>Figure 1: A data sample from the ChartQA dataset. The corresponding chart table is displayed in the top right corner.\n",
      "trated in Fig 1, to answer the question <em>\"What is the value of India Bar\"</em>, the model just needs access to the second row to give the correct answer. In contrast, having redundant table information makes finding the relevant information challenging. To better leverage the table data, we argue that it is important to equip the V+L model with the capability to dynamically interpret the table value from the chart information.</p>\n",
      "p None <p>Therefore, in this paper, we propose <strong>ChartT5</strong>, an OCR-based image-to-text generation model pre-trained on a self-collected chart table pairs corpus. More specifically, ChartT5 learns how to uncover a masked table with two proposed pre-training objectives: Masked Header Prediction (MHP), and Masked Value Prediction (MVP). MHP helps improve the model's capability of linking scene text to the corresponding table headers. MVP requires the model to perform mathematical reasoning over chart structure units and the scene text to predict the correct data value.</p>\n",
      "p \"We evaluate our ChartT5 on two tasks and benchmarks: ChartQA and Chart-to-Text. In ChartQA, ChartT5 outperforms all the non-pretraining methods that use extracted tables by at least (8\\\\%) performance gains. ChartT5 also beats the pre-training table-based methods, which demonstrates the effectiveness of the proposed pre-training strategies. On Chart-to-Text, ChartT5 consistly outperforms the existing SOTA on the content selection metrics Barzilay and Lapata (2005) which values the model's capability to extract the critical information from the chart.\" <p>We evaluate our ChartT5 on two tasks and benchmarks: ChartQA and Chart-to-Text. In ChartQA, ChartT5 outperforms all the non-pretraining methods that use extracted tables by at least (8\\%) performance gains. ChartT5 also beats the pre-training table-based methods, which demonstrates the effectiveness of the proposed pre-training strategies. On Chart-to-Text, ChartT5 consistly outperforms the existing SOTA on the content selection metrics Barzilay and Lapata (2005) which values the model's capability to extract the critical information from the chart.</p>\n",
      "p 'In summary, our contributions are summarized below:' <p>In summary, our contributions are summarized below:</p>\n",
      "[\"Chart figures serve as the visual summary of tabular data, which helps to convey rich context in various documents, such as scientific papers, textbooks, and technical news. An intelligent agent that can understand and communicate chart plots can lead to many useful applications. For example, a virtual doctor who knows how to answer the patient's question on a complex medical report or a reading assistant who can summarize the key findings from scientific papers in brief language. In the past few years, there has been a growing interest in our community to explore chart understanding in vision and language (V\\\\+L) tasks and many related benchmarks like Chart Question Answering **(CQA)**Masry et al. (2022\\\\); Kafle et al. (2018\\\\); Methani et al. (2020\\\\) and Chart Summarization **(CS)**Kantharaj et al. (2022\\\\) are introduced.\\n\\n\", 'While prevalent in the research community, automatic chart understanding remains a challenging problem due to its complex compositions of various shapes, lines, colors, and scene text. Although tremendous success is achieved in the V\\\\+L research, applying these existing methods to handle chart\\\\-related tasks is hard. Recent research ChartQA Masry et al. (2022\\\\) and Chart\\\\-to\\\\-Text Kantharaj et al. (2022\\\\) attempt to first convert chart images to their underlined tables and use the extracted tables to perform chart\\\\-related V\\\\+L task. As the extracted tables always have clean and organized structures, it makes extracting relevant information to solve downstream reasoning tasks much more accessible. Empirically, using tables yields promising results on both CQA and CS.\\n\\n', 'Despite valuing table as a significant ingredient for chart understanding, we have two main concerns about this approach: (1\\\\) Automatic table extraction is unreliable. Existing methods Luo et al. (2021\\\\); Kato et al. (2022\\\\) are often limited to work on a few particular types of chart images and do not generalize well. Moreover, the extracted table is likely to contain incorrect noisy predictions that potentially harm the performance of the following task. (2\\\\) In most cases, the whole table is optional for resolving the chart\\\\-related V\\\\+L task. As illus\\n\\n', 'Figure 1: A data sample from the ChartQA dataset. The corresponding chart table is displayed in the top right corner.\\ntrated in Fig 1, to answer the question *\"What is the value of India Bar\"*, the model just needs access to the second row to give the correct answer. In contrast, having redundant table information makes finding the relevant information challenging. To better leverage the table data, we argue that it is important to equip the V\\\\+L model with the capability to dynamically interpret the table value from the chart information.\\n\\n', \"Therefore, in this paper, we propose **ChartT5**, an OCR\\\\-based image\\\\-to\\\\-text generation model pre\\\\-trained on a self\\\\-collected chart table pairs corpus. More specifically, ChartT5 learns how to uncover a masked table with two proposed pre\\\\-training objectives: Masked Header Prediction (MHP), and Masked Value Prediction (MVP). MHP helps improve the model's capability of linking scene text to the corresponding table headers. MVP requires the model to perform mathematical reasoning over chart structure units and the scene text to predict the correct data value.\\n\\n\", \"We evaluate our ChartT5 on two tasks and benchmarks: ChartQA and Chart\\\\-to\\\\-Text. In ChartQA, ChartT5 outperforms all the non\\\\-pretraining methods that use extracted tables by at least (8\\\\\\\\%) performance gains. ChartT5 also beats the pre\\\\-training table\\\\-based methods, which demonstrates the effectiveness of the proposed pre\\\\-training strategies. On Chart\\\\-to\\\\-Text, ChartT5 consistly outperforms the existing SOTA on the content selection metrics Barzilay and Lapata (2005\\\\) which values the model's capability to extract the critical information from the chart.\\n\\n\", 'In summary, our contributions are summarized below:\\n\\n', '* We propose chart\\\\-to\\\\-table pre\\\\-training for V\\\\+L model to learn the capability of interpreting table data from the chart.\\n* We demonstrate that the pre\\\\-trained model consistently outperforms table\\\\-based methods on two chart understanding tasks.\\n* We conduct comprehensive ablation studies to validate the effectiveness of chart\\\\-to\\\\-table pre\\\\-training and the proposed pre\\\\-training objectives.\\n']\n",
      "Chart figures serve as the visual summary of tabular data, which helps to convey rich context in various documents, such as scientific papers, textbooks, and technical news. An intelligent agent that can understand and communicate chart plots can lead to many useful applications. For example, a virtual doctor who knows how to answer the patient's question on a complex medical report or a reading assistant who can summarize the key findings from scientific papers in brief language. In the past few years, there has been a growing interest in our community to explore chart understanding in vision and language (V\\+L) tasks and many related benchmarks like Chart Question Answering **(CQA)**Masry et al. (2022\\); Kafle et al. (2018\\); Methani et al. (2020\\) and Chart Summarization **(CS)**Kantharaj et al. (2022\\) are introduced.\n",
      "\n",
      "\n",
      "\n",
      "While prevalent in the research community, automatic chart understanding remains a challenging problem due to its complex compositions of various shapes, lines, colors, and scene text. Although tremendous success is achieved in the V\\+L research, applying these existing methods to handle chart\\-related tasks is hard. Recent research ChartQA Masry et al. (2022\\) and Chart\\-to\\-Text Kantharaj et al. (2022\\) attempt to first convert chart images to their underlined tables and use the extracted tables to perform chart\\-related V\\+L task. As the extracted tables always have clean and organized structures, it makes extracting relevant information to solve downstream reasoning tasks much more accessible. Empirically, using tables yields promising results on both CQA and CS.\n",
      "\n",
      "\n",
      "\n",
      "Despite valuing table as a significant ingredient for chart understanding, we have two main concerns about this approach: (1\\) Automatic table extraction is unreliable. Existing methods Luo et al. (2021\\); Kato et al. (2022\\) are often limited to work on a few particular types of chart images and do not generalize well. Moreover, the extracted table is likely to contain incorrect noisy predictions that potentially harm the performance of the following task. (2\\) In most cases, the whole table is optional for resolving the chart\\-related V\\+L task. As illus\n",
      "\n",
      "\n",
      "\n",
      "Figure 1: A data sample from the ChartQA dataset. The corresponding chart table is displayed in the top right corner.\n",
      "trated in Fig 1, to answer the question *\"What is the value of India Bar\"*, the model just needs access to the second row to give the correct answer. In contrast, having redundant table information makes finding the relevant information challenging. To better leverage the table data, we argue that it is important to equip the V\\+L model with the capability to dynamically interpret the table value from the chart information.\n",
      "\n",
      "\n",
      "\n",
      "Therefore, in this paper, we propose **ChartT5**, an OCR\\-based image\\-to\\-text generation model pre\\-trained on a self\\-collected chart table pairs corpus. More specifically, ChartT5 learns how to uncover a masked table with two proposed pre\\-training objectives: Masked Header Prediction (MHP), and Masked Value Prediction (MVP). MHP helps improve the model's capability of linking scene text to the corresponding table headers. MVP requires the model to perform mathematical reasoning over chart structure units and the scene text to predict the correct data value.\n",
      "\n",
      "\n",
      "\n",
      "We evaluate our ChartT5 on two tasks and benchmarks: ChartQA and Chart\\-to\\-Text. In ChartQA, ChartT5 outperforms all the non\\-pretraining methods that use extracted tables by at least (8\\\\%) performance gains. ChartT5 also beats the pre\\-training table\\-based methods, which demonstrates the effectiveness of the proposed pre\\-training strategies. On Chart\\-to\\-Text, ChartT5 consistly outperforms the existing SOTA on the content selection metrics Barzilay and Lapata (2005\\) which values the model's capability to extract the critical information from the chart.\n",
      "\n",
      "\n",
      "\n",
      "In summary, our contributions are summarized below:\n",
      "\n",
      "\n",
      "\n",
      "* We propose chart\\-to\\-table pre\\-training for V\\+L model to learn the capability of interpreting table data from the chart.\n",
      "* We demonstrate that the pre\\-trained model consistently outperforms table\\-based methods on two chart understanding tasks.\n",
      "* We conduct comprehensive ablation studies to validate the effectiveness of chart\\-to\\-table pre\\-training and the proposed pre\\-training objectives.\n",
      "\n",
      "------------------------------\n",
      "CHILD: h2 2 Related Work 2 Related Work\n",
      "p 'Researching chart understanding in V+L tasks is a popular field nowadays. The most prevalent problem is chart question answering (CQA) Kafle et al. (2018); Kahou et al. (2018); Methani et al. (2020); Masry et al. (2022); Chaudhry et al. (2020), where researchers build models to answer complex questions on chart images. Another popular one is chart summarization (CS) Kantharaj et al. (2022); Obeid and Hoque (2020), which requires machine learning models to create a summary of key insights conveyed by a chart. Hsu et al. (2021) collected a large-scale scientific figures captioning dataset from research papers where many images are chart plots.' <p>Researching chart understanding in V+L tasks is a popular field nowadays. The most prevalent problem is chart question answering (CQA) Kafle et al. (2018); Kahou et al. (2018); Methani et al. (2020); Masry et al. (2022); Chaudhry et al. (2020), where researchers build models to answer complex questions on chart images. Another popular one is chart summarization (CS) Kantharaj et al. (2022); Obeid and Hoque (2020), which requires machine learning models to create a summary of key insights conveyed by a chart. Hsu et al. (2021) collected a large-scale scientific figures captioning dataset from research papers where many images are chart plots.</p>\n",
      "p \"There are two main approaches for chart vision and language tasks. The first approach adapts existing visual question answering (VQA) and image captioning models to CQA and CS tasks with some specialized designs for chart images Kafle et al. (2020); Singh and Shekhar (2020); Chaudhry et al. (2020); Kafle et al. (2018); Hsu et al. (2021); Spreafico and Carenini (2020). The other approach assumes the table data of charts is accessible from the dataset Kim et al. (2020); Masry (2021) or can be extracted from the chart images using vision to table techniques Methani et al. (2020); Masry et al. (2022); Kantharaj et al. (2022). Then, the researchers will either use a table-to-text generation model Kim et al. (2020); Masry (2021); Methani et al. (2020) or combine the embedding of tables and charts via a multi-modal fusion method to generate the text output Masry et al. (2022); Kantharaj et al. (2022). It is clear from these efforts that adding tables as the additional representation of charts will dramatically improve the model's capability to understand and interpret chart information.\" <p>There are two main approaches for chart vision and language tasks. The first approach adapts existing visual question answering (VQA) and image captioning models to CQA and CS tasks with some specialized designs for chart images Kafle et al. (2020); Singh and Shekhar (2020); Chaudhry et al. (2020); Kafle et al. (2018); Hsu et al. (2021); Spreafico and Carenini (2020). The other approach assumes the table data of charts is accessible from the dataset Kim et al. (2020); Masry (2021) or can be extracted from the chart images using vision to table techniques Methani et al. (2020); Masry et al. (2022); Kantharaj et al. (2022). Then, the researchers will either use a table-to-text generation model Kim et al. (2020); Masry (2021); Methani et al. (2020) or combine the embedding of tables and charts via a multi-modal fusion method to generate the text output Masry et al. (2022); Kantharaj et al. (2022). It is clear from these efforts that adding tables as the additional representation of charts will dramatically improve the model's capability to understand and interpret chart information.</p>\n",
      "p 'Following the table-based approach, we also value the information provided by the underlined table data of chart images. However, instead of directly concatenating the extracted table into the chart understanding model, we facilitate our model with the capability to interpret the table data from chart images via pre-training on chart-table pairs.' <p>Following the table-based approach, we also value the information provided by the underlined table data of chart images. However, instead of directly concatenating the extracted table into the chart understanding model, we facilitate our model with the capability to interpret the table data from chart images via pre-training on chart-table pairs.</p>\n",
      "p 'Vision and language pre-training has received growing interest over the past few years. Researchers build transformer-based multi-modal fusion models and perform self-supervised learning on a large-scale corpus of image-text pairs to learn robust cross-modal representations that can benefit the performance of various downstream tasks Chen et al. (2020); Lu et al. (2019); Tan and Bansal (2019);\\nSu et al., 2019; Li et al., 2020; Zhang et al., 2021).' <p>Vision and language pre-training has received growing interest over the past few years. Researchers build transformer-based multi-modal fusion models and perform self-supervised learning on a large-scale corpus of image-text pairs to learn robust cross-modal representations that can benefit the performance of various downstream tasks Chen et al. (2020); Lu et al. (2019); Tan and Bansal (2019);\n",
      "Su et al., 2019; Li et al., 2020; Zhang et al., 2021).</p>\n",
      "p 'While the pre-trained models achieve great success on tasks like VQA (Antol et al., 2015) and Image Captioning (Chen et al., 2015), they have only focused on the domain of natural images. However, chart understanding is still challenging for the existing vision and language methods due to their lack of knowledge of scene text and structured visual units such as \"bars\" and \"lines\".' <p>While the pre-trained models achieve great success on tasks like VQA (Antol et al., 2015) and Image Captioning (Chen et al., 2015), they have only focused on the domain of natural images. However, chart understanding is still challenging for the existing vision and language methods due to their lack of knowledge of scene text and structured visual units such as \"bars\" and \"lines\".</p>\n",
      "p None <p>To address the limitation of conventional vision and language pre-training, TAP (Yang et al., 2021) and PreSTU (Kil et al., 2022) propose OCR-based vision and language pre-training frameworks that focus on scene text understanding in natural images where they design various pre-training objectives around the extracted OCR texts. Most recently, Donut (Kim et al., 2022) and Pix2Struct (Lee et al., 2022) propose OCR-free pre-training frameworks, where the pre-trained model directly generates a text output from a raw image input. Donut focuses on document image (<em>e.g.</em>, receipt) understanding, and Pix2Struct aims to handle broader types of synthetic images that contain visually-situated texts such as infographics and user interfaces via parsing web-page screenshots into their HTML Code. Different from these works, we take the first step to explore vision and language pre-training that focuses on chart image understanding. Specifically, we propose novel pre-training objectives to parse charts to their underlined tables.</p>\n",
      "['Researching chart understanding in V\\\\+L tasks is a popular field nowadays. The most prevalent problem is chart question answering (CQA) Kafle et al. (2018\\\\); Kahou et al. (2018\\\\); Methani et al. (2020\\\\); Masry et al. (2022\\\\); Chaudhry et al. (2020\\\\), where researchers build models to answer complex questions on chart images. Another popular one is chart summarization (CS) Kantharaj et al. (2022\\\\); Obeid and Hoque (2020\\\\), which requires machine learning models to create a summary of key insights conveyed by a chart. Hsu et al. (2021\\\\) collected a large\\\\-scale scientific figures captioning dataset from research papers where many images are chart plots.\\n\\n', \"There are two main approaches for chart vision and language tasks. The first approach adapts existing visual question answering (VQA) and image captioning models to CQA and CS tasks with some specialized designs for chart images Kafle et al. (2020\\\\); Singh and Shekhar (2020\\\\); Chaudhry et al. (2020\\\\); Kafle et al. (2018\\\\); Hsu et al. (2021\\\\); Spreafico and Carenini (2020\\\\). The other approach assumes the table data of charts is accessible from the dataset Kim et al. (2020\\\\); Masry (2021\\\\) or can be extracted from the chart images using vision to table techniques Methani et al. (2020\\\\); Masry et al. (2022\\\\); Kantharaj et al. (2022\\\\). Then, the researchers will either use a table\\\\-to\\\\-text generation model Kim et al. (2020\\\\); Masry (2021\\\\); Methani et al. (2020\\\\) or combine the embedding of tables and charts via a multi\\\\-modal fusion method to generate the text output Masry et al. (2022\\\\); Kantharaj et al. (2022\\\\). It is clear from these efforts that adding tables as the additional representation of charts will dramatically improve the model's capability to understand and interpret chart information.\\n\\n\", 'Following the table\\\\-based approach, we also value the information provided by the underlined table data of chart images. However, instead of directly concatenating the extracted table into the chart understanding model, we facilitate our model with the capability to interpret the table data from chart images via pre\\\\-training on chart\\\\-table pairs.\\n\\n', 'Vision and language pre\\\\-training has received growing interest over the past few years. Researchers build transformer\\\\-based multi\\\\-modal fusion models and perform self\\\\-supervised learning on a large\\\\-scale corpus of image\\\\-text pairs to learn robust cross\\\\-modal representations that can benefit the performance of various downstream tasks Chen et al. (2020\\\\); Lu et al. (2019\\\\); Tan and Bansal (2019\\\\);\\nSu et al., 2019; Li et al., 2020; Zhang et al., 2021\\\\).\\n\\n', 'While the pre\\\\-trained models achieve great success on tasks like VQA (Antol et al., 2015\\\\) and Image Captioning (Chen et al., 2015\\\\), they have only focused on the domain of natural images. However, chart understanding is still challenging for the existing vision and language methods due to their lack of knowledge of scene text and structured visual units such as \"bars\" and \"lines\".\\n\\n', 'To address the limitation of conventional vision and language pre\\\\-training, TAP (Yang et al., 2021\\\\) and PreSTU (Kil et al., 2022\\\\) propose OCR\\\\-based vision and language pre\\\\-training frameworks that focus on scene text understanding in natural images where they design various pre\\\\-training objectives around the extracted OCR texts. Most recently, Donut (Kim et al., 2022\\\\) and Pix2Struct (Lee et al., 2022\\\\) propose OCR\\\\-free pre\\\\-training frameworks, where the pre\\\\-trained model directly generates a text output from a raw image input. Donut focuses on document image (*e.g.*, receipt) understanding, and Pix2Struct aims to handle broader types of synthetic images that contain visually\\\\-situated texts such as infographics and user interfaces via parsing web\\\\-page screenshots into their HTML Code. Different from these works, we take the first step to explore vision and language pre\\\\-training that focuses on chart image understanding. Specifically, we propose novel pre\\\\-training objectives to parse charts to their underlined tables.\\n\\n', '###Vision and Language Research on Charts', '###Vision and Language Pre-training']\n",
      "Researching chart understanding in V\\+L tasks is a popular field nowadays. The most prevalent problem is chart question answering (CQA) Kafle et al. (2018\\); Kahou et al. (2018\\); Methani et al. (2020\\); Masry et al. (2022\\); Chaudhry et al. (2020\\), where researchers build models to answer complex questions on chart images. Another popular one is chart summarization (CS) Kantharaj et al. (2022\\); Obeid and Hoque (2020\\), which requires machine learning models to create a summary of key insights conveyed by a chart. Hsu et al. (2021\\) collected a large\\-scale scientific figures captioning dataset from research papers where many images are chart plots.\n",
      "\n",
      "\n",
      "\n",
      "There are two main approaches for chart vision and language tasks. The first approach adapts existing visual question answering (VQA) and image captioning models to CQA and CS tasks with some specialized designs for chart images Kafle et al. (2020\\); Singh and Shekhar (2020\\); Chaudhry et al. (2020\\); Kafle et al. (2018\\); Hsu et al. (2021\\); Spreafico and Carenini (2020\\). The other approach assumes the table data of charts is accessible from the dataset Kim et al. (2020\\); Masry (2021\\) or can be extracted from the chart images using vision to table techniques Methani et al. (2020\\); Masry et al. (2022\\); Kantharaj et al. (2022\\). Then, the researchers will either use a table\\-to\\-text generation model Kim et al. (2020\\); Masry (2021\\); Methani et al. (2020\\) or combine the embedding of tables and charts via a multi\\-modal fusion method to generate the text output Masry et al. (2022\\); Kantharaj et al. (2022\\). It is clear from these efforts that adding tables as the additional representation of charts will dramatically improve the model's capability to understand and interpret chart information.\n",
      "\n",
      "\n",
      "\n",
      "Following the table\\-based approach, we also value the information provided by the underlined table data of chart images. However, instead of directly concatenating the extracted table into the chart understanding model, we facilitate our model with the capability to interpret the table data from chart images via pre\\-training on chart\\-table pairs.\n",
      "\n",
      "\n",
      "\n",
      "Vision and language pre\\-training has received growing interest over the past few years. Researchers build transformer\\-based multi\\-modal fusion models and perform self\\-supervised learning on a large\\-scale corpus of image\\-text pairs to learn robust cross\\-modal representations that can benefit the performance of various downstream tasks Chen et al. (2020\\); Lu et al. (2019\\); Tan and Bansal (2019\\);\n",
      "Su et al., 2019; Li et al., 2020; Zhang et al., 2021\\).\n",
      "\n",
      "\n",
      "\n",
      "While the pre\\-trained models achieve great success on tasks like VQA (Antol et al., 2015\\) and Image Captioning (Chen et al., 2015\\), they have only focused on the domain of natural images. However, chart understanding is still challenging for the existing vision and language methods due to their lack of knowledge of scene text and structured visual units such as \"bars\" and \"lines\".\n",
      "\n",
      "\n",
      "\n",
      "To address the limitation of conventional vision and language pre\\-training, TAP (Yang et al., 2021\\) and PreSTU (Kil et al., 2022\\) propose OCR\\-based vision and language pre\\-training frameworks that focus on scene text understanding in natural images where they design various pre\\-training objectives around the extracted OCR texts. Most recently, Donut (Kim et al., 2022\\) and Pix2Struct (Lee et al., 2022\\) propose OCR\\-free pre\\-training frameworks, where the pre\\-trained model directly generates a text output from a raw image input. Donut focuses on document image (*e.g.*, receipt) understanding, and Pix2Struct aims to handle broader types of synthetic images that contain visually\\-situated texts such as infographics and user interfaces via parsing web\\-page screenshots into their HTML Code. Different from these works, we take the first step to explore vision and language pre\\-training that focuses on chart image understanding. Specifically, we propose novel pre\\-training objectives to parse charts to their underlined tables.\n",
      "\n",
      "\n",
      "\n",
      "###Vision and Language Research on Charts\n",
      "\n",
      "###Vision and Language Pre-training\n",
      "------------------------------\n",
      "CHILD: h2 3 Method 3 Method\n",
      "p 'To collect large-scale pairs of chart-table data, we collect synthetic data from existing chart question-answering corpora, including PlotQA (Methani et al., 2020), DVQA (Kafle et al., 2018), and FigureQA (Kahou et al., 2018). Specifically, DVQA and FigureQA render chart images from synthetic tables that are randomly generated from limited vocabularies. PlotQA first scrapes tables from online resources like World Bank Open Data and then synthesizes the charts from the scraped data, where the tables and charts contain more diverse language information. Our pre-training corpus consists of 495K chart-table pairs, which cover a diverse range of chart types. Our pre-training corpus contains three chart types: bar, line, and pie. The distribution of different chart types from the three chart question-answering benchmarks is summarized in table 1.' <p>To collect large-scale pairs of chart-table data, we collect synthetic data from existing chart question-answering corpora, including PlotQA (Methani et al., 2020), DVQA (Kafle et al., 2018), and FigureQA (Kahou et al., 2018). Specifically, DVQA and FigureQA render chart images from synthetic tables that are randomly generated from limited vocabularies. PlotQA first scrapes tables from online resources like World Bank Open Data and then synthesizes the charts from the scraped data, where the tables and charts contain more diverse language information. Our pre-training corpus consists of 495K chart-table pairs, which cover a diverse range of chart types. Our pre-training corpus contains three chart types: bar, line, and pie. The distribution of different chart types from the three chart question-answering benchmarks is summarized in table 1.</p>\n",
      "p 'ChartT5 is an extension of the existing V+L Pre-training framework, VLT5 (Cho et al., 2021), an encoder-decoder architecture that unifies the vision-language tasks as text generation conditioned on multi-modal inputs. Given a chart image, we first extract the scene texts. For the synthetic chart images that are collected from DVQA (Kafle' <p>ChartT5 is an extension of the existing V+L Pre-training framework, VLT5 (Cho et al., 2021), an encoder-decoder architecture that unifies the vision-language tasks as text generation conditioned on multi-modal inputs. Given a chart image, we first extract the scene texts. For the synthetic chart images that are collected from DVQA (Kafle</p>\n",
      "p '\\\\begin{table}\\n\\\\begin{tabular}{l|c c c|c} \\\\hline \\\\hline Type & PlotQA & DVQA & FigureQA & Total \\\\ \\\\hline Bar & 142,587 & 204,514 & 40,000 & 387,101 \\\\ Line & 48,133 & 0 & 40,000 & 88,133 \\\\ Pie & 0 & 0 & 20,001 & 20,001 \\\\ \\\\hline \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 1: Distribution of the three chart types: bar, line, and pie from different resources in the pre-training corpus.' <p>\\begin{table}\n",
      "\\begin{tabular}{l|c c c|c} \\hline \\hline Type &amp; PlotQA &amp; DVQA &amp; FigureQA &amp; Total \\ \\hline Bar &amp; 142,587 &amp; 204,514 &amp; 40,000 &amp; 387,101 \\ Line &amp; 48,133 &amp; 0 &amp; 40,000 &amp; 88,133 \\ Pie &amp; 0 &amp; 0 &amp; 20,001 &amp; 20,001 \\ \\hline \\hline \\end{tabular}\n",
      "\\end{table}\n",
      "Table 1: Distribution of the three chart types: bar, line, and pie from different resources in the pre-training corpus.</p>\n",
      "p 'Figure 2: An Overview of ChartT5. Given the input chart image and the extracted OCR tokens, ChartT5 predicts the masked values of the table in the output.\\net al., 2018), FigureQA (Kahou et al., 2018), and PlotQA (Methani et al., 2020), the ground-truth scene texts are available. The visual context is then represented as combining visual features extracted from the chart image and the language features obtained on the detected scene text. We then flat the paired table of the chart image into a string and extract the text features via the language encoder. The multi-modal features are then concatenated and fused via the multi-layer encoder, and the output hidden vectors can then be used for various pre-training tasks.' <p>Figure 2: An Overview of ChartT5. Given the input chart image and the extracted OCR tokens, ChartT5 predicts the masked values of the table in the output.\n",
      "et al., 2018), FigureQA (Kahou et al., 2018), and PlotQA (Methani et al., 2020), the ground-truth scene texts are available. The visual context is then represented as combining visual features extracted from the chart image and the language features obtained on the detected scene text. We then flat the paired table of the chart image into a string and extract the text features via the language encoder. The multi-modal features are then concatenated and fused via the multi-layer encoder, and the output hidden vectors can then be used for various pre-training tasks.</p>\n",
      "p None <p>Given an input chart image, to recognize the critical marks (<em>e.g.</em>, bars and lines) of chart images, we first utilize a pre-trained Mask R-CNN object detector from (Masry et al., 2022) to extract the visual region features (\\mathbf{v}={v_{1},v_{2},\\cdots,v_{l^{v}}}). Next, the chart object detector is trained on the synthetic chart images from the previous CQA datasets (Kahou et al., 2018; Kafle et al., 2018; Masry et al., 2022; Methani et al., 2020) which is defined to identify 15 chart-related objects1. For each detected object region, we also extract location features as a 5-d vector: ([\\frac{x_{1}}{W},\\frac{y_{1}}{H},\\frac{x_{2}}{W},\\frac{y_{2}}{H},\\frac{(y_{2} -y_{1})(x_{2}-x_{1})}{W.H}]), which denotes the normalized top left coordinates, bottom right coordinates, and the normalized area of the detected region box. The position feature is then fed through fully-connected layers to be projected to the visual region feature embedding space. The final representation of the visual feature is obtained by summing up the projected region feature and corresponding location feature.</p>\n",
      "p 'Footnote 1: These 15 categories are: Legends, yAxisTitle, ChartTitle, xAxisTitle, LegendPreview, PlotArea, yAxisLabel, xAxisLabel, LegendLabel, PieLabel, bar, pie, pieSlice, line, and dotLine.' <p>Footnote 1: These 15 categories are: Legends, yAxisTitle, ChartTitle, xAxisTitle, LegendPreview, PlotArea, yAxisLabel, xAxisLabel, LegendLabel, PieLabel, bar, pie, pieSlice, line, and dotLine.</p>\n",
      "p \"After extracting the list of OCR words from the chart image, we obtain a set of OCR text embeddings (\\\\mathbf{o}={o_{1},o_{2},\\\\cdots,o_{l^{o}}}) via a learned word embedding layer. We also get each OCR token's 5-d position vector similar to the visual position vector from the OCR token's detected bounding box. We then obtain the position embedding vector using the shared projecting layer from the Chart Image Encoder. The shared position encoding mechanism between OCR tokens and chart object regions would help the model to capture their relative positional relations, which is a critical clue to predict the table data from the chart image. For example, the bar associated with an x-axis label should share a similar x-coordinate position in a vertical bar chart. The final OCR embedding vector is gained by summing up the OCR text token embeddings and the OCR position embedding.\" <p>After extracting the list of OCR words from the chart image, we obtain a set of OCR text embeddings (\\mathbf{o}={o_{1},o_{2},\\cdots,o_{l^{o}}}) via a learned word embedding layer. We also get each OCR token's 5-d position vector similar to the visual position vector from the OCR token's detected bounding box. We then obtain the position embedding vector using the shared projecting layer from the Chart Image Encoder. The shared position encoding mechanism between OCR tokens and chart object regions would help the model to capture their relative positional relations, which is a critical clue to predict the table data from the chart image. For example, the bar associated with an x-axis label should share a similar x-coordinate position in a vertical bar chart. The final OCR embedding vector is gained by summing up the OCR text token embeddings and the OCR position embedding.</p>\n",
      "p \"Following the setting of the original VLT5 (Cho et al., 2021), we add a prefix to the flattened underlying table to indicate different pre-training tasks. We then get the table token embeddings (\\\\mathbf{t}={t_{1},t_{2},\\\\cdots,t_{l^{t}}}) with a shared word embedding layer. We apply the original T5's (Raffel et al., 2020) relative position bias to obtain the position information of each token in the caption and the flattened table. We know that the tables have very different structures compared to natural language captions, and several efforts are exploring specialized position embeddings for tables (Yin et al., 2020; 1). We leave the exploration of the specialized table position embedding for chart table pre-training in the future.\" <p>Following the setting of the original VLT5 (Cho et al., 2021), we add a prefix to the flattened underlying table to indicate different pre-training tasks. We then get the table token embeddings (\\mathbf{t}={t_{1},t_{2},\\cdots,t_{l^{t}}}) with a shared word embedding layer. We apply the original T5's (Raffel et al., 2020) relative position bias to obtain the position information of each token in the caption and the flattened table. We know that the tables have very different structures compared to natural language captions, and several efforts are exploring specialized position embeddings for tables (Yin et al., 2020; 1). We leave the exploration of the specialized table position embedding for chart table pre-training in the future.</p>\n",
      "p None <p><strong>Scene Text Copy Mechanism.</strong> A critical ingredient to the success of chart-to-table translation is the ability to predict the table headers from the corresponding OCR texts. For example, in the horizontal bar chart, the table column header is usually obtained from the x-axis labels, and the row header is often copied from the legend labels. Although presenting OCR text and the table to the model helps link the shared OCR tokens and table values, generating the correct table prediction from the corresponding OCR source is still challenging due to the large candidate token vocabulary. To encourage direct copy from the OCR text to the associated table cell value, we introduce OCR sentinel tokens ({&lt;\\text{ocr_1}&gt;,&lt;\\text{ocr_2}&gt;,\\cdots,&lt;\\text{ocr_1}^{o}&gt;}), which corresponds to the detected OCR texts. As illustrated in Figure 2, we replace each OCR token with a unique corresponding OCR sentinel token. Then, for every OCR token, we find if there is a matched existing table cell value. If a matched pair is found, we replace the table cell value with its paired OCR sentinel token. During pre-training, as all the plot images are synthesized from a paired table, the one-to-one scene text to table value mapping is already provided. With this prepossessing procedure, we successfully distinguish the table values that are copied from OCR tokens and those that need to be generated from the general token vocabularies, encouraging more accurate table pre\n",
      "diction from the relevant resources.</p>\n",
      "p 'Given the chart-table pairs, we propose Masked Header Prediction (MHP) and Masked Value Prediction (MHP) to teach the model to recover incomplete tables with the chart information. Specifically, this objective aims to predict a masked table token (t_{m}) with the remaining table info (t_{\\\\backslash m}) as well as the chart image region (\\\\mathbf{v}) and the scene text (\\\\mathbf{o}). Compared to the traditional masked language modeling applied to the natural language text, we adjust the table masking strategy based on two hypotheses: (1) We alternatively mask just the table headers or numerical table values, as we think interpreting these two types of information requires different skills. Predicting table headers requires retrieving the correct scene text, while predicting numerical table values depends more on the capability to conduct mathematic reasoning over both the visual elements and the scene text. Therefore, it is better to format them as two separate pre-training objectives. (2) We increase the masking rate from 15(\\\\%) to 45(\\\\%), as the masked table token has less dependence on the surrounding table values.' <p>Given the chart-table pairs, we propose Masked Header Prediction (MHP) and Masked Value Prediction (MHP) to teach the model to recover incomplete tables with the chart information. Specifically, this objective aims to predict a masked table token (t_{m}) with the remaining table info (t_{\\backslash m}) as well as the chart image region (\\mathbf{v}) and the scene text (\\mathbf{o}). Compared to the traditional masked language modeling applied to the natural language text, we adjust the table masking strategy based on two hypotheses: (1) We alternatively mask just the table headers or numerical table values, as we think interpreting these two types of information requires different skills. Predicting table headers requires retrieving the correct scene text, while predicting numerical table values depends more on the capability to conduct mathematic reasoning over both the visual elements and the scene text. Therefore, it is better to format them as two separate pre-training objectives. (2) We increase the masking rate from 15(\\%) to 45(\\%), as the masked table token has less dependence on the surrounding table values.</p>\n",
      "p 'In this section, we first introduce the dataset for pre-training. We then go over our ChartT5 model architecture and pre-training objectives to predict masked tables from the chart and OCR information.' <p>In this section, we first introduce the dataset for pre-training. We then go over our ChartT5 model architecture and pre-training objectives to predict masked tables from the chart and OCR information.</p>\n",
      "['To collect large\\\\-scale pairs of chart\\\\-table data, we collect synthetic data from existing chart question\\\\-answering corpora, including PlotQA (Methani et al., 2020\\\\), DVQA (Kafle et al., 2018\\\\), and FigureQA (Kahou et al., 2018\\\\). Specifically, DVQA and FigureQA render chart images from synthetic tables that are randomly generated from limited vocabularies. PlotQA first scrapes tables from online resources like World Bank Open Data and then synthesizes the charts from the scraped data, where the tables and charts contain more diverse language information. Our pre\\\\-training corpus consists of 495K chart\\\\-table pairs, which cover a diverse range of chart types. Our pre\\\\-training corpus contains three chart types: bar, line, and pie. The distribution of different chart types from the three chart question\\\\-answering benchmarks is summarized in table 1\\\\.\\n\\n', 'ChartT5 is an extension of the existing V\\\\+L Pre\\\\-training framework, VLT5 (Cho et al., 2021\\\\), an encoder\\\\-decoder architecture that unifies the vision\\\\-language tasks as text generation conditioned on multi\\\\-modal inputs. Given a chart image, we first extract the scene texts. For the synthetic chart images that are collected from DVQA (Kafle\\n\\n', '\\\\\\\\begin{table}\\n\\\\\\\\begin{tabular}{l\\\\|c c c\\\\|c} \\\\\\\\hline \\\\\\\\hline Type \\\\& PlotQA \\\\& DVQA \\\\& FigureQA \\\\& Total \\\\\\\\ \\\\\\\\hline Bar \\\\& 142,587 \\\\& 204,514 \\\\& 40,000 \\\\& 387,101 \\\\\\\\ Line \\\\& 48,133 \\\\& 0 \\\\& 40,000 \\\\& 88,133 \\\\\\\\ Pie \\\\& 0 \\\\& 0 \\\\& 20,001 \\\\& 20,001 \\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\n\\\\\\\\end{table}\\nTable 1: Distribution of the three chart types: bar, line, and pie from different resources in the pre\\\\-training corpus.\\n\\n', 'Figure 2: An Overview of ChartT5\\\\. Given the input chart image and the extracted OCR tokens, ChartT5 predicts the masked values of the table in the output.\\net al., 2018\\\\), FigureQA (Kahou et al., 2018\\\\), and PlotQA (Methani et al., 2020\\\\), the ground\\\\-truth scene texts are available. The visual context is then represented as combining visual features extracted from the chart image and the language features obtained on the detected scene text. We then flat the paired table of the chart image into a string and extract the text features via the language encoder. The multi\\\\-modal features are then concatenated and fused via the multi\\\\-layer encoder, and the output hidden vectors can then be used for various pre\\\\-training tasks.\\n\\n', '####3.2.1 Chart Image Encoder', 'Given an input chart image, to recognize the critical marks (*e.g.*, bars and lines) of chart images, we first utilize a pre\\\\-trained Mask R\\\\-CNN object detector from (Masry et al., 2022\\\\) to extract the visual region features (\\\\\\\\mathbf{v}\\\\={v\\\\_{1},v\\\\_{2},\\\\\\\\cdots,v\\\\_{l^{v}}}). Next, the chart object detector is trained on the synthetic chart images from the previous CQA datasets (Kahou et al., 2018; Kafle et al., 2018; Masry et al., 2022; Methani et al., 2020\\\\) which is defined to identify 15 chart\\\\-related objects1\\\\. For each detected object region, we also extract location features as a 5\\\\-d vector: (\\\\[\\\\\\\\frac{x\\\\_{1}}{W},\\\\\\\\frac{y\\\\_{1}}{H},\\\\\\\\frac{x\\\\_{2}}{W},\\\\\\\\frac{y\\\\_{2}}{H},\\\\\\\\frac{(y\\\\_{2} \\\\-y\\\\_{1})(x\\\\_{2}\\\\-x\\\\_{1})}{W.H}]), which denotes the normalized top left coordinates, bottom right coordinates, and the normalized area of the detected region box. The position feature is then fed through fully\\\\-connected layers to be projected to the visual region feature embedding space. The final representation of the visual feature is obtained by summing up the projected region feature and corresponding location feature.\\n\\n', 'Footnote 1: These 15 categories are: Legends, yAxisTitle, ChartTitle, xAxisTitle, LegendPreview, PlotArea, yAxisLabel, xAxisLabel, LegendLabel, PieLabel, bar, pie, pieSlice, line, and dotLine.\\n\\n', '####3.2.2 OCR Encoder', \"After extracting the list of OCR words from the chart image, we obtain a set of OCR text embeddings (\\\\\\\\mathbf{o}\\\\={o\\\\_{1},o\\\\_{2},\\\\\\\\cdots,o\\\\_{l^{o}}}) via a learned word embedding layer. We also get each OCR token's 5\\\\-d position vector similar to the visual position vector from the OCR token's detected bounding box. We then obtain the position embedding vector using the shared projecting layer from the Chart Image Encoder. The shared position encoding mechanism between OCR tokens and chart object regions would help the model to capture their relative positional relations, which is a critical clue to predict the table data from the chart image. For example, the bar associated with an x\\\\-axis label should share a similar x\\\\-coordinate position in a vertical bar chart. The final OCR embedding vector is gained by summing up the OCR text token embeddings and the OCR position embedding.\\n\\n\", '####3.2.3 Language Encoder', \"Following the setting of the original VLT5 (Cho et al., 2021\\\\), we add a prefix to the flattened underlying table to indicate different pre\\\\-training tasks. We then get the table token embeddings (\\\\\\\\mathbf{t}\\\\={t\\\\_{1},t\\\\_{2},\\\\\\\\cdots,t\\\\_{l^{t}}}) with a shared word embedding layer. We apply the original T5's (Raffel et al., 2020\\\\) relative position bias to obtain the position information of each token in the caption and the flattened table. We know that the tables have very different structures compared to natural language captions, and several efforts are exploring specialized position embeddings for tables (Yin et al., 2020; 1\\\\). We leave the exploration of the specialized table position embedding for chart table pre\\\\-training in the future.\\n\\n\", '**Scene Text Copy Mechanism.** A critical ingredient to the success of chart\\\\-to\\\\-table translation is the ability to predict the table headers from the corresponding OCR texts. For example, in the horizontal bar chart, the table column header is usually obtained from the x\\\\-axis labels, and the row header is often copied from the legend labels. Although presenting OCR text and the table to the model helps link the shared OCR tokens and table values, generating the correct table prediction from the corresponding OCR source is still challenging due to the large candidate token vocabulary. To encourage direct copy from the OCR text to the associated table cell value, we introduce OCR sentinel tokens ({\\\\<\\\\\\\\text{ocr\\\\_1}\\\\>,\\\\<\\\\\\\\text{ocr\\\\_2}\\\\>,\\\\\\\\cdots,\\\\<\\\\\\\\text{ocr\\\\_1}^{o}\\\\>}), which corresponds to the detected OCR texts. As illustrated in Figure 2, we replace each OCR token with a unique corresponding OCR sentinel token. Then, for every OCR token, we find if there is a matched existing table cell value. If a matched pair is found, we replace the table cell value with its paired OCR sentinel token. During pre\\\\-training, as all the plot images are synthesized from a paired table, the one\\\\-to\\\\-one scene text to table value mapping is already provided. With this prepossessing procedure, we successfully distinguish the table values that are copied from OCR tokens and those that need to be generated from the general token vocabularies, encouraging more accurate table pre\\ndiction from the relevant resources.\\n\\n', 'Given the chart\\\\-table pairs, we propose Masked Header Prediction (MHP) and Masked Value Prediction (MHP) to teach the model to recover incomplete tables with the chart information. Specifically, this objective aims to predict a masked table token (t\\\\_{m}) with the remaining table info (t\\\\_{\\\\\\\\backslash m}) as well as the chart image region (\\\\\\\\mathbf{v}) and the scene text (\\\\\\\\mathbf{o}). Compared to the traditional masked language modeling applied to the natural language text, we adjust the table masking strategy based on two hypotheses: (1\\\\) We alternatively mask just the table headers or numerical table values, as we think interpreting these two types of information requires different skills. Predicting table headers requires retrieving the correct scene text, while predicting numerical table values depends more on the capability to conduct mathematic reasoning over both the visual elements and the scene text. Therefore, it is better to format them as two separate pre\\\\-training objectives. (2\\\\) We increase the masking rate from 15(\\\\\\\\%) to 45(\\\\\\\\%), as the masked table token has less dependence on the surrounding table values.\\n\\n', 'In this section, we first introduce the dataset for pre\\\\-training. We then go over our ChartT5 model architecture and pre\\\\-training objectives to predict masked tables from the chart and OCR information.\\n\\n', '###Pre-training Dataset Collection', '###Model Overview', '###Pre-training Objectives']\n",
      "To collect large\\-scale pairs of chart\\-table data, we collect synthetic data from existing chart question\\-answering corpora, including PlotQA (Methani et al., 2020\\), DVQA (Kafle et al., 2018\\), and FigureQA (Kahou et al., 2018\\). Specifically, DVQA and FigureQA render chart images from synthetic tables that are randomly generated from limited vocabularies. PlotQA first scrapes tables from online resources like World Bank Open Data and then synthesizes the charts from the scraped data, where the tables and charts contain more diverse language information. Our pre\\-training corpus consists of 495K chart\\-table pairs, which cover a diverse range of chart types. Our pre\\-training corpus contains three chart types: bar, line, and pie. The distribution of different chart types from the three chart question\\-answering benchmarks is summarized in table 1\\.\n",
      "\n",
      "\n",
      "\n",
      "ChartT5 is an extension of the existing V\\+L Pre\\-training framework, VLT5 (Cho et al., 2021\\), an encoder\\-decoder architecture that unifies the vision\\-language tasks as text generation conditioned on multi\\-modal inputs. Given a chart image, we first extract the scene texts. For the synthetic chart images that are collected from DVQA (Kafle\n",
      "\n",
      "\n",
      "\n",
      "\\\\begin{table}\n",
      "\\\\begin{tabular}{l\\|c c c\\|c} \\\\hline \\\\hline Type \\& PlotQA \\& DVQA \\& FigureQA \\& Total \\\\ \\\\hline Bar \\& 142,587 \\& 204,514 \\& 40,000 \\& 387,101 \\\\ Line \\& 48,133 \\& 0 \\& 40,000 \\& 88,133 \\\\ Pie \\& 0 \\& 0 \\& 20,001 \\& 20,001 \\\\ \\\\hline \\\\hline \\\\end{tabular}\n",
      "\\\\end{table}\n",
      "Table 1: Distribution of the three chart types: bar, line, and pie from different resources in the pre\\-training corpus.\n",
      "\n",
      "\n",
      "\n",
      "Figure 2: An Overview of ChartT5\\. Given the input chart image and the extracted OCR tokens, ChartT5 predicts the masked values of the table in the output.\n",
      "et al., 2018\\), FigureQA (Kahou et al., 2018\\), and PlotQA (Methani et al., 2020\\), the ground\\-truth scene texts are available. The visual context is then represented as combining visual features extracted from the chart image and the language features obtained on the detected scene text. We then flat the paired table of the chart image into a string and extract the text features via the language encoder. The multi\\-modal features are then concatenated and fused via the multi\\-layer encoder, and the output hidden vectors can then be used for various pre\\-training tasks.\n",
      "\n",
      "\n",
      "\n",
      "####3.2.1 Chart Image Encoder\n",
      "\n",
      "Given an input chart image, to recognize the critical marks (*e.g.*, bars and lines) of chart images, we first utilize a pre\\-trained Mask R\\-CNN object detector from (Masry et al., 2022\\) to extract the visual region features (\\\\mathbf{v}\\={v\\_{1},v\\_{2},\\\\cdots,v\\_{l^{v}}}). Next, the chart object detector is trained on the synthetic chart images from the previous CQA datasets (Kahou et al., 2018; Kafle et al., 2018; Masry et al., 2022; Methani et al., 2020\\) which is defined to identify 15 chart\\-related objects1\\. For each detected object region, we also extract location features as a 5\\-d vector: (\\[\\\\frac{x\\_{1}}{W},\\\\frac{y\\_{1}}{H},\\\\frac{x\\_{2}}{W},\\\\frac{y\\_{2}}{H},\\\\frac{(y\\_{2} \\-y\\_{1})(x\\_{2}\\-x\\_{1})}{W.H}]), which denotes the normalized top left coordinates, bottom right coordinates, and the normalized area of the detected region box. The position feature is then fed through fully\\-connected layers to be projected to the visual region feature embedding space. The final representation of the visual feature is obtained by summing up the projected region feature and corresponding location feature.\n",
      "\n",
      "\n",
      "\n",
      "Footnote 1: These 15 categories are: Legends, yAxisTitle, ChartTitle, xAxisTitle, LegendPreview, PlotArea, yAxisLabel, xAxisLabel, LegendLabel, PieLabel, bar, pie, pieSlice, line, and dotLine.\n",
      "\n",
      "\n",
      "\n",
      "####3.2.2 OCR Encoder\n",
      "\n",
      "After extracting the list of OCR words from the chart image, we obtain a set of OCR text embeddings (\\\\mathbf{o}\\={o\\_{1},o\\_{2},\\\\cdots,o\\_{l^{o}}}) via a learned word embedding layer. We also get each OCR token's 5\\-d position vector similar to the visual position vector from the OCR token's detected bounding box. We then obtain the position embedding vector using the shared projecting layer from the Chart Image Encoder. The shared position encoding mechanism between OCR tokens and chart object regions would help the model to capture their relative positional relations, which is a critical clue to predict the table data from the chart image. For example, the bar associated with an x\\-axis label should share a similar x\\-coordinate position in a vertical bar chart. The final OCR embedding vector is gained by summing up the OCR text token embeddings and the OCR position embedding.\n",
      "\n",
      "\n",
      "\n",
      "####3.2.3 Language Encoder\n",
      "\n",
      "Following the setting of the original VLT5 (Cho et al., 2021\\), we add a prefix to the flattened underlying table to indicate different pre\\-training tasks. We then get the table token embeddings (\\\\mathbf{t}\\={t\\_{1},t\\_{2},\\\\cdots,t\\_{l^{t}}}) with a shared word embedding layer. We apply the original T5's (Raffel et al., 2020\\) relative position bias to obtain the position information of each token in the caption and the flattened table. We know that the tables have very different structures compared to natural language captions, and several efforts are exploring specialized position embeddings for tables (Yin et al., 2020; 1\\). We leave the exploration of the specialized table position embedding for chart table pre\\-training in the future.\n",
      "\n",
      "\n",
      "\n",
      "**Scene Text Copy Mechanism.** A critical ingredient to the success of chart\\-to\\-table translation is the ability to predict the table headers from the corresponding OCR texts. For example, in the horizontal bar chart, the table column header is usually obtained from the x\\-axis labels, and the row header is often copied from the legend labels. Although presenting OCR text and the table to the model helps link the shared OCR tokens and table values, generating the correct table prediction from the corresponding OCR source is still challenging due to the large candidate token vocabulary. To encourage direct copy from the OCR text to the associated table cell value, we introduce OCR sentinel tokens ({\\<\\\\text{ocr\\_1}\\>,\\<\\\\text{ocr\\_2}\\>,\\\\cdots,\\<\\\\text{ocr\\_1}^{o}\\>}), which corresponds to the detected OCR texts. As illustrated in Figure 2, we replace each OCR token with a unique corresponding OCR sentinel token. Then, for every OCR token, we find if there is a matched existing table cell value. If a matched pair is found, we replace the table cell value with its paired OCR sentinel token. During pre\\-training, as all the plot images are synthesized from a paired table, the one\\-to\\-one scene text to table value mapping is already provided. With this prepossessing procedure, we successfully distinguish the table values that are copied from OCR tokens and those that need to be generated from the general token vocabularies, encouraging more accurate table pre\n",
      "diction from the relevant resources.\n",
      "\n",
      "\n",
      "\n",
      "Given the chart\\-table pairs, we propose Masked Header Prediction (MHP) and Masked Value Prediction (MHP) to teach the model to recover incomplete tables with the chart information. Specifically, this objective aims to predict a masked table token (t\\_{m}) with the remaining table info (t\\_{\\\\backslash m}) as well as the chart image region (\\\\mathbf{v}) and the scene text (\\\\mathbf{o}). Compared to the traditional masked language modeling applied to the natural language text, we adjust the table masking strategy based on two hypotheses: (1\\) We alternatively mask just the table headers or numerical table values, as we think interpreting these two types of information requires different skills. Predicting table headers requires retrieving the correct scene text, while predicting numerical table values depends more on the capability to conduct mathematic reasoning over both the visual elements and the scene text. Therefore, it is better to format them as two separate pre\\-training objectives. (2\\) We increase the masking rate from 15(\\\\%) to 45(\\\\%), as the masked table token has less dependence on the surrounding table values.\n",
      "\n",
      "\n",
      "\n",
      "In this section, we first introduce the dataset for pre\\-training. We then go over our ChartT5 model architecture and pre\\-training objectives to predict masked tables from the chart and OCR information.\n",
      "\n",
      "\n",
      "\n",
      "###Pre-training Dataset Collection\n",
      "\n",
      "###Model Overview\n",
      "\n",
      "###Pre-training Objectives\n",
      "------------------------------\n",
      "CHILD: h2 4 Experiment 4 Experiment\n",
      "p 'We first compare ChartT5 to various state-of-the-art methods with or without pre-training on the two downstream tasks.' <p>We first compare ChartT5 to various state-of-the-art methods with or without pre-training on the two downstream tasks.</p>\n",
      "p None <p>We compare ChartT5 with SOTA non-pretraining and pre-training methods on CQA tasks. The best-performed non-pretraining baselines are introduced in (Masry et al., 2022). The authors first predict the table data from the chart image via an automatic data extraction tool (Luo et al., 2021). Then they extend various language-only models (T5, Tapas) and multi-modal models (VLT5, VisionTapas) to predict the answer conditioned on the extracted table. On the line of pre-training baselines, we compare to VLT5({}<em>{pre}) and VisionTapas({}</em>{pre}) which pre-trains VLT5 and Vision Tapas on PlotQA with the visual question answering tasks. We also compare chartT5 to the current SOTA method Pix2Struct which is pre-trained on 80 million webpage screenshots to HTML code parsing objectives. The result is summarized in Table 2.</p>\n",
      "p None <p><strong>Comparison to Non-Pretraining Method</strong> Even without access to the predicted tables, ChartT5 has outperformed all non-pretraining methods by a large margin (a minimum 7.3(\\%) gain on the overall performance). ChartT5 also outperforms all non-pretraining baselines on the human-written questions and machine-generated questions. Although the predicted table covers 54(\\%) of the answers in the test data of ChartQA, simply feeding it as an input does not make the existing models fully leverage the valuable information. The significant improvement achieved by ChartT5 indicates the effectiveness of the proposed pre-training to help the model to obtain the relevant table information for chart understanding.</p>\n",
      "p None <p><strong>Comparison to Pre-training Method</strong> Although the performance of VLT5 and VisionTapas is improved significantly by pre-training on additional CQA data, ChartT5 still outperform them by at least 1.3(\\%). Specifically, on machine-augmented questions, ChartT5 outperforms VLT5({}<em>{pre}) by 8(\\%). However, both visionTapas({}</em>{pre}) and VLT5({}_{pre}) achieve better accuracy on the human split, which means that the in-domain question answering objectives helps the model to improve the numerical reasoning capability. ChartT5 underperforms Pix2Struct by 2.3(\\%) on the overall test split. However, pix2struct is pre-trained on a more than 100 times larger pre-training corpus than the rest of the pre-training methods. Given the same scale of the pre-training dataset, we expect to gain additional performance improvement, and we leave this for future exploration.</p>\n",
      "p 'For the chart summarization task, we compare ChartT5 to the best non-pretraining approaches introduced in (Kantharaj et al., 2022). Given a chart image, The authors build the chart summarization models by extending the pre-trained language generation model T5 (Raffel et al., 2020) and BART(Lewis et al., 2019) whose generation processes are conditioned on: (1) a set of scene texts extracted by a trained OCR detector. (2) the ground truth table that is paired with the chart. The evaluation result is summarized in Table 3.' <p>For the chart summarization task, we compare ChartT5 to the best non-pretraining approaches introduced in (Kantharaj et al., 2022). Given a chart image, The authors build the chart summarization models by extending the pre-trained language generation model T5 (Raffel et al., 2020) and BART(Lewis et al., 2019) whose generation processes are conditioned on: (1) a set of scene texts extracted by a trained OCR detector. (2) the ground truth table that is paired with the chart. The evaluation result is summarized in Table 3.</p>\n",
      "p 'From Table 3, we can see that on Statista, ChartT5 outperforms all baseline methods on BLUE score, but only a slight improvement is achieved over the best baseline. On Pew, ChartT5 underperforms T5-OCR by almost 1.5 percent. The proposed ChartT5 also slightly underperforms against the baseline methods in CIDER on both datasets. However, ChartT5 consistently outperforms all baselines on content selection scores' <p>From Table 3, we can see that on Statista, ChartT5 outperforms all baseline methods on BLUE score, but only a slight improvement is achieved over the best baseline. On Pew, ChartT5 underperforms T5-OCR by almost 1.5 percent. The proposed ChartT5 also slightly underperforms against the baseline methods in CIDER on both datasets. However, ChartT5 consistently outperforms all baselines on content selection scores</p>\n",
      "p None <p>\\begin{table}\n",
      "\\begin{tabular}{l|c c c} \\hline \\hline \\multirow{2}{<em>}{Model} &amp; \\multicolumn{3}{c}{ChartQA} \\  &amp; Human &amp; Augment &amp; Overall \\ \\hline T5 &amp; 25.12 &amp; 56.96 &amp; 41.56 \\ Tapas &amp; 28.72 &amp; 53.84 &amp; 41.28 \\ VLT5 &amp; 26.24 &amp; 56.88 &amp; 41.56 \\ VisionTapas &amp; 29.60 &amp; 61.44 &amp; 45.52 \\ \\hline VLT5({}_{pre}) &amp; </em><em>40.08</em><em> &amp; 63.60 &amp; 51.84 \\ VisionTapas({}_{pre}) &amp; 32.56 &amp; 61.60 &amp; 47.08 \\ Pix2Struct &amp; - &amp; - &amp; </em><em>56.00</em><em> \\ ChartT5 &amp; 31.8 &amp; </em><em>74.4</em>* &amp; 53.16 \\ \\hline \\hline \\end{tabular}\n",
      "\\end{table}\n",
      "Table 2: Evaluation results on ChartQA. We report relaxed accuracy on the test split annotated by humans and that generated by the machine. In the last column, we report the overall accuracy by computing the mean values with human split and augment split.\n",
      "across both Statista and Pew sets. The under-performance on BLEU and CIDER indicates that Chart-table pre-training is limited to benefit high-quality natural language generation. However, the strong performance on content selection, which values the key information appearance in the generation, suggests the advantage of chart-table pre-training on extracting relevant chart information. Therefore, a potential direction to explore is combining different types of pre-training objectives, such as chart-to-text pre-training and chart-table pre-training goals, to facilitate the model with diverse strengths.</p>\n",
      "p 'We conduct ablation experiments to validate the effectiveness of chart-table pre-training and the pre-training objectives. We also evaluate the effectiveness of the proposed scene text copy mechanism.' <p>We conduct ablation experiments to validate the effectiveness of chart-table pre-training and the pre-training objectives. We also evaluate the effectiveness of the proposed scene text copy mechanism.</p>\n",
      "p \"We conduct detailed analyses on the effectiveness of chart-table pre-training. First, we measure the performance gain from the chart-table pre-training on the full test set of ChartQA data. We then study what type of questions benefit most from the chart-table pre-training by picking three subsets of questions that measure different capabilities of the model: (1) Human-written questions, (2) Machine-generated questions, and (3) Table covered questions, where the answers can be directly found in the ground truth tables. The results are summarized in Table 4. From Table 4, we find that after chart-table pre-training the model's performance on these three sets of questions is all improved. The most significant gain is obtained on machine-generated questions, which mainly focus on extractive-type questions. This indicates that chart-table pre-training benefits the model to localize and retrieve the requested information presented on Chart Image. The second biggest gain is achieved on table-cover questions, where the model demonstrates significant improvement in the capability of chart-to-table interpretation.\" <p>We conduct detailed analyses on the effectiveness of chart-table pre-training. First, we measure the performance gain from the chart-table pre-training on the full test set of ChartQA data. We then study what type of questions benefit most from the chart-table pre-training by picking three subsets of questions that measure different capabilities of the model: (1) Human-written questions, (2) Machine-generated questions, and (3) Table covered questions, where the answers can be directly found in the ground truth tables. The results are summarized in Table 4. From Table 4, we find that after chart-table pre-training the model's performance on these three sets of questions is all improved. The most significant gain is obtained on machine-generated questions, which mainly focus on extractive-type questions. This indicates that chart-table pre-training benefits the model to localize and retrieve the requested information presented on Chart Image. The second biggest gain is achieved on table-cover questions, where the model demonstrates significant improvement in the capability of chart-to-table interpretation.</p>\n",
      "p 'We validate the effectiveness of the two pre-training objectives, Masked Header Prediction and Masked Value Prediction. We remove one pre-training objective at a time and pre-train the ChartTSwith only one table prediction task. The pre-trained model' <p>We validate the effectiveness of the two pre-training objectives, Masked Header Prediction and Masked Value Prediction. We remove one pre-training objective at a time and pre-train the ChartTSwith only one table prediction task. The pre-trained model</p>\n",
      "p None <p>\\begin{table}\n",
      "\\begin{tabular}{l|c c c} \\hline \\hline \\multirow{2}{<em>}{Pretraining?} &amp; \\multicolumn{3}{c}{Question Types} \\  &amp; Table &amp; Human &amp; Augment \\ \\hline No &amp; 60.7 &amp; 30.8 &amp; 66.7 \\ Yes &amp; </em><em>64.7</em><em> &amp; </em><em>31.8</em><em> &amp; </em><em>74.4</em>* \\ \\hline \\hline \\end{tabular}\n",
      "\\end{table}\n",
      "Table 4: Ablation Study on Chart Table Pre-training with ChartQA Dataset. We report results on three subsets of questions: Table cover questions, human-written questions, and machine-generated questions.</p>\n",
      "p None <p>\\begin{table}\n",
      "\\begin{tabular}{l|c c c|c c c} \\hline \\hline \\multirow{2}{<em>}{Model} &amp; \\multicolumn{3}{c|}{Statista} &amp; \\multicolumn{3}{c}{Pew} \\  &amp; BLEU &amp; CS &amp; CIDER &amp; BLEU &amp; CS &amp; CIDER \\ \\hline T5-OCR &amp; 35.29 &amp; 73.77 &amp; 4.43 &amp; </em><em>10.49</em><em> &amp; 40.87 &amp; </em><em>2.20</em><em> \\ BART-OCR &amp; - &amp; - &amp; - &amp; 9.09 &amp; 39.99 &amp; 1.97 \\ T5-TAB &amp; 37.01 &amp; 75.72 &amp; </em><em>4.68</em><em> &amp; - &amp; - &amp; - \\ BART-TAB &amp; 36.36 &amp; 77.14 &amp; 4.40 &amp; - &amp; - &amp; - \\ ChartT5 &amp; </em><em>37.51</em><em> &amp; </em><em>82.16</em><em> &amp; 3.45 &amp; 9.05 &amp; </em><em>55.1</em>* &amp; 1.23 \\ \\hline \\hline \\end{tabular}\n",
      "\\end{table}\n",
      "Table 3: Evaluation results on Chart Summarization. We display BLEU, CS and CIDER scores for the Pew and Statista Split. The ground truth table is not available to Pew thus the table-based method does not have results on Pew split.</p>\n",
      "p \"\\\\begin{table}\\n\\\\begin{tabular}{l|c c c} \\\\hline \\\\hline  & \\\\multicolumn{3}{c}{Question Types} \\\\  & Human & Augment & Overall \\\\ \\\\hline Full & 31.8 & 74.4 & 53.1 \\\\ - MVP & 30.9 & 73.7 & 52.3 \\\\ - MHP & 31.2 & 68.3 & 49.7 \\\\ - STC & 30.8 & 72.4 & 51.6 \\\\ \\\\hline \\\\hline \\\\end{tabular}\\n\\\\end{table}\\nTable 5: Ablation Study on the two proposed pre-training objectives and the Scene Text Copy Mechanism (STC). The first row is the result of the full ChartT5 model. Then we remove one of the pre-training objectives and the scene-text-copy mechanism. We report the results of different ablation experiments on both human and machine-generated splits as well as the overall performance.\\nis then fine-tuned and evaluated on the human and augmented split for comparison. The result is displayed in table 5. As can be seen from the table, removing Masked Value Prediction Loss has a negligible impact on the performance of ChartT5 on ChartQA dataset. There is a slightly more drop in human written questions which suggests that predicting table numerical values still has a miner positive impact on helping the model's mathematical reasoning. Remove Masked Header Prediction have a significant impact on the machine-generated question-answering accuracy. As expected, Masked header modeling mainly helps the model learn how to link the scene text to the table headers, which is a critical ability to extract relevant information given a specific query.\" <p>\\begin{table}\n",
      "\\begin{tabular}{l|c c c} \\hline \\hline  &amp; \\multicolumn{3}{c}{Question Types} \\  &amp; Human &amp; Augment &amp; Overall \\ \\hline Full &amp; 31.8 &amp; 74.4 &amp; 53.1 \\ - MVP &amp; 30.9 &amp; 73.7 &amp; 52.3 \\ - MHP &amp; 31.2 &amp; 68.3 &amp; 49.7 \\ - STC &amp; 30.8 &amp; 72.4 &amp; 51.6 \\ \\hline \\hline \\end{tabular}\n",
      "\\end{table}\n",
      "Table 5: Ablation Study on the two proposed pre-training objectives and the Scene Text Copy Mechanism (STC). The first row is the result of the full ChartT5 model. Then we remove one of the pre-training objectives and the scene-text-copy mechanism. We report the results of different ablation experiments on both human and machine-generated splits as well as the overall performance.\n",
      "is then fine-tuned and evaluated on the human and augmented split for comparison. The result is displayed in table 5. As can be seen from the table, removing Masked Value Prediction Loss has a negligible impact on the performance of ChartT5 on ChartQA dataset. There is a slightly more drop in human written questions which suggests that predicting table numerical values still has a miner positive impact on helping the model's mathematical reasoning. Remove Masked Header Prediction have a significant impact on the machine-generated question-answering accuracy. As expected, Masked header modeling mainly helps the model learn how to link the scene text to the table headers, which is a critical ability to extract relevant information given a specific query.</p>\n",
      "p 'We also validate the effectiveness of the scene-text-copy mechanism, where we train a ChartT5model by simply representing OCR tokens in their original text format. The model is fine-tuned and evaluated on the human and augmented split of the chartQA dataset to compare against the full ChartT5. The result is displayed in Table 5. Disabling the scene-text-copy mechanism leads to a 1.5(\\\\%) overall performance drop on ChartQA tasks. Specifically, it leads to more degradation on the augmented split than the human split, as scene-text-copy helps enhance the alignment between OCR and table values to benefit accurate information extraction from the chart.' <p>We also validate the effectiveness of the scene-text-copy mechanism, where we train a ChartT5model by simply representing OCR tokens in their original text format. The model is fine-tuned and evaluated on the human and augmented split of the chartQA dataset to compare against the full ChartT5. The result is displayed in Table 5. Disabling the scene-text-copy mechanism leads to a 1.5(\\%) overall performance drop on ChartQA tasks. Specifically, it leads to more degradation on the augmented split than the human split, as scene-text-copy helps enhance the alignment between OCR and table values to benefit accurate information extraction from the chart.</p>\n",
      "p 'We have manually analyzed model predictions to understand its limitation. We found that our model suffers most from noisy OCR detection and complex question that requires multi-hop reasoning.' <p>We have manually analyzed model predictions to understand its limitation. We found that our model suffers most from noisy OCR detection and complex question that requires multi-hop reasoning.</p>\n",
      "p None <p><strong>Noisy OCR Prediction.</strong> As an OCR-based model, ChartT5 often suffers from a wrong OCR detection. An example is shown in Figure 3; the model localizes the right scene text \"1.18\" to answer the question, but the OCR text is mistakenly detected as \"1:18\". To further understand the limitation of OCR detection, we randomly sample 20K PlotQA test split and compare the performance of our model using detected OCRs against Ground Truth OCRs. We observe a 5(\\%) performance drop when using detected OCRs. We can improve the OCR detector for future work by training on a large Plot scene-text detection benchmark. Another promising direction is to attempt OCR-free end-to-end plot recognition method like Pix2Struct (Lee et al., 2022).</p>\n",
      "p None <p><strong>Multi-Hop Reasoning.</strong> Our model is also quite vulnerable to handling complex questions requiring multi-hop reasoning. An example is shown in Figure 4; the model cannot perform the complex logic reasoning to add the stats of the two smallest bars and compare that to the large bar. We will consider exploring pre-training on the mathematic reasoning datasets to address this limitation.</p>\n",
      "p 'In this section, We detailed our experiment setups to evaluate the proposed ChartT5 on two tasks: chart question answering and chart summarization. We then introduce the main results of the two evaluation tasks. Finally, we present the ablation study on chart-table pre-training and the two pre-training objectives.' <p>In this section, We detailed our experiment setups to evaluate the proposed ChartT5 on two tasks: chart question answering and chart summarization. We then introduce the main results of the two evaluation tasks. Finally, we present the ablation study on chart-table pre-training and the two pre-training objectives.</p>\n",
      "p None <p><strong>Chart Question Answering.</strong> Given a chart image and a query question, the goal for the model is to provide an accurate answer string by interpreting the provided chart image. For this task, we consider the ChartQA dataset Masry et al. (2022), which collects question-answer pairs on realistic chart images scraped from the internet. Their annotations are collected in two fashions: (1) Human-written question-answer pairs; and (2) machine-generated question-answer pairs derived from the human-written chart summaries. In total 32.7K question-answer pairs are collected on 21.9K scraped chart images, where about 9.6K question-and-answer pairs are human-written. Compared to the previously collected CQA datasets, ChartQA is more challenging to handle due to the diverse visual style from the realistic chart images and the complex language from human annotations. Following previous work Masry et al. (2022); Methani et al. (2020), we also apply the relaxed accuracy to measure the performance on the CQA task, which allows a minor inaccuracy on numerical value prediction (within 5(\\%) of the gold answer). For non-numerical answers, the prediction needs to be exactly matched to the gold-standard answer.</p>\n",
      "p None <p><strong>Chart Summarization.</strong> Given a chart image, the target is to summarize the key insights of the chart in natural language. For this task, we evaluate our model on the most recently proposed Chart-to-Text benchmark Kantharaj et al. (2022), which collects roughly 36.5K chart images with one summary for each image. They split the collected charts into two sets: Statista and Pew, representing the two separate websites from which the chart plots come. The summaries in Statista are human-written which is well grounded on the chart image. Meanwhile, the summaries from Pew are automatically extracted from the news paragraphs surrounding the chart images. Pew is noisier and more challenging to handle. We follow Kantharaj et al. (2022) to split the two sets for training and testing. We adopt BLEU-4, Content Selection, and CIDER as the evaluation metrics to measure the quality of the generated summary following Kantharaj et al. (2022).</p>\n",
      "p None <p><strong>Implementation details.</strong> We initialized our ChartT5 from T5({}_{\\text{base}}) and pre-trained on our self-collected corpus for 30 epochs with a batch size of 60. We used Adam optimizer Kingma and Ba (2015) with a linear warm-up for the first 5(\\%) training steps, and the peak learning rate is set as 1e-4. After warming up, a linear decayed learning-rate scheduler gradually drops the learning rate for the rest of the training steps. The pre-training experiments are conducted on 2 Nvidia TITAN RTX GPUs, and it roughly takes two days to accomplish the experiment. We kept the last checkpoint of each pre-training run as our final checkpoint for fine-tuning.</p>\n",
      "p 'We also applied warming-up for downstream fine-tuning to gradually increase the learning rate to the pick value during the first 5(\\\\%) of training epochs. After that, a linear decayed learning-rate scheduler gradually drops the learning rate for the remaining training. For CQA task, we set batch size as 24 and fine-tune ChartT5 for 60 epochs with a peak learning rate 2e-4 on 2 Nvidia TITAN RTX GPUs. The best checkpoint was saved as the one that achieves the highest accuracy on the validation\\nsplit. On the CS task, we use batch size 20 and a peak learning rate 5e-5. On the Pew split, we fine-tune ChartT5for 20 epochs, and on Statista, we fine-tune ChartT5for 25 epochs. The best checkpoint is also saved as achieving the best BLEU score on the validation split. All the reported numbers are one-time runs.' <p>We also applied warming-up for downstream fine-tuning to gradually increase the learning rate to the pick value during the first 5(\\%) of training epochs. After that, a linear decayed learning-rate scheduler gradually drops the learning rate for the remaining training. For CQA task, we set batch size as 24 and fine-tune ChartT5 for 60 epochs with a peak learning rate 2e-4 on 2 Nvidia TITAN RTX GPUs. The best checkpoint was saved as the one that achieves the highest accuracy on the validation\n",
      "split. On the CS task, we use batch size 20 and a peak learning rate 5e-5. On the Pew split, we fine-tune ChartT5for 20 epochs, and on Statista, we fine-tune ChartT5for 25 epochs. The best checkpoint is also saved as achieving the best BLEU score on the validation split. All the reported numbers are one-time runs.</p>\n",
      "['We first compare ChartT5 to various state\\\\-of\\\\-the\\\\-art methods with or without pre\\\\-training on the two downstream tasks.\\n\\n', '####4.1.1 Evaluation on CQA', 'We compare ChartT5 with SOTA non\\\\-pretraining and pre\\\\-training methods on CQA tasks. The best\\\\-performed non\\\\-pretraining baselines are introduced in (Masry et al., 2022\\\\). The authors first predict the table data from the chart image via an automatic data extraction tool (Luo et al., 2021\\\\). Then they extend various language\\\\-only models (T5, Tapas) and multi\\\\-modal models (VLT5, VisionTapas) to predict the answer conditioned on the extracted table. On the line of pre\\\\-training baselines, we compare to VLT5({}*{pre}) and VisionTapas({}*{pre}) which pre\\\\-trains VLT5 and Vision Tapas on PlotQA with the visual question answering tasks. We also compare chartT5 to the current SOTA method Pix2Struct which is pre\\\\-trained on 80 million webpage screenshots to HTML code parsing objectives. The result is summarized in Table 2\\\\.\\n\\n', '**Comparison to Non\\\\-Pretraining Method** Even without access to the predicted tables, ChartT5 has outperformed all non\\\\-pretraining methods by a large margin (a minimum 7\\\\.3(\\\\\\\\%) gain on the overall performance). ChartT5 also outperforms all non\\\\-pretraining baselines on the human\\\\-written questions and machine\\\\-generated questions. Although the predicted table covers 54(\\\\\\\\%) of the answers in the test data of ChartQA, simply feeding it as an input does not make the existing models fully leverage the valuable information. The significant improvement achieved by ChartT5 indicates the effectiveness of the proposed pre\\\\-training to help the model to obtain the relevant table information for chart understanding.\\n\\n', '**Comparison to Pre\\\\-training Method** Although the performance of VLT5 and VisionTapas is improved significantly by pre\\\\-training on additional CQA data, ChartT5 still outperform them by at least 1\\\\.3(\\\\\\\\%). Specifically, on machine\\\\-augmented questions, ChartT5 outperforms VLT5({}*{pre}) by 8(\\\\\\\\%). However, both visionTapas({}*{pre}) and VLT5({}\\\\_{pre}) achieve better accuracy on the human split, which means that the in\\\\-domain question answering objectives helps the model to improve the numerical reasoning capability. ChartT5 underperforms Pix2Struct by 2\\\\.3(\\\\\\\\%) on the overall test split. However, pix2struct is pre\\\\-trained on a more than 100 times larger pre\\\\-training corpus than the rest of the pre\\\\-training methods. Given the same scale of the pre\\\\-training dataset, we expect to gain additional performance improvement, and we leave this for future exploration.\\n\\n', '####4.1.2 Evaluation on Chart Summarization', 'For the chart summarization task, we compare ChartT5 to the best non\\\\-pretraining approaches introduced in (Kantharaj et al., 2022\\\\). Given a chart image, The authors build the chart summarization models by extending the pre\\\\-trained language generation model T5 (Raffel et al., 2020\\\\) and BART(Lewis et al., 2019\\\\) whose generation processes are conditioned on: (1\\\\) a set of scene texts extracted by a trained OCR detector. (2\\\\) the ground truth table that is paired with the chart. The evaluation result is summarized in Table 3\\\\.\\n\\n', 'From Table 3, we can see that on Statista, ChartT5 outperforms all baseline methods on BLUE score, but only a slight improvement is achieved over the best baseline. On Pew, ChartT5 underperforms T5\\\\-OCR by almost 1\\\\.5 percent. The proposed ChartT5 also slightly underperforms against the baseline methods in CIDER on both datasets. However, ChartT5 consistently outperforms all baselines on content selection scores\\n\\n', '\\\\\\\\begin{table}\\n\\\\\\\\begin{tabular}{l\\\\|c c c} \\\\\\\\hline \\\\\\\\hline \\\\\\\\multirow{2}{*}{Model} \\\\& \\\\\\\\multicolumn{3}{c}{ChartQA} \\\\\\\\ \\\\& Human \\\\& Augment \\\\& Overall \\\\\\\\ \\\\\\\\hline T5 \\\\& 25\\\\.12 \\\\& 56\\\\.96 \\\\& 41\\\\.56 \\\\\\\\ Tapas \\\\& 28\\\\.72 \\\\& 53\\\\.84 \\\\& 41\\\\.28 \\\\\\\\ VLT5 \\\\& 26\\\\.24 \\\\& 56\\\\.88 \\\\& 41\\\\.56 \\\\\\\\ VisionTapas \\\\& 29\\\\.60 \\\\& 61\\\\.44 \\\\& 45\\\\.52 \\\\\\\\ \\\\\\\\hline VLT5({}\\\\_{pre}) \\\\&* *40\\\\.08* *\\\\& 63\\\\.60 \\\\& 51\\\\.84 \\\\\\\\ VisionTapas({}\\\\_{pre}) \\\\& 32\\\\.56 \\\\& 61\\\\.60 \\\\& 47\\\\.08 \\\\\\\\ Pix2Struct \\\\& \\\\- \\\\& \\\\- \\\\&* *56\\\\.00* *\\\\\\\\ ChartT5 \\\\& 31\\\\.8 \\\\&* *74\\\\.4*\\\\* \\\\& 53\\\\.16 \\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\n\\\\\\\\end{table}\\nTable 2: Evaluation results on ChartQA. We report relaxed accuracy on the test split annotated by humans and that generated by the machine. In the last column, we report the overall accuracy by computing the mean values with human split and augment split.\\nacross both Statista and Pew sets. The under\\\\-performance on BLEU and CIDER indicates that Chart\\\\-table pre\\\\-training is limited to benefit high\\\\-quality natural language generation. However, the strong performance on content selection, which values the key information appearance in the generation, suggests the advantage of chart\\\\-table pre\\\\-training on extracting relevant chart information. Therefore, a potential direction to explore is combining different types of pre\\\\-training objectives, such as chart\\\\-to\\\\-text pre\\\\-training and chart\\\\-table pre\\\\-training goals, to facilitate the model with diverse strengths.\\n\\n', 'We conduct ablation experiments to validate the effectiveness of chart\\\\-table pre\\\\-training and the pre\\\\-training objectives. We also evaluate the effectiveness of the proposed scene text copy mechanism.\\n\\n', '####4.2.1 Chart-Table Pre-training', \"We conduct detailed analyses on the effectiveness of chart\\\\-table pre\\\\-training. First, we measure the performance gain from the chart\\\\-table pre\\\\-training on the full test set of ChartQA data. We then study what type of questions benefit most from the chart\\\\-table pre\\\\-training by picking three subsets of questions that measure different capabilities of the model: (1\\\\) Human\\\\-written questions, (2\\\\) Machine\\\\-generated questions, and (3\\\\) Table covered questions, where the answers can be directly found in the ground truth tables. The results are summarized in Table 4\\\\. From Table 4, we find that after chart\\\\-table pre\\\\-training the model's performance on these three sets of questions is all improved. The most significant gain is obtained on machine\\\\-generated questions, which mainly focus on extractive\\\\-type questions. This indicates that chart\\\\-table pre\\\\-training benefits the model to localize and retrieve the requested information presented on Chart Image. The second biggest gain is achieved on table\\\\-cover questions, where the model demonstrates significant improvement in the capability of chart\\\\-to\\\\-table interpretation.\\n\\n\", '####4.2.2 Pre-training Objectives', 'We validate the effectiveness of the two pre\\\\-training objectives, Masked Header Prediction and Masked Value Prediction. We remove one pre\\\\-training objective at a time and pre\\\\-train the ChartTSwith only one table prediction task. The pre\\\\-trained model\\n\\n', '\\\\\\\\begin{table}\\n\\\\\\\\begin{tabular}{l\\\\|c c c} \\\\\\\\hline \\\\\\\\hline \\\\\\\\multirow{2}{*}{Pretraining?} \\\\& \\\\\\\\multicolumn{3}{c}{Question Types} \\\\\\\\ \\\\& Table \\\\& Human \\\\& Augment \\\\\\\\ \\\\\\\\hline No \\\\& 60\\\\.7 \\\\& 30\\\\.8 \\\\& 66\\\\.7 \\\\\\\\ Yes \\\\&* *64\\\\.7* *\\\\&* *31\\\\.8* *\\\\&* *74\\\\.4*\\\\* \\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\n\\\\\\\\end{table}\\nTable 4: Ablation Study on Chart Table Pre\\\\-training with ChartQA Dataset. We report results on three subsets of questions: Table cover questions, human\\\\-written questions, and machine\\\\-generated questions.\\n\\n', '\\\\\\\\begin{table}\\n\\\\\\\\begin{tabular}{l\\\\|c c c\\\\|c c c} \\\\\\\\hline \\\\\\\\hline \\\\\\\\multirow{2}{*}{Model} \\\\& \\\\\\\\multicolumn{3}{c\\\\|}{Statista} \\\\& \\\\\\\\multicolumn{3}{c}{Pew} \\\\\\\\ \\\\& BLEU \\\\& CS \\\\& CIDER \\\\& BLEU \\\\& CS \\\\& CIDER \\\\\\\\ \\\\\\\\hline T5\\\\-OCR \\\\& 35\\\\.29 \\\\& 73\\\\.77 \\\\& 4\\\\.43 \\\\&* *10\\\\.49* *\\\\& 40\\\\.87 \\\\&* *2\\\\.20* *\\\\\\\\ BART\\\\-OCR \\\\& \\\\- \\\\& \\\\- \\\\& \\\\- \\\\& 9\\\\.09 \\\\& 39\\\\.99 \\\\& 1\\\\.97 \\\\\\\\ T5\\\\-TAB \\\\& 37\\\\.01 \\\\& 75\\\\.72 \\\\&* *4\\\\.68* *\\\\& \\\\- \\\\& \\\\- \\\\& \\\\- \\\\\\\\ BART\\\\-TAB \\\\& 36\\\\.36 \\\\& 77\\\\.14 \\\\& 4\\\\.40 \\\\& \\\\- \\\\& \\\\- \\\\& \\\\- \\\\\\\\ ChartT5 \\\\&* *37\\\\.51* *\\\\&* *82\\\\.16* *\\\\& 3\\\\.45 \\\\& 9\\\\.05 \\\\&* *55\\\\.1*\\\\* \\\\& 1\\\\.23 \\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\n\\\\\\\\end{table}\\nTable 3: Evaluation results on Chart Summarization. We display BLEU, CS and CIDER scores for the Pew and Statista Split. The ground truth table is not available to Pew thus the table\\\\-based method does not have results on Pew split.\\n\\n', \"\\\\\\\\begin{table}\\n\\\\\\\\begin{tabular}{l\\\\|c c c} \\\\\\\\hline \\\\\\\\hline \\\\& \\\\\\\\multicolumn{3}{c}{Question Types} \\\\\\\\ \\\\& Human \\\\& Augment \\\\& Overall \\\\\\\\ \\\\\\\\hline Full \\\\& 31\\\\.8 \\\\& 74\\\\.4 \\\\& 53\\\\.1 \\\\\\\\ \\\\- MVP \\\\& 30\\\\.9 \\\\& 73\\\\.7 \\\\& 52\\\\.3 \\\\\\\\ \\\\- MHP \\\\& 31\\\\.2 \\\\& 68\\\\.3 \\\\& 49\\\\.7 \\\\\\\\ \\\\- STC \\\\& 30\\\\.8 \\\\& 72\\\\.4 \\\\& 51\\\\.6 \\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\n\\\\\\\\end{table}\\nTable 5: Ablation Study on the two proposed pre\\\\-training objectives and the Scene Text Copy Mechanism (STC). The first row is the result of the full ChartT5 model. Then we remove one of the pre\\\\-training objectives and the scene\\\\-text\\\\-copy mechanism. We report the results of different ablation experiments on both human and machine\\\\-generated splits as well as the overall performance.\\nis then fine\\\\-tuned and evaluated on the human and augmented split for comparison. The result is displayed in table 5\\\\. As can be seen from the table, removing Masked Value Prediction Loss has a negligible impact on the performance of ChartT5 on ChartQA dataset. There is a slightly more drop in human written questions which suggests that predicting table numerical values still has a miner positive impact on helping the model's mathematical reasoning. Remove Masked Header Prediction have a significant impact on the machine\\\\-generated question\\\\-answering accuracy. As expected, Masked header modeling mainly helps the model learn how to link the scene text to the table headers, which is a critical ability to extract relevant information given a specific query.\\n\\n\", '####4.2.3 Scene Text Copy', 'We also validate the effectiveness of the scene\\\\-text\\\\-copy mechanism, where we train a ChartT5model by simply representing OCR tokens in their original text format. The model is fine\\\\-tuned and evaluated on the human and augmented split of the chartQA dataset to compare against the full ChartT5\\\\. The result is displayed in Table 5\\\\. Disabling the scene\\\\-text\\\\-copy mechanism leads to a 1\\\\.5(\\\\\\\\%) overall performance drop on ChartQA tasks. Specifically, it leads to more degradation on the augmented split than the human split, as scene\\\\-text\\\\-copy helps enhance the alignment between OCR and table values to benefit accurate information extraction from the chart.\\n\\n', 'We have manually analyzed model predictions to understand its limitation. We found that our model suffers most from noisy OCR detection and complex question that requires multi\\\\-hop reasoning.\\n\\n', '**Noisy OCR Prediction.** As an OCR\\\\-based model, ChartT5 often suffers from a wrong OCR detection. An example is shown in Figure 3; the model localizes the right scene text \"1\\\\.18\" to answer the question, but the OCR text is mistakenly detected as \"1:18\". To further understand the limitation of OCR detection, we randomly sample 20K PlotQA test split and compare the performance of our model using detected OCRs against Ground Truth OCRs. We observe a 5(\\\\\\\\%) performance drop when using detected OCRs. We can improve the OCR detector for future work by training on a large Plot scene\\\\-text detection benchmark. Another promising direction is to attempt OCR\\\\-free end\\\\-to\\\\-end plot recognition method like Pix2Struct (Lee et al., 2022\\\\).\\n\\n', '**Multi\\\\-Hop Reasoning.** Our model is also quite vulnerable to handling complex questions requiring multi\\\\-hop reasoning. An example is shown in Figure 4; the model cannot perform the complex logic reasoning to add the stats of the two smallest bars and compare that to the large bar. We will consider exploring pre\\\\-training on the mathematic reasoning datasets to address this limitation.\\n\\n', 'In this section, We detailed our experiment setups to evaluate the proposed ChartT5 on two tasks: chart question answering and chart summarization. We then introduce the main results of the two evaluation tasks. Finally, we present the ablation study on chart\\\\-table pre\\\\-training and the two pre\\\\-training objectives.\\n\\n', '**Chart Question Answering.** Given a chart image and a query question, the goal for the model is to provide an accurate answer string by interpreting the provided chart image. For this task, we consider the ChartQA dataset Masry et al. (2022\\\\), which collects question\\\\-answer pairs on realistic chart images scraped from the internet. Their annotations are collected in two fashions: (1\\\\) Human\\\\-written question\\\\-answer pairs; and (2\\\\) machine\\\\-generated question\\\\-answer pairs derived from the human\\\\-written chart summaries. In total 32\\\\.7K question\\\\-answer pairs are collected on 21\\\\.9K scraped chart images, where about 9\\\\.6K question\\\\-and\\\\-answer pairs are human\\\\-written. Compared to the previously collected CQA datasets, ChartQA is more challenging to handle due to the diverse visual style from the realistic chart images and the complex language from human annotations. Following previous work Masry et al. (2022\\\\); Methani et al. (2020\\\\), we also apply the relaxed accuracy to measure the performance on the CQA task, which allows a minor inaccuracy on numerical value prediction (within 5(\\\\\\\\%) of the gold answer). For non\\\\-numerical answers, the prediction needs to be exactly matched to the gold\\\\-standard answer.\\n\\n', '**Chart Summarization.** Given a chart image, the target is to summarize the key insights of the chart in natural language. For this task, we evaluate our model on the most recently proposed Chart\\\\-to\\\\-Text benchmark Kantharaj et al. (2022\\\\), which collects roughly 36\\\\.5K chart images with one summary for each image. They split the collected charts into two sets: Statista and Pew, representing the two separate websites from which the chart plots come. The summaries in Statista are human\\\\-written which is well grounded on the chart image. Meanwhile, the summaries from Pew are automatically extracted from the news paragraphs surrounding the chart images. Pew is noisier and more challenging to handle. We follow Kantharaj et al. (2022\\\\) to split the two sets for training and testing. We adopt BLEU\\\\-4, Content Selection, and CIDER as the evaluation metrics to measure the quality of the generated summary following Kantharaj et al. (2022\\\\).\\n\\n', '**Implementation details.** We initialized our ChartT5 from T5({}\\\\_{\\\\\\\\text{base}}) and pre\\\\-trained on our self\\\\-collected corpus for 30 epochs with a batch size of 60\\\\. We used Adam optimizer Kingma and Ba (2015\\\\) with a linear warm\\\\-up for the first 5(\\\\\\\\%) training steps, and the peak learning rate is set as 1e\\\\-4\\\\. After warming up, a linear decayed learning\\\\-rate scheduler gradually drops the learning rate for the rest of the training steps. The pre\\\\-training experiments are conducted on 2 Nvidia TITAN RTX GPUs, and it roughly takes two days to accomplish the experiment. We kept the last checkpoint of each pre\\\\-training run as our final checkpoint for fine\\\\-tuning.\\n\\n', 'We also applied warming\\\\-up for downstream fine\\\\-tuning to gradually increase the learning rate to the pick value during the first 5(\\\\\\\\%) of training epochs. After that, a linear decayed learning\\\\-rate scheduler gradually drops the learning rate for the remaining training. For CQA task, we set batch size as 24 and fine\\\\-tune ChartT5 for 60 epochs with a peak learning rate 2e\\\\-4 on 2 Nvidia TITAN RTX GPUs. The best checkpoint was saved as the one that achieves the highest accuracy on the validation\\nsplit. On the CS task, we use batch size 20 and a peak learning rate 5e\\\\-5\\\\. On the Pew split, we fine\\\\-tune ChartT5for 20 epochs, and on Statista, we fine\\\\-tune ChartT5for 25 epochs. The best checkpoint is also saved as achieving the best BLEU score on the validation split. All the reported numbers are one\\\\-time runs.\\n\\n', '###Main Results', '###Ablation Study', '###Qualitative Error Analysis']\n",
      "We first compare ChartT5 to various state\\-of\\-the\\-art methods with or without pre\\-training on the two downstream tasks.\n",
      "\n",
      "\n",
      "\n",
      "####4.1.1 Evaluation on CQA\n",
      "\n",
      "We compare ChartT5 with SOTA non\\-pretraining and pre\\-training methods on CQA tasks. The best\\-performed non\\-pretraining baselines are introduced in (Masry et al., 2022\\). The authors first predict the table data from the chart image via an automatic data extraction tool (Luo et al., 2021\\). Then they extend various language\\-only models (T5, Tapas) and multi\\-modal models (VLT5, VisionTapas) to predict the answer conditioned on the extracted table. On the line of pre\\-training baselines, we compare to VLT5({}*{pre}) and VisionTapas({}*{pre}) which pre\\-trains VLT5 and Vision Tapas on PlotQA with the visual question answering tasks. We also compare chartT5 to the current SOTA method Pix2Struct which is pre\\-trained on 80 million webpage screenshots to HTML code parsing objectives. The result is summarized in Table 2\\.\n",
      "\n",
      "\n",
      "\n",
      "**Comparison to Non\\-Pretraining Method** Even without access to the predicted tables, ChartT5 has outperformed all non\\-pretraining methods by a large margin (a minimum 7\\.3(\\\\%) gain on the overall performance). ChartT5 also outperforms all non\\-pretraining baselines on the human\\-written questions and machine\\-generated questions. Although the predicted table covers 54(\\\\%) of the answers in the test data of ChartQA, simply feeding it as an input does not make the existing models fully leverage the valuable information. The significant improvement achieved by ChartT5 indicates the effectiveness of the proposed pre\\-training to help the model to obtain the relevant table information for chart understanding.\n",
      "\n",
      "\n",
      "\n",
      "**Comparison to Pre\\-training Method** Although the performance of VLT5 and VisionTapas is improved significantly by pre\\-training on additional CQA data, ChartT5 still outperform them by at least 1\\.3(\\\\%). Specifically, on machine\\-augmented questions, ChartT5 outperforms VLT5({}*{pre}) by 8(\\\\%). However, both visionTapas({}*{pre}) and VLT5({}\\_{pre}) achieve better accuracy on the human split, which means that the in\\-domain question answering objectives helps the model to improve the numerical reasoning capability. ChartT5 underperforms Pix2Struct by 2\\.3(\\\\%) on the overall test split. However, pix2struct is pre\\-trained on a more than 100 times larger pre\\-training corpus than the rest of the pre\\-training methods. Given the same scale of the pre\\-training dataset, we expect to gain additional performance improvement, and we leave this for future exploration.\n",
      "\n",
      "\n",
      "\n",
      "####4.1.2 Evaluation on Chart Summarization\n",
      "\n",
      "For the chart summarization task, we compare ChartT5 to the best non\\-pretraining approaches introduced in (Kantharaj et al., 2022\\). Given a chart image, The authors build the chart summarization models by extending the pre\\-trained language generation model T5 (Raffel et al., 2020\\) and BART(Lewis et al., 2019\\) whose generation processes are conditioned on: (1\\) a set of scene texts extracted by a trained OCR detector. (2\\) the ground truth table that is paired with the chart. The evaluation result is summarized in Table 3\\.\n",
      "\n",
      "\n",
      "\n",
      "From Table 3, we can see that on Statista, ChartT5 outperforms all baseline methods on BLUE score, but only a slight improvement is achieved over the best baseline. On Pew, ChartT5 underperforms T5\\-OCR by almost 1\\.5 percent. The proposed ChartT5 also slightly underperforms against the baseline methods in CIDER on both datasets. However, ChartT5 consistently outperforms all baselines on content selection scores\n",
      "\n",
      "\n",
      "\n",
      "\\\\begin{table}\n",
      "\\\\begin{tabular}{l\\|c c c} \\\\hline \\\\hline \\\\multirow{2}{*}{Model} \\& \\\\multicolumn{3}{c}{ChartQA} \\\\ \\& Human \\& Augment \\& Overall \\\\ \\\\hline T5 \\& 25\\.12 \\& 56\\.96 \\& 41\\.56 \\\\ Tapas \\& 28\\.72 \\& 53\\.84 \\& 41\\.28 \\\\ VLT5 \\& 26\\.24 \\& 56\\.88 \\& 41\\.56 \\\\ VisionTapas \\& 29\\.60 \\& 61\\.44 \\& 45\\.52 \\\\ \\\\hline VLT5({}\\_{pre}) \\&* *40\\.08* *\\& 63\\.60 \\& 51\\.84 \\\\ VisionTapas({}\\_{pre}) \\& 32\\.56 \\& 61\\.60 \\& 47\\.08 \\\\ Pix2Struct \\& \\- \\& \\- \\&* *56\\.00* *\\\\ ChartT5 \\& 31\\.8 \\&* *74\\.4*\\* \\& 53\\.16 \\\\ \\\\hline \\\\hline \\\\end{tabular}\n",
      "\\\\end{table}\n",
      "Table 2: Evaluation results on ChartQA. We report relaxed accuracy on the test split annotated by humans and that generated by the machine. In the last column, we report the overall accuracy by computing the mean values with human split and augment split.\n",
      "across both Statista and Pew sets. The under\\-performance on BLEU and CIDER indicates that Chart\\-table pre\\-training is limited to benefit high\\-quality natural language generation. However, the strong performance on content selection, which values the key information appearance in the generation, suggests the advantage of chart\\-table pre\\-training on extracting relevant chart information. Therefore, a potential direction to explore is combining different types of pre\\-training objectives, such as chart\\-to\\-text pre\\-training and chart\\-table pre\\-training goals, to facilitate the model with diverse strengths.\n",
      "\n",
      "\n",
      "\n",
      "We conduct ablation experiments to validate the effectiveness of chart\\-table pre\\-training and the pre\\-training objectives. We also evaluate the effectiveness of the proposed scene text copy mechanism.\n",
      "\n",
      "\n",
      "\n",
      "####4.2.1 Chart-Table Pre-training\n",
      "\n",
      "We conduct detailed analyses on the effectiveness of chart\\-table pre\\-training. First, we measure the performance gain from the chart\\-table pre\\-training on the full test set of ChartQA data. We then study what type of questions benefit most from the chart\\-table pre\\-training by picking three subsets of questions that measure different capabilities of the model: (1\\) Human\\-written questions, (2\\) Machine\\-generated questions, and (3\\) Table covered questions, where the answers can be directly found in the ground truth tables. The results are summarized in Table 4\\. From Table 4, we find that after chart\\-table pre\\-training the model's performance on these three sets of questions is all improved. The most significant gain is obtained on machine\\-generated questions, which mainly focus on extractive\\-type questions. This indicates that chart\\-table pre\\-training benefits the model to localize and retrieve the requested information presented on Chart Image. The second biggest gain is achieved on table\\-cover questions, where the model demonstrates significant improvement in the capability of chart\\-to\\-table interpretation.\n",
      "\n",
      "\n",
      "\n",
      "####4.2.2 Pre-training Objectives\n",
      "\n",
      "We validate the effectiveness of the two pre\\-training objectives, Masked Header Prediction and Masked Value Prediction. We remove one pre\\-training objective at a time and pre\\-train the ChartTSwith only one table prediction task. The pre\\-trained model\n",
      "\n",
      "\n",
      "\n",
      "\\\\begin{table}\n",
      "\\\\begin{tabular}{l\\|c c c} \\\\hline \\\\hline \\\\multirow{2}{*}{Pretraining?} \\& \\\\multicolumn{3}{c}{Question Types} \\\\ \\& Table \\& Human \\& Augment \\\\ \\\\hline No \\& 60\\.7 \\& 30\\.8 \\& 66\\.7 \\\\ Yes \\&* *64\\.7* *\\&* *31\\.8* *\\&* *74\\.4*\\* \\\\ \\\\hline \\\\hline \\\\end{tabular}\n",
      "\\\\end{table}\n",
      "Table 4: Ablation Study on Chart Table Pre\\-training with ChartQA Dataset. We report results on three subsets of questions: Table cover questions, human\\-written questions, and machine\\-generated questions.\n",
      "\n",
      "\n",
      "\n",
      "\\\\begin{table}\n",
      "\\\\begin{tabular}{l\\|c c c\\|c c c} \\\\hline \\\\hline \\\\multirow{2}{*}{Model} \\& \\\\multicolumn{3}{c\\|}{Statista} \\& \\\\multicolumn{3}{c}{Pew} \\\\ \\& BLEU \\& CS \\& CIDER \\& BLEU \\& CS \\& CIDER \\\\ \\\\hline T5\\-OCR \\& 35\\.29 \\& 73\\.77 \\& 4\\.43 \\&* *10\\.49* *\\& 40\\.87 \\&* *2\\.20* *\\\\ BART\\-OCR \\& \\- \\& \\- \\& \\- \\& 9\\.09 \\& 39\\.99 \\& 1\\.97 \\\\ T5\\-TAB \\& 37\\.01 \\& 75\\.72 \\&* *4\\.68* *\\& \\- \\& \\- \\& \\- \\\\ BART\\-TAB \\& 36\\.36 \\& 77\\.14 \\& 4\\.40 \\& \\- \\& \\- \\& \\- \\\\ ChartT5 \\&* *37\\.51* *\\&* *82\\.16* *\\& 3\\.45 \\& 9\\.05 \\&* *55\\.1*\\* \\& 1\\.23 \\\\ \\\\hline \\\\hline \\\\end{tabular}\n",
      "\\\\end{table}\n",
      "Table 3: Evaluation results on Chart Summarization. We display BLEU, CS and CIDER scores for the Pew and Statista Split. The ground truth table is not available to Pew thus the table\\-based method does not have results on Pew split.\n",
      "\n",
      "\n",
      "\n",
      "\\\\begin{table}\n",
      "\\\\begin{tabular}{l\\|c c c} \\\\hline \\\\hline \\& \\\\multicolumn{3}{c}{Question Types} \\\\ \\& Human \\& Augment \\& Overall \\\\ \\\\hline Full \\& 31\\.8 \\& 74\\.4 \\& 53\\.1 \\\\ \\- MVP \\& 30\\.9 \\& 73\\.7 \\& 52\\.3 \\\\ \\- MHP \\& 31\\.2 \\& 68\\.3 \\& 49\\.7 \\\\ \\- STC \\& 30\\.8 \\& 72\\.4 \\& 51\\.6 \\\\ \\\\hline \\\\hline \\\\end{tabular}\n",
      "\\\\end{table}\n",
      "Table 5: Ablation Study on the two proposed pre\\-training objectives and the Scene Text Copy Mechanism (STC). The first row is the result of the full ChartT5 model. Then we remove one of the pre\\-training objectives and the scene\\-text\\-copy mechanism. We report the results of different ablation experiments on both human and machine\\-generated splits as well as the overall performance.\n",
      "is then fine\\-tuned and evaluated on the human and augmented split for comparison. The result is displayed in table 5\\. As can be seen from the table, removing Masked Value Prediction Loss has a negligible impact on the performance of ChartT5 on ChartQA dataset. There is a slightly more drop in human written questions which suggests that predicting table numerical values still has a miner positive impact on helping the model's mathematical reasoning. Remove Masked Header Prediction have a significant impact on the machine\\-generated question\\-answering accuracy. As expected, Masked header modeling mainly helps the model learn how to link the scene text to the table headers, which is a critical ability to extract relevant information given a specific query.\n",
      "\n",
      "\n",
      "\n",
      "####4.2.3 Scene Text Copy\n",
      "\n",
      "We also validate the effectiveness of the scene\\-text\\-copy mechanism, where we train a ChartT5model by simply representing OCR tokens in their original text format. The model is fine\\-tuned and evaluated on the human and augmented split of the chartQA dataset to compare against the full ChartT5\\. The result is displayed in Table 5\\. Disabling the scene\\-text\\-copy mechanism leads to a 1\\.5(\\\\%) overall performance drop on ChartQA tasks. Specifically, it leads to more degradation on the augmented split than the human split, as scene\\-text\\-copy helps enhance the alignment between OCR and table values to benefit accurate information extraction from the chart.\n",
      "\n",
      "\n",
      "\n",
      "We have manually analyzed model predictions to understand its limitation. We found that our model suffers most from noisy OCR detection and complex question that requires multi\\-hop reasoning.\n",
      "\n",
      "\n",
      "\n",
      "**Noisy OCR Prediction.** As an OCR\\-based model, ChartT5 often suffers from a wrong OCR detection. An example is shown in Figure 3; the model localizes the right scene text \"1\\.18\" to answer the question, but the OCR text is mistakenly detected as \"1:18\". To further understand the limitation of OCR detection, we randomly sample 20K PlotQA test split and compare the performance of our model using detected OCRs against Ground Truth OCRs. We observe a 5(\\\\%) performance drop when using detected OCRs. We can improve the OCR detector for future work by training on a large Plot scene\\-text detection benchmark. Another promising direction is to attempt OCR\\-free end\\-to\\-end plot recognition method like Pix2Struct (Lee et al., 2022\\).\n",
      "\n",
      "\n",
      "\n",
      "**Multi\\-Hop Reasoning.** Our model is also quite vulnerable to handling complex questions requiring multi\\-hop reasoning. An example is shown in Figure 4; the model cannot perform the complex logic reasoning to add the stats of the two smallest bars and compare that to the large bar. We will consider exploring pre\\-training on the mathematic reasoning datasets to address this limitation.\n",
      "\n",
      "\n",
      "\n",
      "In this section, We detailed our experiment setups to evaluate the proposed ChartT5 on two tasks: chart question answering and chart summarization. We then introduce the main results of the two evaluation tasks. Finally, we present the ablation study on chart\\-table pre\\-training and the two pre\\-training objectives.\n",
      "\n",
      "\n",
      "\n",
      "**Chart Question Answering.** Given a chart image and a query question, the goal for the model is to provide an accurate answer string by interpreting the provided chart image. For this task, we consider the ChartQA dataset Masry et al. (2022\\), which collects question\\-answer pairs on realistic chart images scraped from the internet. Their annotations are collected in two fashions: (1\\) Human\\-written question\\-answer pairs; and (2\\) machine\\-generated question\\-answer pairs derived from the human\\-written chart summaries. In total 32\\.7K question\\-answer pairs are collected on 21\\.9K scraped chart images, where about 9\\.6K question\\-and\\-answer pairs are human\\-written. Compared to the previously collected CQA datasets, ChartQA is more challenging to handle due to the diverse visual style from the realistic chart images and the complex language from human annotations. Following previous work Masry et al. (2022\\); Methani et al. (2020\\), we also apply the relaxed accuracy to measure the performance on the CQA task, which allows a minor inaccuracy on numerical value prediction (within 5(\\\\%) of the gold answer). For non\\-numerical answers, the prediction needs to be exactly matched to the gold\\-standard answer.\n",
      "\n",
      "\n",
      "\n",
      "**Chart Summarization.** Given a chart image, the target is to summarize the key insights of the chart in natural language. For this task, we evaluate our model on the most recently proposed Chart\\-to\\-Text benchmark Kantharaj et al. (2022\\), which collects roughly 36\\.5K chart images with one summary for each image. They split the collected charts into two sets: Statista and Pew, representing the two separate websites from which the chart plots come. The summaries in Statista are human\\-written which is well grounded on the chart image. Meanwhile, the summaries from Pew are automatically extracted from the news paragraphs surrounding the chart images. Pew is noisier and more challenging to handle. We follow Kantharaj et al. (2022\\) to split the two sets for training and testing. We adopt BLEU\\-4, Content Selection, and CIDER as the evaluation metrics to measure the quality of the generated summary following Kantharaj et al. (2022\\).\n",
      "\n",
      "\n",
      "\n",
      "**Implementation details.** We initialized our ChartT5 from T5({}\\_{\\\\text{base}}) and pre\\-trained on our self\\-collected corpus for 30 epochs with a batch size of 60\\. We used Adam optimizer Kingma and Ba (2015\\) with a linear warm\\-up for the first 5(\\\\%) training steps, and the peak learning rate is set as 1e\\-4\\. After warming up, a linear decayed learning\\-rate scheduler gradually drops the learning rate for the rest of the training steps. The pre\\-training experiments are conducted on 2 Nvidia TITAN RTX GPUs, and it roughly takes two days to accomplish the experiment. We kept the last checkpoint of each pre\\-training run as our final checkpoint for fine\\-tuning.\n",
      "\n",
      "\n",
      "\n",
      "We also applied warming\\-up for downstream fine\\-tuning to gradually increase the learning rate to the pick value during the first 5(\\\\%) of training epochs. After that, a linear decayed learning\\-rate scheduler gradually drops the learning rate for the remaining training. For CQA task, we set batch size as 24 and fine\\-tune ChartT5 for 60 epochs with a peak learning rate 2e\\-4 on 2 Nvidia TITAN RTX GPUs. The best checkpoint was saved as the one that achieves the highest accuracy on the validation\n",
      "split. On the CS task, we use batch size 20 and a peak learning rate 5e\\-5\\. On the Pew split, we fine\\-tune ChartT5for 20 epochs, and on Statista, we fine\\-tune ChartT5for 25 epochs. The best checkpoint is also saved as achieving the best BLEU score on the validation split. All the reported numbers are one\\-time runs.\n",
      "\n",
      "\n",
      "\n",
      "###Main Results\n",
      "\n",
      "###Ablation Study\n",
      "\n",
      "###Qualitative Error Analysis\n",
      "------------------------------\n",
      "CHILD: h2 5 Conclusion 5 Conclusion\n",
      "p \"We propose ChartT5 to enhance the vision language model's ability to understand chart images via chart-table pre-training. The model learns to interpret the masked tables via our proposed masked header prediction and masked value prediction objectives. ChartT5 achieves significant improvement over table-based non-pretraining SOTA methods on the ChartQA dataset, especially on the extractive question sets. We also achieve a new SOTA Content Selection Score on the Chart-to-text summarization dataset. We conduct comprehensive ablation studies to identify the impact of chart-table pre-training, and we find that the proposed pre-training is extremely helpful to extract accurate\" <p>We propose ChartT5 to enhance the vision language model's ability to understand chart images via chart-table pre-training. The model learns to interpret the masked tables via our proposed masked header prediction and masked value prediction objectives. ChartT5 achieves significant improvement over table-based non-pretraining SOTA methods on the ChartQA dataset, especially on the extractive question sets. We also achieve a new SOTA Content Selection Score on the Chart-to-text summarization dataset. We conduct comprehensive ablation studies to identify the impact of chart-table pre-training, and we find that the proposed pre-training is extremely helpful to extract accurate</p>\n",
      "p 'Figure 4: An error prediction from our model due to complex multi-hop reasoning' <p>Figure 4: An error prediction from our model due to complex multi-hop reasoning</p>\n",
      "p 'Figure 3: An error prediction from our model due to noisy OCR prediction\\ninformation from the Chart. For future research directions, we believe it may also be meaningful to explore chart understanding under data-efficient settings Hsu et al. (2022); Zeng et al. (2023) and for evidence retrieval tasks Lu et al. (2022); Ji et al. (2023).' <p>Figure 3: An error prediction from our model due to noisy OCR prediction\n",
      "information from the Chart. For future research directions, we believe it may also be meaningful to explore chart understanding under data-efficient settings Hsu et al. (2022); Zeng et al. (2023) and for evidence retrieval tasks Lu et al. (2022); Ji et al. (2023).</p>\n",
      "[\"We propose ChartT5 to enhance the vision language model's ability to understand chart images via chart\\\\-table pre\\\\-training. The model learns to interpret the masked tables via our proposed masked header prediction and masked value prediction objectives. ChartT5 achieves significant improvement over table\\\\-based non\\\\-pretraining SOTA methods on the ChartQA dataset, especially on the extractive question sets. We also achieve a new SOTA Content Selection Score on the Chart\\\\-to\\\\-text summarization dataset. We conduct comprehensive ablation studies to identify the impact of chart\\\\-table pre\\\\-training, and we find that the proposed pre\\\\-training is extremely helpful to extract accurate\\n\\n\", 'Figure 4: An error prediction from our model due to complex multi\\\\-hop reasoning\\n\\n', 'Figure 3: An error prediction from our model due to noisy OCR prediction\\ninformation from the Chart. For future research directions, we believe it may also be meaningful to explore chart understanding under data\\\\-efficient settings Hsu et al. (2022\\\\); Zeng et al. (2023\\\\) and for evidence retrieval tasks Lu et al. (2022\\\\); Ji et al. (2023\\\\).\\n\\n']\n",
      "We propose ChartT5 to enhance the vision language model's ability to understand chart images via chart\\-table pre\\-training. The model learns to interpret the masked tables via our proposed masked header prediction and masked value prediction objectives. ChartT5 achieves significant improvement over table\\-based non\\-pretraining SOTA methods on the ChartQA dataset, especially on the extractive question sets. We also achieve a new SOTA Content Selection Score on the Chart\\-to\\-text summarization dataset. We conduct comprehensive ablation studies to identify the impact of chart\\-table pre\\-training, and we find that the proposed pre\\-training is extremely helpful to extract accurate\n",
      "\n",
      "\n",
      "\n",
      "Figure 4: An error prediction from our model due to complex multi\\-hop reasoning\n",
      "\n",
      "\n",
      "\n",
      "Figure 3: An error prediction from our model due to noisy OCR prediction\n",
      "information from the Chart. For future research directions, we believe it may also be meaningful to explore chart understanding under data\\-efficient settings Hsu et al. (2022\\); Zeng et al. (2023\\) and for evidence retrieval tasks Lu et al. (2022\\); Ji et al. (2023\\).\n",
      "\n",
      "\n",
      "------------------------------\n",
      "CHILD: h2 6 Limitations 6 Limitations\n",
      "p \"Although introducing chart value prediction objective, it only provides minor improvement to the model's performance on doing complex reasoning. There is still a large room to improve the model's capability in math calculation. Our model also suffers from the noisy OCR prediction of off-the-shelf object detector, whose performance will depend highly on the extracted OCR text qualities. Another possible limitation of our approach is the quality of the pre-training data, which only contains synthetic images. Although the proposed model works fairly well on the ChartQA dataset, it is unclear if the improved performance can be generalized to other realistic chart images.\" <p>Although introducing chart value prediction objective, it only provides minor improvement to the model's performance on doing complex reasoning. There is still a large room to improve the model's capability in math calculation. Our model also suffers from the noisy OCR prediction of off-the-shelf object detector, whose performance will depend highly on the extracted OCR text qualities. Another possible limitation of our approach is the quality of the pre-training data, which only contains synthetic images. Although the proposed model works fairly well on the ChartQA dataset, it is unclear if the improved performance can be generalized to other realistic chart images.</p>\n",
      "[\"Although introducing chart value prediction objective, it only provides minor improvement to the model's performance on doing complex reasoning. There is still a large room to improve the model's capability in math calculation. Our model also suffers from the noisy OCR prediction of off\\\\-the\\\\-shelf object detector, whose performance will depend highly on the extracted OCR text qualities. Another possible limitation of our approach is the quality of the pre\\\\-training data, which only contains synthetic images. Although the proposed model works fairly well on the ChartQA dataset, it is unclear if the improved performance can be generalized to other realistic chart images.\\n\\n\"]\n",
      "Although introducing chart value prediction objective, it only provides minor improvement to the model's performance on doing complex reasoning. There is still a large room to improve the model's capability in math calculation. Our model also suffers from the noisy OCR prediction of off\\-the\\-shelf object detector, whose performance will depend highly on the extracted OCR text qualities. Another possible limitation of our approach is the quality of the pre\\-training data, which only contains synthetic images. Although the proposed model works fairly well on the ChartQA dataset, it is unclear if the improved performance can be generalized to other realistic chart images.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "CHILD: h2 7 Ethics Statement 7 Ethics Statement\n",
      "p \"When we collect the pre-training dataset, we ensure we respect the intellectual property of dataset sources. All the ChartQA dataset we used for the collection of chart-table pairs allows public access for research. To ensure the reproducibility of our experiment results, we provide details of the hyperparameter setting in our paper, and we will also publish our code later. Our models can mislead the public's understanding of chart content due to the potential bias from our training corpus. Therefore, we don't recommend using our model for any real-world decision on chart images.\" <p>When we collect the pre-training dataset, we ensure we respect the intellectual property of dataset sources. All the ChartQA dataset we used for the collection of chart-table pairs allows public access for research. To ensure the reproducibility of our experiment results, we provide details of the hyperparameter setting in our paper, and we will also publish our code later. Our models can mislead the public's understanding of chart content due to the potential bias from our training corpus. Therefore, we don't recommend using our model for any real-world decision on chart images.</p>\n",
      "[\"When we collect the pre\\\\-training dataset, we ensure we respect the intellectual property of dataset sources. All the ChartQA dataset we used for the collection of chart\\\\-table pairs allows public access for research. To ensure the reproducibility of our experiment results, we provide details of the hyperparameter setting in our paper, and we will also publish our code later. Our models can mislead the public's understanding of chart content due to the potential bias from our training corpus. Therefore, we don't recommend using our model for any real\\\\-world decision on chart images.\\n\\n\"]\n",
      "When we collect the pre\\-training dataset, we ensure we respect the intellectual property of dataset sources. All the ChartQA dataset we used for the collection of chart\\-table pairs allows public access for research. To ensure the reproducibility of our experiment results, we provide details of the hyperparameter setting in our paper, and we will also publish our code later. Our models can mislead the public's understanding of chart content due to the potential bias from our training corpus. Therefore, we don't recommend using our model for any real\\-world decision on chart images.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "CHILD: h2 Acknowledgement Acknowledgement\n",
      "p 'This research work is supported by U.S DARPA SemaFor Program No. HR001120C0123. The views and conclusions contained in this work only belong to the authors and should not represent the official policies implied by DARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. We also thank Ahmed Masry and Shankar Kantharaj for providing us with ChartQA and Chart Summary-related data and baseline model outputs.' <p>This research work is supported by U.S DARPA SemaFor Program No. HR001120C0123. The views and conclusions contained in this work only belong to the authors and should not represent the official policies implied by DARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. We also thank Ahmed Masry and Shankar Kantharaj for providing us with ChartQA and Chart Summary-related data and baseline model outputs.</p>\n",
      "['This research work is supported by U.S DARPA SemaFor Program No. HR001120C0123\\\\. The views and conclusions contained in this work only belong to the authors and should not represent the official policies implied by DARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. We also thank Ahmed Masry and Shankar Kantharaj for providing us with ChartQA and Chart Summary\\\\-related data and baseline model outputs.\\n\\n']\n",
      "This research work is supported by U.S DARPA SemaFor Program No. HR001120C0123\\. The views and conclusions contained in this work only belong to the authors and should not represent the official policies implied by DARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. We also thank Ahmed Masry and Shankar Kantharaj for providing us with ChartQA and Chart Summary\\-related data and baseline model outputs.\n",
      "\n",
      "\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ArxivPaperSection(header='h2', title='1 Introduction', text='Chart figures serve as the visual summary of tabular data, which helps to convey rich context in various documents, such as scientific papers, textbooks, and technical news. An intelligent agent that can understand and communicate chart plots can lead to many useful applications. For example, a virtual doctor who knows how to answer the patient\\'s question on a complex medical report or a reading assistant who can summarize the key findings from scientific papers in brief language. In the past few years, there has been a growing interest in our community to explore chart understanding in vision and language (V\\\\+L) tasks and many related benchmarks like Chart Question Answering **(CQA)**Masry et al. (2022\\\\); Kafle et al. (2018\\\\); Methani et al. (2020\\\\) and Chart Summarization **(CS)**Kantharaj et al. (2022\\\\) are introduced.\\n\\n\\n\\nWhile prevalent in the research community, automatic chart understanding remains a challenging problem due to its complex compositions of various shapes, lines, colors, and scene text. Although tremendous success is achieved in the V\\\\+L research, applying these existing methods to handle chart\\\\-related tasks is hard. Recent research ChartQA Masry et al. (2022\\\\) and Chart\\\\-to\\\\-Text Kantharaj et al. (2022\\\\) attempt to first convert chart images to their underlined tables and use the extracted tables to perform chart\\\\-related V\\\\+L task. As the extracted tables always have clean and organized structures, it makes extracting relevant information to solve downstream reasoning tasks much more accessible. Empirically, using tables yields promising results on both CQA and CS.\\n\\n\\n\\nDespite valuing table as a significant ingredient for chart understanding, we have two main concerns about this approach: (1\\\\) Automatic table extraction is unreliable. Existing methods Luo et al. (2021\\\\); Kato et al. (2022\\\\) are often limited to work on a few particular types of chart images and do not generalize well. Moreover, the extracted table is likely to contain incorrect noisy predictions that potentially harm the performance of the following task. (2\\\\) In most cases, the whole table is optional for resolving the chart\\\\-related V\\\\+L task. As illus\\n\\n\\n\\nFigure 1: A data sample from the ChartQA dataset. The corresponding chart table is displayed in the top right corner.\\ntrated in Fig 1, to answer the question *\"What is the value of India Bar\"*, the model just needs access to the second row to give the correct answer. In contrast, having redundant table information makes finding the relevant information challenging. To better leverage the table data, we argue that it is important to equip the V\\\\+L model with the capability to dynamically interpret the table value from the chart information.\\n\\n\\n\\nTherefore, in this paper, we propose **ChartT5**, an OCR\\\\-based image\\\\-to\\\\-text generation model pre\\\\-trained on a self\\\\-collected chart table pairs corpus. More specifically, ChartT5 learns how to uncover a masked table with two proposed pre\\\\-training objectives: Masked Header Prediction (MHP), and Masked Value Prediction (MVP). MHP helps improve the model\\'s capability of linking scene text to the corresponding table headers. MVP requires the model to perform mathematical reasoning over chart structure units and the scene text to predict the correct data value.\\n\\n\\n\\nWe evaluate our ChartT5 on two tasks and benchmarks: ChartQA and Chart\\\\-to\\\\-Text. In ChartQA, ChartT5 outperforms all the non\\\\-pretraining methods that use extracted tables by at least (8\\\\\\\\%) performance gains. ChartT5 also beats the pre\\\\-training table\\\\-based methods, which demonstrates the effectiveness of the proposed pre\\\\-training strategies. On Chart\\\\-to\\\\-Text, ChartT5 consistly outperforms the existing SOTA on the content selection metrics Barzilay and Lapata (2005\\\\) which values the model\\'s capability to extract the critical information from the chart.\\n\\n\\n\\nIn summary, our contributions are summarized below:\\n\\n\\n\\n* We propose chart\\\\-to\\\\-table pre\\\\-training for V\\\\+L model to learn the capability of interpreting table data from the chart.\\n* We demonstrate that the pre\\\\-trained model consistently outperforms table\\\\-based methods on two chart understanding tasks.\\n* We conduct comprehensive ablation studies to validate the effectiveness of chart\\\\-to\\\\-table pre\\\\-training and the proposed pre\\\\-training objectives.\\n', children=[]),\n",
       " ArxivPaperSection(header='h2', title='2 Related Work', text='Researching chart understanding in V\\\\+L tasks is a popular field nowadays. The most prevalent problem is chart question answering (CQA) Kafle et al. (2018\\\\); Kahou et al. (2018\\\\); Methani et al. (2020\\\\); Masry et al. (2022\\\\); Chaudhry et al. (2020\\\\), where researchers build models to answer complex questions on chart images. Another popular one is chart summarization (CS) Kantharaj et al. (2022\\\\); Obeid and Hoque (2020\\\\), which requires machine learning models to create a summary of key insights conveyed by a chart. Hsu et al. (2021\\\\) collected a large\\\\-scale scientific figures captioning dataset from research papers where many images are chart plots.\\n\\n\\n\\nThere are two main approaches for chart vision and language tasks. The first approach adapts existing visual question answering (VQA) and image captioning models to CQA and CS tasks with some specialized designs for chart images Kafle et al. (2020\\\\); Singh and Shekhar (2020\\\\); Chaudhry et al. (2020\\\\); Kafle et al. (2018\\\\); Hsu et al. (2021\\\\); Spreafico and Carenini (2020\\\\). The other approach assumes the table data of charts is accessible from the dataset Kim et al. (2020\\\\); Masry (2021\\\\) or can be extracted from the chart images using vision to table techniques Methani et al. (2020\\\\); Masry et al. (2022\\\\); Kantharaj et al. (2022\\\\). Then, the researchers will either use a table\\\\-to\\\\-text generation model Kim et al. (2020\\\\); Masry (2021\\\\); Methani et al. (2020\\\\) or combine the embedding of tables and charts via a multi\\\\-modal fusion method to generate the text output Masry et al. (2022\\\\); Kantharaj et al. (2022\\\\). It is clear from these efforts that adding tables as the additional representation of charts will dramatically improve the model\\'s capability to understand and interpret chart information.\\n\\n\\n\\nFollowing the table\\\\-based approach, we also value the information provided by the underlined table data of chart images. However, instead of directly concatenating the extracted table into the chart understanding model, we facilitate our model with the capability to interpret the table data from chart images via pre\\\\-training on chart\\\\-table pairs.\\n\\n\\n\\nVision and language pre\\\\-training has received growing interest over the past few years. Researchers build transformer\\\\-based multi\\\\-modal fusion models and perform self\\\\-supervised learning on a large\\\\-scale corpus of image\\\\-text pairs to learn robust cross\\\\-modal representations that can benefit the performance of various downstream tasks Chen et al. (2020\\\\); Lu et al. (2019\\\\); Tan and Bansal (2019\\\\);\\nSu et al., 2019; Li et al., 2020; Zhang et al., 2021\\\\).\\n\\n\\n\\nWhile the pre\\\\-trained models achieve great success on tasks like VQA (Antol et al., 2015\\\\) and Image Captioning (Chen et al., 2015\\\\), they have only focused on the domain of natural images. However, chart understanding is still challenging for the existing vision and language methods due to their lack of knowledge of scene text and structured visual units such as \"bars\" and \"lines\".\\n\\n\\n\\nTo address the limitation of conventional vision and language pre\\\\-training, TAP (Yang et al., 2021\\\\) and PreSTU (Kil et al., 2022\\\\) propose OCR\\\\-based vision and language pre\\\\-training frameworks that focus on scene text understanding in natural images where they design various pre\\\\-training objectives around the extracted OCR texts. Most recently, Donut (Kim et al., 2022\\\\) and Pix2Struct (Lee et al., 2022\\\\) propose OCR\\\\-free pre\\\\-training frameworks, where the pre\\\\-trained model directly generates a text output from a raw image input. Donut focuses on document image (*e.g.*, receipt) understanding, and Pix2Struct aims to handle broader types of synthetic images that contain visually\\\\-situated texts such as infographics and user interfaces via parsing web\\\\-page screenshots into their HTML Code. Different from these works, we take the first step to explore vision and language pre\\\\-training that focuses on chart image understanding. Specifically, we propose novel pre\\\\-training objectives to parse charts to their underlined tables.\\n\\n\\n\\n###Vision and Language Research on Charts\\n\\n###Vision and Language Pre-training', children=[]),\n",
       " ArxivPaperSection(header='h2', title='3 Method', text=\"To collect large\\\\-scale pairs of chart\\\\-table data, we collect synthetic data from existing chart question\\\\-answering corpora, including PlotQA (Methani et al., 2020\\\\), DVQA (Kafle et al., 2018\\\\), and FigureQA (Kahou et al., 2018\\\\). Specifically, DVQA and FigureQA render chart images from synthetic tables that are randomly generated from limited vocabularies. PlotQA first scrapes tables from online resources like World Bank Open Data and then synthesizes the charts from the scraped data, where the tables and charts contain more diverse language information. Our pre\\\\-training corpus consists of 495K chart\\\\-table pairs, which cover a diverse range of chart types. Our pre\\\\-training corpus contains three chart types: bar, line, and pie. The distribution of different chart types from the three chart question\\\\-answering benchmarks is summarized in table 1\\\\.\\n\\n\\n\\nChartT5 is an extension of the existing V\\\\+L Pre\\\\-training framework, VLT5 (Cho et al., 2021\\\\), an encoder\\\\-decoder architecture that unifies the vision\\\\-language tasks as text generation conditioned on multi\\\\-modal inputs. Given a chart image, we first extract the scene texts. For the synthetic chart images that are collected from DVQA (Kafle\\n\\n\\n\\n\\\\\\\\begin{table}\\n\\\\\\\\begin{tabular}{l\\\\|c c c\\\\|c} \\\\\\\\hline \\\\\\\\hline Type \\\\& PlotQA \\\\& DVQA \\\\& FigureQA \\\\& Total \\\\\\\\ \\\\\\\\hline Bar \\\\& 142,587 \\\\& 204,514 \\\\& 40,000 \\\\& 387,101 \\\\\\\\ Line \\\\& 48,133 \\\\& 0 \\\\& 40,000 \\\\& 88,133 \\\\\\\\ Pie \\\\& 0 \\\\& 0 \\\\& 20,001 \\\\& 20,001 \\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\n\\\\\\\\end{table}\\nTable 1: Distribution of the three chart types: bar, line, and pie from different resources in the pre\\\\-training corpus.\\n\\n\\n\\nFigure 2: An Overview of ChartT5\\\\. Given the input chart image and the extracted OCR tokens, ChartT5 predicts the masked values of the table in the output.\\net al., 2018\\\\), FigureQA (Kahou et al., 2018\\\\), and PlotQA (Methani et al., 2020\\\\), the ground\\\\-truth scene texts are available. The visual context is then represented as combining visual features extracted from the chart image and the language features obtained on the detected scene text. We then flat the paired table of the chart image into a string and extract the text features via the language encoder. The multi\\\\-modal features are then concatenated and fused via the multi\\\\-layer encoder, and the output hidden vectors can then be used for various pre\\\\-training tasks.\\n\\n\\n\\n####3.2.1 Chart Image Encoder\\n\\nGiven an input chart image, to recognize the critical marks (*e.g.*, bars and lines) of chart images, we first utilize a pre\\\\-trained Mask R\\\\-CNN object detector from (Masry et al., 2022\\\\) to extract the visual region features (\\\\\\\\mathbf{v}\\\\={v\\\\_{1},v\\\\_{2},\\\\\\\\cdots,v\\\\_{l^{v}}}). Next, the chart object detector is trained on the synthetic chart images from the previous CQA datasets (Kahou et al., 2018; Kafle et al., 2018; Masry et al., 2022; Methani et al., 2020\\\\) which is defined to identify 15 chart\\\\-related objects1\\\\. For each detected object region, we also extract location features as a 5\\\\-d vector: (\\\\[\\\\\\\\frac{x\\\\_{1}}{W},\\\\\\\\frac{y\\\\_{1}}{H},\\\\\\\\frac{x\\\\_{2}}{W},\\\\\\\\frac{y\\\\_{2}}{H},\\\\\\\\frac{(y\\\\_{2} \\\\-y\\\\_{1})(x\\\\_{2}\\\\-x\\\\_{1})}{W.H}]), which denotes the normalized top left coordinates, bottom right coordinates, and the normalized area of the detected region box. The position feature is then fed through fully\\\\-connected layers to be projected to the visual region feature embedding space. The final representation of the visual feature is obtained by summing up the projected region feature and corresponding location feature.\\n\\n\\n\\nFootnote 1: These 15 categories are: Legends, yAxisTitle, ChartTitle, xAxisTitle, LegendPreview, PlotArea, yAxisLabel, xAxisLabel, LegendLabel, PieLabel, bar, pie, pieSlice, line, and dotLine.\\n\\n\\n\\n####3.2.2 OCR Encoder\\n\\nAfter extracting the list of OCR words from the chart image, we obtain a set of OCR text embeddings (\\\\\\\\mathbf{o}\\\\={o\\\\_{1},o\\\\_{2},\\\\\\\\cdots,o\\\\_{l^{o}}}) via a learned word embedding layer. We also get each OCR token's 5\\\\-d position vector similar to the visual position vector from the OCR token's detected bounding box. We then obtain the position embedding vector using the shared projecting layer from the Chart Image Encoder. The shared position encoding mechanism between OCR tokens and chart object regions would help the model to capture their relative positional relations, which is a critical clue to predict the table data from the chart image. For example, the bar associated with an x\\\\-axis label should share a similar x\\\\-coordinate position in a vertical bar chart. The final OCR embedding vector is gained by summing up the OCR text token embeddings and the OCR position embedding.\\n\\n\\n\\n####3.2.3 Language Encoder\\n\\nFollowing the setting of the original VLT5 (Cho et al., 2021\\\\), we add a prefix to the flattened underlying table to indicate different pre\\\\-training tasks. We then get the table token embeddings (\\\\\\\\mathbf{t}\\\\={t\\\\_{1},t\\\\_{2},\\\\\\\\cdots,t\\\\_{l^{t}}}) with a shared word embedding layer. We apply the original T5's (Raffel et al., 2020\\\\) relative position bias to obtain the position information of each token in the caption and the flattened table. We know that the tables have very different structures compared to natural language captions, and several efforts are exploring specialized position embeddings for tables (Yin et al., 2020; 1\\\\). We leave the exploration of the specialized table position embedding for chart table pre\\\\-training in the future.\\n\\n\\n\\n**Scene Text Copy Mechanism.** A critical ingredient to the success of chart\\\\-to\\\\-table translation is the ability to predict the table headers from the corresponding OCR texts. For example, in the horizontal bar chart, the table column header is usually obtained from the x\\\\-axis labels, and the row header is often copied from the legend labels. Although presenting OCR text and the table to the model helps link the shared OCR tokens and table values, generating the correct table prediction from the corresponding OCR source is still challenging due to the large candidate token vocabulary. To encourage direct copy from the OCR text to the associated table cell value, we introduce OCR sentinel tokens ({\\\\<\\\\\\\\text{ocr\\\\_1}\\\\>,\\\\<\\\\\\\\text{ocr\\\\_2}\\\\>,\\\\\\\\cdots,\\\\<\\\\\\\\text{ocr\\\\_1}^{o}\\\\>}), which corresponds to the detected OCR texts. As illustrated in Figure 2, we replace each OCR token with a unique corresponding OCR sentinel token. Then, for every OCR token, we find if there is a matched existing table cell value. If a matched pair is found, we replace the table cell value with its paired OCR sentinel token. During pre\\\\-training, as all the plot images are synthesized from a paired table, the one\\\\-to\\\\-one scene text to table value mapping is already provided. With this prepossessing procedure, we successfully distinguish the table values that are copied from OCR tokens and those that need to be generated from the general token vocabularies, encouraging more accurate table pre\\ndiction from the relevant resources.\\n\\n\\n\\nGiven the chart\\\\-table pairs, we propose Masked Header Prediction (MHP) and Masked Value Prediction (MHP) to teach the model to recover incomplete tables with the chart information. Specifically, this objective aims to predict a masked table token (t\\\\_{m}) with the remaining table info (t\\\\_{\\\\\\\\backslash m}) as well as the chart image region (\\\\\\\\mathbf{v}) and the scene text (\\\\\\\\mathbf{o}). Compared to the traditional masked language modeling applied to the natural language text, we adjust the table masking strategy based on two hypotheses: (1\\\\) We alternatively mask just the table headers or numerical table values, as we think interpreting these two types of information requires different skills. Predicting table headers requires retrieving the correct scene text, while predicting numerical table values depends more on the capability to conduct mathematic reasoning over both the visual elements and the scene text. Therefore, it is better to format them as two separate pre\\\\-training objectives. (2\\\\) We increase the masking rate from 15(\\\\\\\\%) to 45(\\\\\\\\%), as the masked table token has less dependence on the surrounding table values.\\n\\n\\n\\nIn this section, we first introduce the dataset for pre\\\\-training. We then go over our ChartT5 model architecture and pre\\\\-training objectives to predict masked tables from the chart and OCR information.\\n\\n\\n\\n###Pre-training Dataset Collection\\n\\n###Model Overview\\n\\n###Pre-training Objectives\", children=[]),\n",
       " ArxivPaperSection(header='h2', title='4 Experiment', text='We first compare ChartT5 to various state\\\\-of\\\\-the\\\\-art methods with or without pre\\\\-training on the two downstream tasks.\\n\\n\\n\\n####4.1.1 Evaluation on CQA\\n\\nWe compare ChartT5 with SOTA non\\\\-pretraining and pre\\\\-training methods on CQA tasks. The best\\\\-performed non\\\\-pretraining baselines are introduced in (Masry et al., 2022\\\\). The authors first predict the table data from the chart image via an automatic data extraction tool (Luo et al., 2021\\\\). Then they extend various language\\\\-only models (T5, Tapas) and multi\\\\-modal models (VLT5, VisionTapas) to predict the answer conditioned on the extracted table. On the line of pre\\\\-training baselines, we compare to VLT5({}*{pre}) and VisionTapas({}*{pre}) which pre\\\\-trains VLT5 and Vision Tapas on PlotQA with the visual question answering tasks. We also compare chartT5 to the current SOTA method Pix2Struct which is pre\\\\-trained on 80 million webpage screenshots to HTML code parsing objectives. The result is summarized in Table 2\\\\.\\n\\n\\n\\n**Comparison to Non\\\\-Pretraining Method** Even without access to the predicted tables, ChartT5 has outperformed all non\\\\-pretraining methods by a large margin (a minimum 7\\\\.3(\\\\\\\\%) gain on the overall performance). ChartT5 also outperforms all non\\\\-pretraining baselines on the human\\\\-written questions and machine\\\\-generated questions. Although the predicted table covers 54(\\\\\\\\%) of the answers in the test data of ChartQA, simply feeding it as an input does not make the existing models fully leverage the valuable information. The significant improvement achieved by ChartT5 indicates the effectiveness of the proposed pre\\\\-training to help the model to obtain the relevant table information for chart understanding.\\n\\n\\n\\n**Comparison to Pre\\\\-training Method** Although the performance of VLT5 and VisionTapas is improved significantly by pre\\\\-training on additional CQA data, ChartT5 still outperform them by at least 1\\\\.3(\\\\\\\\%). Specifically, on machine\\\\-augmented questions, ChartT5 outperforms VLT5({}*{pre}) by 8(\\\\\\\\%). However, both visionTapas({}*{pre}) and VLT5({}\\\\_{pre}) achieve better accuracy on the human split, which means that the in\\\\-domain question answering objectives helps the model to improve the numerical reasoning capability. ChartT5 underperforms Pix2Struct by 2\\\\.3(\\\\\\\\%) on the overall test split. However, pix2struct is pre\\\\-trained on a more than 100 times larger pre\\\\-training corpus than the rest of the pre\\\\-training methods. Given the same scale of the pre\\\\-training dataset, we expect to gain additional performance improvement, and we leave this for future exploration.\\n\\n\\n\\n####4.1.2 Evaluation on Chart Summarization\\n\\nFor the chart summarization task, we compare ChartT5 to the best non\\\\-pretraining approaches introduced in (Kantharaj et al., 2022\\\\). Given a chart image, The authors build the chart summarization models by extending the pre\\\\-trained language generation model T5 (Raffel et al., 2020\\\\) and BART(Lewis et al., 2019\\\\) whose generation processes are conditioned on: (1\\\\) a set of scene texts extracted by a trained OCR detector. (2\\\\) the ground truth table that is paired with the chart. The evaluation result is summarized in Table 3\\\\.\\n\\n\\n\\nFrom Table 3, we can see that on Statista, ChartT5 outperforms all baseline methods on BLUE score, but only a slight improvement is achieved over the best baseline. On Pew, ChartT5 underperforms T5\\\\-OCR by almost 1\\\\.5 percent. The proposed ChartT5 also slightly underperforms against the baseline methods in CIDER on both datasets. However, ChartT5 consistently outperforms all baselines on content selection scores\\n\\n\\n\\n\\\\\\\\begin{table}\\n\\\\\\\\begin{tabular}{l\\\\|c c c} \\\\\\\\hline \\\\\\\\hline \\\\\\\\multirow{2}{*}{Model} \\\\& \\\\\\\\multicolumn{3}{c}{ChartQA} \\\\\\\\ \\\\& Human \\\\& Augment \\\\& Overall \\\\\\\\ \\\\\\\\hline T5 \\\\& 25\\\\.12 \\\\& 56\\\\.96 \\\\& 41\\\\.56 \\\\\\\\ Tapas \\\\& 28\\\\.72 \\\\& 53\\\\.84 \\\\& 41\\\\.28 \\\\\\\\ VLT5 \\\\& 26\\\\.24 \\\\& 56\\\\.88 \\\\& 41\\\\.56 \\\\\\\\ VisionTapas \\\\& 29\\\\.60 \\\\& 61\\\\.44 \\\\& 45\\\\.52 \\\\\\\\ \\\\\\\\hline VLT5({}\\\\_{pre}) \\\\&* *40\\\\.08* *\\\\& 63\\\\.60 \\\\& 51\\\\.84 \\\\\\\\ VisionTapas({}\\\\_{pre}) \\\\& 32\\\\.56 \\\\& 61\\\\.60 \\\\& 47\\\\.08 \\\\\\\\ Pix2Struct \\\\& \\\\- \\\\& \\\\- \\\\&* *56\\\\.00* *\\\\\\\\ ChartT5 \\\\& 31\\\\.8 \\\\&* *74\\\\.4*\\\\* \\\\& 53\\\\.16 \\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\n\\\\\\\\end{table}\\nTable 2: Evaluation results on ChartQA. We report relaxed accuracy on the test split annotated by humans and that generated by the machine. In the last column, we report the overall accuracy by computing the mean values with human split and augment split.\\nacross both Statista and Pew sets. The under\\\\-performance on BLEU and CIDER indicates that Chart\\\\-table pre\\\\-training is limited to benefit high\\\\-quality natural language generation. However, the strong performance on content selection, which values the key information appearance in the generation, suggests the advantage of chart\\\\-table pre\\\\-training on extracting relevant chart information. Therefore, a potential direction to explore is combining different types of pre\\\\-training objectives, such as chart\\\\-to\\\\-text pre\\\\-training and chart\\\\-table pre\\\\-training goals, to facilitate the model with diverse strengths.\\n\\n\\n\\nWe conduct ablation experiments to validate the effectiveness of chart\\\\-table pre\\\\-training and the pre\\\\-training objectives. We also evaluate the effectiveness of the proposed scene text copy mechanism.\\n\\n\\n\\n####4.2.1 Chart-Table Pre-training\\n\\nWe conduct detailed analyses on the effectiveness of chart\\\\-table pre\\\\-training. First, we measure the performance gain from the chart\\\\-table pre\\\\-training on the full test set of ChartQA data. We then study what type of questions benefit most from the chart\\\\-table pre\\\\-training by picking three subsets of questions that measure different capabilities of the model: (1\\\\) Human\\\\-written questions, (2\\\\) Machine\\\\-generated questions, and (3\\\\) Table covered questions, where the answers can be directly found in the ground truth tables. The results are summarized in Table 4\\\\. From Table 4, we find that after chart\\\\-table pre\\\\-training the model\\'s performance on these three sets of questions is all improved. The most significant gain is obtained on machine\\\\-generated questions, which mainly focus on extractive\\\\-type questions. This indicates that chart\\\\-table pre\\\\-training benefits the model to localize and retrieve the requested information presented on Chart Image. The second biggest gain is achieved on table\\\\-cover questions, where the model demonstrates significant improvement in the capability of chart\\\\-to\\\\-table interpretation.\\n\\n\\n\\n####4.2.2 Pre-training Objectives\\n\\nWe validate the effectiveness of the two pre\\\\-training objectives, Masked Header Prediction and Masked Value Prediction. We remove one pre\\\\-training objective at a time and pre\\\\-train the ChartTSwith only one table prediction task. The pre\\\\-trained model\\n\\n\\n\\n\\\\\\\\begin{table}\\n\\\\\\\\begin{tabular}{l\\\\|c c c} \\\\\\\\hline \\\\\\\\hline \\\\\\\\multirow{2}{*}{Pretraining?} \\\\& \\\\\\\\multicolumn{3}{c}{Question Types} \\\\\\\\ \\\\& Table \\\\& Human \\\\& Augment \\\\\\\\ \\\\\\\\hline No \\\\& 60\\\\.7 \\\\& 30\\\\.8 \\\\& 66\\\\.7 \\\\\\\\ Yes \\\\&* *64\\\\.7* *\\\\&* *31\\\\.8* *\\\\&* *74\\\\.4*\\\\* \\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\n\\\\\\\\end{table}\\nTable 4: Ablation Study on Chart Table Pre\\\\-training with ChartQA Dataset. We report results on three subsets of questions: Table cover questions, human\\\\-written questions, and machine\\\\-generated questions.\\n\\n\\n\\n\\\\\\\\begin{table}\\n\\\\\\\\begin{tabular}{l\\\\|c c c\\\\|c c c} \\\\\\\\hline \\\\\\\\hline \\\\\\\\multirow{2}{*}{Model} \\\\& \\\\\\\\multicolumn{3}{c\\\\|}{Statista} \\\\& \\\\\\\\multicolumn{3}{c}{Pew} \\\\\\\\ \\\\& BLEU \\\\& CS \\\\& CIDER \\\\& BLEU \\\\& CS \\\\& CIDER \\\\\\\\ \\\\\\\\hline T5\\\\-OCR \\\\& 35\\\\.29 \\\\& 73\\\\.77 \\\\& 4\\\\.43 \\\\&* *10\\\\.49* *\\\\& 40\\\\.87 \\\\&* *2\\\\.20* *\\\\\\\\ BART\\\\-OCR \\\\& \\\\- \\\\& \\\\- \\\\& \\\\- \\\\& 9\\\\.09 \\\\& 39\\\\.99 \\\\& 1\\\\.97 \\\\\\\\ T5\\\\-TAB \\\\& 37\\\\.01 \\\\& 75\\\\.72 \\\\&* *4\\\\.68* *\\\\& \\\\- \\\\& \\\\- \\\\& \\\\- \\\\\\\\ BART\\\\-TAB \\\\& 36\\\\.36 \\\\& 77\\\\.14 \\\\& 4\\\\.40 \\\\& \\\\- \\\\& \\\\- \\\\& \\\\- \\\\\\\\ ChartT5 \\\\&* *37\\\\.51* *\\\\&* *82\\\\.16* *\\\\& 3\\\\.45 \\\\& 9\\\\.05 \\\\&* *55\\\\.1*\\\\* \\\\& 1\\\\.23 \\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\n\\\\\\\\end{table}\\nTable 3: Evaluation results on Chart Summarization. We display BLEU, CS and CIDER scores for the Pew and Statista Split. The ground truth table is not available to Pew thus the table\\\\-based method does not have results on Pew split.\\n\\n\\n\\n\\\\\\\\begin{table}\\n\\\\\\\\begin{tabular}{l\\\\|c c c} \\\\\\\\hline \\\\\\\\hline \\\\& \\\\\\\\multicolumn{3}{c}{Question Types} \\\\\\\\ \\\\& Human \\\\& Augment \\\\& Overall \\\\\\\\ \\\\\\\\hline Full \\\\& 31\\\\.8 \\\\& 74\\\\.4 \\\\& 53\\\\.1 \\\\\\\\ \\\\- MVP \\\\& 30\\\\.9 \\\\& 73\\\\.7 \\\\& 52\\\\.3 \\\\\\\\ \\\\- MHP \\\\& 31\\\\.2 \\\\& 68\\\\.3 \\\\& 49\\\\.7 \\\\\\\\ \\\\- STC \\\\& 30\\\\.8 \\\\& 72\\\\.4 \\\\& 51\\\\.6 \\\\\\\\ \\\\\\\\hline \\\\\\\\hline \\\\\\\\end{tabular}\\n\\\\\\\\end{table}\\nTable 5: Ablation Study on the two proposed pre\\\\-training objectives and the Scene Text Copy Mechanism (STC). The first row is the result of the full ChartT5 model. Then we remove one of the pre\\\\-training objectives and the scene\\\\-text\\\\-copy mechanism. We report the results of different ablation experiments on both human and machine\\\\-generated splits as well as the overall performance.\\nis then fine\\\\-tuned and evaluated on the human and augmented split for comparison. The result is displayed in table 5\\\\. As can be seen from the table, removing Masked Value Prediction Loss has a negligible impact on the performance of ChartT5 on ChartQA dataset. There is a slightly more drop in human written questions which suggests that predicting table numerical values still has a miner positive impact on helping the model\\'s mathematical reasoning. Remove Masked Header Prediction have a significant impact on the machine\\\\-generated question\\\\-answering accuracy. As expected, Masked header modeling mainly helps the model learn how to link the scene text to the table headers, which is a critical ability to extract relevant information given a specific query.\\n\\n\\n\\n####4.2.3 Scene Text Copy\\n\\nWe also validate the effectiveness of the scene\\\\-text\\\\-copy mechanism, where we train a ChartT5model by simply representing OCR tokens in their original text format. The model is fine\\\\-tuned and evaluated on the human and augmented split of the chartQA dataset to compare against the full ChartT5\\\\. The result is displayed in Table 5\\\\. Disabling the scene\\\\-text\\\\-copy mechanism leads to a 1\\\\.5(\\\\\\\\%) overall performance drop on ChartQA tasks. Specifically, it leads to more degradation on the augmented split than the human split, as scene\\\\-text\\\\-copy helps enhance the alignment between OCR and table values to benefit accurate information extraction from the chart.\\n\\n\\n\\nWe have manually analyzed model predictions to understand its limitation. We found that our model suffers most from noisy OCR detection and complex question that requires multi\\\\-hop reasoning.\\n\\n\\n\\n**Noisy OCR Prediction.** As an OCR\\\\-based model, ChartT5 often suffers from a wrong OCR detection. An example is shown in Figure 3; the model localizes the right scene text \"1\\\\.18\" to answer the question, but the OCR text is mistakenly detected as \"1:18\". To further understand the limitation of OCR detection, we randomly sample 20K PlotQA test split and compare the performance of our model using detected OCRs against Ground Truth OCRs. We observe a 5(\\\\\\\\%) performance drop when using detected OCRs. We can improve the OCR detector for future work by training on a large Plot scene\\\\-text detection benchmark. Another promising direction is to attempt OCR\\\\-free end\\\\-to\\\\-end plot recognition method like Pix2Struct (Lee et al., 2022\\\\).\\n\\n\\n\\n**Multi\\\\-Hop Reasoning.** Our model is also quite vulnerable to handling complex questions requiring multi\\\\-hop reasoning. An example is shown in Figure 4; the model cannot perform the complex logic reasoning to add the stats of the two smallest bars and compare that to the large bar. We will consider exploring pre\\\\-training on the mathematic reasoning datasets to address this limitation.\\n\\n\\n\\nIn this section, We detailed our experiment setups to evaluate the proposed ChartT5 on two tasks: chart question answering and chart summarization. We then introduce the main results of the two evaluation tasks. Finally, we present the ablation study on chart\\\\-table pre\\\\-training and the two pre\\\\-training objectives.\\n\\n\\n\\n**Chart Question Answering.** Given a chart image and a query question, the goal for the model is to provide an accurate answer string by interpreting the provided chart image. For this task, we consider the ChartQA dataset Masry et al. (2022\\\\), which collects question\\\\-answer pairs on realistic chart images scraped from the internet. Their annotations are collected in two fashions: (1\\\\) Human\\\\-written question\\\\-answer pairs; and (2\\\\) machine\\\\-generated question\\\\-answer pairs derived from the human\\\\-written chart summaries. In total 32\\\\.7K question\\\\-answer pairs are collected on 21\\\\.9K scraped chart images, where about 9\\\\.6K question\\\\-and\\\\-answer pairs are human\\\\-written. Compared to the previously collected CQA datasets, ChartQA is more challenging to handle due to the diverse visual style from the realistic chart images and the complex language from human annotations. Following previous work Masry et al. (2022\\\\); Methani et al. (2020\\\\), we also apply the relaxed accuracy to measure the performance on the CQA task, which allows a minor inaccuracy on numerical value prediction (within 5(\\\\\\\\%) of the gold answer). For non\\\\-numerical answers, the prediction needs to be exactly matched to the gold\\\\-standard answer.\\n\\n\\n\\n**Chart Summarization.** Given a chart image, the target is to summarize the key insights of the chart in natural language. For this task, we evaluate our model on the most recently proposed Chart\\\\-to\\\\-Text benchmark Kantharaj et al. (2022\\\\), which collects roughly 36\\\\.5K chart images with one summary for each image. They split the collected charts into two sets: Statista and Pew, representing the two separate websites from which the chart plots come. The summaries in Statista are human\\\\-written which is well grounded on the chart image. Meanwhile, the summaries from Pew are automatically extracted from the news paragraphs surrounding the chart images. Pew is noisier and more challenging to handle. We follow Kantharaj et al. (2022\\\\) to split the two sets for training and testing. We adopt BLEU\\\\-4, Content Selection, and CIDER as the evaluation metrics to measure the quality of the generated summary following Kantharaj et al. (2022\\\\).\\n\\n\\n\\n**Implementation details.** We initialized our ChartT5 from T5({}\\\\_{\\\\\\\\text{base}}) and pre\\\\-trained on our self\\\\-collected corpus for 30 epochs with a batch size of 60\\\\. We used Adam optimizer Kingma and Ba (2015\\\\) with a linear warm\\\\-up for the first 5(\\\\\\\\%) training steps, and the peak learning rate is set as 1e\\\\-4\\\\. After warming up, a linear decayed learning\\\\-rate scheduler gradually drops the learning rate for the rest of the training steps. The pre\\\\-training experiments are conducted on 2 Nvidia TITAN RTX GPUs, and it roughly takes two days to accomplish the experiment. We kept the last checkpoint of each pre\\\\-training run as our final checkpoint for fine\\\\-tuning.\\n\\n\\n\\nWe also applied warming\\\\-up for downstream fine\\\\-tuning to gradually increase the learning rate to the pick value during the first 5(\\\\\\\\%) of training epochs. After that, a linear decayed learning\\\\-rate scheduler gradually drops the learning rate for the remaining training. For CQA task, we set batch size as 24 and fine\\\\-tune ChartT5 for 60 epochs with a peak learning rate 2e\\\\-4 on 2 Nvidia TITAN RTX GPUs. The best checkpoint was saved as the one that achieves the highest accuracy on the validation\\nsplit. On the CS task, we use batch size 20 and a peak learning rate 5e\\\\-5\\\\. On the Pew split, we fine\\\\-tune ChartT5for 20 epochs, and on Statista, we fine\\\\-tune ChartT5for 25 epochs. The best checkpoint is also saved as achieving the best BLEU score on the validation split. All the reported numbers are one\\\\-time runs.\\n\\n\\n\\n###Main Results\\n\\n###Ablation Study\\n\\n###Qualitative Error Analysis', children=[]),\n",
       " ArxivPaperSection(header='h2', title='5 Conclusion', text=\"We propose ChartT5 to enhance the vision language model's ability to understand chart images via chart\\\\-table pre\\\\-training. The model learns to interpret the masked tables via our proposed masked header prediction and masked value prediction objectives. ChartT5 achieves significant improvement over table\\\\-based non\\\\-pretraining SOTA methods on the ChartQA dataset, especially on the extractive question sets. We also achieve a new SOTA Content Selection Score on the Chart\\\\-to\\\\-text summarization dataset. We conduct comprehensive ablation studies to identify the impact of chart\\\\-table pre\\\\-training, and we find that the proposed pre\\\\-training is extremely helpful to extract accurate\\n\\n\\n\\nFigure 4: An error prediction from our model due to complex multi\\\\-hop reasoning\\n\\n\\n\\nFigure 3: An error prediction from our model due to noisy OCR prediction\\ninformation from the Chart. For future research directions, we believe it may also be meaningful to explore chart understanding under data\\\\-efficient settings Hsu et al. (2022\\\\); Zeng et al. (2023\\\\) and for evidence retrieval tasks Lu et al. (2022\\\\); Ji et al. (2023\\\\).\\n\\n\", children=[]),\n",
       " ArxivPaperSection(header='h2', title='6 Limitations', text=\"Although introducing chart value prediction objective, it only provides minor improvement to the model's performance on doing complex reasoning. There is still a large room to improve the model's capability in math calculation. Our model also suffers from the noisy OCR prediction of off\\\\-the\\\\-shelf object detector, whose performance will depend highly on the extracted OCR text qualities. Another possible limitation of our approach is the quality of the pre\\\\-training data, which only contains synthetic images. Although the proposed model works fairly well on the ChartQA dataset, it is unclear if the improved performance can be generalized to other realistic chart images.\\n\\n\", children=[]),\n",
       " ArxivPaperSection(header='h2', title='7 Ethics Statement', text=\"When we collect the pre\\\\-training dataset, we ensure we respect the intellectual property of dataset sources. All the ChartQA dataset we used for the collection of chart\\\\-table pairs allows public access for research. To ensure the reproducibility of our experiment results, we provide details of the hyperparameter setting in our paper, and we will also publish our code later. Our models can mislead the public's understanding of chart content due to the potential bias from our training corpus. Therefore, we don't recommend using our model for any real\\\\-world decision on chart images.\\n\\n\", children=[]),\n",
       " ArxivPaperSection(header='h2', title='Acknowledgement', text='This research work is supported by U.S DARPA SemaFor Program No. HR001120C0123\\\\. The views and conclusions contained in this work only belong to the authors and should not represent the official policies implied by DARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. We also thank Ahmed Masry and Shankar Kantharaj for providing us with ChartQA and Chart Summary\\\\-related data and baseline model outputs.\\n\\n', children=[])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_case1(toc: TreeOfContents):\n",
    "    \"\"\"Case 1: contents under h1 title\"\"\"\n",
    "\n",
    "def split_case2(document: TreeOfContents):\n",
    "    \"\"\"Case 2: title & contents in same level\"\"\"\n",
    "    sections = []\n",
    "    for child in document.branches:\n",
    "        # p - title, h6 - abstract\n",
    "        if not child.name==\"h2\":\n",
    "            continue\n",
    "        print(\"CHILD:\", child.name, child.string, str(child))\n",
    "        child_text = get_toc_text(child)\n",
    "        print(child_text)\n",
    "        \n",
    "        print('-'*30)\n",
    "        # for subchild in child.expandDescendants(child):\n",
    "        #     print(subchild.name, subchild.string)\n",
    "        #     print(md(str(subchild)))\n",
    "        #     print(vars(subchild).keys())\n",
    "        #     print(len(subchild.contents))\n",
    "        #     print(repr(subchild.contents))\n",
    "        \n",
    "        # section_texts = [\n",
    "            \n",
    "        #     for subchild in child.expandDescendants(child)\n",
    "        # ]\n",
    "        section = ArxivPaperSection(\n",
    "            header = child.name,\n",
    "            title = child.string,\n",
    "            text = child_text\n",
    "        )\n",
    "        sections.append(section)\n",
    "    return sections\n",
    "\n",
    "def split_markdown_text(text: str) -> List[ArxivPaperSection]:\n",
    "    toc = md2py(text) # md -> html -> bs4\n",
    "    print(vars(toc).keys())\n",
    "    print(\"TOC\", toc.name)\n",
    "    l1_branches = toc.branches\n",
    "    print(len(l1_branches))\n",
    "    \n",
    "    sections = []\n",
    "    ## Case 1 (h1 title)\n",
    "    if len(l1_branches)==1 and l1_branches[0].name==\"h1\":\n",
    "        print(l1_branches[0])\n",
    "        pass\n",
    "    ## Case2\n",
    "    elif len(l1_branches)>1:\n",
    "        sections = split_case2(toc)\n",
    "        \n",
    "    return sections\n",
    "    \n",
    "idx = 0 ## Case 1\n",
    "idx = 14 ## Case 2\n",
    "sample = df.iloc[idx]['markdown']\n",
    "split_markdown_text(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arxiv_paper(row):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
