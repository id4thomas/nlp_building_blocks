{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "\n",
    "class EnvSettings(BaseSettings):\n",
    "    model_config = SettingsConfigDict(\n",
    "        env_file=\"../.env\", env_file_encoding=\"utf-8\", extra=\"ignore\"\n",
    "    )\n",
    "    embedding_base_url: str\n",
    "    embedding_api_key: str\n",
    "    embedding_model: str\n",
    "    embedding_model_dir: str\n",
    "    \n",
    "    sample_data_dir: str\n",
    "    pipeline_src_dir: str\n",
    "settings = EnvSettings()\n",
    "\n",
    "import sys\n",
    "sys.path.append(settings.pipeline_src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pgvector_llamaindex\n"
     ]
    }
   ],
   "source": [
    "class DBSettings(BaseSettings):\n",
    "    model_config = SettingsConfigDict(\n",
    "        env_file=\"database/.env\", env_file_encoding=\"utf-8\", extra=\"ignore\"\n",
    "    )\n",
    "    postgres_user: str\n",
    "    postgres_password: str\n",
    "    postgres_db: str\n",
    "    postgres_url: str\n",
    "    postgres_port: str\n",
    "\n",
    "db_settings = DBSettings()\n",
    "print(db_settings.postgres_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 7) Index(['id', 'title', 'abstract', 'authors', 'published_date', 'link',\n",
      "       'markdown'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## Load Sample\n",
    "df = pd.read_parquet(settings.sample_data_dir)\n",
    "df = df.sample(100)\n",
    "print(df.shape, df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Embedding & Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.text_embeddings_inference import (\n",
    "    TextEmbeddingsInference,\n",
    ")\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "from sqlalchemy import make_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.llamaindex.ai/en/stable/examples/embeddings/text_embedding_inference/\n",
    "embed_model = TextEmbeddingsInference(\n",
    "    model_name=settings.embedding_model,\n",
    "    base_url=settings.embedding_base_url,\n",
    "    timeout=60,\n",
    "    embed_batch_size=10,\n",
    ")\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB: pgvector_llamaindex\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "# connection_string = \"postgresql://{}:{}@localhost:{}/{}\".format(\n",
    "connection_string = \"postgresql://{}:{}@localhost:{}/{}\".format(\n",
    "    db_settings.postgres_user,\n",
    "    db_settings.postgres_password,\n",
    "    db_settings.postgres_port,\n",
    "    db_settings.postgres_db\n",
    ")\n",
    "\n",
    "db_name = db_settings.postgres_db\n",
    "print(f\"DB: {db_name}\")\n",
    "conn = psycopg2.connect(connection_string)\n",
    "conn.autocommit=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize vector store instance\n",
    "url = make_url(connection_string)\n",
    "\n",
    "## hnsw indexing config\n",
    "hnsw_config = {\n",
    "    \"hnsw_m\": 16,\n",
    "    \"hnsw_ef_construction\": 64,\n",
    "    \"hnsw_ef_search\": 40,\n",
    "    \"hnsw_dist_method\": \"vector_cosine_ops\",\n",
    "}\n",
    "abstract_vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=url.host,\n",
    "    password=url.password,\n",
    "    port=url.port,\n",
    "    user=url.username,\n",
    "    table_name=\"paper_abstract\",\n",
    "    embed_dim=1024,  #bge-m3\n",
    "    hnsw_kwargs=hnsw_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstact_storage_index = VectorStoreIndex.from_vector_store(abstract_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic Retriever\n",
    "retriever_args = {\"similarity_top_k\": 10}\n",
    "retriever = abstact_storage_index.as_retriever(filters=[], **retriever_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='7eb94f34-af50-4f8f-967a-6b779229ef2d', embedding=None, metadata={'paper_information_id': 4, 'published_date': '2023-08-25 19:35:58'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7eb5f76f-a9a4-43ed-9b14-77f26e097a9e', node_type='4', metadata={'paper_information_id': 4, 'published_date': '2023-08-25 19:35:58'}, hash='302d9152de24a6f6fdac54b594c9f83f506776238daab61b8ab92d02a0bd1ec4')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Title: Emulating Radiative Transfer with Artificial Neural Networks\\nAbstract:\\nForward-modeling observables from galaxy simulations enables direct\\ncomparisons between theory and observations. To generate synthetic spectral\\nenergy distributions (SEDs) that include dust absorption, re-emission, and\\nscattering, Monte Carlo radiative transfer is often used in post-processing on\\na galaxy-by-galaxy basis. However, this is computationally expensive,\\nespecially if one wants to make predictions for suites of many cosmological\\nsimulations. To alleviate this computational burden, we have developed a\\nradiative transfer emulator using an artificial neural network (ANN),\\nANNgelina, that can reliably predict SEDs of simulated galaxies using a small\\nnumber of integrated properties of the simulated galaxies: star formation rate,\\nstellar and dust masses, and mass-weighted metallicities of all star particles\\nand of only star particles with age <10 Myr. Here, we present the methodology\\nand quantify the accuracy of the predictions. We train the ANN on SEDs computed\\nfor galaxies from the IllustrisTNG project's TNG50 cosmological\\nmagnetohydrodynamical simulation. ANNgelina is able to predict the SEDs of\\nTNG50 galaxies in the ultraviolet (UV) to millimetre regime with a typical\\nmedian absolute error of ~7 per cent. The prediction error is the greatest in\\nthe UV, possibly due to the viewing-angle dependence being greatest in this\\nwavelength regime. Our results demonstrate that our ANN-based emulator is a\\npromising computationally inexpensive alternative for forward-modeling galaxy\\nSEDs from cosmological simulations.\", mimetype='text/plain', start_char_idx=0, end_char_idx=1617, metadata_seperator='\\n', text_template='{content}'), score=0.5374557331257223),\n",
       " NodeWithScore(node=TextNode(id_='e6c7a6b1-4d05-4cb4-8b30-4994abdba8bd', embedding=None, metadata={'paper_information_id': 97, 'published_date': '2023-10-01 18:50:29'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8688dfa5-a460-4d35-a1bc-f772479a0059', node_type='4', metadata={'paper_information_id': 97, 'published_date': '2023-10-01 18:50:29'}, hash='8f4eb7018b11d633f9d7d4b27213989723622bcca6297ac6fa839c09f0f127e6')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Title: Counterfactual Image Generation for adversarially robust and\\n  interpretable Classifiers\\nAbstract:\\nNeural Image Classifiers are effective but inherently hard to interpret and\\nsusceptible to adversarial attacks. Solutions to both problems exist, among\\nothers, in the form of counterfactual examples generation to enhance\\nexplainability or adversarially augment training datasets for improved\\nrobustness. However, existing methods exclusively address only one of the\\nissues. We propose a unified framework leveraging image-to-image translation\\nGenerative Adversarial Networks (GANs) to produce counterfactual samples that\\nhighlight salient regions for interpretability and act as adversarial samples\\nto augment the dataset for more robustness. This is achieved by combining the\\nclassifier and discriminator into a single model that attributes real images to\\ntheir respective classes and flags generated images as \"fake\". We assess the\\nmethod\\'s effectiveness by evaluating (i) the produced explainability masks on a\\nsemantic segmentation task for concrete cracks and (ii) the model\\'s resilience\\nagainst the Projected Gradient Descent (PGD) attack on a fruit defects\\ndetection problem. Our produced saliency maps are highly descriptive, achieving\\ncompetitive IoU values compared to classical segmentation models despite being\\ntrained exclusively on classification labels. Furthermore, the model exhibits\\nimproved robustness to adversarial attacks, and we show how the discriminator\\'s\\n\"fakeness\" value serves as an uncertainty measure of the predictions.', mimetype='text/plain', start_char_idx=0, end_char_idx=1556, metadata_seperator='\\n', text_template='{content}'), score=0.5299918967480607),\n",
       " NodeWithScore(node=TextNode(id_='a083c017-d3e6-401b-8902-43eebe40ff63', embedding=None, metadata={'paper_information_id': 17, 'published_date': '2023-07-17 11:55:20'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d679e788-c88e-46e0-b8d9-611e1a0bf7d9', node_type='4', metadata={'paper_information_id': 17, 'published_date': '2023-07-17 11:55:20'}, hash='ff7a0f23deeaf8b91c6741e78511203f9b6792a813635ad63d303c8d1826b4fb')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Title: Active Learning for Object Detection with Non-Redundant Informative\\n  Sampling\\nAbstract:\\nCurating an informative and representative dataset is essential for enhancing\\nthe performance of 2D object detectors. We present a novel active learning\\nsampling strategy that addresses both the informativeness and diversity of the\\nselections. Our strategy integrates uncertainty and diversity-based selection\\nprinciples into a joint selection objective by measuring the collective\\ninformation score of the selected samples. Specifically, our proposed NORIS\\nalgorithm quantifies the impact of training with a sample on the\\ninformativeness of other similar samples. By exclusively selecting samples that\\nare simultaneously informative and distant from other highly informative\\nsamples, we effectively avoid redundancy while maintaining a high level of\\ninformativeness. Moreover, instead of utilizing whole image features to\\ncalculate distances between samples, we leverage features extracted from\\ndetected object regions within images to define object features. This allows us\\nto construct a dataset encompassing diverse object types, shapes, and angles.\\nExtensive experiments on object detection and image classification tasks\\ndemonstrate the effectiveness of our strategy over the state-of-the-art\\nbaselines. Specifically, our selection strategy achieves a 20% and 30%\\nreduction in labeling costs compared to random selection for PASCAL-VOC and\\nKITTI, respectively.', mimetype='text/plain', start_char_idx=0, end_char_idx=1462, metadata_seperator='\\n', text_template='{content}'), score=0.5222774147987366),\n",
       " NodeWithScore(node=TextNode(id_='c1af34c3-7b02-4370-b4f3-45ae41767851', embedding=None, metadata={'paper_information_id': 1, 'published_date': '2023-08-22 21:03:58'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c1b63ced-5357-40cd-92c4-1ae8bdc4894f', node_type='4', metadata={'paper_information_id': 1, 'published_date': '2023-08-22 21:03:58'}, hash='4b3632450e2ce00867ebb5349206fedd1ddd0f7be86c498e79267cbfe457d166')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Title: An extensible point-based method for data chart value detection\\nAbstract:\\nWe present an extensible method for identifying semantic points to reverse\\nengineer (i.e. extract the values of) data charts, particularly those in\\nscientific articles. Our method uses a point proposal network (akin to region\\nproposal networks for object detection) to directly predict the position of\\npoints of interest in a chart, and it is readily extensible to multiple chart\\ntypes and chart elements. We focus on complex bar charts in the scientific\\nliterature, on which our model is able to detect salient points with an\\naccuracy of 0.8705 F1 (@1.5-cell max deviation); it achieves 0.9810 F1 on\\nsynthetically-generated charts similar to those used in prior works. We also\\nexplore training exclusively on synthetic data with novel augmentations,\\nreaching surprisingly competent performance in this way (0.6621 F1) on real\\ncharts with widely varying appearance, and we further demonstrate our unchanged\\nmethod applied directly to synthetic pie charts (0.8343 F1). Datasets, trained\\nmodels, and evaluation code are available at\\nhttps://github.com/BNLNLP/PPN_model.', mimetype='text/plain', start_char_idx=0, end_char_idx=1148, metadata_seperator='\\n', text_template='{content}'), score=0.5082253672086051),\n",
       " NodeWithScore(node=TextNode(id_='f6d03f51-fda1-436f-b00f-6c91810e84d4', embedding=None, metadata={'paper_information_id': 86, 'published_date': '2023-04-22 09:35:51'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='40480af8-e670-44bb-9ca4-9e81517311d9', node_type='4', metadata={'paper_information_id': 86, 'published_date': '2023-04-22 09:35:51'}, hash='23c749497506a840f159b0063ea9595e45d0926e92bd314f81895962b867d157')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Title: Learning Symbolic Representations Through Joint GEnerative and\\n  DIscriminative Training\\nAbstract:\\nWe introduce GEDI, a Bayesian framework that combines existing\\nself-supervised learning objectives with likelihood-based generative models.\\nThis framework leverages the benefits of both GEnerative and DIscriminative\\napproaches, resulting in improved symbolic representations over standalone\\nsolutions. Additionally, GEDI can be easily integrated and trained jointly with\\nexisting neuro-symbolic frameworks without the need for additional supervision\\nor costly pre-training steps. We demonstrate through experiments on real-world\\ndata, including SVHN, CIFAR10, and CIFAR100, that GEDI outperforms existing\\nself-supervised learning strategies in terms of clustering performance by a\\nsignificant margin. The symbolic component further allows it to leverage\\nknowledge in the form of logical constraints to improve performance in the\\nsmall data regime.', mimetype='text/plain', start_char_idx=0, end_char_idx=953, metadata_seperator='\\n', text_template='{content}'), score=0.5052429732535506),\n",
       " NodeWithScore(node=TextNode(id_='fb4a47f8-0ced-4e26-884a-de489d72f62e', embedding=None, metadata={'paper_information_id': 13, 'published_date': '2023-06-27 02:46:08'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d264db76-ab75-4a3a-a64f-fe1a0cc8d8d3', node_type='4', metadata={'paper_information_id': 13, 'published_date': '2023-06-27 02:46:08'}, hash='74dd6f38c4509f52b9fbde9a775a6cb4495195be8e65282b8386d54762c9c240')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Title: DSRM: Boost Textual Adversarial Training with Distribution Shift Risk\\n  Minimization\\nAbstract:\\nAdversarial training is one of the best-performing methods in improving the\\nrobustness of deep language models. However, robust models come at the cost of\\nhigh time consumption, as they require multi-step gradient ascents or word\\nsubstitutions to obtain adversarial samples. In addition, these generated\\nsamples are deficient in grammatical quality and semantic consistency, which\\nimpairs the effectiveness of adversarial training. To address these problems,\\nwe introduce a novel, effective procedure for instead adversarial training with\\nonly clean data. Our procedure, distribution shift risk minimization (DSRM),\\nestimates the adversarial loss by perturbing the input data's probability\\ndistribution rather than their embeddings. This formulation results in a robust\\nmodel that minimizes the expected global loss under adversarial attacks. Our\\napproach requires zero adversarial samples for training and reduces time\\nconsumption by up to 70\\\\% compared to current best-performing adversarial\\ntraining methods. Experiments demonstrate that DSRM considerably improves\\nBERT's resistance to textual adversarial attacks and achieves state-of-the-art\\nrobust accuracy on various benchmarks.\", mimetype='text/plain', start_char_idx=0, end_char_idx=1287, metadata_seperator='\\n', text_template='{content}'), score=0.5021710395812988),\n",
       " NodeWithScore(node=TextNode(id_='e55f77d6-4eb4-428a-ae93-7c0351f176e5', embedding=None, metadata={'paper_information_id': 47, 'published_date': '2023-04-24 13:24:00'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b3d61df3-d05b-4ce9-96eb-be1b693a666b', node_type='4', metadata={'paper_information_id': 47, 'published_date': '2023-04-24 13:24:00'}, hash='0e7488ad3314419c00b7a417d30f25a43e1b470204106479e236c36c48166d4e')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Title: Quality-Diversity Optimisation on a Physical Robot Through\\n  Dynamics-Aware and Reset-Free Learning\\nAbstract:\\nLearning algorithms, like Quality-Diversity (QD), can be used to acquire\\nrepertoires of diverse robotics skills. This learning is commonly done via\\ncomputer simulation due to the large number of evaluations required. However,\\ntraining in a virtual environment generates a gap between simulation and\\nreality. Here, we build upon the Reset-Free QD (RF-QD) algorithm to learn\\ncontrollers directly on a physical robot. This method uses a dynamics model,\\nlearned from interactions between the robot and the environment, to predict the\\nrobot's behaviour and improve sample efficiency. A behaviour selection policy\\nfilters out uninteresting or unsafe policies predicted by the model. RF-QD also\\nincludes a recovery policy that returns the robot to a safe zone when it has\\nwalked outside of it, allowing continuous learning. We demonstrate that our\\nmethod enables a physical quadruped robot to learn a repertoire of behaviours\\nin two hours without human supervision. We successfully test the solution\\nrepertoire using a maze navigation task. Finally, we compare our approach to\\nthe MAP-Elites algorithm. We show that dynamics awareness and a recovery policy\\nare required for training on a physical robot for optimal archive generation.\\nVideo available at https://youtu.be/BgGNvIsRh7Q\", mimetype='text/plain', start_char_idx=0, end_char_idx=1392, metadata_seperator='\\n', text_template='{content}'), score=0.501980721950531),\n",
       " NodeWithScore(node=TextNode(id_='dacb3972-b204-41a5-9379-94a6ef891f6f', embedding=None, metadata={'paper_information_id': 53, 'published_date': '2023-06-18 15:50:57'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='acb9733f-8483-4ac1-9ff7-687107e0ff3e', node_type='4', metadata={'paper_information_id': 53, 'published_date': '2023-06-18 15:50:57'}, hash='f38cdada5440720f18f6731dcf7ac5dbd9aecf49749fbb67bfddab3074f89c90')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Title: Acceleration in Policy Optimization\\nAbstract:\\nWe work towards a unifying paradigm for accelerating policy optimization\\nmethods in reinforcement learning (RL) by integrating foresight in the policy\\nimprovement step via optimistic and adaptive updates. Leveraging the connection\\nbetween policy iteration and policy gradient methods, we view policy\\noptimization algorithms as iteratively solving a sequence of surrogate\\nobjectives, local lower bounds on the original objective. We define optimism as\\npredictive modelling of the future behavior of a policy, and adaptivity as\\ntaking immediate and anticipatory corrective actions to mitigate accumulating\\nerrors from overshooting predictions or delayed responses to change. We use\\nthis shared lens to jointly express other well-known algorithms, including\\nmodel-based policy improvement based on forward search, and optimistic\\nmeta-learning algorithms. We analyze properties of this formulation, and show\\nconnections to other accelerated optimization algorithms. Then, we design an\\noptimistic policy gradient algorithm, adaptive via meta-gradient learning, and\\nempirically highlight several design choices pertaining to acceleration, in an\\nillustrative task.', mimetype='text/plain', start_char_idx=0, end_char_idx=1210, metadata_seperator='\\n', text_template='{content}'), score=0.4949085718502352),\n",
       " NodeWithScore(node=TextNode(id_='72d27886-3bf5-4d30-a3a6-52f31e59c3ae', embedding=None, metadata={'paper_information_id': 14, 'published_date': '2023-09-20 14:59:06'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0ad9984a-ba0c-4154-a12f-e37b9b743750', node_type='4', metadata={'paper_information_id': 14, 'published_date': '2023-09-20 14:59:06'}, hash='29cb1b4acec8fd4ff2fa94822c1848e0eeb8bae6cccaa8dc50eb917d032fb240')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Title: Incremental Blockwise Beam Search for Simultaneous Speech Translation\\n  with Controllable Quality-Latency Tradeoff\\nAbstract:\\nBlockwise self-attentional encoder models have recently emerged as one\\npromising end-to-end approach to simultaneous speech translation. These models\\nemploy a blockwise beam search with hypothesis reliability scoring to determine\\nwhen to wait for more input speech before translating further. However, this\\nmethod maintains multiple hypotheses until the entire speech input is consumed\\n-- this scheme cannot directly show a single \\\\textit{incremental} translation\\nto users. Further, this method lacks mechanisms for \\\\textit{controlling} the\\nquality vs. latency tradeoff. We propose a modified incremental blockwise beam\\nsearch incorporating local agreement or hold-$n$ policies for quality-latency\\ncontrol. We apply our framework to models trained for online or offline\\ntranslation and demonstrate that both types can be effectively used in online\\nmode.\\n  Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing\\nlatency or 0.8-1.4 s latency improvement without changing quality.', mimetype='text/plain', start_char_idx=0, end_char_idx=1132, metadata_seperator='\\n', text_template='{content}'), score=0.4947289824485779),\n",
       " NodeWithScore(node=TextNode(id_='7b463ee3-bbc5-48ce-9a38-6603166c564c', embedding=None, metadata={'paper_information_id': 24, 'published_date': '2023-02-20 21:05:17'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0a7a25d9-8d9c-4d5d-aa08-ba715c2d3671', node_type='4', metadata={'paper_information_id': 24, 'published_date': '2023-02-20 21:05:17'}, hash='0583ae52c3435040c895d81d1024f2c629751495d379673bcab4f9940fda1d75')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Title: Hadamard Layer to Improve Semantic Segmentation\\nAbstract:\\nThe Hadamard Layer, a simple and computationally efficient way to improve\\nresults in semantic segmentation tasks, is presented. This layer has no free\\nparameters that require to be trained. Therefore it does not increase the\\nnumber of model parameters, and the extra computational cost is marginal.\\nExperimental results show that the new Hadamard layer substantially improves\\nthe performance of the investigated models (variants of the Pix2Pix model). The\\nperformance's improvement can be explained by the Hadamard layer forcing the\\nnetwork to produce an internal encoding of the classes so that all bins are\\nactive. Therefore, the network computation is more distributed. In a sort that\\nthe Hadamard layer requires that to change the predicted class, it is necessary\\nto modify $2^{k-1}$ bins, assuming $k$ bins in the encoding. A specific loss\\nfunction allows a stable and fast training convergence.\", mimetype='text/plain', start_char_idx=0, end_char_idx=965, metadata_seperator='\\n', text_template='{content}'), score=0.48583659614106534)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Simple query\n",
    "query = \"Retrieval Augmented Generation\"\n",
    "nodes = retriever.retrieve(query)\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 0 - 0.537\n",
      "4 Subsurface cosmogenic and radiogenic production of ^{42}Ar 2023-08-25 19:35:58\n",
      "Radioactive decays from ^{42}Ar and its progeny ^{42}K are potential\n",
      "background sources in large-sca\n",
      "------------------------------\n",
      "Result 1 - 0.530\n",
      "97 Vector Bundles over non-Hausdorff Manifolds 2023-10-01 18:50:29\n",
      "In this paper we generalise the theory of real vector bundles to a certain\n",
      "class of non-Hausdorff ma\n",
      "------------------------------\n",
      "Result 2 - 0.522\n",
      "17 Nonequilibrium Seebeck and spin Seebeck effects in nanoscale junctions 2023-07-17 11:55:20\n",
      "The spin-resolved thermoelectric transport properties of correlated nanoscale\n",
      "junctions, consisting \n",
      "------------------------------\n",
      "Result 3 - 0.508\n",
      "1 Characterizing and correcting electron and hole trapping in germanium\n",
      "  cross-strip detectors 2023-08-22 21:03:58\n",
      "We present measurements of electron and hole trapping in three COSI germanium\n",
      "cross-strip detectors.\n",
      "------------------------------\n",
      "Result 4 - 0.505\n",
      "86 Incorporating Nonlocal Traffic Flow Model in Physics-informed Neural\n",
      "  Networks 2023-04-22 09:35:51\n",
      "This research contributes to the advancement of traffic state estimation\n",
      "methods by leveraging the b\n",
      "------------------------------\n",
      "Result 5 - 0.502\n",
      "13 Synthesizing and multiplexing autonomous quantum coherences 2023-06-27 02:46:08\n",
      "Quantum coherence is a crucial prerequisite for quantum technologies.\n",
      "Therefore, the robust generati\n",
      "------------------------------\n",
      "Result 6 - 0.502\n",
      "47 Use of a Socially Assistive Robot as a Online Shopping Digital Skills\n",
      "  Assistan 2023-04-24 13:24:00\n",
      "This work proposes and analyses the application of a robotic platform as an\n",
      "digital skills assistant\n",
      "------------------------------\n",
      "Result 7 - 0.495\n",
      "53 A thin plate approximation for ocean wave interactions with an ice shelf 2023-06-18 15:50:57\n",
      "A variational principle is proposed to derive the governing equations for the\n",
      "problem of ocean wave \n",
      "------------------------------\n",
      "Result 8 - 0.495\n",
      "14 Tunable optical multistability induced by a single cavity mode in cavity\n",
      "  quantum electrodynamics system 2023-09-20 14:59:06\n",
      "A tunable optical multistability scheme based on a single cavity mode coupled\n",
      "with two separate atom\n",
      "------------------------------\n",
      "Result 9 - 0.486\n",
      "24 Finite State Automata Design using 1T1R ReRAM Crossbar 2023-02-20 21:05:17\n",
      "Data movement costs constitute a significant bottleneck in modern machine\n",
      "learning (ML) systems. Whe\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, node in enumerate(nodes):\n",
    "    ## Score\n",
    "    score = node.score\n",
    "    ## Source\n",
    "    paper_info_id = node.metadata['paper_information_id']\n",
    "    title = df.iloc[paper_info_id][\"title\"]\n",
    "    abstract = df.iloc[paper_info_id][\"abstract\"]\n",
    "    \n",
    "    published_date = node.metadata[\"published_date\"]\n",
    "    print(\"Result {} - {:.3f}\".format(i, score))\n",
    "    print(paper_info_id, title, published_date)\n",
    "    print(abstract[:100])\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Metadata Filtering\n",
    "from llama_index.core.vector_stores.types import (\n",
    "    MetadataFilter,\n",
    "    MetadataFilters,\n",
    ")\n",
    "retriever_args = {\"similarity_top_k\": 10}\n",
    "\n",
    "## Filter documents published between 23.05.01 ~ 23.12.31\n",
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(\n",
    "            key=\"published_date\", value=\"2023-05-01\", operator=\">=\"\n",
    "        ),\n",
    "        MetadataFilter(\n",
    "            key=\"published_date\", value=\"2023-12-31\", operator=\"<=\"\n",
    "        ),\n",
    "    ],\n",
    "    condition=\"and\",\n",
    ")\n",
    "\n",
    "\n",
    "retriever = abstact_storage_index.as_retriever(filters=filters, **retriever_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='7eb94f34-af50-4f8f-967a-6b779229ef2d', embedding=None, metadata={'paper_information_id': 4, 'published_date': '2023-08-25 19:35:58'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7eb5f76f-a9a4-43ed-9b14-77f26e097a9e', node_type='4', metadata={'paper_information_id': 4, 'published_date': '2023-08-25 19:35:58'}, hash='302d9152de24a6f6fdac54b594c9f83f506776238daab61b8ab92d02a0bd1ec4')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Title: Emulating Radiative Transfer with Artificial Neural Networks\\nAbstract:\\nForward-modeling observables from galaxy simulations enables direct\\ncomparisons between theory and observations. To generate synthetic spectral\\nenergy distributions (SEDs) that include dust absorption, re-emission, and\\nscattering, Monte Carlo radiative transfer is often used in post-processing on\\na galaxy-by-galaxy basis. However, this is computationally expensive,\\nespecially if one wants to make predictions for suites of many cosmological\\nsimulations. To alleviate this computational burden, we have developed a\\nradiative transfer emulator using an artificial neural network (ANN),\\nANNgelina, that can reliably predict SEDs of simulated galaxies using a small\\nnumber of integrated properties of the simulated galaxies: star formation rate,\\nstellar and dust masses, and mass-weighted metallicities of all star particles\\nand of only star particles with age <10 Myr. Here, we present the methodology\\nand quantify the accuracy of the predictions. We train the ANN on SEDs computed\\nfor galaxies from the IllustrisTNG project's TNG50 cosmological\\nmagnetohydrodynamical simulation. ANNgelina is able to predict the SEDs of\\nTNG50 galaxies in the ultraviolet (UV) to millimetre regime with a typical\\nmedian absolute error of ~7 per cent. The prediction error is the greatest in\\nthe UV, possibly due to the viewing-angle dependence being greatest in this\\nwavelength regime. Our results demonstrate that our ANN-based emulator is a\\npromising computationally inexpensive alternative for forward-modeling galaxy\\nSEDs from cosmological simulations.\", mimetype='text/plain', start_char_idx=0, end_char_idx=1617, metadata_seperator='\\n', text_template='{content}'), score=0.5374557331257223),\n",
       " NodeWithScore(node=TextNode(id_='e6c7a6b1-4d05-4cb4-8b30-4994abdba8bd', embedding=None, metadata={'paper_information_id': 97, 'published_date': '2023-10-01 18:50:29'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='8688dfa5-a460-4d35-a1bc-f772479a0059', node_type='4', metadata={'paper_information_id': 97, 'published_date': '2023-10-01 18:50:29'}, hash='8f4eb7018b11d633f9d7d4b27213989723622bcca6297ac6fa839c09f0f127e6')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Title: Counterfactual Image Generation for adversarially robust and\\n  interpretable Classifiers\\nAbstract:\\nNeural Image Classifiers are effective but inherently hard to interpret and\\nsusceptible to adversarial attacks. Solutions to both problems exist, among\\nothers, in the form of counterfactual examples generation to enhance\\nexplainability or adversarially augment training datasets for improved\\nrobustness. However, existing methods exclusively address only one of the\\nissues. We propose a unified framework leveraging image-to-image translation\\nGenerative Adversarial Networks (GANs) to produce counterfactual samples that\\nhighlight salient regions for interpretability and act as adversarial samples\\nto augment the dataset for more robustness. This is achieved by combining the\\nclassifier and discriminator into a single model that attributes real images to\\ntheir respective classes and flags generated images as \"fake\". We assess the\\nmethod\\'s effectiveness by evaluating (i) the produced explainability masks on a\\nsemantic segmentation task for concrete cracks and (ii) the model\\'s resilience\\nagainst the Projected Gradient Descent (PGD) attack on a fruit defects\\ndetection problem. Our produced saliency maps are highly descriptive, achieving\\ncompetitive IoU values compared to classical segmentation models despite being\\ntrained exclusively on classification labels. Furthermore, the model exhibits\\nimproved robustness to adversarial attacks, and we show how the discriminator\\'s\\n\"fakeness\" value serves as an uncertainty measure of the predictions.', mimetype='text/plain', start_char_idx=0, end_char_idx=1556, metadata_seperator='\\n', text_template='{content}'), score=0.5299918967480607),\n",
       " NodeWithScore(node=TextNode(id_='a083c017-d3e6-401b-8902-43eebe40ff63', embedding=None, metadata={'paper_information_id': 17, 'published_date': '2023-07-17 11:55:20'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d679e788-c88e-46e0-b8d9-611e1a0bf7d9', node_type='4', metadata={'paper_information_id': 17, 'published_date': '2023-07-17 11:55:20'}, hash='ff7a0f23deeaf8b91c6741e78511203f9b6792a813635ad63d303c8d1826b4fb')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Title: Active Learning for Object Detection with Non-Redundant Informative\\n  Sampling\\nAbstract:\\nCurating an informative and representative dataset is essential for enhancing\\nthe performance of 2D object detectors. We present a novel active learning\\nsampling strategy that addresses both the informativeness and diversity of the\\nselections. Our strategy integrates uncertainty and diversity-based selection\\nprinciples into a joint selection objective by measuring the collective\\ninformation score of the selected samples. Specifically, our proposed NORIS\\nalgorithm quantifies the impact of training with a sample on the\\ninformativeness of other similar samples. By exclusively selecting samples that\\nare simultaneously informative and distant from other highly informative\\nsamples, we effectively avoid redundancy while maintaining a high level of\\ninformativeness. Moreover, instead of utilizing whole image features to\\ncalculate distances between samples, we leverage features extracted from\\ndetected object regions within images to define object features. This allows us\\nto construct a dataset encompassing diverse object types, shapes, and angles.\\nExtensive experiments on object detection and image classification tasks\\ndemonstrate the effectiveness of our strategy over the state-of-the-art\\nbaselines. Specifically, our selection strategy achieves a 20% and 30%\\nreduction in labeling costs compared to random selection for PASCAL-VOC and\\nKITTI, respectively.', mimetype='text/plain', start_char_idx=0, end_char_idx=1462, metadata_seperator='\\n', text_template='{content}'), score=0.5222774147987366),\n",
       " NodeWithScore(node=TextNode(id_='c1af34c3-7b02-4370-b4f3-45ae41767851', embedding=None, metadata={'paper_information_id': 1, 'published_date': '2023-08-22 21:03:58'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c1b63ced-5357-40cd-92c4-1ae8bdc4894f', node_type='4', metadata={'paper_information_id': 1, 'published_date': '2023-08-22 21:03:58'}, hash='4b3632450e2ce00867ebb5349206fedd1ddd0f7be86c498e79267cbfe457d166')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Title: An extensible point-based method for data chart value detection\\nAbstract:\\nWe present an extensible method for identifying semantic points to reverse\\nengineer (i.e. extract the values of) data charts, particularly those in\\nscientific articles. Our method uses a point proposal network (akin to region\\nproposal networks for object detection) to directly predict the position of\\npoints of interest in a chart, and it is readily extensible to multiple chart\\ntypes and chart elements. We focus on complex bar charts in the scientific\\nliterature, on which our model is able to detect salient points with an\\naccuracy of 0.8705 F1 (@1.5-cell max deviation); it achieves 0.9810 F1 on\\nsynthetically-generated charts similar to those used in prior works. We also\\nexplore training exclusively on synthetic data with novel augmentations,\\nreaching surprisingly competent performance in this way (0.6621 F1) on real\\ncharts with widely varying appearance, and we further demonstrate our unchanged\\nmethod applied directly to synthetic pie charts (0.8343 F1). Datasets, trained\\nmodels, and evaluation code are available at\\nhttps://github.com/BNLNLP/PPN_model.', mimetype='text/plain', start_char_idx=0, end_char_idx=1148, metadata_seperator='\\n', text_template='{content}'), score=0.5082253672086051),\n",
       " NodeWithScore(node=TextNode(id_='fb4a47f8-0ced-4e26-884a-de489d72f62e', embedding=None, metadata={'paper_information_id': 13, 'published_date': '2023-06-27 02:46:08'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d264db76-ab75-4a3a-a64f-fe1a0cc8d8d3', node_type='4', metadata={'paper_information_id': 13, 'published_date': '2023-06-27 02:46:08'}, hash='74dd6f38c4509f52b9fbde9a775a6cb4495195be8e65282b8386d54762c9c240')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Title: DSRM: Boost Textual Adversarial Training with Distribution Shift Risk\\n  Minimization\\nAbstract:\\nAdversarial training is one of the best-performing methods in improving the\\nrobustness of deep language models. However, robust models come at the cost of\\nhigh time consumption, as they require multi-step gradient ascents or word\\nsubstitutions to obtain adversarial samples. In addition, these generated\\nsamples are deficient in grammatical quality and semantic consistency, which\\nimpairs the effectiveness of adversarial training. To address these problems,\\nwe introduce a novel, effective procedure for instead adversarial training with\\nonly clean data. Our procedure, distribution shift risk minimization (DSRM),\\nestimates the adversarial loss by perturbing the input data's probability\\ndistribution rather than their embeddings. This formulation results in a robust\\nmodel that minimizes the expected global loss under adversarial attacks. Our\\napproach requires zero adversarial samples for training and reduces time\\nconsumption by up to 70\\\\% compared to current best-performing adversarial\\ntraining methods. Experiments demonstrate that DSRM considerably improves\\nBERT's resistance to textual adversarial attacks and achieves state-of-the-art\\nrobust accuracy on various benchmarks.\", mimetype='text/plain', start_char_idx=0, end_char_idx=1287, metadata_seperator='\\n', text_template='{content}'), score=0.5021710395812988),\n",
       " NodeWithScore(node=TextNode(id_='dacb3972-b204-41a5-9379-94a6ef891f6f', embedding=None, metadata={'paper_information_id': 53, 'published_date': '2023-06-18 15:50:57'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='acb9733f-8483-4ac1-9ff7-687107e0ff3e', node_type='4', metadata={'paper_information_id': 53, 'published_date': '2023-06-18 15:50:57'}, hash='f38cdada5440720f18f6731dcf7ac5dbd9aecf49749fbb67bfddab3074f89c90')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Title: Acceleration in Policy Optimization\\nAbstract:\\nWe work towards a unifying paradigm for accelerating policy optimization\\nmethods in reinforcement learning (RL) by integrating foresight in the policy\\nimprovement step via optimistic and adaptive updates. Leveraging the connection\\nbetween policy iteration and policy gradient methods, we view policy\\noptimization algorithms as iteratively solving a sequence of surrogate\\nobjectives, local lower bounds on the original objective. We define optimism as\\npredictive modelling of the future behavior of a policy, and adaptivity as\\ntaking immediate and anticipatory corrective actions to mitigate accumulating\\nerrors from overshooting predictions or delayed responses to change. We use\\nthis shared lens to jointly express other well-known algorithms, including\\nmodel-based policy improvement based on forward search, and optimistic\\nmeta-learning algorithms. We analyze properties of this formulation, and show\\nconnections to other accelerated optimization algorithms. Then, we design an\\noptimistic policy gradient algorithm, adaptive via meta-gradient learning, and\\nempirically highlight several design choices pertaining to acceleration, in an\\nillustrative task.', mimetype='text/plain', start_char_idx=0, end_char_idx=1210, metadata_seperator='\\n', text_template='{content}'), score=0.4949085718502352),\n",
       " NodeWithScore(node=TextNode(id_='72d27886-3bf5-4d30-a3a6-52f31e59c3ae', embedding=None, metadata={'paper_information_id': 14, 'published_date': '2023-09-20 14:59:06'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0ad9984a-ba0c-4154-a12f-e37b9b743750', node_type='4', metadata={'paper_information_id': 14, 'published_date': '2023-09-20 14:59:06'}, hash='29cb1b4acec8fd4ff2fa94822c1848e0eeb8bae6cccaa8dc50eb917d032fb240')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Title: Incremental Blockwise Beam Search for Simultaneous Speech Translation\\n  with Controllable Quality-Latency Tradeoff\\nAbstract:\\nBlockwise self-attentional encoder models have recently emerged as one\\npromising end-to-end approach to simultaneous speech translation. These models\\nemploy a blockwise beam search with hypothesis reliability scoring to determine\\nwhen to wait for more input speech before translating further. However, this\\nmethod maintains multiple hypotheses until the entire speech input is consumed\\n-- this scheme cannot directly show a single \\\\textit{incremental} translation\\nto users. Further, this method lacks mechanisms for \\\\textit{controlling} the\\nquality vs. latency tradeoff. We propose a modified incremental blockwise beam\\nsearch incorporating local agreement or hold-$n$ policies for quality-latency\\ncontrol. We apply our framework to models trained for online or offline\\ntranslation and demonstrate that both types can be effectively used in online\\nmode.\\n  Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing\\nlatency or 0.8-1.4 s latency improvement without changing quality.', mimetype='text/plain', start_char_idx=0, end_char_idx=1132, metadata_seperator='\\n', text_template='{content}'), score=0.4947289824485779),\n",
       " NodeWithScore(node=TextNode(id_='e250108a-1d23-4bb9-87d8-9296da3d79e3', embedding=None, metadata={'paper_information_id': 34, 'published_date': '2023-06-30 16:07:12'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='cc6cb50e-2a5a-4801-8606-85ce2b554931', node_type='4', metadata={'paper_information_id': 34, 'published_date': '2023-06-30 16:07:12'}, hash='b3d30cee77317559b819f3de1166a1e8aabf357b7a63ef272a97203f39e6cc52')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Title: An Improved Deterministic Algorithm for the Online Min-Sum Set Cover\\n  Problem\\nAbstract:\\nWe study the online variant of the Min-Sum Set Cover (MSSC) problem, a\\ngeneralization of the well-known list update problem. In the MSSC problem, an\\nalgorithm has to maintain the time-varying permutation of the list of $n$\\nelements, and serve a sequence of requests $R_1, R_2, \\\\dots, R_t, \\\\dots$. Each\\n$R_t$ is a subset of elements of cardinality at most $r$. For a requested set\\n$R_t$, an online algorithm has to pay the cost equal to the position of the\\nfirst element from $R_t$ on its list. Then, it may arbitrarily permute its\\nlist, paying the number of swapped adjacent element pairs.\\n  We present the first constructive deterministic algorithm for this problem,\\nwhose competitive ratio does not depend on $n$. Our algorithm is\\n$O(r^2)$-competitive, which beats both the existential upper bound of $O(r^4)$\\nby Bienkowski and Mucha [AAAI '23] and the previous constructive bound of\\n$O(r^{3/2} \\\\cdot \\\\sqrt{n})$ by Fotakis et al. [ICALP '20]. Furthermore, we show\\nthat our algorithm attains an asymptotically optimal competitive ratio of\\n$O(r)$ when compared to the best fixed permutation of elements.\", mimetype='text/plain', start_char_idx=0, end_char_idx=1199, metadata_seperator='\\n', text_template='{content}'), score=0.47758044235648067),\n",
       " NodeWithScore(node=TextNode(id_='83781e04-6523-4aa4-9638-c487e6d41866', embedding=None, metadata={'paper_information_id': 30, 'published_date': '2023-07-18 05:59:27'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='cc48d9a4-1b0b-4500-8c52-1965b8ecfad2', node_type='4', metadata={'paper_information_id': 30, 'published_date': '2023-07-18 05:59:27'}, hash='4cef1be9e1146fd2f697d54a7f9f174682e8c0ed3827266d3bf8cf8d846b7c1d')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Title: EVIL: Evidential Inference Learning for Trustworthy Semi-supervised\\n  Medical Image Segmentation\\nAbstract:\\nRecently, uncertainty-aware methods have attracted increasing attention in\\nsemi-supervised medical image segmentation. However, current methods usually\\nsuffer from the drawback that it is difficult to balance the computational\\ncost, estimation accuracy, and theoretical support in a unified framework. To\\nalleviate this problem, we introduce the Dempster-Shafer Theory of Evidence\\n(DST) into semi-supervised medical image segmentation, dubbed Evidential\\nInference Learning (EVIL). EVIL provides a theoretically guaranteed solution to\\ninfer accurate uncertainty quantification in a single forward pass. Trustworthy\\npseudo labels on unlabeled data are generated after uncertainty estimation. The\\nrecently proposed consistency regularization-based training paradigm is adopted\\nin our framework, which enforces the consistency on the perturbed predictions\\nto enhance the generalization with few labeled data. Experimental results show\\nthat EVIL achieves competitive performance in comparison with several\\nstate-of-the-art methods on the public dataset.', mimetype='text/plain', start_char_idx=0, end_char_idx=1162, metadata_seperator='\\n', text_template='{content}'), score=0.474595276987272),\n",
       " NodeWithScore(node=TextNode(id_='6af0d46b-3f29-43f7-90e7-ccdddeac1771', embedding=None, metadata={'paper_information_id': 67, 'published_date': '2023-07-14 12:43:25'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='48336b46-5788-4769-af29-61659c9dc7ad', node_type='4', metadata={'paper_information_id': 67, 'published_date': '2023-07-14 12:43:25'}, hash='53acab2a194089204a651c6c32e69bc97c3451b0a362d61ef8d871353bacd0f1')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Title: Unsourced Random Access Using Multiple Stages of Orthogonal Pilots: MIMO\\n  and Single-Antenna Structures\\nAbstract:\\nWe study the problem of unsourced random access (URA) over Rayleigh\\nblock-fading channels with a receiver equipped with multiple antennas. We\\npropose a slotted structure with multiple stages of orthogonal pilots, each of\\nwhich is randomly picked from a codebook. In the proposed signaling structure,\\neach user encodes its message using a polar code and appends it to the selected\\npilot sequences to construct its transmitted signal. Accordingly, the\\ntransmitted signal is composed of multiple orthogonal pilot parts and a\\npolar-coded part, which is sent through a randomly selected slot. The\\nperformance of the proposed scheme is further improved by randomly dividing\\nusers into different groups each having a unique interleaver-power pair. We\\nalso apply the idea of multiple stages of orthogonal pilots to the case of a\\nsingle receive antenna. In all the set-ups, we use an iterative approach for\\ndecoding the transmitted messages along with a suitable successive interference\\ncancellation technique. The use of orthogonal pilots and the slotted structure\\nlead to improved accuracy and reduced computational complexity in the proposed\\nset-ups, and make the implementation with short blocklengths more viable.\\nPerformance of the proposed set-ups is illustrated via extensive simulation\\nresults which show that the proposed set-ups with multiple antennas perform\\nbetter than the existing MIMO URA solutions for both short and large\\nblocklengths, and that the proposed single-antenna set-ups are superior to the\\nexisting single-antenna URA schemes.', mimetype='text/plain', start_char_idx=0, end_char_idx=1668, metadata_seperator='\\n', text_template='{content}'), score=0.47356927394866943)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Simple query\n",
    "query = \"Retrieval Augmented Generation\"\n",
    "nodes = retriever.retrieve(query)\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 0 - 0.537\n",
      "4 Subsurface cosmogenic and radiogenic production of ^{42}Ar 2023-08-25 19:35:58\n",
      "Result 1 - 0.530\n",
      "97 Vector Bundles over non-Hausdorff Manifolds 2023-10-01 18:50:29\n",
      "Result 2 - 0.522\n",
      "17 Nonequilibrium Seebeck and spin Seebeck effects in nanoscale junctions 2023-07-17 11:55:20\n",
      "Result 3 - 0.508\n",
      "1 Characterizing and correcting electron and hole trapping in germanium\n",
      "  cross-strip detectors 2023-08-22 21:03:58\n",
      "Result 4 - 0.502\n",
      "13 Synthesizing and multiplexing autonomous quantum coherences 2023-06-27 02:46:08\n",
      "Result 5 - 0.495\n",
      "53 A thin plate approximation for ocean wave interactions with an ice shelf 2023-06-18 15:50:57\n",
      "Result 6 - 0.495\n",
      "14 Tunable optical multistability induced by a single cavity mode in cavity\n",
      "  quantum electrodynamics system 2023-09-20 14:59:06\n",
      "Result 7 - 0.478\n",
      "34 Pre-Training to Learn in Context 2023-06-30 16:07:12\n",
      "Result 8 - 0.475\n",
      "30 Current Tomography -- Localization of void fractions in conducting\n",
      "  liquids by measuring the induced magnetic flux density 2023-07-18 05:59:27\n",
      "Result 9 - 0.474\n",
      "67 The discrete collision-induced breakage equation with mass transfer:\n",
      "  well-posedness and stationary solutions 2023-07-14 12:43:25\n"
     ]
    }
   ],
   "source": [
    "for i, node in enumerate(nodes):\n",
    "    ## Score\n",
    "    score = node.score\n",
    "    ## Source\n",
    "    paper_info_id = node.metadata['paper_information_id']\n",
    "    title = df.iloc[paper_info_id][\"title\"]\n",
    "    abstract = df.iloc[paper_info_id][\"abstract\"]\n",
    "    \n",
    "    published_date = node.metadata[\"published_date\"]\n",
    "    print(\"Result {} - {:.3f}\".format(i, score))\n",
    "    print(paper_info_id, title, published_date)\n",
    "    # print(abstract[:100])\n",
    "    # print('-'*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
