{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    model_config = SettingsConfigDict(\n",
    "        env_file=\"../.env\", env_file_encoding=\"utf-8\", extra=\"ignore\"\n",
    "    )\n",
    "    data_dir: str\n",
    "    docling_model_dir: str\n",
    "    \n",
    "settings = Settings()\n",
    "os.environ[\"HF_HOME\"] = settings.docling_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "from core.base.schema import TextNode, ImageNode, TableNode, TextType, TextLabel, Document\n",
    "from core.reader.docling.pdf_reader import DoclingPDFReader\n",
    "from core.processor.document.text_merger import TextNodeMerger\n",
    "from core.splitter.text.langchain_text_splitters import LangchainRecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = DoclingPDFReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num files: 10\n"
     ]
    }
   ],
   "source": [
    "pdf_dir = os.path.join(settings.data_dir, \"allganize-RAG-Evaluation-Dataset-KO/finance\")\n",
    "pdf_fnames =[x for x in os.listdir(pdf_dir) if x.endswith(\".pdf\")]\n",
    "print(\"num files:\", len(pdf_fnames))\n",
    "pdf_fnames[:10]\n",
    "\n",
    "# file_path = os.path.join(pdf_dir, pdf_fnames[0])\n",
    "file_path = \"resources/finance-small-images.pdf\"\n",
    "file_path = \"resources/1706.03762v7.pdf\"\n",
    "# file_path = \"resources/1706.03762v7-sample.pdf\"\n",
    "# file_path = \"resources/list_group_sample_msword.pdf\"\n",
    "# file_path = \"resources/list_group_sample_google.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Reader\n",
    "document = reader.run(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Processor (merge)\n",
    "nodes = document.nodes\n",
    "print(len(nodes))\n",
    "\n",
    "merger = TextNodeMerger()\n",
    "document = merger.run(document)\n",
    "len(document.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPLITTING NODE 0\n",
      "TEXT: 5162 arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n",
      "Provided proper attribution is provided, Google hereby grant\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 648 arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n",
      "Provided proper attribution is provided, Google hereby grant\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 1023 The dominant sequence transduction models are based on complex recurrent or convolutional neural net\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 242 a small fraction of the training costs of the best models from the literature. We show that the Tran\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 431 1 Introduction\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 760 Recurrent models typically factor computation along the symbol positions of the input and output seq\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 743 Attention mechanisms have become an integral part of compelling sequence modeling and transduction m\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 838 2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended N\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 611 Self-attention, sometimes called intra-attention is an attention mechanism relating different positi\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLITTING NODE 3\n",
      "TEXT: 1811 The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 880 The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 930 Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLITTING NODE 7\n",
      "TEXT: 5377 of the values, where the weight assigned to each value is computed by a compatibility function of th\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 751 of the values, where the weight assigned to each value is computed by a compatibility function of th\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 1011 Attention( Q,K,V ) = softmax( QK T √ d k ) V (1)\n",
      "The two most commonly used attention functions are \n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 1008 3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with d model-dimensiona\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 741 In this work we employ h = 8 parallel attention layers, or heads. For each of these we use d k = d v\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 782 - · The encoder contains self-attention layers. In a self-attention layer all of the keys, values an\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 695 3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of the layers in o\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 526 3.4 Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embedding\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLITTING NODE 10\n",
      "TEXT: 5375 3.5 Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the \n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 718 3.5 Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the \n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 846 PE ( pos, 2 i ) = sin ( pos/ 10000 2 i/d model ) PE ( pos, 2 i +1) = cos ( pos/ 10000 2 i/d model )\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 640 4 Why Self-Attention\n",
      "In this section we compare various aspects of self-attention layers to the recu\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 914 The third is the path length between long-range dependencies in the network. Learning long-range dep\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 586 length n is smaller than the representation dimensionality d , which is most often the case with sen\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 745 A single convolutional layer with kernel width k < n does not connect all pairs of input and output \n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 451 As side benefit, self-attention could yield more interpretable models. We inspect attention distribu\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 690 5 Training\n",
      "This section describes the training regime for our models.\n",
      "5.1 Training Data and Batching\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLITTING NODE 11\n",
      "TEXT: 1187 5.2 Hardware and Schedule\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 994 5.2 Hardware and Schedule\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 268 5.4 Regularization\n",
      "We employ three types of regularization during training:\n",
      "Table 2: The Transformer\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLITTING NODE 13\n",
      "TEXT: 2764 Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the su\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 535 Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the su\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 906 6 Results\n",
      "6.1 Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transf\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 840 For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 535 6.2 Model Variations\n",
      "To evaluate the importance of different components of the Transformer, we varie\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLITTING NODE 15\n",
      "TEXT: 2220 development set, newstest2013. We used beam search as described in the previous section, but no chec\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 1009 development set, newstest2013. We used beam search as described in the previous section, but no chec\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 844 6.3 English Constituency Parsing\n",
      "To evaluate if the Transformer can generalize to other tasks we per\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 398 We performed only a small number of experiments to select the dropout, both attention and residual (\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLITTING NODE 17\n",
      "TEXT: 2382 increased the maximum output length to input length + 300 . We used a beam size of 21 and α = 0 . 3 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 798 increased the maximum output length to input length + 300 . We used a beam size of 21 and α = 0 . 3 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 1000 For translation tasks, the Transformer can be trained significantly faster than architectures based \n",
      "----------------------------------------------------------------------------------------------------\n",
      "SPLIT TEXT: 593 References\n",
      "- [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv p\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Splitter for each node\n",
    "\n",
    "processed_nodes = []\n",
    "splitter = LangchainRecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1024,\n",
    "    chunk_overlap = 128\n",
    ")\n",
    "\n",
    "for i, node in enumerate(document.nodes):\n",
    "    if isinstance(node, TextNode):\n",
    "        split_nodes = splitter.run(node)\n",
    "        if len(split_nodes) > 1:\n",
    "            print(f\"SPLITTING NODE {i}\")\n",
    "            print(f\"TEXT: {len(node.text)} {node.text[:100]}\")\n",
    "            print(\"-\"*100)\n",
    "            for split_node in split_nodes:\n",
    "                print(f\"SPLIT TEXT: {len(split_node.text)} {split_node.text[:100]}\")\n",
    "                print(\"-\"*100)\n",
    "            \n",
    "        processed_nodes.extend(splitter.run(node))\n",
    "    else:\n",
    "        processed_nodes.append(node)\n",
    "\n",
    "processed_document = Document(nodes=processed_nodes)\n",
    "len(processed_document.nodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docling",
   "language": "python",
   "name": "docling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
