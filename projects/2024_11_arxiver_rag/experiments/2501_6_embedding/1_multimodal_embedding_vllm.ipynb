{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-30 21:06:47 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import PIL\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from vllm import LLM\n",
    "\n",
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    model_config = SettingsConfigDict(\n",
    "        env_file=\"../.env\", env_file_encoding=\"utf-8\", extra=\"ignore\"\n",
    "    )\n",
    "    model_dir: str\n",
    "    \n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-30 21:06:55 llm_engine.py:232] Initializing an LLM engine (v0.7.0) with config: model='/home/users/yrsong/llm_serving/models/Qwen2-VL-2B', speculative_config=None, tokenizer='/home/users/yrsong/llm_serving/models/Qwen2-VL-2B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/users/yrsong/llm_serving/models/Qwen2-VL-2B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=PoolerConfig(pooling_type=None, normalize=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 01-30 21:06:57 cuda.py:225] Using Flash Attention backend.\n",
      "INFO 01-30 21:07:00 model_runner.py:1110] Starting to load model /home/users/yrsong/llm_serving/models/Qwen2-VL-2B...\n",
      "INFO 01-30 21:07:01 config.py:2924] cudagraph sizes specified by model runner [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256] is overridden by config [256, 128, 2, 1, 4, 136, 8, 144, 16, 152, 24, 160, 32, 168, 40, 176, 48, 184, 56, 192, 64, 200, 72, 208, 80, 216, 88, 120, 224, 96, 232, 104, 240, 112, 248]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbf21cea07b468796d4fbe6ed301ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 01-30 21:07:21 utils.py:173] Unable to collect loaded parameters for module Qwen2ForEmbedding(\n",
      "WARNING 01-30 21:07:21 utils.py:173]   (model): Qwen2Model(\n",
      "WARNING 01-30 21:07:21 utils.py:173]     (embed_tokens): VocabParallelEmbedding(num_embeddings=151936, embedding_dim=1536, org_vocab_size=151936, num_embeddings_padded=151936, tp_size=1)\n",
      "WARNING 01-30 21:07:21 utils.py:173]     (layers): ModuleList(\n",
      "WARNING 01-30 21:07:21 utils.py:173]       (0-27): 28 x Qwen2DecoderLayer(\n",
      "WARNING 01-30 21:07:21 utils.py:173]         (self_attn): Qwen2Attention(\n",
      "WARNING 01-30 21:07:21 utils.py:173]           (qkv_proj): QKVParallelLinear(in_features=1536, output_features=2048, bias=True, tp_size=1, gather_output=False)\n",
      "WARNING 01-30 21:07:21 utils.py:173]           (o_proj): RowParallelLinear(input_features=1536, output_features=1536, bias=False, tp_size=1, reduce_results=True)\n",
      "WARNING 01-30 21:07:21 utils.py:173]           (rotary_emb): MRotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=32768, base=1000000.0, is_neox_style=True)\n",
      "WARNING 01-30 21:07:21 utils.py:173]           (attn): Attention(head_size=128, num_heads=12, num_kv_heads=2, scale=0.08838834764831845, backend=FlashAttentionImpl)\n",
      "WARNING 01-30 21:07:21 utils.py:173]         )\n",
      "WARNING 01-30 21:07:21 utils.py:173]         (mlp): Qwen2MLP(\n",
      "WARNING 01-30 21:07:21 utils.py:173]           (gate_up_proj): MergedColumnParallelLinear(in_features=1536, output_features=17920, bias=False, tp_size=1, gather_output=False)\n",
      "WARNING 01-30 21:07:21 utils.py:173]           (down_proj): RowParallelLinear(input_features=8960, output_features=1536, bias=False, tp_size=1, reduce_results=True)\n",
      "WARNING 01-30 21:07:21 utils.py:173]           (act_fn): SiluAndMul()\n",
      "WARNING 01-30 21:07:21 utils.py:173]         )\n",
      "WARNING 01-30 21:07:21 utils.py:173]         (input_layernorm): RMSNorm(hidden_size=1536, eps=1e-06)\n",
      "WARNING 01-30 21:07:21 utils.py:173]         (post_attention_layernorm): RMSNorm(hidden_size=1536, eps=1e-06)\n",
      "WARNING 01-30 21:07:21 utils.py:173]       )\n",
      "WARNING 01-30 21:07:21 utils.py:173]     )\n",
      "WARNING 01-30 21:07:21 utils.py:173]     (norm): RMSNorm(hidden_size=1536, eps=1e-06)\n",
      "WARNING 01-30 21:07:21 utils.py:173]   )\n",
      "WARNING 01-30 21:07:21 utils.py:173]   (sampler): Sampler()\n",
      "WARNING 01-30 21:07:21 utils.py:173]   (_pooler): LastPool(\n",
      "WARNING 01-30 21:07:21 utils.py:173]     (head): PoolerHead()\n",
      "WARNING 01-30 21:07:21 utils.py:173]   )\n",
      "WARNING 01-30 21:07:21 utils.py:173] )\n",
      "INFO 01-30 21:07:30 model_runner.py:1115] Loading model weights took 4.1273 GB\n"
     ]
    }
   ],
   "source": [
    "model_cache_dir = settings.model_dir\n",
    "model_dir = os.path.join(model_cache_dir, \"Qwen2-VL-2B\")\n",
    "# model_dir = os.path.join(settings.model_dir, \"Qwen2.5-VL-3B-Instruct\") # 2.5 not supported yet\n",
    "\n",
    "llm = LLM(\n",
    "    model=model_dir,\n",
    "    trust_remote_code=True,\n",
    "    max_model_len=8192,\n",
    "    limit_mm_per_prompt={\"image\": 4},\n",
    "    task=\"embed\",\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.vllm.ai/en/stable/serving/multimodal_inputs.html#embedding\n",
    "# image placeholders: https://github.com/huggingface/transformers/blob/365fecb4d0b6c87f20b93561e11c3d4c77938012/src/transformers/models/qwen2_vl/processing_qwen2_vl.py#L63\n",
    "# processor replaces `<|image_pad|>`  with image data\n",
    "text = \"시간표: <|image_pad|><|image_pad|>\"\n",
    "image1 = PIL.Image.open(\"resources/finance-sample/table-1.png\")\n",
    "\n",
    "# https://github.com/vllm-project/vllm/blob/a2769032ca78108e58abc45e2eb0ade8b47a6515/vllm/model_executor/models/qwen2_vl.py#L947\n",
    "prompt = {\n",
    "    \"prompt\": text,\n",
    "    \"multi_modal_data\": {\n",
    "        \"image\": [image1, image1]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.embed(prompt)\n",
    "print(type(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "embedding = outputs[0].outputs.embedding\n",
    "print(len(embedding))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
