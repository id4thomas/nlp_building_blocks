{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColSmolVLM model testing\n",
    "* cookbook\n",
    "    * colpali-engine\n",
    "    * https://huggingface.co/learn/cookbook/multimodal_rag_using_document_retrieval_and_smol_vlm#3-initialize-the-colsmolvlm-multimodal-document-retrieval-model-\n",
    "* vidore benchmark: https://huggingface.co/spaces/vidore/vidore-leaderboard\n",
    "\n",
    "models:\n",
    "* (colSmol-500M)[https://huggingface.co/vidore/colSmol-500M]\n",
    "    * base model: ColSmolVLM-Instruct-500M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from colpali_engine.models import ColIdefics3, ColIdefics3Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/llm/lib/python3.10/site-packages/pydantic/_internal/_fields.py:152: UserWarning: Field \"model_dir\" in Settings has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('settings_',)`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    model_config = SettingsConfigDict(\n",
    "        env_file=\"../.env\", env_file_encoding=\"utf-8\", extra=\"ignore\"\n",
    "    )\n",
    "    model_dir: str\n",
    "    \n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9526c6d32944bb8b1b3afba1494ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361522f6677e41a1a78b8b01d2081388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/921M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Colpali engine\n",
    "model_dir = os.path.join(\n",
    "    settings.model_dir, \"multimodal_retriever/colSmol-500M\"\n",
    ")\n",
    "\n",
    "model = ColIdefics3.from_pretrained(\n",
    "    model_dir,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"mps\",\n",
    "    # attn_implementation=\"flash_attention_2\" # or eager\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: image_seq_len. \n"
     ]
    }
   ],
   "source": [
    "processor = ColIdefics3Processor.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 예시대로\n",
    "images = [\n",
    "    Image.new(\"RGB\", (32, 32), color=\"white\"),\n",
    "    Image.new(\"RGB\", (16, 16), color=\"black\"),\n",
    "]\n",
    "queries = [\n",
    "    \"Is attention really all you need?\",\n",
    "    \"What is the amount of bananas farmed in Salvador?\",\n",
    "]\n",
    "\n",
    "# Process the inputs\n",
    "batch_images = processor.process_images(images).to(model.device)\n",
    "batch_queries = processor.process_queries(queries).to(model.device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    image_embeddings = model(**batch_images)\n",
    "    query_embeddings = model(**batch_queries)\n",
    "\n",
    "scores = processor.score_multi_vector(query_embeddings, image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1135, 128])\n",
      "torch.Size([2, 23, 128])\n",
      "tensor([[5.8438, 5.8750],\n",
      "        [7.6562, 7.8125]])\n"
     ]
    }
   ],
   "source": [
    "print(image_embeddings.shape)\n",
    "print(query_embeddings.shape)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check ColsmolVLM templates\n",
    "* uses ColIdefics3, ColIdefics3Processor\n",
    "    * colpali repo: https://github.com/illuin-tech/colpali/tree/59e94a92790b67bd60507608c3115a2e48f83a07/colpali_engine/models/idefics3/colidefics3\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_images\n",
    "* 검색 대상 문서를 임베딩 할 때 사용\n",
    "    * 이미지 '만' 받는 것을 가정\n",
    "* 템플릿이 코드상 고정되어 있음\n",
    "    * https://github.com/illuin-tech/colpali/blob/59e94a92790b67bd60507608c3115a2e48f83a07/colpali_engine/models/idefics3/colidefics3/processing_colidefics3.py#L37\n",
    "    * \"<|im_start|>User: Describe the image.\" + 이미지 관련 토큰 + \"<end_of_utterance>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>User: Describe the image.<image><end_of_utterance>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# colpali 코드에 프롬프트 템플릿이 픽스되어 있음\n",
    "messages_doc = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Describe the image.\"},\n",
    "            {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "text_doc = processor.apply_chat_template(messages_doc, add_generation_prompt=False)\n",
    "print(text_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이미지 삽입 부분\n",
    "batch_doc = processor(\n",
    "    text=[text_doc],\n",
    "    images=[Image.new(\"RGB\", (32, 32), color=\"white\")],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"longest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>User: Describe the image.<fake_token_around_image><row_1_col_1><fake_token_around_image><row_1_col_2><fake_token_around_image><row_1_col_3><fake_token_around_image><row_1_col_4>\n",
      "<fake_token_around_image><row_2_col_1><fake_token_around_image><row_2_col_2><fake_token_around_image><row_2_col_3><fake_token_around_image><row_2_col_4>\n",
      "<fake_token_around_image><row_3_col_1><fake_token_around_image><row_3_col_2><fake_token_around_image><row_3_col_3><fake_token_around_image><row_3_col_4>\n",
      "<fake_token_around_image><row_4_col_1><fake_token_around_image><row_4_col_2><fake_token_around_image><row_4_col_3><fake_token_around_image><row_4_col_4>\n",
      "\n",
      "<fake_token_around_image><global-img><fake_token_around_image><end_of_utterance>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(batch_doc['input_ids'][0]).replace(\"<image>\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_queries\n",
    "* 쿼리 텍스트 임베딩 할 때 사용\n",
    "* `self.query_prefix + query + suffix + \"\\n\"`로 포매팅 처리\n",
    "    * query_prefix: \"Query: \"\n",
    "    * suffix (default): self.query_augmentation_token * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<end_of_utterance>\n",
      "Query: \n"
     ]
    }
   ],
   "source": [
    "print(processor.query_augmentation_token)\n",
    "print(processor.query_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: sample_query<end_of_utterance><end_of_utterance><end_of_utterance><end_of_utterance><end_of_utterance><end_of_utterance><end_of_utterance><end_of_utterance><end_of_utterance><end_of_utterance>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"sample_query\"\n",
    "input_ids = processor.process_queries([query])['input_ids'][0]\n",
    "print(tokenizer.decode(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## score_multi_vector\n",
    "* https://github.com/illuin-tech/colpali/blob/59e94a92790b67bd60507608c3115a2e48f83a07/colpali_engine/utils/processing_utils.py#L68\n",
    "* each query and passage is represented as a set of multiple embedding vectors, rather than a single vector.\n",
    "\n",
    "Input\n",
    "* qs (`Union[torch.Tensor, List[torch.Tensor]`): Query embeddings.\n",
    "* ps (`Union[torch.Tensor, List[torch.Tensor]`): Passage embeddings.\n",
    "\n",
    "ex.\n",
    "```\n",
    "image: torch.Size([2, 1135, 128])\n",
    "text: torch.Size([2, 23, 128])\n",
    "```\n",
    "\n",
    "### score 계산: (colbert-style late interaction)\n",
    "1. `torch.einsum(\"bnd,csd->bcns\", qs_batch, ps_batch).max(dim=3)[0].sum(dim=2)`\n",
    "* qs_batch shape bnd\n",
    "    * b: batch, n: number of query embeddings (seq length), d: dim\n",
    "* ps_batch shape csd\n",
    "    * c: batch, n: number of passage embeddings, d: dim\n",
    "* Multiplication (bnd * csd):\n",
    "    * Computes dot products between every query vector (N vectors) and every passage vector (S vectors)\n",
    "* output shape is (B, C, N, S): similarity matrix\n",
    "    * B: Number of query batches\n",
    "    * C: Number of passage batches\n",
    "    * N: Number of query tokens\n",
    "    * S: Number of passage tokens\n",
    "    *  dot product similarity between the n-th query token and the s-th passage token.\n",
    "\n",
    "```\n",
    "for b in range(B):  # Loop over queries batch\n",
    "    for c in range(C):  # Loop over passages batch\n",
    "        for n in range(N):  # Loop over query tokens\n",
    "            for s in range(S):  # Loop over passage tokens\n",
    "                # dot product beween query token embed, passage token embed\n",
    "                output[b, c, n, s] = torch.dot(qs_batch[b, n], ps_batch[c, s])\n",
    "```\n",
    "\n",
    "2. max(dim=3)[0]\n",
    "* finds maximum similarity for each query token (n) across all passage tokens (s).\n",
    "* for each query token, we take the **best-matching passage token.**\n",
    "* output shape: (b, c, n)\n",
    "    * best match score for the n-th query token\n",
    "\n",
    "3. aggregation sum(dim=2)\n",
    "* single score per query-passage pair.\n",
    "* Each query token contributes to the total score.\n",
    "* If a passage has multiple highly similar tokens, their scores are accumulated.\n",
    "* output shape: (b, c)\n",
    "    * overall similarity score between query b and passage c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 직접 이미지 + 텍스트 임베딩 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = [\n",
    "#     [\n",
    "#         Image.new(\"RGB\", (32, 32), color=\"white\"),\n",
    "#         Image.new(\"RGB\", (32, 32), color=\"black\"),\n",
    "#     ],\n",
    "#     # Image.new(\"RGB\", (16, 16), color=\"black\"),\n",
    "# ]\n",
    "# queries = [\n",
    "#     \"Is attention really all you need? <image> \\n\\nimage2 <image>\",\n",
    "#     # \"What is the amount of bananas farmed in Salvador?\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>User: Describe the text and image. text1: Is attention really all you need?:\n",
      "image1: <image>\n",
      "image2: <image>\n",
      "text2: maybe? maybe not?<end_of_utterance>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Is attention really all you need?\"\n",
    "text2 = \"maybe? maybe not?\"\n",
    "messages_doc = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\", \"text\": \"Describe the text and image. text1: {}:\".format(text1) # text1\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\", \"text\": \"\\nimage1: \"\n",
    "            },\n",
    "            {\"type\": \"image\"}, # image1\n",
    "            {\n",
    "                \"type\": \"text\", \"text\": \"\\nimage2: \"\n",
    "            },\n",
    "            {\"type\": \"image\"}, # image2\n",
    "            {\n",
    "                \"type\": \"text\", \"text\": \"\\ntext2: {}\".format(text2)\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "text_doc = processor.apply_chat_template(messages_doc, add_generation_prompt=False)\n",
    "print(text_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>User: Describe the text and image. text1: Is attention really all you need?:\n",
      "image1: <fake_token_around_image><row_1_col_1><fake_token_around_image><row_1_col_2><fake_token_around_image><row_1_col_3><fake_token_around_image><row_1_col_4>\n",
      "<fake_token_around_image><row_2_col_1><fake_token_around_image><row_2_col_2><fake_token_around_image><row_2_col_3><fake_token_around_image><row_2_col_4>\n",
      "<fake_token_around_image><row_3_col_1><fake_token_around_image><row_3_col_2><fake_token_around_image><row_3_col_3><fake_token_around_image><row_3_col_4>\n",
      "<fake_token_around_image><row_4_col_1><fake_token_around_image><row_4_col_2><fake_token_around_image><row_4_col_3><fake_token_around_image><row_4_col_4>\n",
      "\n",
      "<fake_token_around_image><global-img><fake_token_around_image>\n",
      "image2: <fake_token_around_image><row_1_col_1><fake_token_around_image><row_1_col_2><fake_token_around_image><row_1_col_3><fake_token_around_image><row_1_col_4>\n",
      "<fake_token_around_image><row_2_col_1><fake_token_around_image><row_2_col_2><fake_token_around_image><row_2_col_3><fake_token_around_image><row_2_col_4>\n",
      "<fake_token_around_image><row_3_col_1><fake_token_around_image><row_3_col_2><fake_token_around_image><row_3_col_3><fake_token_around_image><row_3_col_4>\n",
      "<fake_token_around_image><row_4_col_1><fake_token_around_image><row_4_col_2><fake_token_around_image><row_4_col_3><fake_token_around_image><row_4_col_4>\n",
      "\n",
      "<fake_token_around_image><global-img><fake_token_around_image>\n",
      "text2: maybe? maybe not?<end_of_utterance>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 이미지 삽입 부분\n",
    "# processor 코드\n",
    "# https://github.com/huggingface/transformers/blob/e6f4a4ebbf970c12fe475be79a039f943c28f975/src/transformers/models/idefics3/processing_idefics3.py#L111\n",
    "'''\n",
    ">>> images = [[image1], [image2]]\n",
    "\n",
    ">>> text = [\n",
    "...     \"<image>In this image, we see\",\n",
    "...     \"bla bla bla<image>\",\n",
    "... ]\n",
    ">>> outputs = processor(images=images, text=text, return_tensors=\"pt\", padding=True)\n",
    "'''\n",
    "\n",
    "images = [\n",
    "    Image.new(\"RGB\", (32, 32), color=\"white\"),\n",
    "    Image.new(\"RGB\", (32, 32), color=\"black\"),\n",
    "]\n",
    "\n",
    "batch_doc = processor(\n",
    "    text=[text_doc],\n",
    "    images=[images],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"longest\",\n",
    ")\n",
    "input_ids = batch_doc['input_ids'][0]\n",
    "print(tokenizer.decode(input_ids).replace(\"<image>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embeddings = model(**batch_doc.to(\"mps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2294, 128])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
