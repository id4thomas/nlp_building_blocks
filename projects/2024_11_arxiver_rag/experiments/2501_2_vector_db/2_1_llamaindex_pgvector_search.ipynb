{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from semantic_chunkers import StatisticalChunker\n",
    "# from semantic_router.encoders import OpenAIEncoder\n",
    "from src.encoder import OpenAIEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "\n",
    "class EnvSettings(BaseSettings):\n",
    "    model_config = SettingsConfigDict(\n",
    "        env_file=\"../.env\", env_file_encoding=\"utf-8\", extra=\"ignore\"\n",
    "    )\n",
    "    embedding_base_url: str\n",
    "    embedding_api_key: str\n",
    "    embedding_model: str\n",
    "    embedding_model_dir: str\n",
    "    \n",
    "    sample_data_dir: str\n",
    "    pipeline_src_dir: str\n",
    "settings = EnvSettings()\n",
    "\n",
    "import sys\n",
    "sys.path.append(settings.pipeline_src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pgvector_llamaindex\n"
     ]
    }
   ],
   "source": [
    "class DBSettings(BaseSettings):\n",
    "    model_config = SettingsConfigDict(\n",
    "        env_file=\"database/pgvector_llamaindex/.env\", env_file_encoding=\"utf-8\", extra=\"ignore\"\n",
    "    )\n",
    "    postgres_user: str\n",
    "    postgres_password: str\n",
    "    postgres_db: str\n",
    "    postgres_url: str\n",
    "    postgres_port: str\n",
    "\n",
    "db_settings = DBSettings()\n",
    "print(db_settings.postgres_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nValueError: 'baai/bge-m3' is not a valid OpenAIEmbeddingModelType\\n-> https://github.com/run-llama/llama_index/blob/a8d27fa9c7f7b039768cb0a0685e70de389087be/llama-index-integrations/embeddings/llama-index-embeddings-openai/llama_index/embeddings/openai/base.py#L27\\nembedding model names are fixed in code\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## OpenAIEmbedding class\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding, OpenAIEmbeddingModelType\n",
    "# https://github.com/run-llama/llama_index/blob/a8d27fa9c7f7b039768cb0a0685e70de389087be/llama-index-integrations/embeddings/llama-index-embeddings-openai/llama_index/embeddings/openai/base.py#L271\n",
    "'''\n",
    "mode: str = OpenAIEmbeddingMode.TEXT_SEARCH_MODE,\n",
    "model: str = OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002,\n",
    "embed_batch_size: int = 100,\n",
    "dimensions: Optional[int] = None,\n",
    "additional_kwargs: Optional[Dict[str, Any]] = None,\n",
    "api_key: Optional[str] = None,\n",
    "api_base: Optional[str] = None,\n",
    "api_version: Optional[str] = None,\n",
    "max_retries: int = 10,\n",
    "timeout: float = 60.0,\n",
    "reuse_client: bool = True,\n",
    "callback_manager: Optional[CallbackManager] = None,\n",
    "default_headers: Optional[Dict[str, str]] = None,\n",
    "http_client: Optional[httpx.Client] = None,\n",
    "async_http_client: Optional[httpx.AsyncClient] = None,\n",
    "num_workers: Optional[int] = None,'''\n",
    "\n",
    "'''\n",
    "ValueError: 'baai/bge-m3' is not a valid OpenAIEmbeddingModelType\n",
    "-> https://github.com/run-llama/llama_index/blob/a8d27fa9c7f7b039768cb0a0685e70de389087be/llama-index-integrations/embeddings/llama-index-embeddings-openai/llama_index/embeddings/openai/base.py#L27\n",
    "embedding model names are fixed in code\n",
    "'''\n",
    "\n",
    "# embed_model = OpenAIEmbedding(\n",
    "#     model = settings.embedding_model,\n",
    "#     dimensions = 1024,\n",
    "#     api_key = settings.embedding_api_key,\n",
    "#     api_base = settings.embedding_base_url,\n",
    "#     embed_batch_size=10\n",
    "# )\n",
    "# Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text-embedding-inference\n",
    "from llama_index.embeddings.text_embeddings_inference import (\n",
    "    TextEmbeddingsInference,\n",
    ")\n",
    "# https://docs.llamaindex.ai/en/stable/examples/embeddings/text_embedding_inference/\n",
    "embed_model = TextEmbeddingsInference(\n",
    "    model_name=settings.embedding_model,\n",
    "    base_url=settings.embedding_base_url,\n",
    "    timeout=60,\n",
    "    embed_batch_size=10,\n",
    ")\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## llama index also supports langchain embedder\n",
    "# needs llama-index-embeddings-langchain install\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# embed_model = OpenAIEmbeddings(\n",
    "#     model=settings.embedding_model,\n",
    "#     api_key=settings.embedding_api_key\n",
    "# )\n",
    "# Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare DB\n",
    "* connect to pre-initialized postgresql db (pgvector docker container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "import textwrap\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. create db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB: pgvector_llamaindex\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "# connection_string = \"postgresql://{}:{}@localhost:{}/{}\".format(\n",
    "connection_string = \"postgresql://{}:{}@localhost:{}/{}\".format(\n",
    "    db_settings.postgres_user,\n",
    "    db_settings.postgres_password,\n",
    "    db_settings.postgres_port,\n",
    "    db_settings.postgres_db\n",
    ")\n",
    "\n",
    "db_name = db_settings.postgres_db\n",
    "print(f\"DB: {db_name}\")\n",
    "conn = psycopg2.connect(connection_string)\n",
    "conn.autocommit = True\n",
    "\n",
    "# We already have database created - skip creation\n",
    "# with conn.cursor() as c:\n",
    "#     c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "#     c.execute(f\"CREATE DATABASE {db_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. create index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize vector store instance\n",
    "url = make_url(connection_string)\n",
    "\n",
    "## hnsw indexing config\n",
    "hnsw_config = {\n",
    "    \"hnsw_m\": 16,\n",
    "    \"hnsw_ef_construction\": 64,\n",
    "    \"hnsw_ef_search\": 40,\n",
    "    \"hnsw_dist_method\": \"vector_cosine_ops\",\n",
    "}\n",
    "\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=url.host,\n",
    "    password=url.password,\n",
    "    port=url.port,\n",
    "    user=url.username,\n",
    "    table_name=\"test_documents\",\n",
    "    embed_dim=1024,  #bge-m3\n",
    "    hnsw_kwargs=hnsw_config,\n",
    ")\n",
    "\n",
    "## create storage context\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Insert documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = Document(\n",
    "    text = \"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata = {\"source\": \"tweet\"},\n",
    "    text_template='{content}'\n",
    ")\n",
    "\n",
    "document2 = Document(\n",
    "    text = \"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata = {\"source\": \"news\"},\n",
    "    text_template='{content}'\n",
    ")\n",
    "documents = [document1, document2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd3c9a472034b55ab695eba882debe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2da10794344674b31c34bb31ea4ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Insert Documents\n",
    "'''\n",
    "ProgrammingError: (psycopg2.errors.UndefinedTable) relation \"public.data_test_documents\" does not exist\n",
    "LINE 1: INSERT INTO public.data_test_documents (text, metadata_, nod...\n",
    "'''\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Schemas in the Database:\n",
      "  - public\n",
      "  - information_schema\n",
      "  - pg_catalog\n",
      "  - pg_toast\n",
      "\n",
      "Table: public.data_test_documents\n",
      "  - Column: id\n",
      "    Data Type: bigint\n",
      "    Nullable:  NO\n",
      "    Default:   nextval('data_test_documents_id_seq'::regclass)\n",
      "  - Column: text\n",
      "    Data Type: character varying\n",
      "    Nullable:  NO\n",
      "    Default:   None\n",
      "  - Column: metadata_\n",
      "    Data Type: json\n",
      "    Nullable:  YES\n",
      "    Default:   None\n",
      "  - Column: node_id\n",
      "    Data Type: character varying\n",
      "    Nullable:  YES\n",
      "    Default:   None\n",
      "  - Column: embedding\n",
      "    Data Type: USER-DEFINED\n",
      "    Nullable:  YES\n",
      "    Default:   None\n",
      "\n",
      "Table: public.paper_information\n",
      "  - Column: id\n",
      "    Data Type: integer\n",
      "    Nullable:  NO\n",
      "    Default:   nextval('paper_information_id_seq'::regclass)\n",
      "  - Column: created_at\n",
      "    Data Type: timestamp without time zone\n",
      "    Nullable:  YES\n",
      "    Default:   now()\n",
      "  - Column: updated_at\n",
      "    Data Type: timestamp without time zone\n",
      "    Nullable:  YES\n",
      "    Default:   now()\n",
      "  - Column: paper_id\n",
      "    Data Type: text\n",
      "    Nullable:  NO\n",
      "    Default:   None\n",
      "  - Column: published_date\n",
      "    Data Type: timestamp without time zone\n",
      "    Nullable:  YES\n",
      "    Default:   now()\n",
      "  - Column: title\n",
      "    Data Type: text\n",
      "    Nullable:  NO\n",
      "    Default:   None\n",
      "  - Column: authors\n",
      "    Data Type: text\n",
      "    Nullable:  NO\n",
      "    Default:   ''::text\n",
      "  - Column: link\n",
      "    Data Type: text\n",
      "    Nullable:  NO\n",
      "    Default:   ''::text\n",
      "\n",
      "Table: public.paper_status\n",
      "  - Column: id\n",
      "    Data Type: integer\n",
      "    Nullable:  NO\n",
      "    Default:   nextval('paper_status_id_seq'::regclass)\n",
      "  - Column: created_at\n",
      "    Data Type: timestamp without time zone\n",
      "    Nullable:  YES\n",
      "    Default:   now()\n",
      "  - Column: updated_at\n",
      "    Data Type: timestamp without time zone\n",
      "    Nullable:  YES\n",
      "    Default:   now()\n",
      "  - Column: paper_information_id\n",
      "    Data Type: integer\n",
      "    Nullable:  YES\n",
      "    Default:   None\n",
      "  - Column: file_extension\n",
      "    Data Type: text\n",
      "    Nullable:  NO\n",
      "    Default:   'pdf'::text\n",
      "  - Column: parse_status\n",
      "    Data Type: text\n",
      "    Nullable:  NO\n",
      "    Default:   'PENDING'::text\n",
      "  - Column: extract_status\n",
      "    Data Type: text\n",
      "    Nullable:  NO\n",
      "    Default:   'PENDING'::text\n",
      "  - Column: split_status\n",
      "    Data Type: text\n",
      "    Nullable:  NO\n",
      "    Default:   'PENDING'::text\n",
      "  - Column: embed_status\n",
      "    Data Type: text\n",
      "    Nullable:  NO\n",
      "    Default:   'PENDING'::text\n"
     ]
    }
   ],
   "source": [
    "## Check DB Schema\n",
    "'''\n",
    "Table: public.data_test_documents\n",
    "  - Column: id\n",
    "    Data Type: bigint\n",
    "    Nullable:  NO\n",
    "    Default:   nextval('data_test_documents_id_seq'::regclass)\n",
    "  - Column: text\n",
    "    Data Type: character varying\n",
    "    Nullable:  NO\n",
    "    Default:   None\n",
    "  - Column: metadata_\n",
    "    Data Type: json\n",
    "    Nullable:  YES\n",
    "    Default:   None\n",
    "  - Column: node_id\n",
    "    Data Type: character varying\n",
    "    Nullable:  YES\n",
    "    Default:   None\n",
    "  - Column: embedding\n",
    "    Data Type: USER-DEFINED\n",
    "    Nullable:  YES\n",
    "    Default:   None\n",
    "'''\n",
    "\n",
    "conn = psycopg2.connect(connection_string)\n",
    "with conn.cursor() as cur:\n",
    "    # --- Print out all schema names ---\n",
    "    print(\"All Schemas in the Database:\")\n",
    "    cur.execute(\"SELECT schema_name FROM information_schema.schemata;\")\n",
    "    schemas = cur.fetchall()\n",
    "    for schema in schemas:\n",
    "        print(f\"  - {schema[0]}\")\n",
    "        \n",
    "    ## Print table schemas\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT table_schema, table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema NOT IN ('information_schema', 'pg_catalog')\n",
    "        ORDER BY table_schema, table_name;\n",
    "    \"\"\")\n",
    "    tables = cur.fetchall()\n",
    "\n",
    "    # 3. Print the schema (columns) of each table\n",
    "    for schema_name, table_name in tables:\n",
    "        print(f\"\\nTable: {schema_name}.{table_name}\")\n",
    "        \n",
    "        # Fetch column details from information_schema.columns\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT column_name, data_type, is_nullable, column_default\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_schema = %s\n",
    "            AND table_name   = %s\n",
    "            ORDER BY ordinal_position;\n",
    "        \"\"\", (schema_name, table_name))\n",
    "        \n",
    "        columns = cur.fetchall()\n",
    "        if not columns:\n",
    "            print(\"  (No columns found)\")\n",
    "        else:\n",
    "            for col_name, col_type, is_nullable, default_val in columns:\n",
    "                print(f\"  - Column: {col_name}\")\n",
    "                print(f\"    Data Type: {col_type}\")\n",
    "                print(f\"    Nullable:  {is_nullable}\")\n",
    "                print(f\"    Default:   {default_val}\")\n",
    "\n",
    "    cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_args = {\"similarity_top_k\": 10}\n",
    "retriever = index.as_retriever(**retriever_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever at 0x172193bb0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='539851ee-16da-4e77-896a-d3616d0c808c', embedding=None, metadata={'source': 'tweet'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='188cc4ce-27e6-48a7-b1e4-03541436f117', node_type='4', metadata={'source': 'tweet'}, hash='c211cf902096529c230ab1394516787d54390c9228e4f376fa12fa2c9699a6d9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='I had chocalate chip pancakes and scrambled eggs for breakfast this morning.', mimetype='text/plain', start_char_idx=0, end_char_idx=76, metadata_seperator='\\n', text_template='{content}'), score=0.37611465142790657),\n",
       " NodeWithScore(node=TextNode(id_='29822fbf-3667-4186-a3ae-fc14ed13716b', embedding=None, metadata={'source': 'news'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='78feaa43-4954-4ef9-9778-8bdaec765188', node_type='4', metadata={'source': 'news'}, hash='bc10f715e156bdaa19bc3ba95c1a166cbc6df58a67d90e04641c3dc4ed6eb91a')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.', mimetype='text/plain', start_char_idx=0, end_char_idx=84, metadata_seperator='\\n', text_template='{content}'), score=0.3248268768294533)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Simple query\n",
    "query = \"LangChain provides abstractions to make working with LLMs easy\"\n",
    "nodes = retriever.retrieve(query)\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## metadata filtering\n",
    "from llama_index.core.vector_stores.types import (\n",
    "    MetadataFilter,\n",
    "    MetadataFilters,\n",
    ")\n",
    "\n",
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(key=\"source\", value=\"tweet\"),\n",
    "    ],\n",
    "    condition=\"or\",\n",
    ")\n",
    "\n",
    "retriever = index.as_retriever(\n",
    "    filters=filters,\n",
    "    **retriever_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='539851ee-16da-4e77-896a-d3616d0c808c', embedding=None, metadata={'source': 'tweet'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='188cc4ce-27e6-48a7-b1e4-03541436f117', node_type='4', metadata={'source': 'tweet'}, hash='c211cf902096529c230ab1394516787d54390c9228e4f376fa12fa2c9699a6d9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='I had chocalate chip pancakes and scrambled eggs for breakfast this morning.', mimetype='text/plain', start_char_idx=0, end_char_idx=76, metadata_seperator='\\n', text_template='{content}'), score=0.37611465142790657)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = retriever.retrieve(query)\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nested filters\n",
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilters(\n",
    "            filters=[\n",
    "                MetadataFilter(key=\"source\", value=\"tweet\"),\n",
    "            ],\n",
    "            condition=\"or\",\n",
    "        ),\n",
    "        MetadataFilters(\n",
    "            filters=[\n",
    "                MetadataFilter(key=\"source\", value=\"news\"),\n",
    "            ],\n",
    "            condition=\"or\",\n",
    "        ),\n",
    "    ],\n",
    "    condition=\"or\",\n",
    ")\n",
    "retriever = index.as_retriever(\n",
    "    filters=filters,\n",
    "    **retriever_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='539851ee-16da-4e77-896a-d3616d0c808c', embedding=None, metadata={'source': 'tweet'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='188cc4ce-27e6-48a7-b1e4-03541436f117', node_type='4', metadata={'source': 'tweet'}, hash='c211cf902096529c230ab1394516787d54390c9228e4f376fa12fa2c9699a6d9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='I had chocalate chip pancakes and scrambled eggs for breakfast this morning.', mimetype='text/plain', start_char_idx=0, end_char_idx=76, metadata_seperator='\\n', text_template='{content}'), score=0.37611465142790657),\n",
       " NodeWithScore(node=TextNode(id_='29822fbf-3667-4186-a3ae-fc14ed13716b', embedding=None, metadata={'source': 'news'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='78feaa43-4954-4ef9-9778-8bdaec765188', node_type='4', metadata={'source': 'news'}, hash='bc10f715e156bdaa19bc3ba95c1a166cbc6df58a67d90e04641c3dc4ed6eb91a')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.', mimetype='text/plain', start_char_idx=0, end_char_idx=84, metadata_seperator='\\n', text_template='{content}'), score=0.3248268768294533)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = retriever.retrieve(query)\n",
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. Test 'Query'\n",
    "* using llm call to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "Settings.llm=None\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\n",
      "\n",
      "The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: LangChain provides abstractions to make working with LLMs easy\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
