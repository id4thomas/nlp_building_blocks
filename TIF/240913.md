# 2024.09.13
## (Apple) AdEMAMix Optimizer
* The AdEMAMix Optimizer: Better, Faster, Older
    * https://arxiv.org/abs/2409.03137
    * https://github.com/nanowell/AdEMAMix-Optimizer-Pytorch
* https://x.com/A_K_Nain/status/1834057048939507859
* A Novel Optimization Approach Leveraging Dual Exponential Moving Averages to Enhance Gradient Efficiency and Improve Large-Scale Model Training Performance
    * extend traditional Adam by incorporating mixture of 2 EMA (Exponential Moving Averages)
        * allow optimizer to balance 'need to respond to recent updates' & 'retain valuable older gradients' (often discarde by existing optimizers)
        * older gradients help converge faster and often to lower minima
    * combine 2 EMA with different decay rates
    * significantly slows down model forgetting during training

## (Apple) Sigmoid Attention
* https://x.com/rohanpaul_ai/status/1833654018109055391
* Theory, Analysis, and Best Practices for Sigmoid Self-Attention
    * https://arxiv.org/abs/2409.04431
    * https://github.com/apple/ml-sigmoid-attention
* Replace the traditional Softmax in Attention with a Sigmoid and a constant (not learned) scalar bias based on the sequence length.
    * 17% inference kernel speed-up over FlashAttention-2 on H100
    * Softmax attention in transformers has limitations
        * SigmoidAttn has improved regularity compared to SoftmaxAttn
* Method
    * Replace softmax with sigmoid activation in attention mechanism
    * Introduce bias term b = -log(n) to mitigate large initial attention norms
    * Apply LayerScale and QK norm for improved stability


# Models
## (Google) DataGemma - RAG Gemma / RIG Gemma
* https://x.com/_philschmid/status/1834215774921035937
    * https://huggingface.co/collections/google/datagemma-release-66df7636084d2b150a4e6643
    * 27b instruction models
* DataGemma: Using real-world data to address AI hallucinations
    * https://blog.google/technology/ai/google-datagemma-ai-llm/
    * RAG (Retrieval-Augmented Generation) Gemma:
        * Input: user query
        * Output: list of queries that can be used to answer the user query and can be understood by Data Commons' existing NLI
            * Data Commons: https://docs.datacommons.org/papers/DataGemma-FullPaper.pdf
    * RIG (Retrieval-Integrated Generation):
        * Input: question / prompt
        * Output: generated response with placement for retrieved statistics annotated
            * Denoted as `[__DC__("<query to fetch the stats>") --> "<LLM Generated Stats>"]`
            * Use retrieved information, but also utilize info inside LLM 
```
# RIG Example Output
**2010-2020:** The female population in Sunnyvale has been steadily increasing, reaching a [__DC__("what was the percentage of females in Sunnyvale, CA in 2020?") --> "51.6%"] majority 
```
