# 2024.03.12
## cosine similarity가 정말 similarity일까? - Netflix
* Is Cosine-Similarity of Embeddings Really About Similarity?
	* https://arxiv.org/abs/2403.05440
	* https://news.ycombinator.com/item?id=39675585 - hackernews 쪽 언급, 댓글 내용 유용함
	* RAG위한 retrieval에 영향을 주는 만큼 고민을 해봐야함 

## RAPTOR - Recursive Abstractive Processing for Tree-Organized Retrieval
* RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval
	* https://arxiv.org/abs/2401.18059
	* cluster chunks -> summarize chunks -> construct tree
	* tree-search of flattened tree 
* https://llamahub.ai/l/llama-packs/llama-index-packs-raptor?from
	* llama index 쪽 붙이는 pack
	* https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-raptor/examples/raptor.ipynb

## neuralmagic-vllm fork
* https://github.com/neuralmagic/nm-vllm
	* https://twitter.com/neuralmagic/status/1767207628713664836
	* ex. sparse FP16, 4-bit inference
* https://huggingface.co/neuralmagic
	* hf에 compressed, sparse 모델들 제공

## GEAR - efficient KV Cache compression
* GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM
	* https://arxiv.org/abs/2403.05527
	* https://github.com/opengear-project/GEAR

## OpenAI transformer debugger
* https://github.com/openai/transformer-debugger
	* https://twitter.com/janleike/status/1767347608065106387
	* 내부 디버깅 툴
	* ex. "Why does the model output token A instead of token B for this prompt?" or "Why does attention head H to attend to token T for this prompt?"