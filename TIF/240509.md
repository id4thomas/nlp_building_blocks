# 2024.05.09
## (Microsoft) vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention
* https://arxiv.org/abs/2405.04437
	* vLLM PagedAttention 쪽 non-contiguous virtual memory 캐싱
	* vAttention은 system 단 페이징 활용, contiguous virtual memory 사용
*  However, to be able to allocate physical memory dynamically, PagedAttention changes the layout of KV-cache from contiguous virtual memory to non-contiguous virtual memory
* PagedAttention necessitates re-writing the attention kernel
* In contrast to PagedAttention, vAttention retains KV-cache in contiguous virtual memory and leverages low-level system support for demand paging, that already exists, to enable on-demand physical memory allocation.
## Memory Available vs Free?
* https://www.baeldung.com/linux/free-available-cached-memory
	* Linux systems distinguish free/available
	* Free: not used memory
	* Available: may be in use at the moment, but system can reclaim when needed
		* cached memory that can be freed
* 추가 궁금 내용
	* free 할때 바로 메모리 반환 안되는 경우?
		* https://www.linuxquestions.org/questions/linux-newbie-8/freed-memory-does-not-return-to-system-is-it-able-to-used-by-running-application-4175521572/
		* "though the program freed memory , but it does not return to system immidiately instead, it is kept in the application for reuse. when the program terminated, the memory then return to system."
		* "Over a certain chunk size (which I think is hard wired inside malloc) memory is returned as soon as it is freed."
	* guarantee that when memory is freed OS will reclaim?
		* https://stackoverflow.com/questions/48358229/how-can-i-get-a-guarantee-that-when-a-memory-is-freed-the-os-will-reclaim-that
		* "If you program to a Posix target, you might want to use mmap() directly instead of malloc to guarantee that the memory is returned to the system once deallocated with munmap()"
## torchtune - PyTotch native library for finetuning LLMs
* https://twitter.com/kakemeister/status/1780281318506668370
* https://github.com/pytorch/torchtune
## PyTorch 2 internals 자료
* https://drive.google.com/file/d/1XBox0G3FI-71efQQjmqGh0-VkCd-AHPL/view
## Is flash attention stable
* https://arxiv.org/abs/2405.02803