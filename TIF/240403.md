# 2024.04.03
## Google - Gecko
* Gecko: Versatile Text Embeddings Distilled from Large Language Models
	* https://arxiv.org/abs/2403.20327
	* compact embedding model by distilling knowledge from LLM into a retriever
		* 1.2B model, 2 training stages (pre-finetuning, fine-tuning)
		* novelty in the fine-tuning dataset (FRet - Few-shot Prompted Retrieval dataset)
* 2 step distillation
	* 1. generate synthetic paired data with LLM
	* 2. refine by retrieving set of candidate passages for each query & relabeling positive and hard negative passages using the same LLM
ex.
```
Seed Passage:
Recently, Marvelâ€™s The Eternals has become the topic of a great deal of online discourse, in part because of a scene where Phastos, a character blessed with the power of invention, helps humanity create the atomic bomb. As you can probably imagine, Twitter saw this and lost it.

Generated task:
Given a query, find a passage that has the answer to the query.

Generated Query:
who made the atomic bomb?

LLM-mined Positive & Negative
...
```
## ClovaX technical report
* https://huggingface.co/papers/2404.01954
	* https://arxiv.org/abs/2404.01954
* pretraining
	* Data: comprised of Korean, multilingual, and code segment
	* Tokenizer: trained a morpheme-aware byte-level BPE, vocab size 100,000
	* Training: PSM & SPM training (acquire in-filing performance during pretraining)
	* Context length: 90% of the training is executed with a context length of 4,096, and the last 10% of training with 32,768
## Langchain - RAG From Scratch: Indexing w/ ColBERT
* https://twitter.com/LangChainAI/status/1774117175089144215