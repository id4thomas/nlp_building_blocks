# 2024.03.08
## Gemma 쪽 버그
* https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing
* https://twitter.com/karpathy/status/1765473722985771335
* https://twitter.com/danielhanchen/status/1765446273661075609
* https://unsloth.ai/blog/gemma-bugs
	* bos, eos 관련
		* <bos> 가 꼭 추가 되어야 함
		* "<end_of_turn>model" 이 아니라 "<end_of_turn>"이 맞다고 함
		* newline이 꼭 필요하다. 안넣어주면 "\nSure, here is ..." 같이 값 나온다고 함
	* embedding normalization 부분에서 precision 따라서 rounding 오류가 남
		* sqrt(hidden_dim) 계산 부분에서 55.4256 가 나와야되는게 bfloat에서는 55.5 가 나옴
	* RoPE 관련 precision 오류들
	* GeLU가 exact tanh 가 아닌 approx tanh 여야함
* 보다보니 느낀점
	* 연산이 같아보이더라도 precision 관련 이슈로 학습 결과가 꽤 차이날 수 있나 싶음
	* bos, eos 등 pretrained 인스트럭션 모델 가져다 쓸때 조심해야 할 것
