# 2024.03.05
## AI Collapse - AI 결과물로 AI를 다시 학습하며 생기는 문제
* The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text
	* https://www.youtube.com/watch?si=srGPaH4XVZuSogQy&v=NcH7fHtqGYM&feature=youtu.be
	* https://arxiv.org/abs/2311.09807
	* nlg 태스크에 대해서 lexical, syntactic, and semantic diversity 측정으로 파악
	* diversity가 감소하는 것을 확인
## RAG Context Filtering
* FilCo: “Learning to Filter Context for Retrieval-Augmented Generation
	* Getting rid of irrelevant context (filters out distracting content)
	* https://huggingface.co/zorazrw
	* https://twitter.com/gneubig/status/1765459516064161815
	* https://twitter.com/ZhiruoW/status/1765453652301644152
## Galore
* Memory-Efficient LLM Training by Gradient Low-Rank Projection
	* lora도 결국 fullrank 학습 하는 것 보다 성능 부족 할 수 있음
	* full parameter 학습을 하면서 gradient를 projection 하는 방식
	* https://twitter.com/_akhaliq/status/1765598376312152538
	* https://huggingface.co/papers/2403.03507