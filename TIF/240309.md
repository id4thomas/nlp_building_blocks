# 2024.03.09
## kNN-LM에 
You can't pick your neighbors, or can you? When and how to rely on retrieval in the kNN-LM
* https://arxiv.org/abs/2210.15859
* knn-LM: interpolates any existing LM’s predictions with the output of a k- nearest neighbors model and requires no additional training
	* Retrieval-enhanced LM (condition their predictions on text retrieved from large external datastores)
	* Find 2 trends
		* presence of large overlapping n-grams between datastore & evaluation set plays importatnt factor in strong performance
		* kNN-LM is beneficial when retrieved items have high semantic similarity with the query 
	* define new formulation of kNN-LM that uses retrieval quality to assign the interpolation coefficient
* 질문들
	* Q. datastore & evaluation set이 어떤거 얘기?
	* Q. interpolation coefficient?
## ClearML Fractional GPU
* https://github.com/allegroai/clearml-fractional-gpu
	* cuda 11.x, 12.x 에 작동, pre-built **hard** memory limitation
	* driver-level memory limitation & compute time-slicing
	* multiple containers can be launched on the same GPU ensuring one user cannot allocate the entire host GPU memory
	* license: personal, research만 허용, commercial 별도
	* https://www.reddit.com/r/MachineLearning/comments/1baho3l/p_fractional_gpu_containers/
## Optimum-Nvidia
* https://twitter.com/rohanpaul_ai/status/1766715314358894693
* https://github.com/huggingface/optimum-nvidia
	* Ada Lovelace & Hopper 쪽 float8 사용 가능
```
- from transformers import AutoModelForCausalLM
+ from optimum.nvidia import AutoModelForCausalLM
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf", padding_side="left")

model = AutoModelForCausalLM.from_pretrained(
  "meta-llama/Llama-2-7b-chat-hf",
+ use_fp8=True,  
)
```
## python의 리스트 메모리 사용량 관련
* https://twitter.com/giffmana/status/1766129098261229619
```
>>> a = [1,2,3]
>>> b = [x for x in range(3)]
>>> type(a), type(b)
(<class 'list'>, <class 'list'>)
>>> import sys
>>> sys.getsizeof(a), sys.getsizeof(b)
(120, 88)
```
* 리스트 두개가 같아 보이지만 메모리 사이즈가 다르다
	* [1,2,3]의 경우 parser가 리스트 길이 3임을 안다
	* range로 초기화 하는 경우 parser가 list len을 모름 -> grows the list alloc buffer on the fly 

## tensorflow의 .numpy() 주의
* https://twitter.com/giffmana/status/1766464402402775533
```
type(tf.constant(["Hello"]).numpy()) >>> numpy.ndarray
type(tf.constant("Hello").numpy()) >>> bytes
```
* 원인: https://twitter.com/giffmana/status/1766464402402775533
	* numpy() 메서드 쪽에서 documentation & 동작 차이 - bug 의심