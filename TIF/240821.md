# LLM Serving vram calculation
* https://x.com/rohanpaul_ai/status/1824916606415237289
$$M=\frac{P*4B}{32/Q} * 1.2$$
* M: GPU Memory (GB)
* P: num params (ex. 7B)
* 4B: 4 bytes (32bit)
* Q: precision of loaded model (ex. 4, 8 bit)
* 1.2: Overhead

# [vLLM] llm-compressor: llm quantization & pruning toolkit from vllm
* https://github.com/vllm-project/llm-compressor

# phi-3.5
* https://huggingface.co/microsoft/Phi-3.5-mini-instruct#appendix-a
* built with synthetic & filtered public data (used for phi3)
* 128K context length, 3.8B (mini)
* variants: mini-instruct / MoE-instruct / vision-instruct


# Tree Attention: Topology-aware Decoding for Long-Context Attention on GPU clusters
* faster inference for large sequence lengths
	* derived from scalar energy function interpretation (?) of self-attention
	* tree reduction can be performed across the sequence axis due to associative properties of logsumexp & max operations
* https://x.com/J_Pilault/status/1823022658830954924
	* https://arxiv.org/abs/2408.04093
	* https://github.com/Zyphra/tree_attention
## deeper dive
'scalar energy function interpretation'
* self-attention can be viewed as computing energy function between pairs of elements in sequence
	* attention score between 2 elements of seq is computed by simil of their query & key vectors
	* energy function interpretation:
		* attention score a_ij can be interpreted as an energy function E(x_i,x_j)
		* measuring 'compatibility' or 'interaction' between elements
'associative properties of logsumexp & max'
* 'associative': operation can be grouped in any order
* logsumexp: handle sum of exponentials stablely (log(exp(x_1) + exp(x_2) + ..))
	* associative property: logsumexp(x1, logsumexp(x2, x3)) = logsumexp(logsumexp(x1, x2), x3)
'tree reduction'
* ex. with sequence x1 ~ x8
	* 1st level (pairwise): y1 = x1+x2 / y2 = x3+x4 / ...
	* 2nd level:y1+y2 / ...

# Example of inifinite KV Cache
* https://x.com/awnihannun/status/1824589078068859363
	* https://browse.arxiv.org/abs/2309.17453
	* Efficient Streaming Language Models with Attention Sinks - Xiao et al. ICLR 2024
* always keep the first n tokens
* set maximum cache size 

# KVQuant: KV Cache Quantization
* https://x.com/rohanpaul_ai/status/1825159696401019054
	* https://arxiv.org/abs/2401.18079
	* KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization - Hooper et al.
* incorporate novel methods to compressing KV cache to preserve accuracy compared to previous approaches