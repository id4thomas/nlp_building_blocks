# 2024.04.15
## ring attention - 어떻게 긴 context length를 가져가는가
* https://twitter.com/khshind/status/1778804574528135200
	* "presents a way to split attention calculation across GPUs while hiding the communication overhead in a ring, enabling zero overhead scaling"
* https://coconut-mode.com/posts/ring-attention/
## Measuring Cross-lingual Transfer in Bytes
* We measured the amount of data transferred from a source language to a target language and found that models initial- ized from diverse languages perform similarly to a target language in a cross-lingual setting.
## qdrant - hybrid search
* https://twitter.com/qdrant_engine/status/1779822723616719287
	* "You can now perform Hybrid Search using a single method with our Python SDK!"
* https://qdrant.tech/documentation/tutorials/hybrid-search-fastembed/?null