# 2024.02.27
## DPO 학습 예시
* https://twitter.com/vwxyzjn/status/1762253021486948745
* https://colab.research.google.com/drive/13-xxbWlahN9Vsi22c6zoZafBOWApvqm_?usp=sharing

## LM KL Divergence
PPO 같은 곳에서 원본 logprob와 멀어지지 않도록 KL Divergence를 주는 경우 소스로 확인
* Q. Query & Response 가 있는 학습 데이터 기준이라면 logprob를 Response로만 계산하도록 해야할지? (마스킹)
* trl 패키지 PPOTrainer 코드 기준

* KL Penalty 계산 부분
	* https://github.com/huggingface/trl/blob/2a2676e7ecdb623d6748f8f77a91d519c3869d98/trl/trainer/ppo_trainer.py#L1104
* KL Penalty입력되는 logprob 계산 부분
	* https://github.com/huggingface/trl/blob/2a2676e7ecdb623d6748f8f77a91d519c3869d98/trl/trainer/ppo_trainer.py#L730
* Batched Forward Pass
	* https://github.com/huggingface/trl/blob/2a2676e7ecdb623d6748f8f77a91d519c3869d98/trl/trainer/ppo_trainer.py#L942
	* https://github.com/huggingface/trl/blob/2a2676e7ecdb623d6748f8f77a91d519c3869d98/trl/trainer/ppo_trainer.py#L993
	* query_batch, response_batch 따라서 계산 부분 확인
