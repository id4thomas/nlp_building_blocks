# 2024.03.03
## Torch memory limit
* setting GPU memory limit per process
	* https://stackoverflow.com/questions/49529372/force-gpu-memory-limit-in-pytorch
	* https://pytorch.org/docs/stable/generated/torch.cuda.set_per_process_memory_fraction.html
```
gpu_id = 0
fraction = 0.8
torch.cuda.set_per_process_memory_fraction(fraction, gpu_id)
## Testing Memoty limit
torch.cuda.empty_cache()
total_memory = torch.cuda.get_device_properties(0).total_memory
tmp_tensor = torch.empty(int(total_memory * 0.499), dtype=torch.int8, device='cuda')
del tmp_tensor
torch.cuda.empty_cache()
# this allocation will raise a OOM:
torch.empty(total_memory // 2, dtype=torch.int8, device='cuda')
```

## Vespa Long-context ColBERT
* long-context BERT 
	* https://blog.vespa.ai/announcing-long-context-colbert-in-vespa/
	* https://twitter.com/jobergum/status/1763533608453644440
```
The official CoLBERT model checkpoint uses vanilla BERT as the base model, with a maximum context window size of 512 tokens. Furthermore, the model checkpoint is fine-tuned using a single dataset with short passage-length texts of up to 100 tokens.
```

## bm25 implementation in torch
* GPU-enabled BM25
* https://github.com/jxmorris12/bm25_pt
* https://twitter.com/jxmnop/status/1763586425855873065