# 2024.09.08
## TorchJD - Training models with multiple losses
* https://www.reddit.com/r/MachineLearning/comments/1fbvuhs/r_training_models_with_multiple_losses/
    * https://github.com/TorchJD/torchjd
    * Jacobian Descent For Multi-Objective Optimization
        * https://arxiv.org/abs/2406.16232
* vs training with summed losses
    * If you add the different losses and compute the gradient of the sum, it's exactly equivalent to computing the Jacobian and adding its rows
        * limitations: If you have two gradients that are conflicting (they have a negative inner product), simply summing them can result in an update vector that is conflicting with one of the two gradients
    * avoid this phenomenon by using the information from the Jacobian, and making sure that the update is always beneficial to all of the losses

## (Jina AI) Late Chunking
* https://x.com/rohanpaul_ai/status/1832213873392546025
    * https://weaviate.io/blog/late-chunking
    * https://github.com/jina-ai/late-chunking
* document chunking loses cross-chunk context
    * embeds entire document -> then chunk