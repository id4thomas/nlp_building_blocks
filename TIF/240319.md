# 2024.03.19
## LM이 long-context에서 어떻게 attend 하는가**
* https://twitter.com/Francis_YAO_/status/1765020836237603055
* https://yaofu.notion.site/How-Do-Language-Models-put-Attention-Weights-over-Long-Context-10250219d5ce42e8b465087c383a034e
	* transformer내의 attention 패턴 위주로 분석
	* 1-1 쪽에서 레이어 마다 어텐션 분포 특징 얘기함
## Dynamic memory compression
Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference
* https://arxiv.org/abs/2403.09636
* Dynamic Memory Compression (DMC) chooses whether to accumulate or append current items, resulting in a smaller key– value cache.
	* 해당 timestep kv 값을 append 할지 accumulate 할지 정해서 압축
## punica - run multiple LoRA finetuned
* CUDA 단에서 Segmented Gather Matrix-Vector multiplication (SGMV) 라는 연산 구현
	* n개의 LoRA 모델이 있을 때
	* input batch X := (x1,x2,...,xn) & input each maps to different LoRA
	* the output is Y := X@W + (x1@A1@B1, x2@A2@B2, ..., xn@An@Bn)
		* left-hand-side computes the input batch on the pretrained model
		* right-hand-side가 SGMV 연산으로 계산
* https://twitter.com/rohanpaul_ai/status/1767414843559043111
* https://github.com/punica-ai/punica
