# 2024.09.12
## Speculative Decoding for high-throughput long-context inferenc
* https://x.com/togethercompute/status/1831755765184704700
    * https://www.together.ai/blog/speculative-decoding-for-high-throughput-long-context-inference
* decoding becomes memory-bound in large-scale, large-context scenarios due to KV Cache Size
* Introduce 2 algorithmic improvements
    * MagicDec
        * reduce bottleneck when loading the KV cache
        * uses a fixed context window in the draft model to make the draft model many times faster than the target model (even for large draft models)
    * Adaptive Sequoia trees
        * tune how much we speculate as a function of the context length and batch size
        * use the Sequoia algorithm to construct the optimal tree at each speculation budget

## [RAG] metadata pre-filtering
* https://x.com/mariaKhalusova/status/1833520362602086764
    * similarity search isn't enough
    * Metadata pre-filtering can significantly improve the retrieval results
    * add custom metadata extraction to your unstructured data preprocessing pipeline
* Building an Advanced RAG System With Self-Querying Retrieval
    * https://www.mongodb.com/developer/products/atlas/advanced-rag-self-querying-retrieval/

# Models
## (jina.ai) reader-lm - generate clean markdown directly from noisy raw html
* https://x.com/JinaAI_/status/1833861180445860168
    * https://jina.ai/news/reader-lm-small-language-models-for-cleaning-and-converting-html-to-markdown/?nocache=1
* https://huggingface.co/jinaai/reader-lm-1.5b
    * 0.5b, 1b
    * multilingual and support a context length of up to 256K tokens
    * cc-by-nc

## (Mistral) pixtral-12b vlm
* https://huggingface.co/mistral-community/pixtral-12b-240910
    * https://github.com/mistralai/mistral-common/releases/tag/v1.4.0
* https://x.com/reach_vb/status/1833779749430124692
    * text: Mistral Nemo 12B
    * vision: 400M
        * GeLU (for vision adapter) & 2D RoPE (for vision encoder)
    * Three new special tokens  - `img`, `img_break`, `img_end`

## (Qwen) qwen2-vl
* qwen2 based multilingual visual-language model
* https://x.com/Alibaba_Qwen/status/1829187276179681634
    * qwen2-vl-2b, 7b under apache 2.0
    * https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct

## Llama-Omni: end-to-end speech model
* https://huggingface.co/ICTNLP/Llama-3.1-8B-Omni
* Speech Encoder - Whisper Large v3
* LLM backbone - Llama 3.1 8B Instruct
* Speech Decoder - HuBERT (UnitY)