# 2024.03.11
* DejaVu라는 llm 가속화 기법 2개가 있음 서로 다른 연구
## Deja Vu: Contextual Sparsity for efficient LLM inference - LG
Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time
* https://twitter.com/fly51fly/status/1718219645159670270
* https://arxiv.org/abs/2310.17157
	* predicts contextual sparsity on-the-fly for llm inference
	* formulates paramter sparsity prediction as near-neighbor search
		* asynchronous 하게 predict sparsity for attention head at next layer
* https://github.com/FMInference/DejaVu
## DejaVu: KV-cache Streaming for LLM Serving - MS
DéjàVu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving
* https://arxiv.org/abs/2403.01876
	* KV cache streaming library (De ́ja`VuLib)
	* propose efficient prompt-token disaggregation to reduce pipeline bubbles, microbatch swapping
## LLM vram calculators
* https://twitter.com/rohanpaul_ai/status/1766717801329541221
	* https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator
* Eluether 쪽에서도 만든거 하나 있음
	* https://github.com/EleutherAI/cookbook/tree/main/calc
	* 파라미터, FLOP, 메모리 계산 코드