# 2024.03.04
## 1-bit LLMs
* The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits
	* MS쪽 연구, full-precision급 성능 나온다고 함 웨이트를 -1, 0, 1 범위로 제한
	* Transformer의 nn.Linear -> BitLinear
		* BitNet - BitNet: Scaling 1-bit Transformers for Large Language Models
			* 이거도 기존 ms쪽 연구
			* https://arxiv.org/abs/2310.11453
			* https://github.com/Beomi/BitNet-Transformers - beomi님 재현 테스트
	* https://arxiv.org/abs/2402.17764
## bGPT - byte level transformer
* Beyond Language Models: Byte Models are Digital World Simulators
	* 1 token = 1 patch (token per byte 가 아님)
	* https://arxiv.org/abs/2402.17764
	* https://github.com/sanderwood/bgpt
	* https://byte-gpt.github.io
	* https://www.reddit.com/r/LocalLLaMA/comments/1b4ooec/bgpt_bytelevel_transformer/
		* 기존 meta쪽 MEGABYTE 변형으로 생각된다고 함
		* MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers
			* https://arxiv.org/abs/2305.07185
		* 추가 byte 입력 transformer 연구들
			* MambaByte: Token-free Selective State Space Model
				* https://arxiv.org/abs/2401.13660
