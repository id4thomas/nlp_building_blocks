# Controllable Text Generation for LLMs Survey
* https://huggingface.co/papers/2408.12599
* https://github.com/IAAR-Shanghai/CTGSurvey

# (Linkedin) Liger-Kernel: Efficient Triton Kernels for LLM Training
* https://github.com/linkedin/Liger-Kernel
* hf model compatible

# Stable FP8 recipe for Llama models
* https://x.com/Thom_Wolf/status/1826924774997532799
1. Clip learning rate when 2nd moment estimator is outdated by coming spike
2. SmoothQuant
3. Layer-wise scaling
4. Clipped Softmax
5. Avoid quantizing first and last layer 
6. Accumulate FP8 operations in bfloat16 and keep the residual stream in float32