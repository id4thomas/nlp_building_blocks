# Research
## T-Free: Subword Tokenizer-Free LLMs
* directly embeds words through sparse activation patterns over character triplets -> doesn't require reference corpus
* https://github.com/Aleph-Alpha/trigrams
* https://arxiv.org/abs/2406.19223
* https://x.com/Aleph__Alpha/status/1829086497363939672

## (NAACL 2024 Findings) Training LM on synthetic text impacts linguistic diversity
* The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text
    * https://arxiv.org/abs/2311.09807

# Models
## (answerdotai 2024.08) answerai-colbert-small-v1
* https://x.com/bclavie/status/1823405960406462739
    * https://www.answer.ai/posts/2024-08-13-small-but-mighty-colbert.html
    * https://huggingface.co/answerdotai/answerai-colbert-small-v1
* 33M param model (apache 2.0)

## (nvidia 2024.04) Llama3-ChatQA-1.5
* conversational QA & RAG oriented LLM
    * Trained with recipe from ChatQA paper
    * ChatQA: Surpassing GPT-4 on Conversational QA
and RAG
    * https://arxiv.org/pdf/2401.10225
* https://huggingface.co/nvidia/Llama3-ChatQA-1.5-70B
* 70B params, llama3 license

## (NIPS 2023 Spotlight - Apple) 4M: Masked Multimodal Maked Modeling
* framework for training any-to-any multimodal, multitask models
* https://4m.epfl.ch
* https://arxiv.org/abs/2312.06647

## Rubra - opensource llm trained with tool-calling capabilities
* https://github.com/rubra-ai/rubra

# Frameworks
## (mixedbread-ai 2024.08) batched - dynamic batching framework
* https://github.com/mixedbread-ai/batched

## FlexAttention - PyTorch API for implementing attention variants
* https://pytorch.org/blog/flexattention/
* allow user-defined function score_mod
    * modifies (QK^T/sqrt(d)) before softmax
* torch.compile lowers the function into a single fused FlexAttention kernel

## (Intel) RAG Foundry: Python framework for augmenting LLM for RAG
* RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation
    * https://arxiv.org/abs/2408.02545
    * https://github.com/IntelLabs/RAGFoundry
* help create data for training, help easily train with PEFT

## (mozilla) llamafile - LLM as a single file executable
* https://github.com/mozilla-Ocho/llamafile

# Discussions
## 'Reparameterization Trick' in VAE - trick?
* https://www.reddit.com/r/MachineLearning/comments/1f3ohje/d_clarification_on_the_reparameterization_trick/


## (Karpathy) RLHF is barely RL
* https://x.com/karpathy/status/1821277264996352246

## (Reddit) Why is ReLU considered non-linear activation?
* https://www.reddit.com/r/learnmachinelearning/comments/1ezq1nl/why_is_relu_considered_a_nonlinear_activation/