# Research
## T-Free: Subword Tokenizer-Free LLMs
* directly embeds words through sparse activation patterns over character triplets -> doesn't require reference corpus
* https://github.com/Aleph-Alpha/trigrams
* https://arxiv.org/abs/2406.19223
* https://x.com/Aleph__Alpha/status/1829086497363939672

## (NAACL 2024 Findings) Training LM on synthetic text impacts linguistic diversity
* The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text
    * https://arxiv.org/abs/2311.09807

# Models
## (nvidia 2024.08) NVEagle: vision-lm (7B, 13B, 13B-Chat)
* MoE vision encoders
* https://x.com/mervenoyann/status/1829144958101561681
* https://huggingface.co/collections/merve/nveagle-66d0705108582d73bb235c26
    * cc-by-nc

## (salesforce 2024.08) xGen-MM (BLIP-3): Family of Open large MM
* BLIP-3 is a framework for developing LMMs
    * Introduce xGen 
* https://www.salesforceairesearch.com/opensource/xGen-MM/index.html
    * https://github.com/salesforce/LAVIS/tree/xgen-mm
    * https://huggingface.co/Salesforce/xgen-mm-phi3-mini-instruct-singleimg-r-v1.5
        * ph3-mini based (4b, apache 2.0)
* https://huggingface.co/papers/2408.08872

## (answerdotai 2024.08) answerai-colbert-small-v1
* https://x.com/bclavie/status/1823405960406462739
    * https://www.answer.ai/posts/2024-08-13-small-but-mighty-colbert.html
    * https://huggingface.co/answerdotai/answerai-colbert-small-v1
* 33M param model (apache 2.0)

## (nvidia 2024.04) Llama3-ChatQA-1.5
* conversational QA & RAG oriented LLM
    * Trained with recipe from ChatQA paper
    * ChatQA: Surpassing GPT-4 on Conversational QA
and RAG
    * https://arxiv.org/pdf/2401.10225
* https://huggingface.co/nvidia/Llama3-ChatQA-1.5-70B
* 70B params, llama3 license

## (NIPS 2023 Spotlight - Apple) 4M: Masked Multimodal Maked Modeling
* framework for training any-to-any multimodal, multitask models
* https://4m.epfl.ch
* https://arxiv.org/abs/2312.06647

## Rubra - opensource llm trained with tool-calling capabilities
* https://github.com/rubra-ai/rubra

# Frameworks
## torch compile with torchao
* https://x.com/reach_vb/status/1828892506320159172
    * https://github.com/huggingface/huggingface-llama-recipes/blob/main/torch_compile_with_torchao.ipynb
```
# `torch.compile(model, ...)` is not recommended as you compile callbacks
# and full generate. We recommend compiling only the forward for now. 
# "reduce-overhead" will use cudagraphs. 
# Compile with torchao requires torch 2.5 or torch nighlty if the version is still not on pipy
model.forward = torch.compile(model.forward, mode="reduce-overhead", fullgraph=True)
```

## (mixedbread-ai 2024.08) batched - dynamic batching framework
* https://github.com/mixedbread-ai/batched

## FlexAttention - PyTorch API for implementing attention variants
* https://pytorch.org/blog/flexattention/
* allow user-defined function score_mod
    * modifies (QK^T/sqrt(d)) before softmax
* torch.compile lowers the function into a single fused FlexAttention kernel

## (Intel) RAG Foundry: Python framework for augmenting LLM for RAG
* RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation
    * https://arxiv.org/abs/2408.02545
    * https://github.com/IntelLabs/RAGFoundry
* help create data for training, help easily train with PEFT

## (mozilla) llamafile - LLM as a single file executable
* https://github.com/mozilla-Ocho/llamafile

# Discussions
## 'Reparameterization Trick' in VAE - trick?
* https://www.reddit.com/r/MachineLearning/comments/1f3ohje/d_clarification_on_the_reparameterization_trick/


## (Karpathy) RLHF is barely RL
* https://x.com/karpathy/status/1821277264996352246

## (Reddit) Why is ReLU considered non-linear activation?
* https://www.reddit.com/r/learnmachinelearning/comments/1ezq1nl/why_is_relu_considered_a_nonlinear_activation/