# 2024.03.14
## Torch에서 allocate 한 메모리와 nvidia-smi로 실제 잡혀있는 프로세스 gpu 메모리가 다른부분 디버깅
* https://github.com/pytorch/pytorch/issues/101159
* 상세 분석한 글 - https://github.com/heojae/FoodImageRotationAdmin/issues/33
	* https://discuss.pytorch.org/t/clearing-the-gpu-is-a-headache/84762
		* note that PyTorch loads the CUDA kernels, cudnn, CUDA runtime etc. after the first CUDA operation
		* which will also allocate memory (and cannot be freed until the script exits).
		* Depending on the device, CUDA version etc. this CUDA context might take ~700MB.
	* 요약:
		* CUDA context만 해도 용량을 꽤 잡을 수가 있다
간단하게 실험 가능한 방법: (notebook (interactive) 환경에서는 잘 안되서 스크립트로 하기)
```
model = torch.randn(1024*1024).cuda()
# model 이 cuda로 올라간 상태에서 확인
print(torch.cuda.memory_allocated()/1024**2)	## 4.0
print(torch.cuda.memory_cached()/1024**2)		## 20.0

torch.cuda.empty_cache()
# empty_cache가 의미 있었는지 확인
print(torch.cuda.memory_allocated()/1024**2)	## 4.0
print(torch.cuda.memory_cached()/1024**2)		## 20.0

del model
# del 후 메모리 변경 사항 확인
print(torch.cuda.memory_allocated()/1024**2)	## 0.0
print(torch.cuda.memory_cached()/1024**2)		## 20.0

torch.cuda.empty_cache()
# del 후 cache clear 까지 한 후 확인
print(torch.cuda.memory_allocated()/1024**2)	## 0.0
print(torch.cuda.memory_cached()/1024**2)		## 0.0
```
## Cohere - Command-R
* https://twitter.com/aidangomez/status/1767264315550163024
	* 35b짜리 모델