# Transformer

Studying the Transformer Architecture

## Multi-head Attention
### Scaled Dot-Product Attention

### Multi-head Attention

## Feed-forward Network

## Positional Embedding
Adds ordering information

## Training Transformer Etc.
Warmup learning-rate scheduler


## References
[1] https://hongl.tistory.com/231