# Transformer

Studying the Transformer Architecture

## Multi-head Attention
### Scaled Dot-Product Attention

### Multi-head Attention

## Feed-forward Network

## Positional Embedding
Adds ordering information

## Training Transformer Etc.
Warmup learning-rate scheduler


## References
[1] https://hongl.tistory.com/231<br>
[2] Vaswani, Ashish et al. “Attention is All you Need.” ArXiv abs/1706.03762 (2017): n. pag.<br>