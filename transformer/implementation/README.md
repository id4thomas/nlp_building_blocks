# Transformer Implementation

## attention.py
### MultiHeadAttention
Initializes <i>num_heads</i> number of ScaledDotProduct Attention instances<br>
Concatenates attention results and projects with <i>att_proj</i> layer.
### ScaledDotProductAttention



