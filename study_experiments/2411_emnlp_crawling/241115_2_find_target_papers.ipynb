{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def word_overlap_match(query, candidates, threshold = 90):\n",
    "    def calculate_overlap(query_words, candidate_words):\n",
    "        query_count = Counter(query_words)\n",
    "        candidate_count = Counter(candidate_words)\n",
    "        overlap_count = sum((query_count & candidate_count).values())\n",
    "        return overlap_count / max(len(query_words), len(candidate_words)) * 100\n",
    "\n",
    "    query_words = query.split()\n",
    "    for index, candidate in enumerate(candidates):\n",
    "        candidate_words = candidate.split()\n",
    "        overlap_percentage = calculate_overlap(query_words, candidate_words)\n",
    "        if overlap_percentage > threshold:\n",
    "            return index\n",
    "    return -1  # Return -1 if no candidate meets the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_name = \"emnlp-2024\"\n",
    "conf_id = \"2024emnlp-main\"\n",
    "conf_id = \"2024emnlp-demo\"\n",
    "conf_id = \"2024emnlp-industry\"\n",
    "conf_id = \"2024emnlp-tutorials\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num papers: 1444\n"
     ]
    }
   ],
   "source": [
    "with open(f'{event_name}.json', 'r', encoding='utf8') as f:\n",
    "    paper_list = json.load(f)\n",
    "print(\"Num papers: {}\".format(len(paper_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_titles = [x[1] for x in paper_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "query_name = \"query1\"\n",
    "with open(f\"queries/{query_name}.txt\", \"r\") as f:\n",
    "    queries = [x.strip() for x in f.readlines()]\n",
    "print(len(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "def get_abstract_text(abs_url: str) -> str:\n",
    "    html_doc = requests.get(abs_url).text\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    abstract_span = soup.select_one('div.acl-abstract span')\n",
    "    abstract_text = abstract_span.get_text(strip=True) if abstract_span else \"Abstract not found.\"\n",
    "    return abstract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:10<00:00,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "search_results = []\n",
    "found_count = 0\n",
    "for query in tqdm(queries):\n",
    "    idx = word_overlap_match(query, paper_titles, threshold=80)\n",
    "    if idx==-1:\n",
    "        continue\n",
    "    found_count+=1\n",
    "    # print(idx, query)\n",
    "    search_result = {\n",
    "        \"query\": query,\n",
    "        \"id\": paper_list[idx][0],\n",
    "        \"title\": paper_list[idx][1],\n",
    "        \"abstract_url\": paper_list[idx][2],\n",
    "        \"paper_url\": paper_list[idx][3],\n",
    "        \"abstract\": get_abstract_text(paper_list[idx][2])\n",
    "    }\n",
    "    search_results.append(search_result)\n",
    "print(len(search_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"query\": \"Dense X Retrieval: What Retrieval Granularity Should We Use?\",\n",
      "    \"id\": \"2024.emnlp-main.845\",\n",
      "    \"title\": \"Dense X Retrieval: What Retrieval Granularity Should We Use?\",\n",
      "    \"abstract_url\": \"https://aclanthology.org/2024.emnlp-main.845/\",\n",
      "    \"paper_url\": \"https://aclanthology.org/2024.emnlp-main.845.pdf\",\n",
      "    \"abstract\": \"Dense retrieval has become a prominent method to obtain relevant context or world knowledge in open-domain NLP tasks. When we use a learned dense retriever on a retrieval corpus at inference time, an often-overlooked design choice is the retrieval unit in which the corpus is indexed, e.g. document, passage, or sentence. We discover that the retrieval unit choice significantly impacts the performance of both retrieval and downstream tasks. Distinct from the typical approach of using passages or sentences, we introduce a novel retrieval unit, proposition, for dense retrieval. Propositions are defined as atomic expressions within text, each encapsulating a distinct factoid and presented in a concise, self-contained natural language format. We conduct an empirical comparison of different retrieval granularity. Our experiments reveal that indexing a corpus by fine-grained units such as propositions significantly outperforms passage-level units in retrieval tasks. Moreover, constructing prompts with fine-grained retrieved units for retrieval-augmented language models improves the performance of downstream QA tasks given a specific computation budget.\"\n",
      "}\n",
      "{\n",
      "    \"query\": \"HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs\",\n",
      "    \"id\": \"2024.emnlp-main.59\",\n",
      "    \"title\": \"HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs\",\n",
      "    \"abstract_url\": \"https://aclanthology.org/2024.emnlp-main.59/\",\n",
      "    \"paper_url\": \"https://aclanthology.org/2024.emnlp-main.59.pdf\",\n",
      "    \"abstract\": \"Empathy serves as a cornerstone in enabling prosocial behaviors, and can be evoked through sharing of personal experiences in stories. While empathy is influenced by narrative content, intuitively, people respond to the way a story is told as well, through narrative style. Yet the relationship between empathy and narrative style is not fully understood. In this work, we empirically examine and quantify this relationship between style and empathy using LLMs and large-scale crowdsourcing studies. We introduce a novel, theory-based taxonomy, HEART (Human Empathy and Narrative Taxonomy) that delineates elements of narrative style that can lead to empathy with the narrator of a story. We establish the performance of LLMs in extracting narrative elements from HEART, showing that prompting with our taxonomy leads to reasonable, human-level annotations beyond what prior lexicon-based methods can do. To show empirical use of our taxonomy, we collect a dataset of empathy judgments of stories via a large-scale crowdsourcing study withN=2,624participants. We show that narrative elements extracted via LLMs, in particular, vividness of emotions and plot volume, can elucidate the pathways by which narrative style cultivates empathy towards personal stories. Our work suggests that such models can be used for narrative analyses that lead to human-centered social and behavioral insights.\"\n",
      "}\n",
      "{\n",
      "    \"query\": \"Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models\",\n",
      "    \"id\": \"2024.emnlp-main.248\",\n",
      "    \"title\": \"Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models\",\n",
      "    \"abstract_url\": \"https://aclanthology.org/2024.emnlp-main.248/\",\n",
      "    \"paper_url\": \"https://aclanthology.org/2024.emnlp-main.248.pdf\",\n",
      "    \"abstract\": \"Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of open-source LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2, a more powerful evaluator LM than its predecessor that closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, Prometheus 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available.\"\n",
      "}\n",
      "{\n",
      "    \"query\": \"RAGViz: Diagnose and Visualize Retrieval-Augmented Generation\",\n",
      "    \"id\": \"2024.emnlp-demo.33\",\n",
      "    \"title\": \"RAGViz: Diagnose and Visualize Retrieval-Augmented Generation\",\n",
      "    \"abstract_url\": \"https://aclanthology.org/2024.emnlp-demo.33/\",\n",
      "    \"paper_url\": \"https://aclanthology.org/2024.emnlp-demo.33.pdf\",\n",
      "    \"abstract\": \"Retrieval-augmented generation (RAG) combines knowledge from domain-specific sources into large language models to ground answer generation. Current RAG systems lack customizable visibility on the context documents and the model\\u2019s attentiveness towards such documents. We propose RAGViz, a RAG diagnosis tool that visualizes the attentiveness of the generated tokens in retrieved documents. With a built-in user interface, retrieval index, and Large Language Model (LLM) backbone, RAGViz provides two main functionalities: (1) token and document-level attention visualization, and (2) generation comparison upon context document addition and removal. As an open-source toolkit, RAGViz can be easily hosted with a custom embedding model and HuggingFace-supported LLM backbone. Using a hybrid ANN (Approximate Nearest Neighbor) index, memory-efficient LLM inference tool, and custom context snippet method, RAGViz operates efficiently with a median query time of about 5 seconds on a moderate GPU node. Our code is available at https://github.com/cxcscmu/RAGViz. A demo video of RAGViz can be found at https://youtu.be/cTAbuTu6ur4.\"\n",
      "}\n",
      "{\n",
      "    \"query\": \"PAIRDISTILL: Pairwise Relevance Distillation for Dense Retrieval\",\n",
      "    \"id\": \"2024.emnlp-main.1013\",\n",
      "    \"title\": \"PairDistill: Pairwise Relevance Distillation for Dense Retrieval\",\n",
      "    \"abstract_url\": \"https://aclanthology.org/2024.emnlp-main.1013/\",\n",
      "    \"paper_url\": \"https://aclanthology.org/2024.emnlp-main.1013.pdf\",\n",
      "    \"abstract\": \"Effective information retrieval (IR) from vast datasets relies on advanced techniques to extract relevant information in response to queries. Recent advancements in dense retrieval have showcased remarkable efficacy compared to traditional sparse retrieval methods. To further enhance retrieval performance, knowledge distillation techniques, often leveraging robust cross-encoder rerankers, have been extensively explored. However, existing approaches primarily distill knowledge from pointwise rerankers, which assign absolute relevance scores to documents, thus facing challenges related to inconsistent comparisons. This paper introduces Pairwise Relevance Distillation (PairDistill) to leverage pairwise reranking, offering fine-grained distinctions between similarly relevant documents to enrich the training of dense retrieval models. Our experiments demonstrate that PairDistill outperforms existing methods, achieving new state-of-the-art results across multiple benchmarks. This highlights the potential of PairDistill in advancing dense retrieval techniques effectively. Our source code and trained models are released at https://github.com/MiuLab/PairDistill\"\n",
      "}\n",
      "{\n",
      "    \"query\": \"Statistical Uncertainty in Word Embeddings: GloVe-V\",\n",
      "    \"id\": \"2024.emnlp-main.510\",\n",
      "    \"title\": \"Statistical Uncertainty in Word Embeddings: GloVe-V\",\n",
      "    \"abstract_url\": \"https://aclanthology.org/2024.emnlp-main.510/\",\n",
      "    \"paper_url\": \"https://aclanthology.org/2024.emnlp-main.510.pdf\",\n",
      "    \"abstract\": \"Static word embeddings are ubiquitous in computational social science applications and contribute to practical decision-making in a variety of fields including law and healthcare. However, assessing the statistical uncertainty in downstream conclusions drawn from word embedding statistics has remained challenging. When using only point estimates for embeddings, researchers have no streamlined way of assessing the degree to which their model selection criteria or scientific conclusions are subject to noise due to sparsity in the underlying data used to generate the embeddings. We introduce a method to obtain approximate, easy-to-use, and scalable reconstruction error variance estimates for GloVe, one of the most widely used word embedding models, using an analytical approximation to a multivariate normal model. To demonstrate the value of embeddings with variance (GloVe-V), we illustrate how our approach enables principled hypothesis testing in core word embedding tasks, such as comparing the similarity between different word pairs in vector space, assessing the performance of different models, and analyzing the relative degree of ethnic or gender bias in a corpus using different word lists.\"\n",
      "}\n",
      "{\n",
      "    \"query\": \"Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs\",\n",
      "    \"id\": \"2024.emnlp-main.525\",\n",
      "    \"title\": \"Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs\",\n",
      "    \"abstract_url\": \"https://aclanthology.org/2024.emnlp-main.525/\",\n",
      "    \"paper_url\": \"https://aclanthology.org/2024.emnlp-main.525.pdf\",\n",
      "    \"abstract\": \"Language Model Programs, i.e. sophisticated pipelines of modular language model (LM) calls, are increasingly advancing NLP tasks, but they require crafting prompts that are jointly effective for all modules. We study prompt optimization for LM programs, i.e. how to update these prompts to maximize a downstream metric without access to module-level labels or gradients. To make this tractable, we factorize our problem into optimizing the free-form instructions and few-shot demonstrations of every module and introduce several strategies to craft task-grounded instructions and navigate credit assignment across modules. Our strategies include (i) program- and data-aware techniques for proposing effective instructions, (ii) a stochastic mini-batch evaluation function for learning a surrogate model of our objective, and (iii) a meta-optimization procedure in which we refine how LMs construct proposals over time. Using these insights we develop MIPRO, a novel algorithm for optimizing LM programs. MIPRO outperforms baseline optimizers on five of seven diverse multi-stage LM programs using a best-in-class open-source model (Llama-3-8B), by as high as 13% accuracy. We have released our new optimizers and benchmark in DSPy at [http://dspy.ai](http://dspy.ai).\"\n",
      "}\n",
      "{\n",
      "    \"query\": \"Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together\",\n",
      "    \"id\": \"2024.emnlp-main.597\",\n",
      "    \"title\": \"Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together\",\n",
      "    \"abstract_url\": \"https://aclanthology.org/2024.emnlp-main.597/\",\n",
      "    \"paper_url\": \"https://aclanthology.org/2024.emnlp-main.597.pdf\",\n",
      "    \"abstract\": \"Natural Language Processing (NLP) systems are increasingly taking the form of sophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG), where each module may involve a distinct Language Model (LM) and an associated prompt template. These compound systems often lack intermediate labels or gradient flow to optimize each module, making their end-to-end optimization challenging. Here we seek strategies to optimize both the module-level LM weights and the associated prompt templates of such systems to maximize a downstream task metric. We propose for the first time combining the weight and prompt optimization strategies to optimize a modular LM pipeline by alternating between the two to get the same LM to teach itself. In experiments with multi-hop QA, mathematical reasoning, and feature-based classification using mistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies optimizing the weights and prompts of a pipeline together outperform directly optimizing weights alone and prompts alone by up to 60% and 6%, respectively, on average across LMs and tasks. Our BetterTogether optimizer is released in DSPy at [http://dspy.ai](http://dspy.ai).\"\n",
      "}\n",
      "{\n",
      "    \"query\": \"Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies\",\n",
      "    \"id\": \"2024.emnlp-main.1112\",\n",
      "    \"title\": \"Reasoning in Token Economies: Budget-Aware Evaluation of LLM Reasoning Strategies\",\n",
      "    \"abstract_url\": \"https://aclanthology.org/2024.emnlp-main.1112/\",\n",
      "    \"paper_url\": \"https://aclanthology.org/2024.emnlp-main.1112.pdf\",\n",
      "    \"abstract\": \"A diverse array of reasoning strategies has been proposed to elicit the capabilities of large language models. However, in this paper, we point out that traditional evaluations which focus solely on performance metrics miss a key factor: the increased effectiveness due to additional compute. By overlooking this aspect, a skewed view of strategy efficiency is often presented. This paper introduces a framework that incorporates the compute budget into the evaluation, providing a more informative comparison that takes into account both performance metrics and computational cost. In this budget-aware perspective, we find that complex reasoning strategies often don\\u2019t surpass simpler baselines purely due to algorithmic ingenuity, but rather due to the larger computational resources allocated. When we provide a simple baseline like chain-of-thought self-consistency with comparable compute resources, it frequently outperforms reasoning strategies proposed in the literature. In this scale-aware perspective, we find that unlike self-consistency, certain strategies such as multi-agent debate or Reflexion can become worse if more compute budget is utilized.\"\n",
      "}\n",
      "{\n",
      "    \"query\": \"MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents\",\n",
      "    \"id\": \"2024.emnlp-main.499\",\n",
      "    \"title\": \"MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents\",\n",
      "    \"abstract_url\": \"https://aclanthology.org/2024.emnlp-main.499/\",\n",
      "    \"paper_url\": \"https://aclanthology.org/2024.emnlp-main.499.pdf\",\n",
      "    \"abstract\": \"Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of fact-checking are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to a model to check a single response. In this work, we show how to build small fact-checking models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify datasets from recent work on fact-checking and grounding LLM generations into a new benchmark, LLM-AggreFact. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models.\"\n",
      "}\n",
      "{\n",
      "    \"query\": \"Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language Models\",\n",
      "    \"id\": \"2024.emnlp-main.461\",\n",
      "    \"title\": \"Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language Models\",\n",
      "    \"abstract_url\": \"https://aclanthology.org/2024.emnlp-main.461/\",\n",
      "    \"paper_url\": \"https://aclanthology.org/2024.emnlp-main.461.pdf\",\n",
      "    \"abstract\": \"Data are crucial element in large language model (LLM) alignment. Recent studies have explored using LLMs for efficient data collection. However, LLM-generated data often suffers from quality issues, with underrepresented or absent aspects and low-quality datapoints. To address these problems, we propose Data Advisor, an enhanced LLM-based method for generating data that takes into account the characteristics of the desired dataset. Starting from a set of pre-defined principles in hand, Data Advisor monitors the status of the generated data, identifies weaknesses in the current dataset, and advises the next iteration of data generation accordingly. Data Advisor can be easily integrated into existing data generation methods to enhance data quality and coverage. Experiments on safety alignment of three representative LLMs (i.e., Mistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in enhancing model safety against various fine-grained safety issues without sacrificing model utility.\"\n",
      "}\n",
      "{\n",
      "    \"query\": \"EfficientRAG: Efficient Retriever for Multi-Hop Question Answering\",\n",
      "    \"id\": \"2024.emnlp-main.199\",\n",
      "    \"title\": \"EfficientRAG: Efficient Retriever for Multi-Hop Question Answering\",\n",
      "    \"abstract_url\": \"https://aclanthology.org/2024.emnlp-main.199/\",\n",
      "    \"paper_url\": \"https://aclanthology.org/2024.emnlp-main.199.pdf\",\n",
      "    \"abstract\": \"Retrieval-augmented generation (RAG) methods encounter difficulties when addressing complex questions like multi-hop queries.While iterative retrieval methods improve performance by gathering additional information, current approaches often rely on multiple calls of large language models (LLMs).In this paper, we introduce EfficientRAG, an efficient retriever for multi-hop question answering.EfficientRAG iteratively generates new queries without the need for LLM calls at each iteration and filters out irrelevant information.Experimental results demonstrate that EfficientRAG surpasses existing RAG methods on three open-domain multi-hop question-answering datasets.The code is available in [aka.ms/efficientrag](https://github.com/NIL-zhuang/EfficientRAG-official).\"\n",
      "}\n",
      "{\n",
      "    \"query\": \"Hierarchical Deconstruction of LLM Reasoning: A Graph-Based Framework for Analyzing Knowledge Utilization\",\n",
      "    \"id\": \"2024.emnlp-main.288\",\n",
      "    \"title\": \"Hierarchical Deconstruction of LLM Reasoning: A Graph-Based Framework for Analyzing Knowledge Utilization\",\n",
      "    \"abstract_url\": \"https://aclanthology.org/2024.emnlp-main.288/\",\n",
      "    \"paper_url\": \"https://aclanthology.org/2024.emnlp-main.288.pdf\",\n",
      "    \"abstract\": \"Despite the advances in large language models (LLMs), how they use their knowledge for reasoning is not yet well understood.In this study, we propose a method that deconstructs complex real-world questions into a graph, representing each question as a node with predecessors of background knowledge needed to solve the question. We develop the DepthQA dataset, deconstructing questions into three depths: (i) recalling conceptual knowledge, (ii) applying procedural knowledge, and (iii) analyzing strategic knowledge. Based on a hierarchical graph, we quantify forward discrepancy, a discrepancy in LLM performance on simpler sub-problems versus complex questions. We also measure backward discrepancy where LLMs answer complex questions but struggle with simpler ones. Our analysis shows that smaller models exhibit more discrepancies than larger models. Distinct patterns of discrepancies are observed across model capacity and possibility of training data memorization. Additionally, guiding models from simpler to complex questions through multi-turn interactions improves performance across model sizes, highlighting the importance of structured intermediate steps in knowledge reasoning. This work enhances our understanding of LLM reasoning and suggests ways to improve their problem-solving abilities.\"\n",
      "}\n",
      "{\n",
      "    \"query\": \"mDPO: Conditional Preference Optimization for Multimodal Large Language Models\",\n",
      "    \"id\": \"2024.emnlp-main.460\",\n",
      "    \"title\": \"mDPO: Conditional Preference Optimization for Multimodal Large Language Models\",\n",
      "    \"abstract_url\": \"https://aclanthology.org/2024.emnlp-main.460/\",\n",
      "    \"paper_url\": \"https://aclanthology.org/2024.emnlp-main.460.pdf\",\n",
      "    \"abstract\": \"Direct preference optimization (DPO) has shown to be an effective method for large language model (LLM) alignment. Recent works have attempted to apply DPO to multimodal scenarios but have found it challenging to achieve consistent improvement. Through a comparative experiment, we identify the unconditional preference problem in multimodal preference optimization, where the model overlooks the image condition. To address this problem, we propose mDPO, a multimodal DPO objective that prevents the over-prioritization of language-only preferences by also optimizing image preference. Moreover, we introduce a reward anchor that forces the reward to be positive for chosen responses, thereby avoiding the decrease in their likelihood\\u2014an intrinsic problem of relative preference optimization. Experiments on two multimodal LLMs of different sizes and three widely used benchmarks demonstrate that mDPO effectively addresses the unconditional preference problem in multimodal preference optimization and significantly improves model performance, particularly in reducing hallucination.\"\n",
      "}\n",
      "{\n",
      "    \"query\": \"Atomic Inference for NLI with Generated Facts as Atoms\",\n",
      "    \"id\": \"2024.emnlp-main.569\",\n",
      "    \"title\": \"Atomic Inference for NLI with Generated Facts as Atoms\",\n",
      "    \"abstract_url\": \"https://aclanthology.org/2024.emnlp-main.569/\",\n",
      "    \"paper_url\": \"https://aclanthology.org/2024.emnlp-main.569.pdf\",\n",
      "    \"abstract\": \"With recent advances, neural models can achieve human-level performance on various natural language tasks. However, there are no guarantees that any explanations from these models are faithful, i.e. that they reflect the inner workings of the model. Atomic inference overcomes this issue, providing interpretable and faithful model decisions. This approach involves making predictions for different components (or atoms) of an instance, before using interpretable and deterministic rules to derive the overall prediction based on the individual atom-level predictions. We investigate the effectiveness of using LLM-generated facts as atoms, decomposing Natural Language Inference premises into lists of facts. While directly using generated facts in atomic inference systems can result in worse performance, with 1) a multi-stage fact generation process, and 2) a training regime that incorporates the facts, our fact-based method outperforms other approaches.\"\n",
      "}\n",
      "{\n",
      "    \"query\": \"CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds\",\n",
      "    \"id\": \"2024.emnlp-main.39\",\n",
      "    \"title\": \"CoCoLoFa: A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds\",\n",
      "    \"abstract_url\": \"https://aclanthology.org/2024.emnlp-main.39/\",\n",
      "    \"paper_url\": \"https://aclanthology.org/2024.emnlp-main.39.pdf\",\n",
      "    \"abstract\": \"Detecting logical fallacies in texts can help users spot argument flaws, but automating this detection is not easy. Manually annotating fallacies in large-scale, real-world text data to create datasets for developing and validating detection models is costly. This paper introduces CoCoLoFa, the largest known logical fallacy dataset, containing 7,706 comments for 648 news articles, with each comment labeled for fallacy presence and type. We recruited 143 crowd workers to write comments embodying specific fallacy types (e.g., slippery slope) in response to news articles. Recognizing the complexity of this writing task, we built an LLM-powered assistant into the workers\\u2019 interface to aid in drafting and refining their comments. Experts rated the writing quality and labeling validity of CoCoLoFa as high and reliable. BERT-based models fine-tuned using CoCoLoFa achieved the highest fallacy detection (F1=0.86) and classification (F1=0.87) performance on its test set, outperforming the state-of-the-art LLMs. Our work shows that combining crowdsourcing and LLMs enables us to more effectively construct datasets for complex linguistic phenomena that crowd workers find challenging to produce on their own.\"\n",
      "}\n",
      "{\n",
      "    \"query\": \"Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition\",\n",
      "    \"id\": \"2024.emnlp-main.985\",\n",
      "    \"title\": \"Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition\",\n",
      "    \"abstract_url\": \"https://aclanthology.org/2024.emnlp-main.985/\",\n",
      "    \"paper_url\": \"https://aclanthology.org/2024.emnlp-main.985.pdf\",\n",
      "    \"abstract\": \"Accurately attributing answer text to its source document is crucial for developing a reliable question-answering system. However, attribution for long documents remains largely unexplored. Post-hoc attribution systems are designed to map answer text back to the source document, yet the granularity of this mapping has not been addressed. Furthermore, a critical question arises: What exactly should be attributed? This involves identifying the specific information units within an answer that require grounding. In this paper, we propose and investigate a novel approach to the factual decomposition of generated answers for attribution, employing template-based in-context learning. To accomplish this, we utilize the question and integrate negative sampling during few-shot in-context learning for decomposition. This approach enhances the semantic understanding of both abstractive and extractive answers. We examine the impact of answer decomposition by providing a thorough examination of various attribution approaches, ranging from retrieval-based techniques to LLM-based attributors.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for result in search_results:\n",
    "    print(json.dumps(result, indent = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"results/{query_name}_search_results.json\", \"w\") as f:\n",
    "    f.write(json.dumps(search_results, indent = \"\\t\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
