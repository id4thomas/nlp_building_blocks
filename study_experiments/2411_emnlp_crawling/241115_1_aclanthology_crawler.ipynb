{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACL Anthology Crawler\n",
    "* https://github.com/srhthu/ACL-Anthology-Crawler/blob/main/crawl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://aclanthology.org\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://aclanthology.org/events/emnlp-2024\n"
     ]
    }
   ],
   "source": [
    "event_name = \"emnlp-2024\"\n",
    "event_url = urllib.parse.urljoin(base = base_url, url = f\"events/{event_name}\")\n",
    "print(event_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf_paper_list(soup, conf_id) -> list:\n",
    "    papers = soup.find('div', id = conf_id).find_all('p', class_ = \"d-sm-flex\")\n",
    "    paper_list = []\n",
    "    for paper_p in papers:\n",
    "        pdf_url = paper_p.contents[0].contents[0]['href']\n",
    "        paper_span = paper_p.contents[-1]\n",
    "        assert paper_span.name == 'span'\n",
    "        paper_a = paper_span.strong.a\n",
    "        title = paper_a.get_text()\n",
    "        url = \"https://aclanthology.org\" + paper_a['href']\n",
    "        paper_id = paper_a['href'].replace(\"/\", \"\")\n",
    "        # ID, title, abs_url, pdf_url\n",
    "        paper_list.append([paper_id, title, url, pdf_url])\n",
    "    return paper_list\n",
    "    \n",
    "def get_paper_list(event_url: str, conf_ids: List[str]):\n",
    "    html_doc = requests.get(event_url).text\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    paper_list = []\n",
    "    for conf_id in conf_ids:\n",
    "        conf_paper_list = get_conf_paper_list(soup, conf_id)\n",
    "        paper_list.extend(conf_paper_list)\n",
    "    return paper_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_ids = [\n",
    "    \"2024emnlp-main\",\n",
    "    \"2024emnlp-demo\",\n",
    "    \"2024emnlp-industry\",\n",
    "]\n",
    "paper_list = get_paper_list(event_url, conf_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2024.emnlp-main.0',\n",
       " 'Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing',\n",
       " 'https://aclanthology.org/2024.emnlp-main.0/',\n",
       " 'https://aclanthology.org/2024.emnlp-main.0.pdf']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num papers: 1444\n"
     ]
    }
   ],
   "source": [
    "# Main: Num papers: 1269\n",
    "print(\"Num papers: {}\".format(len(paper_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dump\n",
    "'''\n",
    "[\n",
    "  [\n",
    "    \"2024.emnlp-main.0\",\n",
    "    \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\",\n",
    "    \"https://aclanthology.org/2024.emnlp-main.0/\",\n",
    "    \"https://aclanthology.org/2024.emnlp-main.0.pdf\"\n",
    "  ],\n",
    "  ...\n",
    "]\n",
    "'''\n",
    "with open(f'{event_name}.json', 'w', encoding='utf8') as f:\n",
    "    json.dump(paper_list, f, indent = 2, ensure_ascii= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting Paper PDF File\n",
    "# illegal_chr = r'\\/:*?<>|'\n",
    "# table = ''.maketrans('', '', illegal_chr)\n",
    "# ## Sample\n",
    "# paper_idx = 2\n",
    "# paper = paper_list[paper_idx]\n",
    "\n",
    "# r = requests.get(paper[3])\n",
    "# # ex. 2.Multi-News+ Cost-efficient Dataset Cleansing via LLM-based Data Annotation.pdf\n",
    "# n = '{}.{}.pdf'.format(paper_idx, paper[0].translate(table))\n",
    "# with open(n, 'wb') as f:\n",
    "#     f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Abstract\n",
    "paper_abs_url = paper[2]\n",
    "html_doc = requests.get(paper_abs_url).text\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span>The quality of the dataset is crucial for ensuring optimal performance and reliability of downstream task models. However, datasets often contain noisy data inadvertently included during the construction process. Numerous attempts have been made to correct this issue through human annotators. However, hiring and managing human annotators is expensive and time-consuming. As an alternative, recent studies are exploring the use of large language models (LLMs) for data annotation.In this study, we present a case study that extends the application of LLM-based data annotation to enhance the quality of existing datasets through a cleansing strategy. Specifically, we leverage approaches such as chain-of-thought and majority voting to imitate human annotation and classify unrelated documents from the Multi-News dataset, which is widely used for the multi-document summarization task. Through our proposed cleansing method, we introduce an enhanced Multi-News+. By employing LLMs for data cleansing, we demonstrate an efficient and effective approach to improving dataset quality without relying on expensive human annotation efforts.</span>\n",
      "The quality of the dataset is crucial for ensuring optimal performance and reliability of downstream task models. However, datasets often contain noisy data inadvertently included during the construction process. Numerous attempts have been made to correct this issue through human annotators. However, hiring and managing human annotators is expensive and time-consuming. As an alternative, recent studies are exploring the use of large language models (LLMs) for data annotation.In this study, we present a case study that extends the application of LLM-based data annotation to enhance the quality of existing datasets through a cleansing strategy. Specifically, we leverage approaches such as chain-of-thought and majority voting to imitate human annotation and classify unrelated documents from the Multi-News dataset, which is widely used for the multi-document summarization task. Through our proposed cleansing method, we introduce an enhanced Multi-News+. By employing LLMs for data cleansing, we demonstrate an efficient and effective approach to improving dataset quality without relying on expensive human annotation efforts.\n"
     ]
    }
   ],
   "source": [
    "# Locate the abstract content\n",
    "# abstract_div = soup.find('div', class_='acl-abstract')\n",
    "# print(abstract_div)\n",
    "# abstract_text = abstract_div.get_text(strip=True) if abstract_div else \"Abstract not found.\"\n",
    "\n",
    "# # Print the extracted abstract\n",
    "# print(abstract_text)\n",
    "abstract_span = soup.select_one('div.acl-abstract span')\n",
    "print(abstract_span)\n",
    "\n",
    "abstract_text = abstract_span.get_text(strip=True) if abstract_span else \"Abstract not found.\"\n",
    "\n",
    "# Print the extracted abstract\n",
    "print(abstract_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
