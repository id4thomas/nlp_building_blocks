{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/id4thomas/miniforge3/envs/torch2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/id4thomas/miniforge3/envs/torch2/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: peft\n",
      "Version: 0.7.1\n",
      "Summary: Parameter-Efficient Fine-Tuning (PEFT)\n",
      "Home-page: https://github.com/huggingface/peft\n",
      "Author: The HuggingFace team\n",
      "Author-email: sourab@huggingface.co\n",
      "License: Apache\n",
      "Location: /Users/id4thomas/miniforge3/envs/torch2/lib/python3.10/site-packages\n",
      "Requires: accelerate, huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch, tqdm, transformers\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(30080, 2048)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=2048, out_features=30080, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load Model & Tokenizer\n",
    "model_name = \"EleutherAI/polyglot-ko-1.3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "\ttask_type = TaskType.CAUSAL_LM,\n",
    "\tr =  8,\n",
    "\tlora_alpha = 16,\n",
    "\tlora_dropout = 0.1,\n",
    "\tinference_mode=False,\n",
    "\t# init_lora_weights = \"gaussian\"\n",
    "\tinit_lora_weights = False # random init\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "GPTNeoXAttention(\n",
      "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  (query_key_value): lora.Linear(\n",
      "    (base_layer): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "    (lora_dropout): ModuleDict(\n",
      "      (default): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (lora_A): ModuleDict(\n",
      "      (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "    )\n",
      "    (lora_B): ModuleDict(\n",
      "      (default): Linear(in_features=8, out_features=6144, bias=False)\n",
      "    )\n",
      "    (lora_embedding_A): ParameterDict()\n",
      "    (lora_embedding_B): ParameterDict()\n",
      "  )\n",
      "  (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Analyze Lora Model\n",
    "print(type(lora_model))\n",
    "# print(lora_model.base_model.model.gpt_neox.layers[0])\n",
    "print(lora_model.base_model.gpt_neox.layers[0].attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6144, 2048])\n",
      "2.0\n",
      "torch.Size([8, 2048]) torch.Size([6144, 8])\n"
     ]
    }
   ],
   "source": [
    "## Layers Before Merge\n",
    "# following https://github.com/huggingface/peft/blob/bd544bb2ceae4a2b272e583e69b8f5fcdb022ff5/src/peft/tuners/lora/layer.py#L330\n",
    "qkv_bef = torch.clone(lora_model.base_model.model.gpt_neox.layers[0].attention.query_key_value.weight)\n",
    "print(qkv_bef.shape)\n",
    "\n",
    "lora_scale_val = lora_model.base_model.model.gpt_neox.layers[0].attention.query_key_value.scaling[\"default\"]\n",
    "lora_a = torch.clone(lora_model.base_model.model.gpt_neox.layers[0].attention.query_key_value.lora_A.default.weight)\n",
    "lora_b = torch.clone(lora_model.base_model.model.gpt_neox.layers[0].attention.query_key_value.lora_B.default.weight)\n",
    "print(lora_scale_val)\n",
    "print(lora_a.shape, lora_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6144, 2048])\n",
      "torch.Size([6144, 2048])\n"
     ]
    }
   ],
   "source": [
    "merged_lora = lora_b.matmul(lora_a)\n",
    "# merged_lora = lora_b@lora_a\n",
    "merged_lora = merged_lora * lora_scale_val\n",
    "print(merged_lora.shape)\n",
    "\n",
    "\n",
    "merged_linear = qkv_bef+merged_lora\n",
    "print(merged_linear.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MERGE\n",
    "# https://cdn-lfs.huggingface.co/datasets/huggingface/documentation-images/4313422c5f2755897fb8ddfc5b99251358f679647ec0f2d120a3f1ff060defe7?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27lora_diagram.png%3B+filename%3D%22lora_diagram.png%22%3B&response-content-type=image%2Fpng&Expires=1702993287&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjk5MzI4N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9odWdnaW5nZmFjZS9kb2N1bWVudGF0aW9uLWltYWdlcy80MzEzNDIyYzVmMjc1NTg5N2ZiOGRkZmM1Yjk5MjUxMzU4ZjY3OTY0N2VjMGYyZDEyMGEzZjFmZjA2MGRlZmU3P3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=EMrBr7qZbWFT56xOWykG81wdmcCNckVdh0OnnRNF%7EMiQBqwkUmZcgMdX89hvxHHMN8I6dwhpyqDmi-Ar2MrFW8HllSX3PxY1cugNSOsCLvlnEOpoFcMl1aGWWsVzApO%7E2tajrI5eQXZ56u8lbWMYSNkADJUglKFD882DupJg2M8x4yOsUAyE1kGHvuMwcvTvaFCeccCBce0bpD3Uta30PvnT9NizZ49bKor2m3e1taHtZx4jjfxlPoHitzO15m4UCWPLAcEtBT5t50zlv%7EmYAFMWEXdYPUWHVB12OnYYP1a2aMJFOSVfoZ0l%7EA4oLFkBrcvRa7ivYsIkytMQTTtanw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
    "\"\"\"\n",
    "X: (batch_size, 2048)\n",
    "With Adapter: \n",
    "query_key_value(X) + lora_B(X*lora_A)\n",
    "(b, 2048) * (2048, 6144) + (b, 2048)*(2048*8)*(8*6144)\n",
    "\n",
    "Merged:\n",
    "merged_layer = query_key_value + lora_B(lora_A)\n",
    "merged_layer(X)\n",
    "\"\"\"\n",
    "merged_model = lora_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "    (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(merged_model))\n",
    "merged_model.gpt_neox.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6144, 2048])\n"
     ]
    }
   ],
   "source": [
    "## Test Difference with real merged layer\n",
    "real_merged_linear = torch.clone(lora_model.base_model.model.gpt_neox.layers[0].attention.query_key_value.weight)\n",
    "print(real_merged_linear.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIFF WITH MY MERGED tensor(0., grad_fn=<SumBackward0>)\n",
      "DIFF WITH OG tensor(11.0746)\n"
     ]
    }
   ],
   "source": [
    "print(\"DIFF WITH MY MERGED\", torch.sum(real_merged_linear-merged_linear))\n",
    "print(\"DIFF WITH OG\", torch.sum(real_merged_linear-qkv_bef))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "torch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
