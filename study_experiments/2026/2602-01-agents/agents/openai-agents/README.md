# OpenAI Agents SDK
[OpenAI Agents SDK](https://openai.github.io/openai-agents-python/)

## í”„ë ˆì„ì›Œí¬ íŒŒì•…


## êµ¬í˜„ ê¸°ì´ˆ
### Client (Model) ì„ ì–¸
OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ `OpenAIChatCompletionsModel`/`OpenAIResponsesModel`ì— ì œê³µí•´ì„œ ì„ ì–¸

ì˜ˆì‹œ:
```
llm_client=AsyncOpenAI(
    base_url="http://.../v1",
    api_key="sk-123",
)
model = OpenAIChatCompletionsModel(
    model="Qwen3-...",
    openai_client=llm_client,
)
```

### Tool ì—°ê²°
`MCPServerManager`ë¥¼ í†µí•´ ì›ê²© MCP ì„œë²„ë¥¼ ê´€ë¦¬
- ê³µì‹ ê°€ì´ë“œ [[ë§í¬]](https://openai.github.io/openai-agents-python/mcp/)
- `MCPServerStreamableHttp`ë¡œ ì—°ê²° ì„¤ì •ì„ ì •ì˜

```
server1 = MCPServerStreamableHttp(
    name="location",
    params={
        "url": "http://.../mcp",
        "headers": {"x-user-id": user_id},
        "timeout": 60
    },
    cache_tools_list=True,
)
...

# Initialize Manager
servers = [server1, ...]
manager = MCPServerManager(servers)

async with manager:
    agent = Agent(..., mcp_servers=manager.active_servers)
```

### Agent ì„ ì–¸
```
agent = Agent(
    name="WeatherAgent",
    instructions=agent_instruction,
    model=model,
    mcp_servers=mcp_manager.active_servers,
    model_settings=ModelSettings(
        parallel_tool_calls=True,
        temperature=0.8,
        ...
    )
)
```

## Agent ì‹¤í–‰ ì˜ˆì‹œ
Agentë¥¼ ì‹¤í–‰í• ë•ŒëŠ” `Runner`ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
- `RunResult` ê°ì²´ë¥¼ ë°˜í™˜í•¨ ([ê³µì‹ ë¬¸ì„œ](https://openai.github.io/openai-agents-js/openai/agents/classes/runresult/))

### run
```
from agents import Runner

result = await Runner.run(agent, input=query)
```

ê²°ê³¼ í™•ì¸
- new_items: í•´ë‹¹ runìœ¼ë¡œ ìƒì„±ëœ ì•„ì´í…œë“¤
- final_output: ìµœì¢… ìƒì„± ê²°ê³¼
```
for item in result.new_items:
    print(item.type)
    if item.type == "tool_call_item":
        raw = item.raw_item
        # MCP íˆ´ì¸ ê²½ìš°
        tool_name = getattr(raw, "name", None) or getattr(raw, "tool_name", None)
        arguments = getattr(raw, "arguments", None)
        print(f"[tool_call] {tool_name}  args={arguments}")

    elif item.type == "tool_call_output_item":
        print(f"[tool_result] {item.output}")

    elif item.type == "message_output_item":
        print(f"[message] {ItemHelpers.text_message_output(item)}")

print(f"\n[final] {result.final_output}")
```

### stream
```
from openai.types.responses import ResponseTextDeltaEvent

current_text = ""
in_tool_call = False

result = Runner.run_streamed(agent, input=query)

async for event in result.stream_events():
    if event.type == "raw_response_event":
        ...
    elif event.type == "run_item_stream_event":
        item = event.item
        if item.type == "tool_call_item":
            # tool call ì‹¤í–‰ ìš”ì²­
            ...
        elif item.type == "tool_call_output_item":
            # tool í˜¸ì¶œ ê²°ê³¼
            ...
        elif item.type == "message_output_item":
            # í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì´ ëë‚¬ìŒì„ í‘œì‹œ
            ...
    elif event.type == "agent_updated_stream_event":
        # agent ê°€ í• ë‹¹ë¨
        print(f"\nğŸ¤– [agent] {event.new_agent.name}\n")
```