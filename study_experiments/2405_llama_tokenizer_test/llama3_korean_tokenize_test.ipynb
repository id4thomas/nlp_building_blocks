{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/id4thomas/miniforge3/envs/torch2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer,LlamaTokenizer \n",
    "with open(\"key.json\", \"r\") as f:\n",
    "\tkeys = json.load(f)\n",
    "os.environ[\"HF_KEY\"] = keys[\"hf_key\"]\n",
    "\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 51.0k/51.0k [00:00<00:00, 358kB/s]\n",
      "tokenizer.json: 100%|██████████| 9.09M/9.09M [00:00<00:00, 13.5MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 73.0/73.0 [00:00<00:00, 344kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# hf version - only fast available\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", token = keys[\"hf_key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiktoken version\n",
    "# loading code taken from llama repo\n",
    "# https://github.com/meta-llama/llama3/blob/d6e09315954d1a547bf45e37269978c049e73d33/llama/tokenizer.py#L38\n",
    "# tokenizer.model from hf repo\n",
    "mergeable_ranks = load_tiktoken_bpe('tokenizer.model')\n",
    "num_base_tokens = len(mergeable_ranks)\n",
    "num_reserved_special_tokens = 256\n",
    "pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: E\n",
    "\n",
    "special_tokens = [\n",
    "    \"<|begin_of_text|>\",\n",
    "    \"<|end_of_text|>\",\n",
    "    \"<|reserved_special_token_0|>\",\n",
    "    \"<|reserved_special_token_1|>\",\n",
    "    \"<|reserved_special_token_2|>\",\n",
    "    \"<|reserved_special_token_3|>\",\n",
    "    \"<|start_header_id|>\",\n",
    "    \"<|end_header_id|>\",\n",
    "    \"<|reserved_special_token_4|>\",\n",
    "    \"<|eot_id|>\",  # end of turn\n",
    "] + [\n",
    "    f\"<|reserved_special_token_{i}|>\"\n",
    "    for i in range(5, num_reserved_special_tokens - 5)\n",
    "]\n",
    "special_tokens = {\n",
    "    token: num_base_tokens + i for i, token in enumerate(special_tokens)\n",
    "}\n",
    "tiktoken_tokenizer = tiktoken.Encoding(\n",
    "    name=\"llama3-8b\",\n",
    "    pat_str=pat_str,\n",
    "    mergeable_ranks=mergeable_ranks,\n",
    "\tspecial_tokens=special_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요 저는 사람입니다.\n",
      "UTF-8 ENCODED: b'\\xec\\x95\\x88\\xeb\\x85\\x95\\xed\\x95\\x98\\xec\\x84\\xb8\\xec\\x9a\\x94 \\xec\\xa0\\x80\\xeb\\x8a\\x94 \\xec\\x82\\xac\\xeb\\x9e\\x8c\\xec\\x9e\\x85\\xeb\\x8b\\x88\\xeb\\x8b\\xa4.'\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Test Tokenization\n",
    "sample = \"안녕하세요 저는 사람입니다.\"\n",
    "print(sample)\n",
    "print(\"UTF-8 ENCODED:\",sample.encode('utf-8'))\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HF 쪽 tokenize 시 스트링이 깨져서 나옴\n",
    "* \"안녕하세요\" -> \"안\", \"녕하세요\" 토큰화 되야함\n",
    "* hf_tokenizer tokenize 로 반환 받을 경우 'ìķĪ', 'ëħķíķĺìĦ¸ìļĶ' 로 받아짐\n",
    "\t* hf 쪽 vocab 파일 보면 해당 토큰에 매핑되는 값은 맞음\n",
    "\t* tiktoken 버전과 ID 값 같음\n",
    "\t* encode -> id -> decode로 하면 제대로 나오기는 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF ENCODED: {'input_ids': [101193, 124409, 102678, 16969, 102745, 80052, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "<class 'tokenizers.Encoding'>\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "['ìķĪ', 'ëħķíķĺìĦ¸ìļĶ', 'ĠìłĢ', 'ëĬĶ', 'ĠìĤ¬ëŀĮ', 'ìŀħëĭĪëĭ¤', '.']\n",
      "[Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n",
      "Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]) <class 'tokenizers.Encoding'>\n",
      "HF TOKENIZED: ['ìķĪ', 'ëħķíķĺìĦ¸ìļĶ', 'ĠìłĢ', 'ëĬĶ', 'ĠìĤ¬ëŀĮ', 'ìŀħëĭĪëĭ¤', '.']\n",
      "안\n",
      "녕하세요\n"
     ]
    }
   ],
   "source": [
    "## HF Ver\n",
    "encoded = hf_tokenizer.encode(sample, add_special_tokens = False)\n",
    "encoded = hf_tokenizer.encode_plus(sample, add_special_tokens = False)\n",
    "print(\"HF ENCODED:\", encoded)\n",
    "print(type(encoded[0]))\n",
    "print(type(encoded))\n",
    "# print(encoded.tokens)\n",
    "print(encoded.tokens())\n",
    "print(encoded._encodings)\n",
    "print(encoded._encodings[0], type(encoded._encodings[0]))\n",
    "\n",
    "tokenized = hf_tokenizer.tokenize(sample, add_special_tokens = False)\n",
    "print(\"HF TOKENIZED:\", tokenized)\n",
    "\n",
    "decoded = hf_tokenizer.decode([101193])\n",
    "print(decoded)\n",
    "decoded = hf_tokenizer.decode([124409])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIKTOKEN ENCODED: [101193, 124409, 102678, 16969, 102745, 80052, 13]\n",
      "TIKTOKEN DECODE T0: 안\n",
      "TIKTOKEN DECODE T1: 녕하세요\n",
      "TIKTOKEN DECODED: b'\\xec\\x95\\x88\\xeb\\x85\\x95\\xed\\x95\\x98\\xec\\x84\\xb8\\xec\\x9a\\x94 \\xec\\xa0\\x80\\xeb\\x8a\\x94 \\xec\\x82\\xac\\xeb\\x9e\\x8c\\xec\\x9e\\x85\\xeb\\x8b\\x88\\xeb\\x8b\\xa4.'\n",
      "TIKTOKEN DECODED DECODE UTF-8: 안녕하세요 저는 사람입니다.\n"
     ]
    }
   ],
   "source": [
    "## TIKTOKEN VER\n",
    "encoded = tiktoken_tokenizer.encode(sample)\n",
    "print(\"TIKTOKEN ENCODED:\",encoded)\n",
    "print(\"TIKTOKEN DECODE T0:\", tiktoken_tokenizer.decode([encoded[0]]))\n",
    "print(\"TIKTOKEN DECODE T1:\",tiktoken_tokenizer.decode([encoded[1]]))\n",
    "decoded = tiktoken_tokenizer._core_bpe.decode_bytes(encoded)\n",
    "print(\"TIKTOKEN DECODED:\", decoded)\n",
    "print(\"TIKTOKEN DECODED DECODE UTF-8:\", decoded.decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "torch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
