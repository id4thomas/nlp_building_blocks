{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Methods\n",
    "* OneHot Encoding\n",
    "* CountVectorizer\n",
    "* TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: scikit-learn\n",
      "Version: 1.0.1\n",
      "Summary: A set of python modules for machine learning and data mining\n",
      "Home-page: http://scikit-learn.org\n",
      "Author: None\n",
      "Author-email: None\n",
      "License: new BSD\n",
      "Location: /home/tslab/anaconda3/envs/comet2020/lib/python3.8/site-packages\n",
      "Requires: threadpoolctl, numpy, scipy, joblib\n",
      "Required-by: seqeval, sentence-transformers, flair, allennlp\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "!pip show scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Corpus from sklearn\n",
    "train_corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "test_corpus = [\n",
    "    'This is the fourth document.',\n",
    "    'This document is the one.',\n",
    "    'This document is new.',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram\n",
      "['This', 'is', 'the', 'first', 'document']\n",
      "Train CV [[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "Vocab ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "Test CV [[0 1 0 1 0 0 1 0 1]\n",
      " [0 1 0 1 1 0 1 0 1]\n",
      " [0 1 0 1 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "\n",
    "#Unigram\n",
    "print(\"Unigram\")\n",
    "cv = CountVectorizer()\n",
    "\n",
    "#cv tokenizer\n",
    "cv_tokenizer=cv.build_tokenizer()\n",
    "print(cv_tokenizer(train_corpus[0]))\n",
    "\n",
    "train_cv=cv.fit_transform(train_corpus)\n",
    "print(\"Train CV\",train_cv.toarray())\n",
    "print(\"Vocab\",cv.get_feature_names_out())\n",
    "\n",
    "test_cv=cv.transform(test_corpus)\n",
    "print(\"Test CV\",test_cv.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV [[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "Vocab ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "Test CV [[0 1 0 1 0 0 1 0 1]\n",
      " [0 1 0 1 1 0 1 0 1]\n",
      " [0 1 0 1 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#OneHot Using Count Vectorizer\n",
    "#binary=True reports non-zero as 1\n",
    "cv = CountVectorizer(binary=True)\n",
    "\n",
    "train_cv=cv.fit_transform(train_corpus)\n",
    "print(\"Train CV\",train_cv.toarray())\n",
    "print(\"Vocab\",cv.get_feature_names_out())\n",
    "\n",
    "test_cv=cv.transform(test_corpus)\n",
    "print(\"Test CV\",test_cv.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CV [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
      "Vocab ['and this' 'document is' 'first document' 'is the' 'is this'\n",
      " 'second document' 'the first' 'the second' 'the third' 'third one'\n",
      " 'this document' 'this is' 'this the']\n",
      "Test CV [[0 0 0 1 0 0 0 0 0 0 0 1 0]\n",
      " [0 1 0 1 0 0 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#Bigram\n",
    "cv = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "train_cv=cv.fit_transform(train_corpus)\n",
    "print(\"Train CV\",train_cv.toarray())\n",
    "print(\"Vocab\",cv.get_feature_names_out())\n",
    "\n",
    "test_cv=cv.transform(test_corpus)\n",
    "print(\"Test CV\",test_cv.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TFIDF [[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n",
      "Vocab ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "Test TFIDF [[0.         0.57684669 0.         0.47160997 0.         0.\n",
      "  0.47160997 0.         0.47160997]\n",
      " [0.         0.42796959 0.         0.34989318 0.67049706 0.\n",
      "  0.34989318 0.         0.34989318]\n",
      " [0.         0.65416415 0.         0.53482206 0.         0.\n",
      "  0.         0.         0.53482206]]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorizer\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "train_tfidf=tfidf_vec.fit_transform(train_corpus)\n",
    "print(\"Train TFIDF\",train_tfidf.toarray())\n",
    "print(\"Vocab\",tfidf_vec.get_feature_names_out())\n",
    "\n",
    "test_tfidf=tfidf_vec.transform(test_corpus)\n",
    "print(\"Test TFIDF\",test_tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TFIDF [[0.         0.         0.52303503 0.42344193 0.         0.\n",
      "  0.52303503 0.         0.         0.         0.         0.52303503\n",
      "  0.        ]\n",
      " [0.         0.47633035 0.         0.30403549 0.         0.47633035\n",
      "  0.         0.47633035 0.         0.         0.47633035 0.\n",
      "  0.        ]\n",
      " [0.49819711 0.         0.         0.31799276 0.         0.\n",
      "  0.         0.         0.49819711 0.49819711 0.         0.39278432\n",
      "  0.        ]\n",
      " [0.         0.         0.43779123 0.         0.55528266 0.\n",
      "  0.43779123 0.         0.         0.         0.         0.\n",
      "  0.55528266]]\n",
      "Vocab ['and this' 'document is' 'first document' 'is the' 'is this'\n",
      " 'second document' 'the first' 'the second' 'the third' 'third one'\n",
      " 'this document' 'this is' 'this the']\n",
      "Test TFIDF [[0.         0.         0.         0.62922751 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.77722116\n",
      "  0.        ]\n",
      " [0.         0.64450299 0.         0.41137791 0.         0.\n",
      "  0.         0.         0.         0.         0.64450299 0.\n",
      "  0.        ]\n",
      " [0.         0.70710678 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.70710678 0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Bigram\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(2,2))\n",
    "\n",
    "train_tfidf=tfidf_vec.fit_transform(train_corpus)\n",
    "print(\"Train TFIDF\",train_tfidf.toarray())\n",
    "print(\"Vocab\",tfidf_vec.get_feature_names_out())\n",
    "\n",
    "test_tfidf=tfidf_vec.transform(test_corpus)\n",
    "print(\"Test TFIDF\",test_tfidf.toarray())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "490463624bb20166a8667b7fa728dcad49f92cff15a6e94f417ad6118e23fbcd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('comet2020': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
